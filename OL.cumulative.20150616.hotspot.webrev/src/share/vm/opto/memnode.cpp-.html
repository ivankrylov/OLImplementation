<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/opto/memnode.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "compiler/compileLog.hpp"
  28 #include "memory/allocation.inline.hpp"
  29 #include "oops/objArrayKlass.hpp"
  30 #include "opto/addnode.hpp"
  31 #include "opto/cfgnode.hpp"
  32 #include "opto/compile.hpp"
  33 #include "opto/connode.hpp"
  34 #include "opto/loopnode.hpp"
  35 #include "opto/machnode.hpp"
  36 #include "opto/matcher.hpp"
  37 #include "opto/memnode.hpp"
  38 #include "opto/mulnode.hpp"
  39 #include "opto/phaseX.hpp"
  40 #include "opto/regmask.hpp"
  41 
  42 // Portions of code courtesy of Clifford Click
  43 
  44 // Optimization - Graph Style
  45 
  46 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  47 
  48 //=============================================================================
  49 uint MemNode::size_of() const { return sizeof(*this); }
  50 
  51 const TypePtr *MemNode::adr_type() const {
  52   Node* adr = in(Address);
  53   const TypePtr* cross_check = NULL;
  54   DEBUG_ONLY(cross_check = _adr_type);
  55   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  56 }
  57 
  58 #ifndef PRODUCT
  59 void MemNode::dump_spec(outputStream *st) const {
  60   if (in(Address) == NULL)  return; // node is dead
  61 #ifndef ASSERT
  62   // fake the missing field
  63   const TypePtr* _adr_type = NULL;
  64   if (in(Address) != NULL)
  65     _adr_type = in(Address)-&gt;bottom_type()-&gt;isa_ptr();
  66 #endif
  67   dump_adr_type(this, _adr_type, st);
  68 
  69   Compile* C = Compile::current();
  70   if( C-&gt;alias_type(_adr_type)-&gt;is_volatile() )
  71     st-&gt;print(" Volatile!");
  72 }
  73 
  74 void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {
  75   st-&gt;print(" @");
  76   if (adr_type == NULL) {
  77     st-&gt;print("NULL");
  78   } else {
  79     adr_type-&gt;dump_on(st);
  80     Compile* C = Compile::current();
  81     Compile::AliasType* atp = NULL;
  82     if (C-&gt;have_alias_type(adr_type))  atp = C-&gt;alias_type(adr_type);
  83     if (atp == NULL)
  84       st-&gt;print(", idx=?\?;");
  85     else if (atp-&gt;index() == Compile::AliasIdxBot)
  86       st-&gt;print(", idx=Bot;");
  87     else if (atp-&gt;index() == Compile::AliasIdxTop)
  88       st-&gt;print(", idx=Top;");
  89     else if (atp-&gt;index() == Compile::AliasIdxRaw)
  90       st-&gt;print(", idx=Raw;");
  91     else {
  92       ciField* field = atp-&gt;field();
  93       if (field) {
  94         st-&gt;print(", name=");
  95         field-&gt;print_name_on(st);
  96       }
  97       st-&gt;print(", idx=%d;", atp-&gt;index());
  98     }
  99   }
 100 }
 101 
 102 extern void print_alias_types();
 103 
 104 #endif
 105 
 106 Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {
 107   assert((t_oop != NULL), "sanity");
 108   bool is_instance = t_oop-&gt;is_known_instance_field();
 109   bool is_boxed_value_load = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp;
 110                              (load != NULL) &amp;&amp; load-&gt;is_Load() &amp;&amp;
 111                              (phase-&gt;is_IterGVN() != NULL);
 112   if (!(is_instance || is_boxed_value_load))
 113     return mchain;  // don't try to optimize non-instance types
 114   uint instance_id = t_oop-&gt;instance_id();
 115   Node *start_mem = phase-&gt;C-&gt;start()-&gt;proj_out(TypeFunc::Memory);
 116   Node *prev = NULL;
 117   Node *result = mchain;
 118   while (prev != result) {
 119     prev = result;
 120     if (result == start_mem)
 121       break;  // hit one of our sentinels
 122     // skip over a call which does not affect this memory slice
 123     if (result-&gt;is_Proj() &amp;&amp; result-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
 124       Node *proj_in = result-&gt;in(0);
 125       if (proj_in-&gt;is_Allocate() &amp;&amp; proj_in-&gt;_idx == instance_id) {
 126         break;  // hit one of our sentinels
 127       } else if (proj_in-&gt;is_Call()) {
 128         CallNode *call = proj_in-&gt;as_Call();
 129         if (!call-&gt;may_modify(t_oop, phase)) { // returns false for instances
 130           result = call-&gt;in(TypeFunc::Memory);
 131         }
 132       } else if (proj_in-&gt;is_Initialize()) {
 133         AllocateNode* alloc = proj_in-&gt;as_Initialize()-&gt;allocation();
 134         // Stop if this is the initialization for the object instance which
 135         // which contains this memory slice, otherwise skip over it.
 136         if ((alloc == NULL) || (alloc-&gt;_idx == instance_id)) {
 137           break;
 138         }
 139         if (is_instance) {
 140           result = proj_in-&gt;in(TypeFunc::Memory);
 141         } else if (is_boxed_value_load) {
 142           Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
 143           const TypeKlassPtr* tklass = phase-&gt;type(klass)-&gt;is_klassptr();
 144           if (tklass-&gt;klass_is_exact() &amp;&amp; !tklass-&gt;klass()-&gt;equals(t_oop-&gt;klass())) {
 145             result = proj_in-&gt;in(TypeFunc::Memory); // not related allocation
 146           }
 147         }
 148       } else if (proj_in-&gt;is_MemBar()) {
 149         result = proj_in-&gt;in(TypeFunc::Memory);
 150       } else {
 151         assert(false, "unexpected projection");
 152       }
 153     } else if (result-&gt;is_ClearArray()) {
 154       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 155         // Can not bypass initialization of the instance
 156         // we are looking for.
 157         break;
 158       }
 159       // Otherwise skip it (the call updated 'result' value).
 160     } else if (result-&gt;is_MergeMem()) {
 161       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 162     }
 163   }
 164   return result;
 165 }
 166 
 167 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 168   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 169   if (t_oop == NULL)
 170     return mchain;  // don't try to optimize non-oop types
 171   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 172   bool is_instance = t_oop-&gt;is_known_instance_field();
 173   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 174   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {
 175     PhiNode *mphi = result-&gt;as_Phi();
 176     assert(mphi-&gt;bottom_type() == Type::MEMORY, "memory phi required");
 177     const TypePtr *t = mphi-&gt;adr_type();
 178     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 179         t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 180         t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 181          -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 182          -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop) {
 183       // clone the Phi with our address type
 184       result = mphi-&gt;split_out_instance(t_adr, igvn);
 185     } else {
 186       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), "correct memory chain");
 187     }
 188   }
 189   return result;
 190 }
 191 
 192 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 193   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 194   Node *mem = mmem;
 195 #ifdef ASSERT
 196   {
 197     // Check that current type is consistent with the alias index used during graph construction
 198     assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not be a bad alias_idx");
 199     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 200                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 201     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 202     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
 203                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;
 204         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 205         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 206           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 207           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 208       // don't assert if it is dead code.
 209       consistent = true;
 210     }
 211     if( !consistent ) {
 212       st-&gt;print("alias_idx==%d, adr_check==", alias_idx);
 213       if( adr_check == NULL ) {
 214         st-&gt;print("NULL");
 215       } else {
 216         adr_check-&gt;dump();
 217       }
 218       st-&gt;cr();
 219       print_alias_types();
 220       assert(consistent, "adr_check must match alias idx");
 221     }
 222   }
 223 #endif
 224   // TypeOopPtr::NOTNULL+any is an OOP with unknown offset - generally
 225   // means an array I have not precisely typed yet.  Do not do any
 226   // alias stuff with it any time soon.
 227   const TypeOopPtr *toop = tp-&gt;isa_oopptr();
 228   if( tp-&gt;base() != Type::AnyPtr &amp;&amp;
 229       !(toop &amp;&amp;
 230         toop-&gt;klass() != NULL &amp;&amp;
 231         toop-&gt;klass()-&gt;is_java_lang_Object() &amp;&amp;
 232         toop-&gt;offset() == Type::OffsetBot) ) {
 233     // compress paths and change unreachable cycles to TOP
 234     // If not, we can update the input infinitely along a MergeMem cycle
 235     // Equivalent code in PhiNode::Ideal
 236     Node* m  = phase-&gt;transform(mmem);
 237     // If transformed to a MergeMem, get the desired slice
 238     // Otherwise the returned node represents memory for every slice
 239     mem = (m-&gt;is_MergeMem())? m-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : m;
 240     // Update input if it is progress over what we have now
 241   }
 242   return mem;
 243 }
 244 
 245 //--------------------------Ideal_common---------------------------------------
 246 // Look for degenerate control and memory inputs.  Bypass MergeMem inputs.
 247 // Unhook non-raw memories from complete (macro-expanded) initializations.
 248 Node *MemNode::Ideal_common(PhaseGVN *phase, bool can_reshape) {
 249   // If our control input is a dead region, kill all below the region
 250   Node *ctl = in(MemNode::Control);
 251   if (ctl &amp;&amp; remove_dead_region(phase, can_reshape))
 252     return this;
 253   ctl = in(MemNode::Control);
 254   // Don't bother trying to transform a dead node
 255   if (ctl &amp;&amp; ctl-&gt;is_top())  return NodeSentinel;
 256 
 257   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 258   // Wait if control on the worklist.
 259   if (ctl &amp;&amp; can_reshape &amp;&amp; igvn != NULL) {
 260     Node* bol = NULL;
 261     Node* cmp = NULL;
 262     if (ctl-&gt;in(0)-&gt;is_If()) {
 263       assert(ctl-&gt;is_IfTrue() || ctl-&gt;is_IfFalse(), "sanity");
 264       bol = ctl-&gt;in(0)-&gt;in(1);
 265       if (bol-&gt;is_Bool())
 266         cmp = ctl-&gt;in(0)-&gt;in(1)-&gt;in(1);
 267     }
 268     if (igvn-&gt;_worklist.member(ctl) ||
 269         (bol != NULL &amp;&amp; igvn-&gt;_worklist.member(bol)) ||
 270         (cmp != NULL &amp;&amp; igvn-&gt;_worklist.member(cmp)) ) {
 271       // This control path may be dead.
 272       // Delay this memory node transformation until the control is processed.
 273       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 274       return NodeSentinel; // caller will return NULL
 275     }
 276   }
 277   // Ignore if memory is dead, or self-loop
 278   Node *mem = in(MemNode::Memory);
 279   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 280   assert(mem != this, "dead loop in MemNode::Ideal");
 281 
 282   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 283     // This memory slice may be dead.
 284     // Delay this mem node transformation until the memory is processed.
 285     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 286     return NodeSentinel; // caller will return NULL
 287   }
 288 
 289   Node *address = in(MemNode::Address);
 290   const Type *t_adr = phase-&gt;type(address);
 291   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 292 
 293   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 294       (igvn-&gt;_worklist.member(address) ||
 295        igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; (t_adr != adr_type())) ) {
 296     // The address's base and type may change when the address is processed.
 297     // Delay this mem node transformation until the address is processed.
 298     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 299     return NodeSentinel; // caller will return NULL
 300   }
 301 
 302   // Do NOT remove or optimize the next lines: ensure a new alias index
 303   // is allocated for an oop pointer type before Escape Analysis.
 304   // Note: C++ will not remove it since the call has side effect.
 305   if (t_adr-&gt;isa_oopptr()) {
 306     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 307   }
 308 
 309   Node* base = NULL;
 310   if (address-&gt;is_AddP()) {
 311     base = address-&gt;in(AddPNode::Base);
 312   }
 313   if (base != NULL &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR) &amp;&amp;
 314       !t_adr-&gt;isa_rawptr()) {
 315     // Note: raw address has TOP base and top-&gt;higher_equal(TypePtr::NULL_PTR) is true.
 316     // Skip this node optimization if its address has TOP base.
 317     return NodeSentinel; // caller will return NULL
 318   }
 319 
 320   // Avoid independent memory operations
 321   Node* old_mem = mem;
 322 
 323   // The code which unhooks non-raw memories from complete (macro-expanded)
 324   // initializations was removed. After macro-expansion all stores catched
 325   // by Initialize node became raw stores and there is no information
 326   // which memory slices they modify. So it is unsafe to move any memory
 327   // operation above these stores. Also in most cases hooked non-raw memories
 328   // were already unhooked by using information from detect_ptr_independence()
 329   // and find_previous_store().
 330 
 331   if (mem-&gt;is_MergeMem()) {
 332     MergeMemNode* mmem = mem-&gt;as_MergeMem();
 333     const TypePtr *tp = t_adr-&gt;is_ptr();
 334 
 335     mem = step_through_mergemem(phase, mmem, tp, adr_type(), tty);
 336   }
 337 
 338   if (mem != old_mem) {
 339     set_req(MemNode::Memory, mem);
 340     if (can_reshape &amp;&amp; old_mem-&gt;outcnt() == 0) {
 341         igvn-&gt;_worklist.push(old_mem);
 342     }
 343     if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel;
 344     return this;
 345   }
 346 
 347   // let the subclass continue analyzing...
 348   return NULL;
 349 }
 350 
 351 // Helper function for proving some simple control dominations.
 352 // Attempt to prove that all control inputs of 'dom' dominate 'sub'.
 353 // Already assumes that 'dom' is available at 'sub', and that 'sub'
 354 // is not a constant (dominated by the method's StartNode).
 355 // Used by MemNode::find_previous_store to prove that the
 356 // control input of a memory operation predates (dominates)
 357 // an allocation it wants to look past.
 358 bool MemNode::all_controls_dominate(Node* dom, Node* sub) {
 359   if (dom == NULL || dom-&gt;is_top() || sub == NULL || sub-&gt;is_top())
 360     return false; // Conservative answer for dead code
 361 
 362   // Check 'dom'. Skip Proj and CatchProj nodes.
 363   dom = dom-&gt;find_exact_control(dom);
 364   if (dom == NULL || dom-&gt;is_top())
 365     return false; // Conservative answer for dead code
 366 
 367   if (dom == sub) {
 368     // For the case when, for example, 'sub' is Initialize and the original
 369     // 'dom' is Proj node of the 'sub'.
 370     return false;
 371   }
 372 
 373   if (dom-&gt;is_Con() || dom-&gt;is_Start() || dom-&gt;is_Root() || dom == sub)
 374     return true;
 375 
 376   // 'dom' dominates 'sub' if its control edge and control edges
 377   // of all its inputs dominate or equal to sub's control edge.
 378 
 379   // Currently 'sub' is either Allocate, Initialize or Start nodes.
 380   // Or Region for the check in LoadNode::Ideal();
 381   // 'sub' should have sub-&gt;in(0) != NULL.
 382   assert(sub-&gt;is_Allocate() || sub-&gt;is_Initialize() || sub-&gt;is_Start() ||
 383          sub-&gt;is_Region() || sub-&gt;is_Call(), "expecting only these nodes");
 384 
 385   // Get control edge of 'sub'.
 386   Node* orig_sub = sub;
 387   sub = sub-&gt;find_exact_control(sub-&gt;in(0));
 388   if (sub == NULL || sub-&gt;is_top())
 389     return false; // Conservative answer for dead code
 390 
 391   assert(sub-&gt;is_CFG(), "expecting control");
 392 
 393   if (sub == dom)
 394     return true;
 395 
 396   if (sub-&gt;is_Start() || sub-&gt;is_Root())
 397     return false;
 398 
 399   {
 400     // Check all control edges of 'dom'.
 401 
 402     ResourceMark rm;
 403     Arena* arena = Thread::current()-&gt;resource_area();
 404     Node_List nlist(arena);
 405     Unique_Node_List dom_list(arena);
 406 
 407     dom_list.push(dom);
 408     bool only_dominating_controls = false;
 409 
 410     for (uint next = 0; next &lt; dom_list.size(); next++) {
 411       Node* n = dom_list.at(next);
 412       if (n == orig_sub)
 413         return false; // One of dom's inputs dominated by sub.
 414       if (!n-&gt;is_CFG() &amp;&amp; n-&gt;pinned()) {
 415         // Check only own control edge for pinned non-control nodes.
 416         n = n-&gt;find_exact_control(n-&gt;in(0));
 417         if (n == NULL || n-&gt;is_top())
 418           return false; // Conservative answer for dead code
 419         assert(n-&gt;is_CFG(), "expecting control");
 420         dom_list.push(n);
 421       } else if (n-&gt;is_Con() || n-&gt;is_Start() || n-&gt;is_Root()) {
 422         only_dominating_controls = true;
 423       } else if (n-&gt;is_CFG()) {
 424         if (n-&gt;dominates(sub, nlist))
 425           only_dominating_controls = true;
 426         else
 427           return false;
 428       } else {
 429         // First, own control edge.
 430         Node* m = n-&gt;find_exact_control(n-&gt;in(0));
 431         if (m != NULL) {
 432           if (m-&gt;is_top())
 433             return false; // Conservative answer for dead code
 434           dom_list.push(m);
 435         }
 436         // Now, the rest of edges.
 437         uint cnt = n-&gt;req();
 438         for (uint i = 1; i &lt; cnt; i++) {
 439           m = n-&gt;find_exact_control(n-&gt;in(i));
 440           if (m == NULL || m-&gt;is_top())
 441             continue;
 442           dom_list.push(m);
 443         }
 444       }
 445     }
 446     return only_dominating_controls;
 447   }
 448 }
 449 
 450 //---------------------detect_ptr_independence---------------------------------
 451 // Used by MemNode::find_previous_store to prove that two base
 452 // pointers are never equal.
 453 // The pointers are accompanied by their associated allocations,
 454 // if any, which have been previously discovered by the caller.
 455 bool MemNode::detect_ptr_independence(Node* p1, AllocateNode* a1,
 456                                       Node* p2, AllocateNode* a2,
 457                                       PhaseTransform* phase) {
 458   // Attempt to prove that these two pointers cannot be aliased.
 459   // They may both manifestly be allocations, and they should differ.
 460   // Or, if they are not both allocations, they can be distinct constants.
 461   // Otherwise, one is an allocation and the other a pre-existing value.
 462   if (a1 == NULL &amp;&amp; a2 == NULL) {           // neither an allocation
 463     return (p1 != p2) &amp;&amp; p1-&gt;is_Con() &amp;&amp; p2-&gt;is_Con();
 464   } else if (a1 != NULL &amp;&amp; a2 != NULL) {    // both allocations
 465     return (a1 != a2);
 466   } else if (a1 != NULL) {                  // one allocation a1
 467     // (Note:  p2-&gt;is_Con implies p2-&gt;in(0)-&gt;is_Root, which dominates.)
 468     return all_controls_dominate(p2, a1);
 469   } else { //(a2 != NULL)                   // one allocation a2
 470     return all_controls_dominate(p1, a2);
 471   }
 472   return false;
 473 }
 474 
 475 
 476 // The logic for reordering loads and stores uses four steps:
 477 // (a) Walk carefully past stores and initializations which we
 478 //     can prove are independent of this load.
 479 // (b) Observe that the next memory state makes an exact match
 480 //     with self (load or store), and locate the relevant store.
 481 // (c) Ensure that, if we were to wire self directly to the store,
 482 //     the optimizer would fold it up somehow.
 483 // (d) Do the rewiring, and return, depending on some other part of
 484 //     the optimizer to fold up the load.
 485 // This routine handles steps (a) and (b).  Steps (c) and (d) are
 486 // specific to loads and stores, so they are handled by the callers.
 487 // (Currently, only LoadNode::Ideal has steps (c), (d).  More later.)
 488 //
 489 Node* MemNode::find_previous_store(PhaseTransform* phase) {
 490   Node*         ctrl   = in(MemNode::Control);
 491   Node*         adr    = in(MemNode::Address);
 492   intptr_t      offset = 0;
 493   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
 494   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
 495 
 496   if (offset == Type::OffsetBot)
 497     return NULL;            // cannot unalias unless there are precise offsets
 498 
 499   const TypeOopPtr *addr_t = adr-&gt;bottom_type()-&gt;isa_oopptr();
 500 
 501   intptr_t size_in_bytes = memory_size();
 502 
 503   Node* mem = in(MemNode::Memory);   // start searching here...
 504 
 505   int cnt = 50;             // Cycle limiter
 506   for (;;) {                // While we can dance past unrelated stores...
 507     if (--cnt &lt; 0)  break;  // Caught in cycle or a complicated dance?
 508 
 509     if (mem-&gt;is_Store()) {
 510       Node* st_adr = mem-&gt;in(MemNode::Address);
 511       intptr_t st_offset = 0;
 512       Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);
 513       if (st_base == NULL)
 514         break;              // inscrutable pointer
 515       if (st_offset != offset &amp;&amp; st_offset != Type::OffsetBot) {
 516         const int MAX_STORE = BytesPerLong;
 517         if (st_offset &gt;= offset + size_in_bytes ||
 518             st_offset &lt;= offset - MAX_STORE ||
 519             st_offset &lt;= offset - mem-&gt;as_Store()-&gt;memory_size()) {
 520           // Success:  The offsets are provably independent.
 521           // (You may ask, why not just test st_offset != offset and be done?
 522           // The answer is that stores of different sizes can co-exist
 523           // in the same sequence of RawMem effects.  We sometimes initialize
 524           // a whole 'tile' of array elements with a single jint or jlong.)
 525           mem = mem-&gt;in(MemNode::Memory);
 526           continue;           // (a) advance through independent store memory
 527         }
 528       }
 529       if (st_base != base &amp;&amp;
 530           detect_ptr_independence(base, alloc,
 531                                   st_base,
 532                                   AllocateNode::Ideal_allocation(st_base, phase),
 533                                   phase)) {
 534         // Success:  The bases are provably independent.
 535         mem = mem-&gt;in(MemNode::Memory);
 536         continue;           // (a) advance through independent store memory
 537       }
 538 
 539       // (b) At this point, if the bases or offsets do not agree, we lose,
 540       // since we have not managed to prove 'this' and 'mem' independent.
 541       if (st_base == base &amp;&amp; st_offset == offset) {
 542         return mem;         // let caller handle steps (c), (d)
 543       }
 544 
 545     } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
 546       InitializeNode* st_init = mem-&gt;in(0)-&gt;as_Initialize();
 547       AllocateNode*  st_alloc = st_init-&gt;allocation();
 548       if (st_alloc == NULL)
 549         break;              // something degenerated
 550       bool known_identical = false;
 551       bool known_independent = false;
 552       if (alloc == st_alloc)
 553         known_identical = true;
 554       else if (alloc != NULL)
 555         known_independent = true;
 556       else if (all_controls_dominate(this, st_alloc))
 557         known_independent = true;
 558 
 559       if (known_independent) {
 560         // The bases are provably independent: Either they are
 561         // manifestly distinct allocations, or else the control
 562         // of this load dominates the store's allocation.
 563         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 564         if (alias_idx == Compile::AliasIdxRaw) {
 565           mem = st_alloc-&gt;in(TypeFunc::Memory);
 566         } else {
 567           mem = st_init-&gt;memory(alias_idx);
 568         }
 569         continue;           // (a) advance through independent store memory
 570       }
 571 
 572       // (b) at this point, if we are not looking at a store initializing
 573       // the same allocation we are loading from, we lose.
 574       if (known_identical) {
 575         // From caller, can_see_stored_value will consult find_captured_store.
 576         return mem;         // let caller handle steps (c), (d)
 577       }
 578 
 579     } else if (addr_t != NULL &amp;&amp; addr_t-&gt;is_known_instance_field()) {
 580       // Can't use optimize_simple_memory_chain() since it needs PhaseGVN.
 581       if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Call()) {
 582         CallNode *call = mem-&gt;in(0)-&gt;as_Call();
 583         if (!call-&gt;may_modify(addr_t, phase)) {
 584           mem = call-&gt;in(TypeFunc::Memory);
 585           continue;         // (a) advance through independent call memory
 586         }
 587       } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_MemBar()) {
 588         mem = mem-&gt;in(0)-&gt;in(TypeFunc::Memory);
 589         continue;           // (a) advance through independent MemBar memory
 590       } else if (mem-&gt;is_ClearArray()) {
 591         if (ClearArrayNode::step_through(&amp;mem, (uint)addr_t-&gt;instance_id(), phase)) {
 592           // (the call updated 'mem' value)
 593           continue;         // (a) advance through independent allocation memory
 594         } else {
 595           // Can not bypass initialization of the instance
 596           // we are looking for.
 597           return mem;
 598         }
 599       } else if (mem-&gt;is_MergeMem()) {
 600         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 601         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 602         continue;           // (a) advance through independent MergeMem memory
 603       }
 604     }
 605 
 606     // Unless there is an explicit 'continue', we must bail out here,
 607     // because 'mem' is an inscrutable memory state (e.g., a call).
 608     break;
 609   }
 610 
 611   return NULL;              // bail out
 612 }
 613 
 614 //----------------------calculate_adr_type-------------------------------------
 615 // Helper function.  Notices when the given type of address hits top or bottom.
 616 // Also, asserts a cross-check of the type against the expected address type.
 617 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 618   if (t == Type::TOP)  return NULL; // does not touch memory any more?
 619   #ifdef PRODUCT
 620   cross_check = NULL;
 621   #else
 622   if (!VerifyAliases || is_error_reported() || Node::in_dump())  cross_check = NULL;
 623   #endif
 624   const TypePtr* tp = t-&gt;isa_ptr();
 625   if (tp == NULL) {
 626     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, "expected memory type must be wide");
 627     return TypePtr::BOTTOM;           // touches lots of memory
 628   } else {
 629     #ifdef ASSERT
 630     // %%%% [phh] We don't check the alias index if cross_check is
 631     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 632     if (cross_check != NULL &amp;&amp;
 633         cross_check != TypePtr::BOTTOM &amp;&amp;
 634         cross_check != TypeRawPtr::BOTTOM) {
 635       // Recheck the alias index, to see if it has changed (due to a bug).
 636       Compile* C = Compile::current();
 637       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 638              "must stay in the original alias category");
 639       // The type of the address must be contained in the adr_type,
 640       // disregarding "null"-ness.
 641       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 642       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 643       assert(cross_check-&gt;meet(tp_notnull) == cross_check-&gt;remove_speculative(),
 644              "real address must not escape from expected memory type");
 645     }
 646     #endif
 647     return tp;
 648   }
 649 }
 650 
 651 //------------------------adr_phi_is_loop_invariant----------------------------
 652 // A helper function for Ideal_DU_postCCP to check if a Phi in a counted
 653 // loop is loop invariant. Make a quick traversal of Phi and associated
 654 // CastPP nodes, looking to see if they are a closed group within the loop.
 655 bool MemNode::adr_phi_is_loop_invariant(Node* adr_phi, Node* cast) {
 656   // The idea is that the phi-nest must boil down to only CastPP nodes
 657   // with the same data. This implies that any path into the loop already
 658   // includes such a CastPP, and so the original cast, whatever its input,
 659   // must be covered by an equivalent cast, with an earlier control input.
 660   ResourceMark rm;
 661 
 662   // The loop entry input of the phi should be the unique dominating
 663   // node for every Phi/CastPP in the loop.
 664   Unique_Node_List closure;
 665   closure.push(adr_phi-&gt;in(LoopNode::EntryControl));
 666 
 667   // Add the phi node and the cast to the worklist.
 668   Unique_Node_List worklist;
 669   worklist.push(adr_phi);
 670   if( cast != NULL ){
 671     if( !cast-&gt;is_ConstraintCast() ) return false;
 672     worklist.push(cast);
 673   }
 674 
 675   // Begin recursive walk of phi nodes.
 676   while( worklist.size() ){
 677     // Take a node off the worklist
 678     Node *n = worklist.pop();
 679     if( !closure.member(n) ){
 680       // Add it to the closure.
 681       closure.push(n);
 682       // Make a sanity check to ensure we don't waste too much time here.
 683       if( closure.size() &gt; 20) return false;
 684       // This node is OK if:
 685       //  - it is a cast of an identical value
 686       //  - or it is a phi node (then we add its inputs to the worklist)
 687       // Otherwise, the node is not OK, and we presume the cast is not invariant
 688       if( n-&gt;is_ConstraintCast() ){
 689         worklist.push(n-&gt;in(1));
 690       } else if( n-&gt;is_Phi() ) {
 691         for( uint i = 1; i &lt; n-&gt;req(); i++ ) {
 692           worklist.push(n-&gt;in(i));
 693         }
 694       } else {
 695         return false;
 696       }
 697     }
 698   }
 699 
 700   // Quit when the worklist is empty, and we've found no offending nodes.
 701   return true;
 702 }
 703 
 704 //------------------------------Ideal_DU_postCCP-------------------------------
 705 // Find any cast-away of null-ness and keep its control.  Null cast-aways are
 706 // going away in this pass and we need to make this memory op depend on the
 707 // gating null check.
 708 Node *MemNode::Ideal_DU_postCCP( PhaseCCP *ccp ) {
 709   return Ideal_common_DU_postCCP(ccp, this, in(MemNode::Address));
 710 }
 711 
 712 // I tried to leave the CastPP's in.  This makes the graph more accurate in
 713 // some sense; we get to keep around the knowledge that an oop is not-null
 714 // after some test.  Alas, the CastPP's interfere with GVN (some values are
 715 // the regular oop, some are the CastPP of the oop, all merge at Phi's which
 716 // cannot collapse, etc).  This cost us 10% on SpecJVM, even when I removed
 717 // some of the more trivial cases in the optimizer.  Removing more useless
 718 // Phi's started allowing Loads to illegally float above null checks.  I gave
 719 // up on this approach.  CNC 10/20/2000
 720 // This static method may be called not from MemNode (EncodePNode calls it).
 721 // Only the control edge of the node 'n' might be updated.
 722 Node *MemNode::Ideal_common_DU_postCCP( PhaseCCP *ccp, Node* n, Node* adr ) {
 723   Node *skipped_cast = NULL;
 724   // Need a null check?  Regular static accesses do not because they are
 725   // from constant addresses.  Array ops are gated by the range check (which
 726   // always includes a NULL check).  Just check field ops.
 727   if( n-&gt;in(MemNode::Control) == NULL ) {
 728     // Scan upwards for the highest location we can place this memory op.
 729     while( true ) {
 730       switch( adr-&gt;Opcode() ) {
 731 
 732       case Op_AddP:             // No change to NULL-ness, so peek thru AddP's
 733         adr = adr-&gt;in(AddPNode::Base);
 734         continue;
 735 
 736       case Op_DecodeN:         // No change to NULL-ness, so peek thru
 737       case Op_DecodeNKlass:
 738         adr = adr-&gt;in(1);
 739         continue;
 740 
 741       case Op_EncodeP:
 742       case Op_EncodePKlass:
 743         // EncodeP node's control edge could be set by this method
 744         // when EncodeP node depends on CastPP node.
 745         //
 746         // Use its control edge for memory op because EncodeP may go away
 747         // later when it is folded with following or preceding DecodeN node.
 748         if (adr-&gt;in(0) == NULL) {
 749           // Keep looking for cast nodes.
 750           adr = adr-&gt;in(1);
 751           continue;
 752         }
 753         ccp-&gt;hash_delete(n);
 754         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 755         ccp-&gt;hash_insert(n);
 756         return n;
 757 
 758       case Op_CastPP:
 759         // If the CastPP is useless, just peek on through it.
 760         if( ccp-&gt;type(adr) == ccp-&gt;type(adr-&gt;in(1)) ) {
 761           // Remember the cast that we've peeked though. If we peek
 762           // through more than one, then we end up remembering the highest
 763           // one, that is, if in a loop, the one closest to the top.
 764           skipped_cast = adr;
 765           adr = adr-&gt;in(1);
 766           continue;
 767         }
 768         // CastPP is going away in this pass!  We need this memory op to be
 769         // control-dependent on the test that is guarding the CastPP.
 770         ccp-&gt;hash_delete(n);
 771         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 772         ccp-&gt;hash_insert(n);
 773         return n;
 774 
 775       case Op_Phi:
 776         // Attempt to float above a Phi to some dominating point.
 777         if (adr-&gt;in(0) != NULL &amp;&amp; adr-&gt;in(0)-&gt;is_CountedLoop()) {
 778           // If we've already peeked through a Cast (which could have set the
 779           // control), we can't float above a Phi, because the skipped Cast
 780           // may not be loop invariant.
 781           if (adr_phi_is_loop_invariant(adr, skipped_cast)) {
 782             adr = adr-&gt;in(1);
 783             continue;
 784           }
 785         }
 786 
 787         // Intentional fallthrough!
 788 
 789         // No obvious dominating point.  The mem op is pinned below the Phi
 790         // by the Phi itself.  If the Phi goes away (no true value is merged)
 791         // then the mem op can float, but not indefinitely.  It must be pinned
 792         // behind the controls leading to the Phi.
 793       case Op_CheckCastPP:
 794         // These usually stick around to change address type, however a
 795         // useless one can be elided and we still need to pick up a control edge
 796         if (adr-&gt;in(0) == NULL) {
 797           // This CheckCastPP node has NO control and is likely useless. But we
 798           // need check further up the ancestor chain for a control input to keep
 799           // the node in place. 4959717.
 800           skipped_cast = adr;
 801           adr = adr-&gt;in(1);
 802           continue;
 803         }
 804         ccp-&gt;hash_delete(n);
 805         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 806         ccp-&gt;hash_insert(n);
 807         return n;
 808 
 809         // List of "safe" opcodes; those that implicitly block the memory
 810         // op below any null check.
 811       case Op_CastX2P:          // no null checks on native pointers
 812       case Op_Parm:             // 'this' pointer is not null
 813       case Op_LoadP:            // Loading from within a klass
 814       case Op_LoadN:            // Loading from within a klass
 815       case Op_LoadKlass:        // Loading from within a klass
 816       case Op_LoadNKlass:       // Loading from within a klass
 817       case Op_ConP:             // Loading from a klass
 818       case Op_ConN:             // Loading from a klass
 819       case Op_ConNKlass:        // Loading from a klass
 820       case Op_CreateEx:         // Sucking up the guts of an exception oop
 821       case Op_Con:              // Reading from TLS
 822       case Op_CMoveP:           // CMoveP is pinned
 823       case Op_CMoveN:           // CMoveN is pinned
 824         break;                  // No progress
 825 
 826       case Op_Proj:             // Direct call to an allocation routine
 827       case Op_SCMemProj:        // Memory state from store conditional ops
 828 #ifdef ASSERT
 829         {
 830           assert(adr-&gt;as_Proj()-&gt;_con == TypeFunc::Parms, "must be return value");
 831           const Node* call = adr-&gt;in(0);
 832           if (call-&gt;is_CallJava()) {
 833             const CallJavaNode* call_java = call-&gt;as_CallJava();
 834             const TypeTuple *r = call_java-&gt;tf()-&gt;range();
 835             assert(r-&gt;cnt() &gt; TypeFunc::Parms, "must return value");
 836             const Type* ret_type = r-&gt;field_at(TypeFunc::Parms);
 837             assert(ret_type &amp;&amp; ret_type-&gt;isa_ptr(), "must return pointer");
 838             // We further presume that this is one of
 839             // new_instance_Java, new_array_Java, or
 840             // the like, but do not assert for this.
 841           } else if (call-&gt;is_Allocate()) {
 842             // similar case to new_instance_Java, etc.
 843           } else if (!call-&gt;is_CallLeaf()) {
 844             // Projections from fetch_oop (OSR) are allowed as well.
 845             ShouldNotReachHere();
 846           }
 847         }
 848 #endif
 849         break;
 850       default:
 851         ShouldNotReachHere();
 852       }
 853       break;
 854     }
 855   }
 856 
 857   return  NULL;               // No progress
 858 }
 859 
 860 
 861 //=============================================================================
 862 // Should LoadNode::Ideal() attempt to remove control edges?
 863 bool LoadNode::can_remove_control() const {
 864   return true;
 865 }
 866 uint LoadNode::size_of() const { return sizeof(*this); }
 867 uint LoadNode::cmp( const Node &amp;n ) const
 868 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 869 const Type *LoadNode::bottom_type() const { return _type; }
 870 uint LoadNode::ideal_reg() const {
 871   return _type-&gt;ideal_reg();
 872 }
 873 
 874 #ifndef PRODUCT
 875 void LoadNode::dump_spec(outputStream *st) const {
 876   MemNode::dump_spec(st);
 877   if( !Verbose &amp;&amp; !WizardMode ) {
 878     // standard dump does this in Verbose and WizardMode
 879     st-&gt;print(" #"); _type-&gt;dump_on(st);
 880   }
 881 }
 882 #endif
 883 
 884 #ifdef ASSERT
 885 //----------------------------is_immutable_value-------------------------------
 886 // Helper function to allow a raw load without control edge for some cases
 887 bool LoadNode::is_immutable_value(Node* adr) {
 888   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 889           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 890           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 891            in_bytes(JavaThread::osthread_offset())));
 892 }
 893 #endif
 894 
 895 //----------------------------LoadNode::make-----------------------------------
 896 // Polymorphic factory method:
 897 Node *LoadNode::make(PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo) {
 898   Compile* C = gvn.C;
 899 
 900   // sanity check the alias category against the created node type
 901   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 902            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 903          "use LoadKlassNode instead");
 904   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 905            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 906          "use LoadRangeNode instead");
 907   // Check control edge of raw loads
 908   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 909           // oop will be recorded in oop map if load crosses safepoint
 910           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 911           "raw memory operations should have control edge");
 912   switch (bt) {
 913   case T_BOOLEAN: return new (C) LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 914   case T_BYTE:    return new (C) LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 915   case T_INT:     return new (C) LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 916   case T_CHAR:    return new (C) LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 917   case T_SHORT:   return new (C) LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 918   case T_LONG:    return new (C) LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo);
 919   case T_FLOAT:   return new (C) LoadFNode (ctl, mem, adr, adr_type, rt,            mo);
 920   case T_DOUBLE:  return new (C) LoadDNode (ctl, mem, adr, adr_type, rt,            mo);
 921   case T_ADDRESS: return new (C) LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo);
 922   case T_OBJECT:
 923 #ifdef _LP64
 924     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 925       Node* load  = gvn.transform(new (C) LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo));
 926       return new (C) DecodeNNode(load, load-&gt;bottom_type()-&gt;make_ptr());
 927     } else
 928 #endif
 929     {
 930       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), "should have got back a narrow oop");
 931       return new (C) LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_oopptr(), mo);
 932     }
 933   }
 934   ShouldNotReachHere();
 935   return (LoadNode*)NULL;
 936 }
 937 
 938 LoadLNode* LoadLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo) {
 939   bool require_atomic = true;
 940   return new (C) LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, require_atomic);
 941 }
 942 
 943 
 944 
 945 
 946 //------------------------------hash-------------------------------------------
 947 uint LoadNode::hash() const {
 948   // unroll addition of interesting fields
 949   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 950 }
 951 
 952 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 953   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 954     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 955     bool is_stable_ary = FoldStableValues &amp;&amp;
 956                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 957                          tp-&gt;isa_aryptr()-&gt;is_stable();
 958 
 959     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 960   }
 961 
 962   return false;
 963 }
 964 
 965 //---------------------------can_see_stored_value------------------------------
 966 // This routine exists to make sure this set of tests is done the same
 967 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 968 // will change the graph shape in a way which makes memory alive twice at the
 969 // same time (uses the Oracle model of aliasing), then some
 970 // LoadXNode::Identity will fold things back to the equivalence-class model
 971 // of aliasing.
 972 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 973   Node* ld_adr = in(MemNode::Address);
 974   intptr_t ld_off = 0;
 975   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 976   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 977   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
 978   // This is more general than load from boxing objects.
 979   if (skip_through_membars(atp, tp, phase-&gt;C-&gt;eliminate_boxing())) {
 980     uint alias_idx = atp-&gt;index();
 981     bool final = !atp-&gt;is_rewritable();
 982     Node* result = NULL;
 983     Node* current = st;
 984     // Skip through chains of MemBarNodes checking the MergeMems for
 985     // new states for the slice of this load.  Stop once any other
 986     // kind of node is encountered.  Loads from final memory can skip
 987     // through any kind of MemBar but normal loads shouldn't skip
 988     // through MemBarAcquire since the could allow them to move out of
 989     // a synchronized region.
 990     while (current-&gt;is_Proj()) {
 991       int opc = current-&gt;in(0)-&gt;Opcode();
 992       if ((final &amp;&amp; (opc == Op_MemBarAcquire ||
 993                      opc == Op_MemBarAcquireLock ||
 994                      opc == Op_LoadFence)) ||
 995           opc == Op_MemBarRelease ||
 996           opc == Op_StoreFence ||
 997           opc == Op_MemBarReleaseLock ||
 998           opc == Op_MemBarCPUOrder) {
 999         Node* mem = current-&gt;in(0)-&gt;in(TypeFunc::Memory);
1000         if (mem-&gt;is_MergeMem()) {
1001           MergeMemNode* merge = mem-&gt;as_MergeMem();
1002           Node* new_st = merge-&gt;memory_at(alias_idx);
1003           if (new_st == merge-&gt;base_memory()) {
1004             // Keep searching
1005             current = new_st;
1006             continue;
1007           }
1008           // Save the new memory state for the slice and fall through
1009           // to exit.
1010           result = new_st;
1011         }
1012       }
1013       break;
1014     }
1015     if (result != NULL) {
1016       st = result;
1017     }
1018   }
1019 
1020   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1021   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1022   for (int trip = 0; trip &lt;= 1; trip++) {
1023 
1024     if (st-&gt;is_Store()) {
1025       Node* st_adr = st-&gt;in(MemNode::Address);
1026       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1027         // Try harder before giving up...  Match raw and non-raw pointers.
1028         intptr_t st_off = 0;
1029         AllocateNode* alloc = AllocateNode::Ideal_allocation(st_adr, phase, st_off);
1030         if (alloc == NULL)       return NULL;
1031         if (alloc != ld_alloc)   return NULL;
1032         if (ld_off != st_off)    return NULL;
1033         // At this point we have proven something like this setup:
1034         //  A = Allocate(...)
1035         //  L = LoadQ(,  AddP(CastPP(, A.Parm),, #Off))
1036         //  S = StoreQ(, AddP(,        A.Parm  , #Off), V)
1037         // (Actually, we haven't yet proven the Q's are the same.)
1038         // In other words, we are loading from a casted version of
1039         // the same pointer-and-offset that we stored to.
1040         // Thus, we are able to replace L by V.
1041       }
1042       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1043       if (store_Opcode() != st-&gt;Opcode())
1044         return NULL;
1045       return st-&gt;in(MemNode::ValueIn);
1046     }
1047 
1048     // A load from a freshly-created object always returns zero.
1049     // (This can happen after LoadNode::Ideal resets the load's memory input
1050     // to find_captured_store, which returned InitializeNode::zero_memory.)
1051     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1052         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1053         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1054       // return a zero value for the load's basic type
1055       // (This is one of the few places where a generic PhaseTransform
1056       // can create new nodes.  Think of it as lazily manifesting
1057       // virtually pre-existing constants.)
1058       return phase-&gt;zerocon(memory_type());
1059     }
1060 
1061     // A load from an initialization barrier can match a captured store.
1062     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1063       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1064       AllocateNode* alloc = init-&gt;allocation();
1065       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1066         // examine a captured store value
1067         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1068         if (st != NULL)
1069           continue;             // take one more trip around
1070       }
1071     }
1072 
1073     // Load boxed value from result of valueOf() call is input parameter.
1074     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1075         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1076       intptr_t ignore = 0;
1077       Node* base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ignore);
1078       if (base != NULL &amp;&amp; base-&gt;is_Proj() &amp;&amp;
1079           base-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp;
1080           base-&gt;in(0)-&gt;is_CallStaticJava() &amp;&amp;
1081           base-&gt;in(0)-&gt;as_CallStaticJava()-&gt;is_boxing_method()) {
1082         return base-&gt;in(0)-&gt;in(TypeFunc::Parms);
1083       }
1084     }
1085 
1086     break;
1087   }
1088 
1089   return NULL;
1090 }
1091 
1092 //----------------------is_instance_field_load_with_local_phi------------------
1093 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1094   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1095       in(Address)-&gt;is_AddP() ) {
1096     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1097     // Only instances and boxed values.
1098     if( t_oop != NULL &amp;&amp;
1099         (t_oop-&gt;is_ptr_to_boxed_value() ||
1100          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1101         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1102         t_oop-&gt;offset() != Type::OffsetTop) {
1103       return true;
1104     }
1105   }
1106   return false;
1107 }
1108 
1109 //------------------------------Identity---------------------------------------
1110 // Loads are identity if previous store is to same address
1111 Node *LoadNode::Identity( PhaseTransform *phase ) {
1112   // If the previous store-maker is the right kind of Store, and the store is
1113   // to the same address, then we are equal to the value stored.
1114   Node* mem = in(Memory);
1115   Node* value = can_see_stored_value(mem, phase);
1116   if( value ) {
1117     // byte, short &amp; char stores truncate naturally.
1118     // A load has to load the truncated value which requires
1119     // some sort of masking operation and that requires an
1120     // Ideal call instead of an Identity call.
1121     if (memory_size() &lt; BytesPerInt) {
1122       // If the input to the store does not fit with the load's result type,
1123       // it must be truncated via an Ideal call.
1124       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1125         return this;
1126     }
1127     // (This works even when value is a Con, but LoadNode::Value
1128     // usually runs first, producing the singleton type of the Con.)
1129     return value;
1130   }
1131 
1132   // Search for an existing data phi which was generated before for the same
1133   // instance's field to avoid infinite generation of phis in a loop.
1134   Node *region = mem-&gt;in(0);
1135   if (is_instance_field_load_with_local_phi(region)) {
1136     const TypeOopPtr *addr_t = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1137     int this_index  = phase-&gt;C-&gt;get_alias_index(addr_t);
1138     int this_offset = addr_t-&gt;offset();
1139     int this_iid    = addr_t-&gt;instance_id();
1140     if (!addr_t-&gt;is_known_instance() &amp;&amp;
1141          addr_t-&gt;is_ptr_to_boxed_value()) {
1142       // Use _idx of address base (could be Phi node) for boxed values.
1143       intptr_t   ignore = 0;
1144       Node*      base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1145       this_iid = base-&gt;_idx;
1146     }
1147     const Type* this_type = bottom_type();
1148     for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1149       Node* phi = region-&gt;fast_out(i);
1150       if (phi-&gt;is_Phi() &amp;&amp; phi != mem &amp;&amp;
1151           phi-&gt;as_Phi()-&gt;is_same_inst_field(this_type, this_iid, this_index, this_offset)) {
1152         return phi;
1153       }
1154     }
1155   }
1156 
1157   return this;
1158 }
1159 
1160 // We're loading from an object which has autobox behaviour.
1161 // If this object is result of a valueOf call we'll have a phi
1162 // merging a newly allocated object and a load from the cache.
1163 // We want to replace this load with the original incoming
1164 // argument to the valueOf call.
1165 Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {
1166   assert(phase-&gt;C-&gt;eliminate_boxing(), "sanity");
1167   intptr_t ignore = 0;
1168   Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1169   if ((base == NULL) || base-&gt;is_Phi()) {
1170     // Push the loads from the phi that comes from valueOf up
1171     // through it to allow elimination of the loads and the recovery
1172     // of the original value. It is done in split_through_phi().
1173     return NULL;
1174   } else if (base-&gt;is_Load() ||
1175              base-&gt;is_DecodeN() &amp;&amp; base-&gt;in(1)-&gt;is_Load()) {
1176     // Eliminate the load of boxed value for integer types from the cache
1177     // array by deriving the value from the index into the array.
1178     // Capture the offset of the load and then reverse the computation.
1179 
1180     // Get LoadN node which loads a boxing object from 'cache' array.
1181     if (base-&gt;is_DecodeN()) {
1182       base = base-&gt;in(1);
1183     }
1184     if (!base-&gt;in(Address)-&gt;is_AddP()) {
1185       return NULL; // Complex address
1186     }
1187     AddPNode* address = base-&gt;in(Address)-&gt;as_AddP();
1188     Node* cache_base = address-&gt;in(AddPNode::Base);
1189     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_DecodeN()) {
1190       // Get ConP node which is static 'cache' field.
1191       cache_base = cache_base-&gt;in(1);
1192     }
1193     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_Con()) {
1194       const TypeAryPtr* base_type = cache_base-&gt;bottom_type()-&gt;isa_aryptr();
1195       if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1196         Node* elements[4];
1197         int shift = exact_log2(type2aelembytes(T_OBJECT));
1198         int count = address-&gt;unpack_offsets(elements, ARRAY_SIZE(elements));
1199         if ((count &gt;  0) &amp;&amp; elements[0]-&gt;is_Con() &amp;&amp;
1200             ((count == 1) ||
1201              (count == 2) &amp;&amp; elements[1]-&gt;Opcode() == Op_LShiftX &amp;&amp;
1202                              elements[1]-&gt;in(2) == phase-&gt;intcon(shift))) {
1203           ciObjArray* array = base_type-&gt;const_oop()-&gt;as_obj_array();
1204           // Fetch the box object cache[0] at the base of the array and get its value
1205           ciInstance* box = array-&gt;obj_at(0)-&gt;as_instance();
1206           ciInstanceKlass* ik = box-&gt;klass()-&gt;as_instance_klass();
1207           assert(ik-&gt;is_box_klass(), "sanity");
1208           assert(ik-&gt;nof_nonstatic_fields() == 1, "change following code");
1209           if (ik-&gt;nof_nonstatic_fields() == 1) {
1210             // This should be true nonstatic_field_at requires calling
1211             // nof_nonstatic_fields so check it anyway
1212             ciConstant c = box-&gt;field_value(ik-&gt;nonstatic_field_at(0));
1213             BasicType bt = c.basic_type();
1214             // Only integer types have boxing cache.
1215             assert(bt == T_BOOLEAN || bt == T_CHAR  ||
1216                    bt == T_BYTE    || bt == T_SHORT ||
1217                    bt == T_INT     || bt == T_LONG, err_msg_res("wrong type = %s", type2name(bt)));
1218             jlong cache_low = (bt == T_LONG) ? c.as_long() : c.as_int();
1219             if (cache_low != (int)cache_low) {
1220               return NULL; // should not happen since cache is array indexed by value
1221             }
1222             jlong offset = arrayOopDesc::base_offset_in_bytes(T_OBJECT) - (cache_low &lt;&lt; shift);
1223             if (offset != (int)offset) {
1224               return NULL; // should not happen since cache is array indexed by value
1225             }
1226            // Add up all the offsets making of the address of the load
1227             Node* result = elements[0];
1228             for (int i = 1; i &lt; count; i++) {
1229               result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, elements[i]));
1230             }
1231             // Remove the constant offset from the address and then
1232             result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, phase-&gt;MakeConX(-(int)offset)));
1233             // remove the scaling of the offset to recover the original index.
1234             if (result-&gt;Opcode() == Op_LShiftX &amp;&amp; result-&gt;in(2) == phase-&gt;intcon(shift)) {
1235               // Peel the shift off directly but wrap it in a dummy node
1236               // since Ideal can't return existing nodes
1237               result = new (phase-&gt;C) RShiftXNode(result-&gt;in(1), phase-&gt;intcon(0));
1238             } else if (result-&gt;is_Add() &amp;&amp; result-&gt;in(2)-&gt;is_Con() &amp;&amp;
1239                        result-&gt;in(1)-&gt;Opcode() == Op_LShiftX &amp;&amp;
1240                        result-&gt;in(1)-&gt;in(2) == phase-&gt;intcon(shift)) {
1241               // We can't do general optimization: ((X&lt;&lt;Z) + Y) &gt;&gt; Z ==&gt; X + (Y&gt;&gt;Z)
1242               // but for boxing cache access we know that X&lt;&lt;Z will not overflow
1243               // (there is range check) so we do this optimizatrion by hand here.
1244               Node* add_con = new (phase-&gt;C) RShiftXNode(result-&gt;in(2), phase-&gt;intcon(shift));
1245               result = new (phase-&gt;C) AddXNode(result-&gt;in(1)-&gt;in(1), phase-&gt;transform(add_con));
1246             } else {
1247               result = new (phase-&gt;C) RShiftXNode(result, phase-&gt;intcon(shift));
1248             }
1249 #ifdef _LP64
1250             if (bt != T_LONG) {
1251               result = new (phase-&gt;C) ConvL2INode(phase-&gt;transform(result));
1252             }
1253 #else
1254             if (bt == T_LONG) {
1255               result = new (phase-&gt;C) ConvI2LNode(phase-&gt;transform(result));
1256             }
1257 #endif
1258             // Boxing/unboxing can be done from signed &amp; unsigned loads (e.g. LoadUB -&gt; ... -&gt; LoadB pair).
1259             // Need to preserve unboxing load type if it is unsigned.
1260             switch(this-&gt;Opcode()) {
1261               case Op_LoadUB:
1262                 result = new (phase-&gt;C) AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFF));
1263                 break;
1264               case Op_LoadUS:
1265                 result = new (phase-&gt;C) AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFFFF));
1266                 break;
1267             }
1268             return result;
1269           }
1270         }
1271       }
1272     }
1273   }
1274   return NULL;
1275 }
1276 
1277 static bool stable_phi(PhiNode* phi, PhaseGVN *phase) {
1278   Node* region = phi-&gt;in(0);
1279   if (region == NULL) {
1280     return false; // Wait stable graph
1281   }
1282   uint cnt = phi-&gt;req();
1283   for (uint i = 1; i &lt; cnt; i++) {
1284     Node* rc = region-&gt;in(i);
1285     if (rc == NULL || phase-&gt;type(rc) == Type::TOP)
1286       return false; // Wait stable graph
1287     Node* in = phi-&gt;in(i);
1288     if (in == NULL || phase-&gt;type(in) == Type::TOP)
1289       return false; // Wait stable graph
1290   }
1291   return true;
1292 }
1293 //------------------------------split_through_phi------------------------------
1294 // Split instance or boxed field load through Phi.
1295 Node *LoadNode::split_through_phi(PhaseGVN *phase) {
1296   Node* mem     = in(Memory);
1297   Node* address = in(Address);
1298   const TypeOopPtr *t_oop = phase-&gt;type(address)-&gt;isa_oopptr();
1299 
1300   assert((t_oop != NULL) &amp;&amp;
1301          (t_oop-&gt;is_known_instance_field() ||
1302           t_oop-&gt;is_ptr_to_boxed_value()), "invalide conditions");
1303 
1304   Compile* C = phase-&gt;C;
1305   intptr_t ignore = 0;
1306   Node*    base = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1307   bool base_is_phi = (base != NULL) &amp;&amp; base-&gt;is_Phi();
1308   bool load_boxed_values = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp; C-&gt;aggressive_unboxing() &amp;&amp;
1309                            (base != NULL) &amp;&amp; (base == address-&gt;in(AddPNode::Base)) &amp;&amp;
1310                            phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL);
1311 
1312   if (!((mem-&gt;is_Phi() || base_is_phi) &amp;&amp;
1313         (load_boxed_values || t_oop-&gt;is_known_instance_field()))) {
1314     return NULL; // memory is not Phi
1315   }
1316 
1317   if (mem-&gt;is_Phi()) {
1318     if (!stable_phi(mem-&gt;as_Phi(), phase)) {
1319       return NULL; // Wait stable graph
1320     }
1321     uint cnt = mem-&gt;req();
1322     // Check for loop invariant memory.
1323     if (cnt == 3) {
1324       for (uint i = 1; i &lt; cnt; i++) {
1325         Node* in = mem-&gt;in(i);
1326         Node*  m = optimize_memory_chain(in, t_oop, this, phase);
1327         if (m == mem) {
1328           set_req(Memory, mem-&gt;in(cnt - i));
1329           return this; // made change
1330         }
1331       }
1332     }
1333   }
1334   if (base_is_phi) {
1335     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1336       return NULL; // Wait stable graph
1337     }
1338     uint cnt = base-&gt;req();
1339     // Check for loop invariant memory.
1340     if (cnt == 3) {
1341       for (uint i = 1; i &lt; cnt; i++) {
1342         if (base-&gt;in(i) == base) {
1343           return NULL; // Wait stable graph
1344         }
1345       }
1346     }
1347   }
1348 
1349   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));
1350 
1351   // Split through Phi (see original code in loopopts.cpp).
1352   assert(C-&gt;have_alias_type(t_oop), "instance should have alias type");
1353 
1354   // Do nothing here if Identity will find a value
1355   // (to avoid infinite chain of value phis generation).
1356   if (!phase-&gt;eqv(this, this-&gt;Identity(phase)))
1357     return NULL;
1358 
1359   // Select Region to split through.
1360   Node* region;
1361   if (!base_is_phi) {
1362     assert(mem-&gt;is_Phi(), "sanity");
1363     region = mem-&gt;in(0);
1364     // Skip if the region dominates some control edge of the address.
1365     if (!MemNode::all_controls_dominate(address, region))
1366       return NULL;
1367   } else if (!mem-&gt;is_Phi()) {
1368     assert(base_is_phi, "sanity");
1369     region = base-&gt;in(0);
1370     // Skip if the region dominates some control edge of the memory.
1371     if (!MemNode::all_controls_dominate(mem, region))
1372       return NULL;
1373   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1374     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), "sanity");
1375     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1376       region = base-&gt;in(0);
1377     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
1378       region = mem-&gt;in(0);
1379     } else {
1380       return NULL; // complex graph
1381     }
1382   } else {
1383     assert(base-&gt;in(0) == mem-&gt;in(0), "sanity");
1384     region = mem-&gt;in(0);
1385   }
1386 
1387   const Type* this_type = this-&gt;bottom_type();
1388   int this_index  = C-&gt;get_alias_index(t_oop);
1389   int this_offset = t_oop-&gt;offset();
1390   int this_iid    = t_oop-&gt;instance_id();
1391   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1392     // Use _idx of address base for boxed values.
1393     this_iid = base-&gt;_idx;
1394   }
1395   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1396   Node* phi = new (C) PhiNode(region, this_type, NULL, this_iid, this_index, this_offset);
1397   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1398     Node* x;
1399     Node* the_clone = NULL;
1400     if (region-&gt;in(i) == C-&gt;top()) {
1401       x = C-&gt;top();      // Dead path?  Use a dead data op
1402     } else {
1403       x = this-&gt;clone();        // Else clone up the data op
1404       the_clone = x;            // Remember for possible deletion.
1405       // Alter data node to use pre-phi inputs
1406       if (this-&gt;in(0) == region) {
1407         x-&gt;set_req(0, region-&gt;in(i));
1408       } else {
1409         x-&gt;set_req(0, NULL);
1410       }
1411       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1412         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1413       }
1414       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1415         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1416       }
1417       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1418         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1419         Node* adr_x = phase-&gt;transform(new (C) AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1420         x-&gt;set_req(Address, adr_x);
1421       }
1422     }
1423     // Check for a 'win' on some paths
1424     const Type *t = x-&gt;Value(igvn);
1425 
1426     bool singleton = t-&gt;singleton();
1427 
1428     // See comments in PhaseIdealLoop::split_thru_phi().
1429     if (singleton &amp;&amp; t == Type::TOP) {
1430       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1431     }
1432 
1433     if (singleton) {
1434       x = igvn-&gt;makecon(t);
1435     } else {
1436       // We now call Identity to try to simplify the cloned node.
1437       // Note that some Identity methods call phase-&gt;type(this).
1438       // Make sure that the type array is big enough for
1439       // our new node, even though we may throw the node away.
1440       // (This tweaking with igvn only works because x is a new node.)
1441       igvn-&gt;set_type(x, t);
1442       // If x is a TypeNode, capture any more-precise type permanently into Node
1443       // otherwise it will be not updated during igvn-&gt;transform since
1444       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1445       x-&gt;raise_bottom_type(t);
1446       Node *y = x-&gt;Identity(igvn);
1447       if (y != x) {
1448         x = y;
1449       } else {
1450         y = igvn-&gt;hash_find_insert(x);
1451         if (y) {
1452           x = y;
1453         } else {
1454           // Else x is a new node we are keeping
1455           // We do not need register_new_node_with_optimizer
1456           // because set_type has already been called.
1457           igvn-&gt;_worklist.push(x);
1458         }
1459       }
1460     }
1461     if (x != the_clone &amp;&amp; the_clone != NULL) {
1462       igvn-&gt;remove_dead_node(the_clone);
1463     }
1464     phi-&gt;set_req(i, x);
1465   }
1466   // Record Phi
1467   igvn-&gt;register_new_node_with_optimizer(phi);
1468   return phi;
1469 }
1470 
1471 //------------------------------Ideal------------------------------------------
1472 // If the load is from Field memory and the pointer is non-null, it might be possible to
1473 // zero out the control input.
1474 // If the offset is constant and the base is an object allocation,
1475 // try to hook me up to the exact initializing store.
1476 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1477   Node* p = MemNode::Ideal_common(phase, can_reshape);
1478   if (p)  return (p == NodeSentinel) ? NULL : p;
1479 
1480   Node* ctrl    = in(MemNode::Control);
1481   Node* address = in(MemNode::Address);
1482 
1483   // Skip up past a SafePoint control.  Cannot do this for Stores because
1484   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1485   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1486       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw ) {
1487     ctrl = ctrl-&gt;in(0);
1488     set_req(MemNode::Control,ctrl);
1489   }
1490 
1491   intptr_t ignore = 0;
1492   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1493   if (base != NULL
1494       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1495     // Check for useless control edge in some common special cases
1496     if (in(MemNode::Control) != NULL
1497         &amp;&amp; can_remove_control()
1498         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1499         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1500       // A method-invariant, non-null address (constant or 'this' argument).
1501       set_req(MemNode::Control, NULL);
1502     }
1503   }
1504 
1505   Node* mem = in(MemNode::Memory);
1506   const TypePtr *addr_t = phase-&gt;type(address)-&gt;isa_ptr();
1507 
1508   if (can_reshape &amp;&amp; (addr_t != NULL)) {
1509     // try to optimize our memory input
1510     Node* opt_mem = MemNode::optimize_memory_chain(mem, addr_t, this, phase);
1511     if (opt_mem != mem) {
1512       set_req(MemNode::Memory, opt_mem);
1513       if (phase-&gt;type( opt_mem ) == Type::TOP) return NULL;
1514       return this;
1515     }
1516     const TypeOopPtr *t_oop = addr_t-&gt;isa_oopptr();
1517     if ((t_oop != NULL) &amp;&amp;
1518         (t_oop-&gt;is_known_instance_field() ||
1519          t_oop-&gt;is_ptr_to_boxed_value())) {
1520       PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
1521       if (igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(opt_mem)) {
1522         // Delay this transformation until memory Phi is processed.
1523         phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
1524         return NULL;
1525       }
1526       // Split instance field load through Phi.
1527       Node* result = split_through_phi(phase);
1528       if (result != NULL) return result;
1529 
1530       if (t_oop-&gt;is_ptr_to_boxed_value()) {
1531         Node* result = eliminate_autobox(phase);
1532         if (result != NULL) return result;
1533       }
1534     }
1535   }
1536 
1537   // Check for prior store with a different base or offset; make Load
1538   // independent.  Skip through any number of them.  Bail out if the stores
1539   // are in an endless dead cycle and report no progress.  This is a key
1540   // transform for Reflection.  However, if after skipping through the Stores
1541   // we can't then fold up against a prior store do NOT do the transform as
1542   // this amounts to using the 'Oracle' model of aliasing.  It leaves the same
1543   // array memory alive twice: once for the hoisted Load and again after the
1544   // bypassed Store.  This situation only works if EVERYBODY who does
1545   // anti-dependence work knows how to bypass.  I.e. we need all
1546   // anti-dependence checks to ask the same Oracle.  Right now, that Oracle is
1547   // the alias index stuff.  So instead, peek through Stores and IFF we can
1548   // fold up, do so.
1549   Node* prev_mem = find_previous_store(phase);
1550   // Steps (a), (b):  Walk past independent stores to find an exact match.
1551   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1552     // (c) See if we can fold up on the spot, but don't fold up here.
1553     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1554     // just return a prior value, which is done by Identity calls.
1555     if (can_see_stored_value(prev_mem, phase)) {
1556       // Make ready for step (d):
1557       set_req(MemNode::Memory, prev_mem);
1558       return this;
1559     }
1560   }
1561 
1562   return NULL;                  // No further progress
1563 }
1564 
1565 // Helper to recognize certain Klass fields which are invariant across
1566 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1567 const Type*
1568 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1569                                  ciKlass* klass) const {
1570   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1571     // The field is Klass::_modifier_flags.  Return its (constant) value.
1572     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1573     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _modifier_flags");
1574     return TypeInt::make(klass-&gt;modifier_flags());
1575   }
1576   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1577     // The field is Klass::_access_flags.  Return its (constant) value.
1578     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1579     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _access_flags");
1580     return TypeInt::make(klass-&gt;access_flags());
1581   }
1582   if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())) {
1583     // The field is Klass::_layout_helper.  Return its constant value if known.
1584     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _layout_helper");
1585     return TypeInt::make(klass-&gt;layout_helper());
1586   }
1587 
1588   // No match.
1589   return NULL;
1590 }
1591 
1592 // Try to constant-fold a stable array element.
1593 static const Type* fold_stable_ary_elem(const TypeAryPtr* ary, int off, BasicType loadbt) {
1594   assert(ary-&gt;const_oop(), "array should be constant");
1595   assert(ary-&gt;is_stable(), "array should be stable");
1596 
1597   // Decode the results of GraphKit::array_element_address.
1598   ciArray* aobj = ary-&gt;const_oop()-&gt;as_array();
1599   ciConstant con = aobj-&gt;element_value_by_offset(off);
1600 
1601   if (con.basic_type() != T_ILLEGAL &amp;&amp; !con.is_null_or_zero()) {
1602     const Type* con_type = Type::make_from_constant(con);
1603     if (con_type != NULL) {
1604       if (con_type-&gt;isa_aryptr()) {
1605         // Join with the array element type, in case it is also stable.
1606         int dim = ary-&gt;stable_dimension();
1607         con_type = con_type-&gt;is_aryptr()-&gt;cast_to_stable(true, dim-1);
1608       }
1609       if (loadbt == T_NARROWOOP &amp;&amp; con_type-&gt;isa_oopptr()) {
1610         con_type = con_type-&gt;make_narrowoop();
1611       }
1612 #ifndef PRODUCT
1613       if (TraceIterativeGVN) {
1614         tty-&gt;print("FoldStableValues: array element [off=%d]: con_type=", off);
1615         con_type-&gt;dump(); tty-&gt;cr();
1616       }
1617 #endif //PRODUCT
1618       return con_type;
1619     }
1620   }
1621   return NULL;
1622 }
1623 
1624 //------------------------------Value-----------------------------------------
1625 const Type *LoadNode::Value( PhaseTransform *phase ) const {
1626   // Either input is TOP ==&gt; the result is TOP
1627   Node* mem = in(MemNode::Memory);
1628   const Type *t1 = phase-&gt;type(mem);
1629   if (t1 == Type::TOP)  return Type::TOP;
1630   Node* adr = in(MemNode::Address);
1631   const TypePtr* tp = phase-&gt;type(adr)-&gt;isa_ptr();
1632   if (tp == NULL || tp-&gt;empty())  return Type::TOP;
1633   int off = tp-&gt;offset();
1634   assert(off != Type::OffsetTop, "case covered by TypePtr::empty");
1635   Compile* C = phase-&gt;C;
1636 
1637   // Try to guess loaded type from pointer type
1638   if (tp-&gt;isa_aryptr()) {
1639     const TypeAryPtr* ary = tp-&gt;is_aryptr();
1640     const Type* t = ary-&gt;elem();
1641 
1642     // Determine whether the reference is beyond the header or not, by comparing
1643     // the offset against the offset of the start of the array's data.
1644     // Different array types begin at slightly different offsets (12 vs. 16).
1645     // We choose T_BYTE as an example base type that is least restrictive
1646     // as to alignment, which will therefore produce the smallest
1647     // possible base offset.
1648     const int min_base_off = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1649     const bool off_beyond_header = ((uint)off &gt;= (uint)min_base_off);
1650 
1651     // Try to constant-fold a stable array element.
1652     if (FoldStableValues &amp;&amp; ary-&gt;is_stable() &amp;&amp; ary-&gt;const_oop() != NULL) {
1653       // Make sure the reference is not into the header and the offset is constant
1654       if (off_beyond_header &amp;&amp; adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1655         const Type* con_type = fold_stable_ary_elem(ary, off, memory_type());
1656         if (con_type != NULL) {
1657           return con_type;
1658         }
1659       }
1660     }
1661 
1662     // Don't do this for integer types. There is only potential profit if
1663     // the element type t is lower than _type; that is, for int types, if _type is
1664     // more restrictive than t.  This only happens here if one is short and the other
1665     // char (both 16 bits), and in those cases we've made an intentional decision
1666     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1667     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1668     //
1669     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1670     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1671     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1672     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1673     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1674     // In fact, that could have been the original type of p1, and p1 could have
1675     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1676     // expression (LShiftL quux 3) independently optimized to the constant 8.
1677     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1678         &amp;&amp; (_type-&gt;isa_vect() == NULL)
1679         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1680       // t might actually be lower than _type, if _type is a unique
1681       // concrete subclass of abstract class t.
1682       if (off_beyond_header) {  // is the offset beyond the header?
1683         const Type* jt = t-&gt;join_speculative(_type);
1684         // In any case, do not allow the join, per se, to empty out the type.
1685         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1686           // This can happen if a interface-typed array narrows to a class type.
1687           jt = _type;
1688         }
1689 #ifdef ASSERT
1690         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1691           // The pointers in the autobox arrays are always non-null
1692           Node* base = adr-&gt;in(AddPNode::Base);
1693           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1694             // Get LoadN node which loads IntegerCache.cache field
1695             base = base-&gt;in(1);
1696           }
1697           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1698             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1699             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1700               // It could be narrow oop
1701               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,"sanity");
1702             }
1703           }
1704         }
1705 #endif
1706         return jt;
1707       }
1708     }
1709   } else if (tp-&gt;base() == Type::InstPtr) {
1710     ciEnv* env = C-&gt;env();
1711     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1712     ciKlass* klass = tinst-&gt;klass();
1713     assert( off != Type::OffsetBot ||
1714             // arrays can be cast to Objects
1715             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1716             // unsafe field access may not have a constant offset
1717             C-&gt;has_unsafe_access(),
1718             "Field accesses must be precise" );
1719     // For oop loads, we expect the _type to be precise
1720     if (klass == env-&gt;String_klass() &amp;&amp;
1721         adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1722       // For constant Strings treat the final fields as compile time constants.
1723       Node* base = adr-&gt;in(AddPNode::Base);
1724       const TypeOopPtr* t = phase-&gt;type(base)-&gt;isa_oopptr();
1725       if (t != NULL &amp;&amp; t-&gt;singleton()) {
1726         ciField* field = env-&gt;String_klass()-&gt;get_field_by_offset(off, false);
1727         if (field != NULL &amp;&amp; field-&gt;is_final()) {
1728           ciObject* string = t-&gt;const_oop();
1729           ciConstant constant = string-&gt;as_instance()-&gt;field_value(field);
1730           if (constant.basic_type() == T_INT) {
1731             return TypeInt::make(constant.as_int());
1732           } else if (constant.basic_type() == T_ARRAY) {
1733             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1734               return TypeNarrowOop::make_from_constant(constant.as_object(), true);
1735             } else {
1736               return TypeOopPtr::make_from_constant(constant.as_object(), true);
1737             }
1738           }
1739         }
1740       }
1741     }
1742     // Optimizations for constant objects
1743     ciObject* const_oop = tinst-&gt;const_oop();
1744     if (const_oop != NULL) {
1745       // For constant Boxed value treat the target field as a compile time constant.
1746       if (tinst-&gt;is_ptr_to_boxed_value()) {
1747         return tinst-&gt;get_const_boxed_value();
1748       } else
1749       // For constant CallSites treat the target field as a compile time constant.
1750       if (const_oop-&gt;is_call_site()) {
1751         ciCallSite* call_site = const_oop-&gt;as_call_site();
1752         ciField* field = call_site-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_offset(off, /*is_static=*/ false);
1753         if (field != NULL &amp;&amp; field-&gt;is_call_site_target()) {
1754           ciMethodHandle* target = call_site-&gt;get_target();
1755           if (target != NULL) {  // just in case
1756             ciConstant constant(T_OBJECT, target);
1757             const Type* t;
1758             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1759               t = TypeNarrowOop::make_from_constant(constant.as_object(), true);
1760             } else {
1761               t = TypeOopPtr::make_from_constant(constant.as_object(), true);
1762             }
1763             // Add a dependence for invalidation of the optimization.
1764             if (!call_site-&gt;is_constant_call_site()) {
1765               C-&gt;dependencies()-&gt;assert_call_site_target_value(call_site, target);
1766             }
1767             return t;
1768           }
1769         }
1770       }
1771     }
1772   } else if (tp-&gt;base() == Type::KlassPtr) {
1773     assert( off != Type::OffsetBot ||
1774             // arrays can be cast to Objects
1775             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1776             // also allow array-loading from the primary supertype
1777             // array during subtype checks
1778             Opcode() == Op_LoadKlass,
1779             "Field accesses must be precise" );
1780     // For klass/static loads, we expect the _type to be precise
1781   }
1782 
1783   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1784   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1785     ciKlass* klass = tkls-&gt;klass();
1786     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1787       // We are loading a field from a Klass metaobject whose identity
1788       // is known at compile time (the type is "exact" or "precise").
1789       // Check for fields we know are maintained as constants by the VM.
1790       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1791         // The field is Klass::_super_check_offset.  Return its (constant) value.
1792         // (Folds up type checking code.)
1793         assert(Opcode() == Op_LoadI, "must load an int from _super_check_offset");
1794         return TypeInt::make(klass-&gt;super_check_offset());
1795       }
1796       // Compute index into primary_supers array
1797       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1798       // Check for overflowing; use unsigned compare to handle the negative case.
1799       if( depth &lt; ciKlass::primary_super_limit() ) {
1800         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1801         // (Folds up type checking code.)
1802         assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1803         ciKlass *ss = klass-&gt;super_of_depth(depth);
1804         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1805       }
1806       const Type* aift = load_array_final_field(tkls, klass);
1807       if (aift != NULL)  return aift;
1808       if (tkls-&gt;offset() == in_bytes(ArrayKlass::component_mirror_offset())
1809           &amp;&amp; klass-&gt;is_array_klass()) {
1810         // The field is ArrayKlass::_component_mirror.  Return its (constant) value.
1811         // (Folds up aClassConstant.getComponentType, common in Arrays.copyOf.)
1812         assert(Opcode() == Op_LoadP, "must load an oop from _component_mirror");
1813         return TypeInstPtr::make(klass-&gt;as_array_klass()-&gt;component_mirror());
1814       }
1815       if (tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1816         // The field is Klass::_java_mirror.  Return its (constant) value.
1817         // (Folds up the 2nd indirection in anObjConstant.getClass().)
1818         assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
1819         return TypeInstPtr::make(klass-&gt;java_mirror());
1820       }
1821     }
1822 
1823     // We can still check if we are loading from the primary_supers array at a
1824     // shallow enough depth.  Even though the klass is not exact, entries less
1825     // than or equal to its super depth are correct.
1826     if (klass-&gt;is_loaded() ) {
1827       ciType *inner = klass;
1828       while( inner-&gt;is_obj_array_klass() )
1829         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1830       if( inner-&gt;is_instance_klass() &amp;&amp;
1831           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1832         // Compute index into primary_supers array
1833         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1834         // Check for overflowing; use unsigned compare to handle the negative case.
1835         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1836             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1837           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1838           // (Folds up type checking code.)
1839           assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1840           ciKlass *ss = klass-&gt;super_of_depth(depth);
1841           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1842         }
1843       }
1844     }
1845 
1846     // If the type is enough to determine that the thing is not an array,
1847     // we can give the layout_helper a positive interval type.
1848     // This will help short-circuit some reflective code.
1849     if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())
1850         &amp;&amp; !klass-&gt;is_array_klass() // not directly typed as an array
1851         &amp;&amp; !klass-&gt;is_interface()  // specifically not Serializable &amp; Cloneable
1852         &amp;&amp; !klass-&gt;is_java_lang_Object()   // not the supertype of all T[]
1853         ) {
1854       // Note:  When interfaces are reliable, we can narrow the interface
1855       // test to (klass != Serializable &amp;&amp; klass != Cloneable).
1856       assert(Opcode() == Op_LoadI, "must load an int from _layout_helper");
1857       jint min_size = Klass::instance_layout_helper(oopDesc::header_size(), false);
1858       // The key property of this type is that it folds up tests
1859       // for array-ness, since it proves that the layout_helper is positive.
1860       // Thus, a generic value like the basic object layout helper works fine.
1861       return TypeInt::make(min_size, max_jint, Type::WidenMin);
1862     }
1863   }
1864 
1865   // If we are loading from a freshly-allocated object, produce a zero,
1866   // if the load is provably beyond the header of the object.
1867   // (Also allow a variable load from a fresh array to produce zero.)
1868   const TypeOopPtr *tinst = tp-&gt;isa_oopptr();
1869   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1870   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1871   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1872     Node* value = can_see_stored_value(mem,phase);
1873     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1874       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),"sanity");
1875       return value-&gt;bottom_type();
1876     }
1877   }
1878 
1879   if (is_instance) {
1880     // If we have an instance type and our memory input is the
1881     // programs's initial memory state, there is no matching store,
1882     // so just return a zero of the appropriate type
1883     Node *mem = in(MemNode::Memory);
1884     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1885       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, "must be memory Parm");
1886       return Type::get_zero_type(_type-&gt;basic_type());
1887     }
1888   }
1889   return _type;
1890 }
1891 
1892 //------------------------------match_edge-------------------------------------
1893 // Do we Match on this edge index or not?  Match only the address.
1894 uint LoadNode::match_edge(uint idx) const {
1895   return idx == MemNode::Address;
1896 }
1897 
1898 //--------------------------LoadBNode::Ideal--------------------------------------
1899 //
1900 //  If the previous store is to the same address as this load,
1901 //  and the value stored was larger than a byte, replace this load
1902 //  with the value stored truncated to a byte.  If no truncation is
1903 //  needed, the replacement is done in LoadNode::Identity().
1904 //
1905 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1906   Node* mem = in(MemNode::Memory);
1907   Node* value = can_see_stored_value(mem,phase);
1908   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1909     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(24)) );
1910     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(24));
1911   }
1912   // Identity call will handle the case where truncation is not needed.
1913   return LoadNode::Ideal(phase, can_reshape);
1914 }
1915 
1916 const Type* LoadBNode::Value(PhaseTransform *phase) const {
1917   Node* mem = in(MemNode::Memory);
1918   Node* value = can_see_stored_value(mem,phase);
1919   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1920       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1921     // If the input to the store does not fit with the load's result type,
1922     // it must be truncated. We can't delay until Ideal call since
1923     // a singleton Value is needed for split_thru_phi optimization.
1924     int con = value-&gt;get_int();
1925     return TypeInt::make((con &lt;&lt; 24) &gt;&gt; 24);
1926   }
1927   return LoadNode::Value(phase);
1928 }
1929 
1930 //--------------------------LoadUBNode::Ideal-------------------------------------
1931 //
1932 //  If the previous store is to the same address as this load,
1933 //  and the value stored was larger than a byte, replace this load
1934 //  with the value stored truncated to a byte.  If no truncation is
1935 //  needed, the replacement is done in LoadNode::Identity().
1936 //
1937 Node* LoadUBNode::Ideal(PhaseGVN* phase, bool can_reshape) {
1938   Node* mem = in(MemNode::Memory);
1939   Node* value = can_see_stored_value(mem, phase);
1940   if (value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal(_type))
1941     return new (phase-&gt;C) AndINode(value, phase-&gt;intcon(0xFF));
1942   // Identity call will handle the case where truncation is not needed.
1943   return LoadNode::Ideal(phase, can_reshape);
1944 }
1945 
1946 const Type* LoadUBNode::Value(PhaseTransform *phase) const {
1947   Node* mem = in(MemNode::Memory);
1948   Node* value = can_see_stored_value(mem,phase);
1949   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1950       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1951     // If the input to the store does not fit with the load's result type,
1952     // it must be truncated. We can't delay until Ideal call since
1953     // a singleton Value is needed for split_thru_phi optimization.
1954     int con = value-&gt;get_int();
1955     return TypeInt::make(con &amp; 0xFF);
1956   }
1957   return LoadNode::Value(phase);
1958 }
1959 
1960 //--------------------------LoadUSNode::Ideal-------------------------------------
1961 //
1962 //  If the previous store is to the same address as this load,
1963 //  and the value stored was larger than a char, replace this load
1964 //  with the value stored truncated to a char.  If no truncation is
1965 //  needed, the replacement is done in LoadNode::Identity().
1966 //
1967 Node *LoadUSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1968   Node* mem = in(MemNode::Memory);
1969   Node* value = can_see_stored_value(mem,phase);
1970   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) )
1971     return new (phase-&gt;C) AndINode(value,phase-&gt;intcon(0xFFFF));
1972   // Identity call will handle the case where truncation is not needed.
1973   return LoadNode::Ideal(phase, can_reshape);
1974 }
1975 
1976 const Type* LoadUSNode::Value(PhaseTransform *phase) const {
1977   Node* mem = in(MemNode::Memory);
1978   Node* value = can_see_stored_value(mem,phase);
1979   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1980       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1981     // If the input to the store does not fit with the load's result type,
1982     // it must be truncated. We can't delay until Ideal call since
1983     // a singleton Value is needed for split_thru_phi optimization.
1984     int con = value-&gt;get_int();
1985     return TypeInt::make(con &amp; 0xFFFF);
1986   }
1987   return LoadNode::Value(phase);
1988 }
1989 
1990 //--------------------------LoadSNode::Ideal--------------------------------------
1991 //
1992 //  If the previous store is to the same address as this load,
1993 //  and the value stored was larger than a short, replace this load
1994 //  with the value stored truncated to a short.  If no truncation is
1995 //  needed, the replacement is done in LoadNode::Identity().
1996 //
1997 Node *LoadSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1998   Node* mem = in(MemNode::Memory);
1999   Node* value = can_see_stored_value(mem,phase);
2000   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
2001     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(16)) );
2002     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(16));
2003   }
2004   // Identity call will handle the case where truncation is not needed.
2005   return LoadNode::Ideal(phase, can_reshape);
2006 }
2007 
2008 const Type* LoadSNode::Value(PhaseTransform *phase) const {
2009   Node* mem = in(MemNode::Memory);
2010   Node* value = can_see_stored_value(mem,phase);
2011   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2012       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2013     // If the input to the store does not fit with the load's result type,
2014     // it must be truncated. We can't delay until Ideal call since
2015     // a singleton Value is needed for split_thru_phi optimization.
2016     int con = value-&gt;get_int();
2017     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2018   }
2019   return LoadNode::Value(phase);
2020 }
2021 
2022 //=============================================================================
2023 //----------------------------LoadKlassNode::make------------------------------
2024 // Polymorphic factory method:
2025 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node *mem, Node *adr, const TypePtr* at, const TypeKlassPtr *tk) {
2026   Compile* C = gvn.C;
2027   // sanity check the alias category against the created node type
2028   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2029   assert(adr_type != NULL, "expecting TypeKlassPtr");
2030 #ifdef _LP64
2031   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2032     assert(UseCompressedClassPointers, "no compressed klasses");
2033     Node* load_klass = gvn.transform(new (C) LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2034     return new (C) DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2035   }
2036 #endif
2037   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), "should have got back a narrow oop");
2038   return new (C) LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2039 }
2040 
2041 //------------------------------Value------------------------------------------
2042 const Type *LoadKlassNode::Value( PhaseTransform *phase ) const {
2043   return klass_value_common(phase);
2044 }
2045 
2046 // In most cases, LoadKlassNode does not have the control input set. If the control
2047 // input is set, it must not be removed (by LoadNode::Ideal()).
2048 bool LoadKlassNode::can_remove_control() const {
2049   return false;
2050 }
2051 
2052 const Type *LoadNode::klass_value_common( PhaseTransform *phase ) const {
2053   // Either input is TOP ==&gt; the result is TOP
2054   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2055   if (t1 == Type::TOP)  return Type::TOP;
2056   Node *adr = in(MemNode::Address);
2057   const Type *t2 = phase-&gt;type( adr );
2058   if (t2 == Type::TOP)  return Type::TOP;
2059   const TypePtr *tp = t2-&gt;is_ptr();
2060   if (TypePtr::above_centerline(tp-&gt;ptr()) ||
2061       tp-&gt;ptr() == TypePtr::Null)  return Type::TOP;
2062 
2063   // Return a more precise klass, if possible
2064   const TypeInstPtr *tinst = tp-&gt;isa_instptr();
2065   if (tinst != NULL) {
2066     ciInstanceKlass* ik = tinst-&gt;klass()-&gt;as_instance_klass();
2067     int offset = tinst-&gt;offset();
2068     if (ik == phase-&gt;C-&gt;env()-&gt;Class_klass()
2069         &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2070             offset == java_lang_Class::array_klass_offset_in_bytes())) {
2071       // We are loading a special hidden field from a Class mirror object,
2072       // the field which points to the VM's Klass metaobject.
2073       ciType* t = tinst-&gt;java_mirror_type();
2074       // java_mirror_type returns non-null for compile-time Class constants.
2075       if (t != NULL) {
2076         // constant oop =&gt; constant klass
2077         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2078           if (t-&gt;is_void()) {
2079             // We cannot create a void array.  Since void is a primitive type return null
2080             // klass.  Users of this result need to do a null check on the returned klass.
2081             return TypePtr::NULL_PTR;
2082           }
2083           return TypeKlassPtr::make(ciArrayKlass::make(t));
2084         }
2085         if (!t-&gt;is_klass()) {
2086           // a primitive Class (e.g., int.class) has NULL for a klass field
2087           return TypePtr::NULL_PTR;
2088         }
2089         // (Folds up the 1st indirection in aClassConstant.getModifiers().)
2090         return TypeKlassPtr::make(t-&gt;as_klass());
2091       }
2092       // non-constant mirror, so we can't tell what's going on
2093     }
2094     if( !ik-&gt;is_loaded() )
2095       return _type;             // Bail out if not loaded
2096     if (offset == oopDesc::klass_offset_in_bytes()) {
2097       if (tinst-&gt;klass_is_exact()) {
2098         return TypeKlassPtr::make(ik);
2099       }
2100       // See if we can become precise: no subklasses and no interface
2101       // (Note:  We need to support verified interfaces.)
2102       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2103         //assert(!UseExactTypes, "this code should be useless with exact types");
2104         // Add a dependence; if any subclass added we need to recompile
2105         if (!ik-&gt;is_final()) {
2106           // %%% should use stronger assert_unique_concrete_subtype instead
2107           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2108         }
2109         // Return precise klass
2110         return TypeKlassPtr::make(ik);
2111       }
2112 
2113       // Return root of possible klass
2114       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);
2115     }
2116   }
2117 
2118   // Check for loading klass from an array
2119   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
2120   if( tary != NULL ) {
2121     ciKlass *tary_klass = tary-&gt;klass();
2122     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2123         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2124       if (tary-&gt;klass_is_exact()) {
2125         return TypeKlassPtr::make(tary_klass);
2126       }
2127       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();
2128       // If the klass is an object array, we defer the question to the
2129       // array component klass.
2130       if( ak-&gt;is_obj_array_klass() ) {
2131         assert( ak-&gt;is_loaded(), "" );
2132         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
2133         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {
2134           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();
2135           // See if we can become precise: no subklasses and no interface
2136           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2137             //assert(!UseExactTypes, "this code should be useless with exact types");
2138             // Add a dependence; if any subclass added we need to recompile
2139             if (!ik-&gt;is_final()) {
2140               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2141             }
2142             // Return precise array klass
2143             return TypeKlassPtr::make(ak);
2144           }
2145         }
2146         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
2147       } else {                  // Found a type-array?
2148         //assert(!UseExactTypes, "this code should be useless with exact types");
2149         assert( ak-&gt;is_type_array_klass(), "" );
2150         return TypeKlassPtr::make(ak); // These are always precise
2151       }
2152     }
2153   }
2154 
2155   // Check for loading klass from an array klass
2156   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2157   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2158     ciKlass* klass = tkls-&gt;klass();
2159     if( !klass-&gt;is_loaded() )
2160       return _type;             // Bail out if not loaded
2161     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2162         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2163       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2164       // // Always returning precise element type is incorrect,
2165       // // e.g., element type could be object and array may contain strings
2166       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2167 
2168       // The array's TypeKlassPtr was declared 'precise' or 'not precise'
2169       // according to the element type's subclassing.
2170       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);
2171     }
2172     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2173         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2174       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2175       // The field is Klass::_super.  Return its (constant) value.
2176       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2177       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2178     }
2179   }
2180 
2181   // Bailout case
2182   return LoadNode::Value(phase);
2183 }
2184 
2185 //------------------------------Identity---------------------------------------
2186 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2187 // Also feed through the klass in Allocate(...klass...)._klass.
2188 Node* LoadKlassNode::Identity( PhaseTransform *phase ) {
2189   return klass_identity_common(phase);
2190 }
2191 
2192 Node* LoadNode::klass_identity_common(PhaseTransform *phase ) {
2193   Node* x = LoadNode::Identity(phase);
2194   if (x != this)  return x;
2195 
2196   // Take apart the address into an oop and and offset.
2197   // Return 'this' if we cannot.
2198   Node*    adr    = in(MemNode::Address);
2199   intptr_t offset = 0;
2200   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2201   if (base == NULL)     return this;
2202   const TypeOopPtr* toop = phase-&gt;type(adr)-&gt;isa_oopptr();
2203   if (toop == NULL)     return this;
2204 
2205   // We can fetch the klass directly through an AllocateNode.
2206   // This works even if the klass is not constant (clone or newArray).
2207   if (offset == oopDesc::klass_offset_in_bytes()) {
2208     Node* allocated_klass = AllocateNode::Ideal_klass(base, phase);
2209     if (allocated_klass != NULL) {
2210       return allocated_klass;
2211     }
2212   }
2213 
2214   // Simplify k.java_mirror.as_klass to plain k, where k is a Klass*.
2215   // Simplify ak.component_mirror.array_klass to plain ak, ak an ArrayKlass.
2216   // See inline_native_Class_query for occurrences of these patterns.
2217   // Java Example:  x.getClass().isAssignableFrom(y)
2218   // Java Example:  Array.newInstance(x.getClass().getComponentType(), n)
2219   //
2220   // This improves reflective code, often making the Class
2221   // mirror go completely dead.  (Current exception:  Class
2222   // mirrors may appear in debug info, but we could clean them out by
2223   // introducing a new debug info operator for Klass*.java_mirror).
2224   if (toop-&gt;isa_instptr() &amp;&amp; toop-&gt;klass() == phase-&gt;C-&gt;env()-&gt;Class_klass()
2225       &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2226           offset == java_lang_Class::array_klass_offset_in_bytes())) {
2227     // We are loading a special hidden field from a Class mirror,
2228     // the field which points to its Klass or ArrayKlass metaobject.
2229     if (base-&gt;is_Load()) {
2230       Node* adr2 = base-&gt;in(MemNode::Address);
2231       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
2232       if (tkls != NULL &amp;&amp; !tkls-&gt;empty()
2233           &amp;&amp; (tkls-&gt;klass()-&gt;is_instance_klass() ||
2234               tkls-&gt;klass()-&gt;is_array_klass())
2235           &amp;&amp; adr2-&gt;is_AddP()
2236           ) {
2237         int mirror_field = in_bytes(Klass::java_mirror_offset());
2238         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2239           mirror_field = in_bytes(ArrayKlass::component_mirror_offset());
2240         }
2241         if (tkls-&gt;offset() == mirror_field) {
2242           return adr2-&gt;in(AddPNode::Base);
2243         }
2244       }
2245     }
2246   }
2247 
2248   return this;
2249 }
2250 
2251 
2252 //------------------------------Value------------------------------------------
2253 const Type *LoadNKlassNode::Value( PhaseTransform *phase ) const {
2254   const Type *t = klass_value_common(phase);
2255   if (t == Type::TOP)
2256     return t;
2257 
2258   return t-&gt;make_narrowklass();
2259 }
2260 
2261 //------------------------------Identity---------------------------------------
2262 // To clean up reflective code, simplify k.java_mirror.as_klass to narrow k.
2263 // Also feed through the klass in Allocate(...klass...)._klass.
2264 Node* LoadNKlassNode::Identity( PhaseTransform *phase ) {
2265   Node *x = klass_identity_common(phase);
2266 
2267   const Type *t = phase-&gt;type( x );
2268   if( t == Type::TOP ) return x;
2269   if( t-&gt;isa_narrowklass()) return x;
2270   assert (!t-&gt;isa_narrowoop(), "no narrow oop here");
2271 
2272   return phase-&gt;transform(new (phase-&gt;C) EncodePKlassNode(x, t-&gt;make_narrowklass()));
2273 }
2274 
2275 //------------------------------Value-----------------------------------------
2276 const Type *LoadRangeNode::Value( PhaseTransform *phase ) const {
2277   // Either input is TOP ==&gt; the result is TOP
2278   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2279   if( t1 == Type::TOP ) return Type::TOP;
2280   Node *adr = in(MemNode::Address);
2281   const Type *t2 = phase-&gt;type( adr );
2282   if( t2 == Type::TOP ) return Type::TOP;
2283   const TypePtr *tp = t2-&gt;is_ptr();
2284   if (TypePtr::above_centerline(tp-&gt;ptr()))  return Type::TOP;
2285   const TypeAryPtr *tap = tp-&gt;isa_aryptr();
2286   if( !tap ) return _type;
2287   return tap-&gt;size();
2288 }
2289 
2290 //-------------------------------Ideal---------------------------------------
2291 // Feed through the length in AllocateArray(...length...)._length.
2292 Node *LoadRangeNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2293   Node* p = MemNode::Ideal_common(phase, can_reshape);
2294   if (p)  return (p == NodeSentinel) ? NULL : p;
2295 
2296   // Take apart the address into an oop and and offset.
2297   // Return 'this' if we cannot.
2298   Node*    adr    = in(MemNode::Address);
2299   intptr_t offset = 0;
2300   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase,  offset);
2301   if (base == NULL)     return NULL;
2302   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2303   if (tary == NULL)     return NULL;
2304 
2305   // We can fetch the length directly through an AllocateArrayNode.
2306   // This works even if the length is not constant (clone or newArray).
2307   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2308     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2309     if (alloc != NULL) {
2310       Node* allocated_length = alloc-&gt;Ideal_length();
2311       Node* len = alloc-&gt;make_ideal_length(tary, phase);
2312       if (allocated_length != len) {
2313         // New CastII improves on this.
2314         return len;
2315       }
2316     }
2317   }
2318 
2319   return NULL;
2320 }
2321 
2322 //------------------------------Identity---------------------------------------
2323 // Feed through the length in AllocateArray(...length...)._length.
2324 Node* LoadRangeNode::Identity( PhaseTransform *phase ) {
2325   Node* x = LoadINode::Identity(phase);
2326   if (x != this)  return x;
2327 
2328   // Take apart the address into an oop and and offset.
2329   // Return 'this' if we cannot.
2330   Node*    adr    = in(MemNode::Address);
2331   intptr_t offset = 0;
2332   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2333   if (base == NULL)     return this;
2334   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2335   if (tary == NULL)     return this;
2336 
2337   // We can fetch the length directly through an AllocateArrayNode.
2338   // This works even if the length is not constant (clone or newArray).
2339   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2340     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2341     if (alloc != NULL) {
2342       Node* allocated_length = alloc-&gt;Ideal_length();
2343       // Do not allow make_ideal_length to allocate a CastII node.
2344       Node* len = alloc-&gt;make_ideal_length(tary, phase, false);
2345       if (allocated_length == len) {
2346         // Return allocated_length only if it would not be improved by a CastII.
2347         return allocated_length;
2348       }
2349     }
2350   }
2351 
2352   return this;
2353 
2354 }
2355 
2356 //=============================================================================
2357 //---------------------------StoreNode::make-----------------------------------
2358 // Polymorphic factory method:
2359 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2360   assert((mo == unordered || mo == release), "unexpected");
2361   Compile* C = gvn.C;
2362   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2363          ctl != NULL, "raw memory operations should have control edge");
2364 
2365   switch (bt) {
2366   case T_BOOLEAN:
2367   case T_BYTE:    return new (C) StoreBNode(ctl, mem, adr, adr_type, val, mo);
2368   case T_INT:     return new (C) StoreINode(ctl, mem, adr, adr_type, val, mo);
2369   case T_CHAR:
2370   case T_SHORT:   return new (C) StoreCNode(ctl, mem, adr, adr_type, val, mo);
2371   case T_LONG:    return new (C) StoreLNode(ctl, mem, adr, adr_type, val, mo);
2372   case T_FLOAT:   return new (C) StoreFNode(ctl, mem, adr, adr_type, val, mo);
2373   case T_DOUBLE:  return new (C) StoreDNode(ctl, mem, adr, adr_type, val, mo);
2374   case T_METADATA:
2375   case T_ADDRESS:
2376   case T_OBJECT:
2377 #ifdef _LP64
2378     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2379       val = gvn.transform(new (C) EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2380       return new (C) StoreNNode(ctl, mem, adr, adr_type, val, mo);
2381     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2382                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2383                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2384       val = gvn.transform(new (C) EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2385       return new (C) StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2386     }
2387 #endif
2388     {
2389       return new (C) StorePNode(ctl, mem, adr, adr_type, val, mo);
2390     }
2391   }
2392   ShouldNotReachHere();
2393   return (StoreNode*)NULL;
2394 }
2395 
2396 StoreLNode* StoreLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {
2397   bool require_atomic = true;
2398   return new (C) StoreLNode(ctl, mem, adr, adr_type, val, mo, require_atomic);
2399 }
2400 
2401 
2402 //--------------------------bottom_type----------------------------------------
2403 const Type *StoreNode::bottom_type() const {
2404   return Type::MEMORY;
2405 }
2406 
2407 //------------------------------hash-------------------------------------------
2408 uint StoreNode::hash() const {
2409   // unroll addition of interesting fields
2410   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2411 
2412   // Since they are not commoned, do not hash them:
2413   return NO_HASH;
2414 }
2415 
2416 //------------------------------Ideal------------------------------------------
2417 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2418 // When a store immediately follows a relevant allocation/initialization,
2419 // try to capture it into the initialization, or hoist it above.
2420 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2421   Node* p = MemNode::Ideal_common(phase, can_reshape);
2422   if (p)  return (p == NodeSentinel) ? NULL : p;
2423 
2424   Node* mem     = in(MemNode::Memory);
2425   Node* address = in(MemNode::Address);
2426 
2427   // Back-to-back stores to same address?  Fold em up.  Generally
2428   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2429   // since they must follow each StoreP operation.  Redundant StoreCMs
2430   // are eliminated just before matching in final_graph_reshape.
2431   if (mem-&gt;is_Store() &amp;&amp; mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2432       mem-&gt;Opcode() != Op_StoreCM) {
2433     // Looking at a dead closed cycle of memory?
2434     assert(mem != mem-&gt;in(MemNode::Memory), "dead loop in StoreNode::Ideal");
2435 
2436     assert(Opcode() == mem-&gt;Opcode() ||
2437            phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw,
2438            "no mismatched stores, except on raw memory");
2439 
2440     if (mem-&gt;outcnt() == 1 &amp;&amp;           // check for intervening uses
2441         mem-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2442       // If anybody other than 'this' uses 'mem', we cannot fold 'mem' away.
2443       // For example, 'mem' might be the final state at a conditional return.
2444       // Or, 'mem' might be used by some node which is live at the same time
2445       // 'this' is live, which might be unschedulable.  So, require exactly
2446       // ONE user, the 'this' store, until such time as we clone 'mem' for
2447       // each of 'mem's uses (thus making the exactly-1-user-rule hold true).
2448       if (can_reshape) {  // (%%% is this an anachronism?)
2449         set_req_X(MemNode::Memory, mem-&gt;in(MemNode::Memory),
2450                   phase-&gt;is_IterGVN());
2451       } else {
2452         // It's OK to do this in the parser, since DU info is always accurate,
2453         // and the parser always refers to nodes via SafePointNode maps.
2454         set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2455       }
2456       return this;
2457     }
2458   }
2459 
2460   // Capture an unaliased, unconditional, simple store into an initializer.
2461   // Or, if it is independent of the allocation, hoist it above the allocation.
2462   if (ReduceFieldZeroing &amp;&amp; /*can_reshape &amp;&amp;*/
2463       mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
2464     InitializeNode* init = mem-&gt;in(0)-&gt;as_Initialize();
2465     intptr_t offset = init-&gt;can_capture_store(this, phase, can_reshape);
2466     if (offset &gt; 0) {
2467       Node* moved = init-&gt;capture_store(this, offset, phase, can_reshape);
2468       // If the InitializeNode captured me, it made a raw copy of me,
2469       // and I need to disappear.
2470       if (moved != NULL) {
2471         // %%% hack to ensure that Ideal returns a new node:
2472         mem = MergeMemNode::make(phase-&gt;C, mem);
2473         return mem;             // fold me away
2474       }
2475     }
2476   }
2477 
2478   return NULL;                  // No further progress
2479 }
2480 
2481 //------------------------------Value-----------------------------------------
2482 const Type *StoreNode::Value( PhaseTransform *phase ) const {
2483   // Either input is TOP ==&gt; the result is TOP
2484   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2485   if( t1 == Type::TOP ) return Type::TOP;
2486   const Type *t2 = phase-&gt;type( in(MemNode::Address) );
2487   if( t2 == Type::TOP ) return Type::TOP;
2488   const Type *t3 = phase-&gt;type( in(MemNode::ValueIn) );
2489   if( t3 == Type::TOP ) return Type::TOP;
2490   return Type::MEMORY;
2491 }
2492 
2493 //------------------------------Identity---------------------------------------
2494 // Remove redundant stores:
2495 //   Store(m, p, Load(m, p)) changes to m.
2496 //   Store(, p, x) -&gt; Store(m, p, x) changes to Store(m, p, x).
2497 Node *StoreNode::Identity( PhaseTransform *phase ) {
2498   Node* mem = in(MemNode::Memory);
2499   Node* adr = in(MemNode::Address);
2500   Node* val = in(MemNode::ValueIn);
2501 
2502   // Load then Store?  Then the Store is useless
2503   if (val-&gt;is_Load() &amp;&amp;
2504       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2505       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2506       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2507     return mem;
2508   }
2509 
2510   // Two stores in a row of the same value?
2511   if (mem-&gt;is_Store() &amp;&amp;
2512       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2513       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2514       mem-&gt;Opcode() == Opcode()) {
2515     return mem;
2516   }
2517 
2518   // Store of zero anywhere into a freshly-allocated object?
2519   // Then the store is useless.
2520   // (It must already have been captured by the InitializeNode.)
2521   if (ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {
2522     // a newly allocated object is already all-zeroes everywhere
2523     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {
2524       return mem;
2525     }
2526 
2527     // the store may also apply to zero-bits in an earlier object
2528     Node* prev_mem = find_previous_store(phase);
2529     // Steps (a), (b):  Walk past independent stores to find an exact match.
2530     if (prev_mem != NULL) {
2531       Node* prev_val = can_see_stored_value(prev_mem, phase);
2532       if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2533         // prev_val and val might differ by a cast; it would be good
2534         // to keep the more informative of the two.
2535         return mem;
2536       }
2537     }
2538   }
2539 
2540   return this;
2541 }
2542 
2543 //------------------------------match_edge-------------------------------------
2544 // Do we Match on this edge index or not?  Match only memory &amp; value
2545 uint StoreNode::match_edge(uint idx) const {
2546   return idx == MemNode::Address || idx == MemNode::ValueIn;
2547 }
2548 
2549 //------------------------------cmp--------------------------------------------
2550 // Do not common stores up together.  They generally have to be split
2551 // back up anyways, so do not bother.
2552 uint StoreNode::cmp( const Node &amp;n ) const {
2553   return (&amp;n == this);          // Always fail except on self
2554 }
2555 
2556 //------------------------------Ideal_masked_input-----------------------------
2557 // Check for a useless mask before a partial-word store
2558 // (StoreB ... (AndI valIn conIa) )
2559 // If (conIa &amp; mask == mask) this simplifies to
2560 // (StoreB ... (valIn) )
2561 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2562   Node *val = in(MemNode::ValueIn);
2563   if( val-&gt;Opcode() == Op_AndI ) {
2564     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2565     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2566       set_req(MemNode::ValueIn, val-&gt;in(1));
2567       return this;
2568     }
2569   }
2570   return NULL;
2571 }
2572 
2573 
2574 //------------------------------Ideal_sign_extended_input----------------------
2575 // Check for useless sign-extension before a partial-word store
2576 // (StoreB ... (RShiftI _ (LShiftI _ valIn conIL ) conIR) )
2577 // If (conIL == conIR &amp;&amp; conIR &lt;= num_bits)  this simplifies to
2578 // (StoreB ... (valIn) )
2579 Node *StoreNode::Ideal_sign_extended_input(PhaseGVN *phase, int num_bits) {
2580   Node *val = in(MemNode::ValueIn);
2581   if( val-&gt;Opcode() == Op_RShiftI ) {
2582     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2583     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &lt;= num_bits) ) {
2584       Node *shl = val-&gt;in(1);
2585       if( shl-&gt;Opcode() == Op_LShiftI ) {
2586         const TypeInt *t2 = phase-&gt;type( shl-&gt;in(2) )-&gt;isa_int();
2587         if( t2 &amp;&amp; t2-&gt;is_con() &amp;&amp; (t2-&gt;get_con() == t-&gt;get_con()) ) {
2588           set_req(MemNode::ValueIn, shl-&gt;in(1));
2589           return this;
2590         }
2591       }
2592     }
2593   }
2594   return NULL;
2595 }
2596 
2597 //------------------------------value_never_loaded-----------------------------------
2598 // Determine whether there are any possible loads of the value stored.
2599 // For simplicity, we actually check if there are any loads from the
2600 // address stored to, not just for loads of the value stored by this node.
2601 //
2602 bool StoreNode::value_never_loaded( PhaseTransform *phase) const {
2603   Node *adr = in(Address);
2604   const TypeOopPtr *adr_oop = phase-&gt;type(adr)-&gt;isa_oopptr();
2605   if (adr_oop == NULL)
2606     return false;
2607   if (!adr_oop-&gt;is_known_instance_field())
2608     return false; // if not a distinct instance, there may be aliases of the address
2609   for (DUIterator_Fast imax, i = adr-&gt;fast_outs(imax); i &lt; imax; i++) {
2610     Node *use = adr-&gt;fast_out(i);
2611     int opc = use-&gt;Opcode();
2612     if (use-&gt;is_Load() || use-&gt;is_LoadStore()) {
2613       return false;
2614     }
2615   }
2616   return true;
2617 }
2618 
2619 //=============================================================================
2620 //------------------------------Ideal------------------------------------------
2621 // If the store is from an AND mask that leaves the low bits untouched, then
2622 // we can skip the AND operation.  If the store is from a sign-extension
2623 // (a left shift, then right shift) we can skip both.
2624 Node *StoreBNode::Ideal(PhaseGVN *phase, bool can_reshape){
2625   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFF);
2626   if( progress != NULL ) return progress;
2627 
2628   progress = StoreNode::Ideal_sign_extended_input(phase, 24);
2629   if( progress != NULL ) return progress;
2630 
2631   // Finally check the default case
2632   return StoreNode::Ideal(phase, can_reshape);
2633 }
2634 
2635 //=============================================================================
2636 //------------------------------Ideal------------------------------------------
2637 // If the store is from an AND mask that leaves the low bits untouched, then
2638 // we can skip the AND operation
2639 Node *StoreCNode::Ideal(PhaseGVN *phase, bool can_reshape){
2640   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFFFF);
2641   if( progress != NULL ) return progress;
2642 
2643   progress = StoreNode::Ideal_sign_extended_input(phase, 16);
2644   if( progress != NULL ) return progress;
2645 
2646   // Finally check the default case
2647   return StoreNode::Ideal(phase, can_reshape);
2648 }
2649 
2650 //=============================================================================
2651 //------------------------------Identity---------------------------------------
2652 Node *StoreCMNode::Identity( PhaseTransform *phase ) {
2653   // No need to card mark when storing a null ptr
2654   Node* my_store = in(MemNode::OopStore);
2655   if (my_store-&gt;is_Store()) {
2656     const Type *t1 = phase-&gt;type( my_store-&gt;in(MemNode::ValueIn) );
2657     if( t1 == TypePtr::NULL_PTR ) {
2658       return in(MemNode::Memory);
2659     }
2660   }
2661   return this;
2662 }
2663 
2664 //=============================================================================
2665 //------------------------------Ideal---------------------------------------
2666 Node *StoreCMNode::Ideal(PhaseGVN *phase, bool can_reshape){
2667   Node* progress = StoreNode::Ideal(phase, can_reshape);
2668   if (progress != NULL) return progress;
2669 
2670   Node* my_store = in(MemNode::OopStore);
2671   if (my_store-&gt;is_MergeMem()) {
2672     Node* mem = my_store-&gt;as_MergeMem()-&gt;memory_at(oop_alias_idx());
2673     set_req(MemNode::OopStore, mem);
2674     return this;
2675   }
2676 
2677   return NULL;
2678 }
2679 
2680 //------------------------------Value-----------------------------------------
2681 const Type *StoreCMNode::Value( PhaseTransform *phase ) const {
2682   // Either input is TOP ==&gt; the result is TOP
2683   const Type *t = phase-&gt;type( in(MemNode::Memory) );
2684   if( t == Type::TOP ) return Type::TOP;
2685   t = phase-&gt;type( in(MemNode::Address) );
2686   if( t == Type::TOP ) return Type::TOP;
2687   t = phase-&gt;type( in(MemNode::ValueIn) );
2688   if( t == Type::TOP ) return Type::TOP;
2689   // If extra input is TOP ==&gt; the result is TOP
2690   t = phase-&gt;type( in(MemNode::OopStore) );
2691   if( t == Type::TOP ) return Type::TOP;
2692 
2693   return StoreNode::Value( phase );
2694 }
2695 
2696 
2697 //=============================================================================
2698 //----------------------------------SCMemProjNode------------------------------
2699 const Type * SCMemProjNode::Value( PhaseTransform *phase ) const
2700 {
2701   return bottom_type();
2702 }
2703 
2704 //=============================================================================
2705 //----------------------------------LoadStoreNode------------------------------
2706 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2707   : Node(required),
2708     _type(rt),
2709     _adr_type(at)
2710 {
2711   init_req(MemNode::Control, c  );
2712   init_req(MemNode::Memory , mem);
2713   init_req(MemNode::Address, adr);
2714   init_req(MemNode::ValueIn, val);
2715   init_class_id(Class_LoadStore);
2716 }
2717 
2718 uint LoadStoreNode::ideal_reg() const {
2719   return _type-&gt;ideal_reg();
2720 }
2721 
2722 bool LoadStoreNode::result_not_used() const {
2723   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2724     Node *x = fast_out(i);
2725     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2726     return false;
2727   }
2728   return true;
2729 }
2730 
2731 uint LoadStoreNode::size_of() const { return sizeof(*this); }
2732 
2733 //=============================================================================
2734 //----------------------------------LoadStoreConditionalNode--------------------
2735 LoadStoreConditionalNode::LoadStoreConditionalNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex ) : LoadStoreNode(c, mem, adr, val, NULL, TypeInt::BOOL, 5) {
2736   init_req(ExpectedIn, ex );
2737 }
2738 
2739 //=============================================================================
2740 //-------------------------------adr_type--------------------------------------
2741 // Do we Match on this edge index or not?  Do not match memory
2742 const TypePtr* ClearArrayNode::adr_type() const {
2743   Node *adr = in(3);
2744   return MemNode::calculate_adr_type(adr-&gt;bottom_type());
2745 }
2746 
2747 //------------------------------match_edge-------------------------------------
2748 // Do we Match on this edge index or not?  Do not match memory
2749 uint ClearArrayNode::match_edge(uint idx) const {
2750   return idx &gt; 1;
2751 }
2752 
2753 //------------------------------Identity---------------------------------------
2754 // Clearing a zero length array does nothing
2755 Node *ClearArrayNode::Identity( PhaseTransform *phase ) {
2756   return phase-&gt;type(in(2))-&gt;higher_equal(TypeX::ZERO)  ? in(1) : this;
2757 }
2758 
2759 //------------------------------Idealize---------------------------------------
2760 // Clearing a short array is faster with stores
2761 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape){
2762   const int unit = BytesPerLong;
2763   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2764   if (!t)  return NULL;
2765   if (!t-&gt;is_con())  return NULL;
2766   intptr_t raw_count = t-&gt;get_con();
2767   intptr_t size = raw_count;
2768   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2769   // Clearing nothing uses the Identity call.
2770   // Negative clears are possible on dead ClearArrays
2771   // (see jck test stmt114.stmt11402.val).
2772   if (size &lt;= 0 || size % unit != 0)  return NULL;
2773   intptr_t count = size / unit;
2774   // Length too long; use fast hardware clear
2775   if (size &gt; Matcher::init_array_short_size)  return NULL;
2776   Node *mem = in(1);
2777   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2778   Node *adr = in(3);
2779   const Type* at = phase-&gt;type(adr);
2780   if( at==Type::TOP ) return NULL;
2781   const TypePtr* atp = at-&gt;isa_ptr();
2782   // adjust atp to be the correct array element address type
2783   if (atp == NULL)  atp = TypePtr::BOTTOM;
2784   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2785   // Get base for derived pointer purposes
2786   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2787   Node *base = adr-&gt;in(1);
2788 
2789   Node *zero = phase-&gt;makecon(TypeLong::ZERO);
2790   Node *off  = phase-&gt;MakeConX(BytesPerLong);
2791   mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2792   count--;
2793   while( count-- ) {
2794     mem = phase-&gt;transform(mem);
2795     adr = phase-&gt;transform(new (phase-&gt;C) AddPNode(base,adr,off));
2796     mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2797   }
2798   return mem;
2799 }
2800 
2801 //----------------------------step_through----------------------------------
2802 // Return allocation input memory edge if it is different instance
2803 // or itself if it is the one we are looking for.
2804 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2805   Node* n = *np;
2806   assert(n-&gt;is_ClearArray(), "sanity");
2807   intptr_t offset;
2808   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2809   // This method is called only before Allocate nodes are expanded during
2810   // macro nodes expansion. Before that ClearArray nodes are only generated
2811   // in LibraryCallKit::generate_arraycopy() which follows allocations.
2812   assert(alloc != NULL, "should have allocation");
2813   if (alloc-&gt;_idx == instance_id) {
2814     // Can not bypass initialization of the instance we are looking for.
2815     return false;
2816   }
2817   // Otherwise skip it.
2818   InitializeNode* init = alloc-&gt;initialization();
2819   if (init != NULL)
2820     *np = init-&gt;in(TypeFunc::Memory);
2821   else
2822     *np = alloc-&gt;in(TypeFunc::Memory);
2823   return true;
2824 }
2825 
2826 //----------------------------clear_memory-------------------------------------
2827 // Generate code to initialize object storage to zero.
2828 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2829                                    intptr_t start_offset,
2830                                    Node* end_offset,
2831                                    PhaseGVN* phase) {
2832   Compile* C = phase-&gt;C;
2833   intptr_t offset = start_offset;
2834 
2835   int unit = BytesPerLong;
2836   if ((offset % unit) != 0) {
2837     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(offset));
2838     adr = phase-&gt;transform(adr);
2839     const TypePtr* atp = TypeRawPtr::BOTTOM;
2840     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2841     mem = phase-&gt;transform(mem);
2842     offset += BytesPerInt;
2843   }
2844   assert((offset % unit) == 0, "");
2845 
2846   // Initialize the remaining stuff, if any, with a ClearArray.
2847   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);
2848 }
2849 
2850 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2851                                    Node* start_offset,
2852                                    Node* end_offset,
2853                                    PhaseGVN* phase) {
2854   if (start_offset == end_offset) {
2855     // nothing to do
2856     return mem;
2857   }
2858 
2859   Compile* C = phase-&gt;C;
2860   int unit = BytesPerLong;
2861   Node* zbase = start_offset;
2862   Node* zend  = end_offset;
2863 
2864   // Scale to the unit required by the CPU:
2865   if (!Matcher::init_array_count_is_in_bytes) {
2866     Node* shift = phase-&gt;intcon(exact_log2(unit));
2867     zbase = phase-&gt;transform( new(C) URShiftXNode(zbase, shift) );
2868     zend  = phase-&gt;transform( new(C) URShiftXNode(zend,  shift) );
2869   }
2870 
2871   // Bulk clear double-words
2872   Node* zsize = phase-&gt;transform( new(C) SubXNode(zend, zbase) );
2873   Node* adr = phase-&gt;transform( new(C) AddPNode(dest, dest, start_offset) );
2874   mem = new (C) ClearArrayNode(ctl, mem, zsize, adr);
2875   return phase-&gt;transform(mem);
2876 }
2877 
2878 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2879                                    intptr_t start_offset,
2880                                    intptr_t end_offset,
2881                                    PhaseGVN* phase) {
2882   if (start_offset == end_offset) {
2883     // nothing to do
2884     return mem;
2885   }
2886 
2887   Compile* C = phase-&gt;C;
2888   assert((end_offset % BytesPerInt) == 0, "odd end offset");
2889   intptr_t done_offset = end_offset;
2890   if ((done_offset % BytesPerLong) != 0) {
2891     done_offset -= BytesPerInt;
2892   }
2893   if (done_offset &gt; start_offset) {
2894     mem = clear_memory(ctl, mem, dest,
2895                        start_offset, phase-&gt;MakeConX(done_offset), phase);
2896   }
2897   if (done_offset &lt; end_offset) { // emit the final 32-bit store
2898     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
2899     adr = phase-&gt;transform(adr);
2900     const TypePtr* atp = TypeRawPtr::BOTTOM;
2901     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2902     mem = phase-&gt;transform(mem);
2903     done_offset += BytesPerInt;
2904   }
2905   assert(done_offset == end_offset, "");
2906   return mem;
2907 }
2908 
2909 //=============================================================================
2910 // Do not match memory edge.
2911 uint StrIntrinsicNode::match_edge(uint idx) const {
2912   return idx == 2 || idx == 3;
2913 }
2914 
2915 //------------------------------Ideal------------------------------------------
2916 // Return a node which is more "ideal" than the current node.  Strip out
2917 // control copies
2918 Node *StrIntrinsicNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2919   if (remove_dead_region(phase, can_reshape)) return this;
2920   // Don't bother trying to transform a dead node
2921   if (in(0) &amp;&amp; in(0)-&gt;is_top())  return NULL;
2922 
2923   if (can_reshape) {
2924     Node* mem = phase-&gt;transform(in(MemNode::Memory));
2925     // If transformed to a MergeMem, get the desired slice
2926     uint alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
2927     mem = mem-&gt;is_MergeMem() ? mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : mem;
2928     if (mem != in(MemNode::Memory)) {
2929       set_req(MemNode::Memory, mem);
2930       return this;
2931     }
2932   }
2933   return NULL;
2934 }
2935 
2936 //------------------------------Value------------------------------------------
2937 const Type *StrIntrinsicNode::Value( PhaseTransform *phase ) const {
2938   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2939   return bottom_type();
2940 }
2941 
2942 //=============================================================================
2943 //------------------------------match_edge-------------------------------------
2944 // Do not match memory edge
2945 uint EncodeISOArrayNode::match_edge(uint idx) const {
2946   return idx == 2 || idx == 3; // EncodeISOArray src (Binary dst len)
2947 }
2948 
2949 //------------------------------Ideal------------------------------------------
2950 // Return a node which is more "ideal" than the current node.  Strip out
2951 // control copies
2952 Node *EncodeISOArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2953   return remove_dead_region(phase, can_reshape) ? this : NULL;
2954 }
2955 
2956 //------------------------------Value------------------------------------------
2957 const Type *EncodeISOArrayNode::Value(PhaseTransform *phase) const {
2958   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2959   return bottom_type();
2960 }
2961 
2962 //=============================================================================
2963 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
2964   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
2965     _adr_type(C-&gt;get_adr_type(alias_idx))
2966 {
2967   init_class_id(Class_MemBar);
2968   Node* top = C-&gt;top();
2969   init_req(TypeFunc::I_O,top);
2970   init_req(TypeFunc::FramePtr,top);
2971   init_req(TypeFunc::ReturnAdr,top);
2972   if (precedent != NULL)
2973     init_req(TypeFunc::Parms, precedent);
2974 }
2975 
2976 //------------------------------cmp--------------------------------------------
2977 uint MemBarNode::hash() const { return NO_HASH; }
2978 uint MemBarNode::cmp( const Node &amp;n ) const {
2979   return (&amp;n == this);          // Always fail except on self
2980 }
2981 
2982 //------------------------------make-------------------------------------------
2983 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
2984   switch (opcode) {
2985   case Op_MemBarAcquire:     return new(C) MemBarAcquireNode(C, atp, pn);
2986   case Op_LoadFence:         return new(C) LoadFenceNode(C, atp, pn);
2987   case Op_MemBarRelease:     return new(C) MemBarReleaseNode(C, atp, pn);
2988   case Op_StoreFence:        return new(C) StoreFenceNode(C, atp, pn);
2989   case Op_MemBarAcquireLock: return new(C) MemBarAcquireLockNode(C, atp, pn);
2990   case Op_MemBarReleaseLock: return new(C) MemBarReleaseLockNode(C, atp, pn);
2991   case Op_MemBarVolatile:    return new(C) MemBarVolatileNode(C, atp, pn);
2992   case Op_MemBarCPUOrder:    return new(C) MemBarCPUOrderNode(C, atp, pn);
2993   case Op_Initialize:        return new(C) InitializeNode(C, atp, pn);
2994   case Op_MemBarStoreStore:  return new(C) MemBarStoreStoreNode(C, atp, pn);
2995   default: ShouldNotReachHere(); return NULL;
2996   }
2997 }
2998 
2999 //------------------------------Ideal------------------------------------------
3000 // Return a node which is more "ideal" than the current node.  Strip out
3001 // control copies
3002 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
3003   if (remove_dead_region(phase, can_reshape)) return this;
3004   // Don't bother trying to transform a dead node
3005   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
3006     return NULL;
3007   }
3008 
3009   // Eliminate volatile MemBars for scalar replaced objects.
3010   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
3011     bool eliminate = false;
3012     int opc = Opcode();
3013     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
3014       // Volatile field loads and stores.
3015       Node* my_mem = in(MemBarNode::Precedent);
3016       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
3017       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
3018         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
3019         // replace this Precedent (decodeN) with the Load instead.
3020         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
3021           Node* load_node = my_mem-&gt;in(1);
3022           set_req(MemBarNode::Precedent, load_node);
3023           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
3024           my_mem = load_node;
3025         } else {
3026           assert(my_mem-&gt;unique_out() == this, "sanity");
3027           del_req(Precedent);
3028           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem); // remove dead node later
3029           my_mem = NULL;
3030         }
3031       }
3032       if (my_mem != NULL &amp;&amp; my_mem-&gt;is_Mem()) {
3033         const TypeOopPtr* t_oop = my_mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_oopptr();
3034         // Check for scalar replaced object reference.
3035         if( t_oop != NULL &amp;&amp; t_oop-&gt;is_known_instance_field() &amp;&amp;
3036             t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
3037             t_oop-&gt;offset() != Type::OffsetTop) {
3038           eliminate = true;
3039         }
3040       }
3041     } else if (opc == Op_MemBarRelease) {
3042       // Final field stores.
3043       Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);
3044       if ((alloc != NULL) &amp;&amp; alloc-&gt;is_Allocate() &amp;&amp;
3045           alloc-&gt;as_Allocate()-&gt;_is_non_escaping) {
3046         // The allocated object does not escape.
3047         eliminate = true;
3048       }
3049     }
3050     if (eliminate) {
3051       // Replace MemBar projections by its inputs.
3052       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3053       igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3054       igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3055       // Must return either the original node (now dead) or a new node
3056       // (Do not return a top here, since that would break the uniqueness of top.)
3057       return new (phase-&gt;C) ConINode(TypeInt::ZERO);
3058     }
3059   }
3060   return NULL;
3061 }
3062 
3063 //------------------------------Value------------------------------------------
3064 const Type *MemBarNode::Value( PhaseTransform *phase ) const {
3065   if( !in(0) ) return Type::TOP;
3066   if( phase-&gt;type(in(0)) == Type::TOP )
3067     return Type::TOP;
3068   return TypeTuple::MEMBAR;
3069 }
3070 
3071 //------------------------------match------------------------------------------
3072 // Construct projections for memory.
3073 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {
3074   switch (proj-&gt;_con) {
3075   case TypeFunc::Control:
3076   case TypeFunc::Memory:
3077     return new (m-&gt;C) MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3078   }
3079   ShouldNotReachHere();
3080   return NULL;
3081 }
3082 
3083 //===========================InitializeNode====================================
3084 // SUMMARY:
3085 // This node acts as a memory barrier on raw memory, after some raw stores.
3086 // The 'cooked' oop value feeds from the Initialize, not the Allocation.
3087 // The Initialize can 'capture' suitably constrained stores as raw inits.
3088 // It can coalesce related raw stores into larger units (called 'tiles').
3089 // It can avoid zeroing new storage for memory units which have raw inits.
3090 // At macro-expansion, it is marked 'complete', and does not optimize further.
3091 //
3092 // EXAMPLE:
3093 // The object 'new short[2]' occupies 16 bytes in a 32-bit machine.
3094 //   ctl = incoming control; mem* = incoming memory
3095 // (Note:  A star * on a memory edge denotes I/O and other standard edges.)
3096 // First allocate uninitialized memory and fill in the header:
3097 //   alloc = (Allocate ctl mem* 16 #short[].klass ...)
3098 //   ctl := alloc.Control; mem* := alloc.Memory*
3099 //   rawmem = alloc.Memory; rawoop = alloc.RawAddress
3100 // Then initialize to zero the non-header parts of the raw memory block:
3101 //   init = (Initialize alloc.Control alloc.Memory* alloc.RawAddress)
3102 //   ctl := init.Control; mem.SLICE(#short[*]) := init.Memory
3103 // After the initialize node executes, the object is ready for service:
3104 //   oop := (CheckCastPP init.Control alloc.RawAddress #short[])
3105 // Suppose its body is immediately initialized as {1,2}:
3106 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3107 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3108 //   mem.SLICE(#short[*]) := store2
3109 //
3110 // DETAILS:
3111 // An InitializeNode collects and isolates object initialization after
3112 // an AllocateNode and before the next possible safepoint.  As a
3113 // memory barrier (MemBarNode), it keeps critical stores from drifting
3114 // down past any safepoint or any publication of the allocation.
3115 // Before this barrier, a newly-allocated object may have uninitialized bits.
3116 // After this barrier, it may be treated as a real oop, and GC is allowed.
3117 //
3118 // The semantics of the InitializeNode include an implicit zeroing of
3119 // the new object from object header to the end of the object.
3120 // (The object header and end are determined by the AllocateNode.)
3121 //
3122 // Certain stores may be added as direct inputs to the InitializeNode.
3123 // These stores must update raw memory, and they must be to addresses
3124 // derived from the raw address produced by AllocateNode, and with
3125 // a constant offset.  They must be ordered by increasing offset.
3126 // The first one is at in(RawStores), the last at in(req()-1).
3127 // Unlike most memory operations, they are not linked in a chain,
3128 // but are displayed in parallel as users of the rawmem output of
3129 // the allocation.
3130 //
3131 // (See comments in InitializeNode::capture_store, which continue
3132 // the example given above.)
3133 //
3134 // When the associated Allocate is macro-expanded, the InitializeNode
3135 // may be rewritten to optimize collected stores.  A ClearArrayNode
3136 // may also be created at that point to represent any required zeroing.
3137 // The InitializeNode is then marked 'complete', prohibiting further
3138 // capturing of nearby memory operations.
3139 //
3140 // During macro-expansion, all captured initializations which store
3141 // constant values of 32 bits or smaller are coalesced (if advantageous)
3142 // into larger 'tiles' 32 or 64 bits.  This allows an object to be
3143 // initialized in fewer memory operations.  Memory words which are
3144 // covered by neither tiles nor non-constant stores are pre-zeroed
3145 // by explicit stores of zero.  (The code shape happens to do all
3146 // zeroing first, then all other stores, with both sequences occurring
3147 // in order of ascending offsets.)
3148 //
3149 // Alternatively, code may be inserted between an AllocateNode and its
3150 // InitializeNode, to perform arbitrary initialization of the new object.
3151 // E.g., the object copying intrinsics insert complex data transfers here.
3152 // The initialization must then be marked as 'complete' disable the
3153 // built-in zeroing semantics and the collection of initializing stores.
3154 //
3155 // While an InitializeNode is incomplete, reads from the memory state
3156 // produced by it are optimizable if they match the control edge and
3157 // new oop address associated with the allocation/initialization.
3158 // They return a stored value (if the offset matches) or else zero.
3159 // A write to the memory state, if it matches control and address,
3160 // and if it is to a constant offset, may be 'captured' by the
3161 // InitializeNode.  It is cloned as a raw memory operation and rewired
3162 // inside the initialization, to the raw oop produced by the allocation.
3163 // Operations on addresses which are provably distinct (e.g., to
3164 // other AllocateNodes) are allowed to bypass the initialization.
3165 //
3166 // The effect of all this is to consolidate object initialization
3167 // (both arrays and non-arrays, both piecewise and bulk) into a
3168 // single location, where it can be optimized as a unit.
3169 //
3170 // Only stores with an offset less than TrackedInitializationLimit words
3171 // will be considered for capture by an InitializeNode.  This puts a
3172 // reasonable limit on the complexity of optimized initializations.
3173 
3174 //---------------------------InitializeNode------------------------------------
3175 InitializeNode::InitializeNode(Compile* C, int adr_type, Node* rawoop)
3176   : _is_complete(Incomplete), _does_not_escape(false),
3177     MemBarNode(C, adr_type, rawoop)
3178 {
3179   init_class_id(Class_Initialize);
3180 
3181   assert(adr_type == Compile::AliasIdxRaw, "only valid atp");
3182   assert(in(RawAddress) == rawoop, "proper init");
3183   // Note:  allocation() can be NULL, for secondary initialization barriers
3184 }
3185 
3186 // Since this node is not matched, it will be processed by the
3187 // register allocator.  Declare that there are no constraints
3188 // on the allocation of the RawAddress edge.
3189 const RegMask &amp;InitializeNode::in_RegMask(uint idx) const {
3190   // This edge should be set to top, by the set_complete.  But be conservative.
3191   if (idx == InitializeNode::RawAddress)
3192     return *(Compile::current()-&gt;matcher()-&gt;idealreg2spillmask[in(idx)-&gt;ideal_reg()]);
3193   return RegMask::Empty;
3194 }
3195 
3196 Node* InitializeNode::memory(uint alias_idx) {
3197   Node* mem = in(Memory);
3198   if (mem-&gt;is_MergeMem()) {
3199     return mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
3200   } else {
3201     // incoming raw memory is not split
3202     return mem;
3203   }
3204 }
3205 
3206 bool InitializeNode::is_non_zero() {
3207   if (is_complete())  return false;
3208   remove_extra_zeroes();
3209   return (req() &gt; RawStores);
3210 }
3211 
3212 void InitializeNode::set_complete(PhaseGVN* phase) {
3213   assert(!is_complete(), "caller responsibility");
3214   _is_complete = Complete;
3215 
3216   // After this node is complete, it contains a bunch of
3217   // raw-memory initializations.  There is no need for
3218   // it to have anything to do with non-raw memory effects.
3219   // Therefore, tell all non-raw users to re-optimize themselves,
3220   // after skipping the memory effects of this initialization.
3221   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3222   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3223 }
3224 
3225 // convenience function
3226 // return false if the init contains any stores already
3227 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3228   InitializeNode* init = initialization();
3229   if (init == NULL || init-&gt;is_complete())  return false;
3230   init-&gt;remove_extra_zeroes();
3231   // for now, if this allocation has already collected any inits, bail:
3232   if (init-&gt;is_non_zero())  return false;
3233   init-&gt;set_complete(phase);
3234   return true;
3235 }
3236 
3237 void InitializeNode::remove_extra_zeroes() {
3238   if (req() == RawStores)  return;
3239   Node* zmem = zero_memory();
3240   uint fill = RawStores;
3241   for (uint i = fill; i &lt; req(); i++) {
3242     Node* n = in(i);
3243     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3244     if (fill &lt; i)  set_req(fill, n);          // compact
3245     ++fill;
3246   }
3247   // delete any empty spaces created:
3248   while (fill &lt; req()) {
3249     del_req(fill);
3250   }
3251 }
3252 
3253 // Helper for remembering which stores go with which offsets.
3254 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3255   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3256   intptr_t offset = -1;
3257   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3258                                                phase, offset);
3259   if (base == NULL)     return -1;  // something is dead,
3260   if (offset &lt; 0)       return -1;  //        dead, dead
3261   return offset;
3262 }
3263 
3264 // Helper for proving that an initialization expression is
3265 // "simple enough" to be folded into an object initialization.
3266 // Attempts to prove that a store's initial value 'n' can be captured
3267 // within the initialization without creating a vicious cycle, such as:
3268 //     { Foo p = new Foo(); p.next = p; }
3269 // True for constants and parameters and small combinations thereof.
3270 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {
3271   if (n == NULL)      return true;   // (can this really happen?)
3272   if (n-&gt;is_Proj())   n = n-&gt;in(0);
3273   if (n == this)      return false;  // found a cycle
3274   if (n-&gt;is_Con())    return true;
3275   if (n-&gt;is_Start())  return true;   // params, etc., are OK
3276   if (n-&gt;is_Root())   return true;   // even better
3277 
3278   Node* ctl = n-&gt;in(0);
3279   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {
3280     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);
3281     if (ctl == this)  return false;
3282 
3283     // If we already know that the enclosing memory op is pinned right after
3284     // the init, then any control flow that the store has picked up
3285     // must have preceded the init, or else be equal to the init.
3286     // Even after loop optimizations (which might change control edges)
3287     // a store is never pinned *before* the availability of its inputs.
3288     if (!MemNode::all_controls_dominate(n, this))
3289       return false;                  // failed to prove a good control
3290   }
3291 
3292   // Check data edges for possible dependencies on 'this'.
3293   if ((count += 1) &gt; 20)  return false;  // complexity limit
3294   for (uint i = 1; i &lt; n-&gt;req(); i++) {
3295     Node* m = n-&gt;in(i);
3296     if (m == NULL || m == n || m-&gt;is_top())  continue;
3297     uint first_i = n-&gt;find_edge(m);
3298     if (i != first_i)  continue;  // process duplicate edge just once
3299     if (!detect_init_independence(m, count)) {
3300       return false;
3301     }
3302   }
3303 
3304   return true;
3305 }
3306 
3307 // Here are all the checks a Store must pass before it can be moved into
3308 // an initialization.  Returns zero if a check fails.
3309 // On success, returns the (constant) offset to which the store applies,
3310 // within the initialized memory.
3311 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {
3312   const int FAIL = 0;
3313   if (st-&gt;req() != MemNode::ValueIn + 1)
3314     return FAIL;                // an inscrutable StoreNode (card mark?)
3315   Node* ctl = st-&gt;in(MemNode::Control);
3316   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3317     return FAIL;                // must be unconditional after the initialization
3318   Node* mem = st-&gt;in(MemNode::Memory);
3319   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3320     return FAIL;                // must not be preceded by other stores
3321   Node* adr = st-&gt;in(MemNode::Address);
3322   intptr_t offset;
3323   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3324   if (alloc == NULL)
3325     return FAIL;                // inscrutable address
3326   if (alloc != allocation())
3327     return FAIL;                // wrong allocation!  (store needs to float up)
3328   Node* val = st-&gt;in(MemNode::ValueIn);
3329   int complexity_count = 0;
3330   if (!detect_init_independence(val, complexity_count))
3331     return FAIL;                // stored value must be 'simple enough'
3332 
3333   // The Store can be captured only if nothing after the allocation
3334   // and before the Store is using the memory location that the store
3335   // overwrites.
3336   bool failed = false;
3337   // If is_complete_with_arraycopy() is true the shape of the graph is
3338   // well defined and is safe so no need for extra checks.
3339   if (!is_complete_with_arraycopy()) {
3340     // We are going to look at each use of the memory state following
3341     // the allocation to make sure nothing reads the memory that the
3342     // Store writes.
3343     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3344     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3345     ResourceMark rm;
3346     Unique_Node_List mems;
3347     mems.push(mem);
3348     Node* unique_merge = NULL;
3349     for (uint next = 0; next &lt; mems.size(); ++next) {
3350       Node *m  = mems.at(next);
3351       for (DUIterator_Fast jmax, j = m-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3352         Node *n = m-&gt;fast_out(j);
3353         if (n-&gt;outcnt() == 0) {
3354           continue;
3355         }
3356         if (n == st) {
3357           continue;
3358         } else if (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0) != ctl) {
3359           // If the control of this use is different from the control
3360           // of the Store which is right after the InitializeNode then
3361           // this node cannot be between the InitializeNode and the
3362           // Store.
3363           continue;
3364         } else if (n-&gt;is_MergeMem()) {
3365           if (n-&gt;as_MergeMem()-&gt;memory_at(alias_idx) == m) {
3366             // We can hit a MergeMemNode (that will likely go away
3367             // later) that is a direct use of the memory state
3368             // following the InitializeNode on the same slice as the
3369             // store node that we'd like to capture. We need to check
3370             // the uses of the MergeMemNode.
3371             mems.push(n);
3372           }
3373         } else if (n-&gt;is_Mem()) {
3374           Node* other_adr = n-&gt;in(MemNode::Address);
3375           if (other_adr == adr) {
3376             failed = true;
3377             break;
3378           } else {
3379             const TypePtr* other_t_adr = phase-&gt;type(other_adr)-&gt;isa_ptr();
3380             if (other_t_adr != NULL) {
3381               int other_alias_idx = phase-&gt;C-&gt;get_alias_index(other_t_adr);
3382               if (other_alias_idx == alias_idx) {
3383                 // A load from the same memory slice as the store right
3384                 // after the InitializeNode. We check the control of the
3385                 // object/array that is loaded from. If it's the same as
3386                 // the store control then we cannot capture the store.
3387                 assert(!n-&gt;is_Store(), "2 stores to same slice on same control?");
3388                 Node* base = other_adr;
3389                 assert(base-&gt;is_AddP(), err_msg_res("should be addp but is %s", base-&gt;Name()));
3390                 base = base-&gt;in(AddPNode::Base);
3391                 if (base != NULL) {
3392                   base = base-&gt;uncast();
3393                   if (base-&gt;is_Proj() &amp;&amp; base-&gt;in(0) == alloc) {
3394                     failed = true;
3395                     break;
3396                   }
3397                 }
3398               }
3399             }
3400           }
3401         } else {
3402           failed = true;
3403           break;
3404         }
3405       }
3406     }
3407   }
3408   if (failed) {
3409     if (!can_reshape) {
3410       // We decided we couldn't capture the store during parsing. We
3411       // should try again during the next IGVN once the graph is
3412       // cleaner.
3413       phase-&gt;C-&gt;record_for_igvn(st);
3414     }
3415     return FAIL;
3416   }
3417 
3418   return offset;                // success
3419 }
3420 
3421 // Find the captured store in(i) which corresponds to the range
3422 // [start..start+size) in the initialized object.
3423 // If there is one, return its index i.  If there isn't, return the
3424 // negative of the index where it should be inserted.
3425 // Return 0 if the queried range overlaps an initialization boundary
3426 // or if dead code is encountered.
3427 // If size_in_bytes is zero, do not bother with overlap checks.
3428 int InitializeNode::captured_store_insertion_point(intptr_t start,
3429                                                    int size_in_bytes,
3430                                                    PhaseTransform* phase) {
3431   const int FAIL = 0, MAX_STORE = BytesPerLong;
3432 
3433   if (is_complete())
3434     return FAIL;                // arraycopy got here first; punt
3435 
3436   assert(allocation() != NULL, "must be present");
3437 
3438   // no negatives, no header fields:
3439   if (start &lt; (intptr_t) allocation()-&gt;minimum_header_size())  return FAIL;
3440 
3441   // after a certain size, we bail out on tracking all the stores:
3442   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3443   if (start &gt;= ti_limit)  return FAIL;
3444 
3445   for (uint i = InitializeNode::RawStores, limit = req(); ; ) {
3446     if (i &gt;= limit)  return -(int)i; // not found; here is where to put it
3447 
3448     Node*    st     = in(i);
3449     intptr_t st_off = get_store_offset(st, phase);
3450     if (st_off &lt; 0) {
3451       if (st != zero_memory()) {
3452         return FAIL;            // bail out if there is dead garbage
3453       }
3454     } else if (st_off &gt; start) {
3455       // ...we are done, since stores are ordered
3456       if (st_off &lt; start + size_in_bytes) {
3457         return FAIL;            // the next store overlaps
3458       }
3459       return -(int)i;           // not found; here is where to put it
3460     } else if (st_off &lt; start) {
3461       if (size_in_bytes != 0 &amp;&amp;
3462           start &lt; st_off + MAX_STORE &amp;&amp;
3463           start &lt; st_off + st-&gt;as_Store()-&gt;memory_size()) {
3464         return FAIL;            // the previous store overlaps
3465       }
3466     } else {
3467       if (size_in_bytes != 0 &amp;&amp;
3468           st-&gt;as_Store()-&gt;memory_size() != size_in_bytes) {
3469         return FAIL;            // mismatched store size
3470       }
3471       return i;
3472     }
3473 
3474     ++i;
3475   }
3476 }
3477 
3478 // Look for a captured store which initializes at the offset 'start'
3479 // with the given size.  If there is no such store, and no other
3480 // initialization interferes, then return zero_memory (the memory
3481 // projection of the AllocateNode).
3482 Node* InitializeNode::find_captured_store(intptr_t start, int size_in_bytes,
3483                                           PhaseTransform* phase) {
3484   assert(stores_are_sane(phase), "");
3485   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3486   if (i == 0) {
3487     return NULL;                // something is dead
3488   } else if (i &lt; 0) {
3489     return zero_memory();       // just primordial zero bits here
3490   } else {
3491     Node* st = in(i);           // here is the store at this position
3492     assert(get_store_offset(st-&gt;as_Store(), phase) == start, "sanity");
3493     return st;
3494   }
3495 }
3496 
3497 // Create, as a raw pointer, an address within my new object at 'offset'.
3498 Node* InitializeNode::make_raw_address(intptr_t offset,
3499                                        PhaseTransform* phase) {
3500   Node* addr = in(RawAddress);
3501   if (offset != 0) {
3502     Compile* C = phase-&gt;C;
3503     addr = phase-&gt;transform( new (C) AddPNode(C-&gt;top(), addr,
3504                                                  phase-&gt;MakeConX(offset)) );
3505   }
3506   return addr;
3507 }
3508 
3509 // Clone the given store, converting it into a raw store
3510 // initializing a field or element of my new object.
3511 // Caller is responsible for retiring the original store,
3512 // with subsume_node or the like.
3513 //
3514 // From the example above InitializeNode::InitializeNode,
3515 // here are the old stores to be captured:
3516 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3517 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3518 //
3519 // Here is the changed code; note the extra edges on init:
3520 //   alloc = (Allocate ...)
3521 //   rawoop = alloc.RawAddress
3522 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3523 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3524 //   init = (Initialize alloc.Control alloc.Memory rawoop
3525 //                      rawstore1 rawstore2)
3526 //
3527 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
3528                                     PhaseTransform* phase, bool can_reshape) {
3529   assert(stores_are_sane(phase), "");
3530 
3531   if (start &lt; 0)  return NULL;
3532   assert(can_capture_store(st, phase, can_reshape) == start, "sanity");
3533 
3534   Compile* C = phase-&gt;C;
3535   int size_in_bytes = st-&gt;memory_size();
3536   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3537   if (i == 0)  return NULL;     // bail out
3538   Node* prev_mem = NULL;        // raw memory for the captured store
3539   if (i &gt; 0) {
3540     prev_mem = in(i);           // there is a pre-existing store under this one
3541     set_req(i, C-&gt;top());       // temporarily disconnect it
3542     // See StoreNode::Ideal 'st-&gt;outcnt() == 1' for the reason to disconnect.
3543   } else {
3544     i = -i;                     // no pre-existing store
3545     prev_mem = zero_memory();   // a slice of the newly allocated object
3546     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3547       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3548     else
3549       ins_req(i, C-&gt;top());     // build a new edge
3550   }
3551   Node* new_st = st-&gt;clone();
3552   new_st-&gt;set_req(MemNode::Control, in(Control));
3553   new_st-&gt;set_req(MemNode::Memory,  prev_mem);
3554   new_st-&gt;set_req(MemNode::Address, make_raw_address(start, phase));
3555   new_st = phase-&gt;transform(new_st);
3556 
3557   // At this point, new_st might have swallowed a pre-existing store
3558   // at the same offset, or perhaps new_st might have disappeared,
3559   // if it redundantly stored the same value (or zero to fresh memory).
3560 
3561   // In any case, wire it in:
3562   set_req(i, new_st);
3563 
3564   // The caller may now kill the old guy.
3565   DEBUG_ONLY(Node* check_st = find_captured_store(start, size_in_bytes, phase));
3566   assert(check_st == new_st || check_st == NULL, "must be findable");
3567   assert(!is_complete(), "");
3568   return new_st;
3569 }
3570 
3571 static bool store_constant(jlong* tiles, int num_tiles,
3572                            intptr_t st_off, int st_size,
3573                            jlong con) {
3574   if ((st_off &amp; (st_size-1)) != 0)
3575     return false;               // strange store offset (assume size==2**N)
3576   address addr = (address)tiles + st_off;
3577   assert(st_off &gt;= 0 &amp;&amp; addr+st_size &lt;= (address)&amp;tiles[num_tiles], "oob");
3578   switch (st_size) {
3579   case sizeof(jbyte):  *(jbyte*) addr = (jbyte) con; break;
3580   case sizeof(jchar):  *(jchar*) addr = (jchar) con; break;
3581   case sizeof(jint):   *(jint*)  addr = (jint)  con; break;
3582   case sizeof(jlong):  *(jlong*) addr = (jlong) con; break;
3583   default: return false;        // strange store size (detect size!=2**N here)
3584   }
3585   return true;                  // return success to caller
3586 }
3587 
3588 // Coalesce subword constants into int constants and possibly
3589 // into long constants.  The goal, if the CPU permits,
3590 // is to initialize the object with a small number of 64-bit tiles.
3591 // Also, convert floating-point constants to bit patterns.
3592 // Non-constants are not relevant to this pass.
3593 //
3594 // In terms of the running example on InitializeNode::InitializeNode
3595 // and InitializeNode::capture_store, here is the transformation
3596 // of rawstore1 and rawstore2 into rawstore12:
3597 //   alloc = (Allocate ...)
3598 //   rawoop = alloc.RawAddress
3599 //   tile12 = 0x00010002
3600 //   rawstore12 = (StoreI alloc.Control alloc.Memory (+ rawoop 12) tile12)
3601 //   init = (Initialize alloc.Control alloc.Memory rawoop rawstore12)
3602 //
3603 void
3604 InitializeNode::coalesce_subword_stores(intptr_t header_size,
3605                                         Node* size_in_bytes,
3606                                         PhaseGVN* phase) {
3607   Compile* C = phase-&gt;C;
3608 
3609   assert(stores_are_sane(phase), "");
3610   // Note:  After this pass, they are not completely sane,
3611   // since there may be some overlaps.
3612 
3613   int old_subword = 0, old_long = 0, new_int = 0, new_long = 0;
3614 
3615   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3616   intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, ti_limit);
3617   size_limit = MIN2(size_limit, ti_limit);
3618   size_limit = align_size_up(size_limit, BytesPerLong);
3619   int num_tiles = size_limit / BytesPerLong;
3620 
3621   // allocate space for the tile map:
3622   const int small_len = DEBUG_ONLY(true ? 3 :) 30; // keep stack frames small
3623   jlong  tiles_buf[small_len];
3624   Node*  nodes_buf[small_len];
3625   jlong  inits_buf[small_len];
3626   jlong* tiles = ((num_tiles &lt;= small_len) ? &amp;tiles_buf[0]
3627                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3628   Node** nodes = ((num_tiles &lt;= small_len) ? &amp;nodes_buf[0]
3629                   : NEW_RESOURCE_ARRAY(Node*, num_tiles));
3630   jlong* inits = ((num_tiles &lt;= small_len) ? &amp;inits_buf[0]
3631                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3632   // tiles: exact bitwise model of all primitive constants
3633   // nodes: last constant-storing node subsumed into the tiles model
3634   // inits: which bytes (in each tile) are touched by any initializations
3635 
3636   //// Pass A: Fill in the tile model with any relevant stores.
3637 
3638   Copy::zero_to_bytes(tiles, sizeof(tiles[0]) * num_tiles);
3639   Copy::zero_to_bytes(nodes, sizeof(nodes[0]) * num_tiles);
3640   Copy::zero_to_bytes(inits, sizeof(inits[0]) * num_tiles);
3641   Node* zmem = zero_memory(); // initially zero memory state
3642   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3643     Node* st = in(i);
3644     intptr_t st_off = get_store_offset(st, phase);
3645 
3646     // Figure out the store's offset and constant value:
3647     if (st_off &lt; header_size)             continue; //skip (ignore header)
3648     if (st-&gt;in(MemNode::Memory) != zmem)  continue; //skip (odd store chain)
3649     int st_size = st-&gt;as_Store()-&gt;memory_size();
3650     if (st_off + st_size &gt; size_limit)    break;
3651 
3652     // Record which bytes are touched, whether by constant or not.
3653     if (!store_constant(inits, num_tiles, st_off, st_size, (jlong) -1))
3654       continue;                 // skip (strange store size)
3655 
3656     const Type* val = phase-&gt;type(st-&gt;in(MemNode::ValueIn));
3657     if (!val-&gt;singleton())                continue; //skip (non-con store)
3658     BasicType type = val-&gt;basic_type();
3659 
3660     jlong con = 0;
3661     switch (type) {
3662     case T_INT:    con = val-&gt;is_int()-&gt;get_con();  break;
3663     case T_LONG:   con = val-&gt;is_long()-&gt;get_con(); break;
3664     case T_FLOAT:  con = jint_cast(val-&gt;getf());    break;
3665     case T_DOUBLE: con = jlong_cast(val-&gt;getd());   break;
3666     default:                              continue; //skip (odd store type)
3667     }
3668 
3669     if (type == T_LONG &amp;&amp; Matcher::isSimpleConstant64(con) &amp;&amp;
3670         st-&gt;Opcode() == Op_StoreL) {
3671       continue;                 // This StoreL is already optimal.
3672     }
3673 
3674     // Store down the constant.
3675     store_constant(tiles, num_tiles, st_off, st_size, con);
3676 
3677     intptr_t j = st_off &gt;&gt; LogBytesPerLong;
3678 
3679     if (type == T_INT &amp;&amp; st_size == BytesPerInt
3680         &amp;&amp; (st_off &amp; BytesPerInt) == BytesPerInt) {
3681       jlong lcon = tiles[j];
3682       if (!Matcher::isSimpleConstant64(lcon) &amp;&amp;
3683           st-&gt;Opcode() == Op_StoreI) {
3684         // This StoreI is already optimal by itself.
3685         jint* intcon = (jint*) &amp;tiles[j];
3686         intcon[1] = 0;  // undo the store_constant()
3687 
3688         // If the previous store is also optimal by itself, back up and
3689         // undo the action of the previous loop iteration... if we can.
3690         // But if we can't, just let the previous half take care of itself.
3691         st = nodes[j];
3692         st_off -= BytesPerInt;
3693         con = intcon[0];
3694         if (con != 0 &amp;&amp; st != NULL &amp;&amp; st-&gt;Opcode() == Op_StoreI) {
3695           assert(st_off &gt;= header_size, "still ignoring header");
3696           assert(get_store_offset(st, phase) == st_off, "must be");
3697           assert(in(i-1) == zmem, "must be");
3698           DEBUG_ONLY(const Type* tcon = phase-&gt;type(st-&gt;in(MemNode::ValueIn)));
3699           assert(con == tcon-&gt;is_int()-&gt;get_con(), "must be");
3700           // Undo the effects of the previous loop trip, which swallowed st:
3701           intcon[0] = 0;        // undo store_constant()
3702           set_req(i-1, st);     // undo set_req(i, zmem)
3703           nodes[j] = NULL;      // undo nodes[j] = st
3704           --old_subword;        // undo ++old_subword
3705         }
3706         continue;               // This StoreI is already optimal.
3707       }
3708     }
3709 
3710     // This store is not needed.
3711     set_req(i, zmem);
3712     nodes[j] = st;              // record for the moment
3713     if (st_size &lt; BytesPerLong) // something has changed
3714           ++old_subword;        // includes int/float, but who's counting...
3715     else  ++old_long;
3716   }
3717 
3718   if ((old_subword + old_long) == 0)
3719     return;                     // nothing more to do
3720 
3721   //// Pass B: Convert any non-zero tiles into optimal constant stores.
3722   // Be sure to insert them before overlapping non-constant stores.
3723   // (E.g., byte[] x = { 1,2,y,4 }  =&gt;  x[int 0] = 0x01020004, x[2]=y.)
3724   for (int j = 0; j &lt; num_tiles; j++) {
3725     jlong con  = tiles[j];
3726     jlong init = inits[j];
3727     if (con == 0)  continue;
3728     jint con0,  con1;           // split the constant, address-wise
3729     jint init0, init1;          // split the init map, address-wise
3730     { union { jlong con; jint intcon[2]; } u;
3731       u.con = con;
3732       con0  = u.intcon[0];
3733       con1  = u.intcon[1];
3734       u.con = init;
3735       init0 = u.intcon[0];
3736       init1 = u.intcon[1];
3737     }
3738 
3739     Node* old = nodes[j];
3740     assert(old != NULL, "need the prior store");
3741     intptr_t offset = (j * BytesPerLong);
3742 
3743     bool split = !Matcher::isSimpleConstant64(con);
3744 
3745     if (offset &lt; header_size) {
3746       assert(offset + BytesPerInt &gt;= header_size, "second int counts");
3747       assert(*(jint*)&amp;tiles[j] == 0, "junk in header");
3748       split = true;             // only the second word counts
3749       // Example:  int a[] = { 42 ... }
3750     } else if (con0 == 0 &amp;&amp; init0 == -1) {
3751       split = true;             // first word is covered by full inits
3752       // Example:  int a[] = { ... foo(), 42 ... }
3753     } else if (con1 == 0 &amp;&amp; init1 == -1) {
3754       split = true;             // second word is covered by full inits
3755       // Example:  int a[] = { ... 42, foo() ... }
3756     }
3757 
3758     // Here's a case where init0 is neither 0 nor -1:
3759     //   byte a[] = { ... 0,0,foo(),0,  0,0,0,42 ... }
3760     // Assuming big-endian memory, init0, init1 are 0x0000FF00, 0x000000FF.
3761     // In this case the tile is not split; it is (jlong)42.
3762     // The big tile is stored down, and then the foo() value is inserted.
3763     // (If there were foo(),foo() instead of foo(),0, init0 would be -1.)
3764 
3765     Node* ctl = old-&gt;in(MemNode::Control);
3766     Node* adr = make_raw_address(offset, phase);
3767     const TypePtr* atp = TypeRawPtr::BOTTOM;
3768 
3769     // One or two coalesced stores to plop down.
3770     Node*    st[2];
3771     intptr_t off[2];
3772     int  nst = 0;
3773     if (!split) {
3774       ++new_long;
3775       off[nst] = offset;
3776       st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3777                                   phase-&gt;longcon(con), T_LONG, MemNode::unordered);
3778     } else {
3779       // Omit either if it is a zero.
3780       if (con0 != 0) {
3781         ++new_int;
3782         off[nst]  = offset;
3783         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3784                                     phase-&gt;intcon(con0), T_INT, MemNode::unordered);
3785       }
3786       if (con1 != 0) {
3787         ++new_int;
3788         offset += BytesPerInt;
3789         adr = make_raw_address(offset, phase);
3790         off[nst]  = offset;
3791         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3792                                     phase-&gt;intcon(con1), T_INT, MemNode::unordered);
3793       }
3794     }
3795 
3796     // Insert second store first, then the first before the second.
3797     // Insert each one just before any overlapping non-constant stores.
3798     while (nst &gt; 0) {
3799       Node* st1 = st[--nst];
3800       C-&gt;copy_node_notes_to(st1, old);
3801       st1 = phase-&gt;transform(st1);
3802       offset = off[nst];
3803       assert(offset &gt;= header_size, "do not smash header");
3804       int ins_idx = captured_store_insertion_point(offset, /*size:*/0, phase);
3805       guarantee(ins_idx != 0, "must re-insert constant store");
3806       if (ins_idx &lt; 0)  ins_idx = -ins_idx;  // never overlap
3807       if (ins_idx &gt; InitializeNode::RawStores &amp;&amp; in(ins_idx-1) == zmem)
3808         set_req(--ins_idx, st1);
3809       else
3810         ins_req(ins_idx, st1);
3811     }
3812   }
3813 
3814   if (PrintCompilation &amp;&amp; WizardMode)
3815     tty-&gt;print_cr("Changed %d/%d subword/long constants into %d/%d int/long",
3816                   old_subword, old_long, new_int, new_long);
3817   if (C-&gt;log() != NULL)
3818     C-&gt;log()-&gt;elem("comment that='%d/%d subword/long to %d/%d int/long'",
3819                    old_subword, old_long, new_int, new_long);
3820 
3821   // Clean up any remaining occurrences of zmem:
3822   remove_extra_zeroes();
3823 }
3824 
3825 // Explore forward from in(start) to find the first fully initialized
3826 // word, and return its offset.  Skip groups of subword stores which
3827 // together initialize full words.  If in(start) is itself part of a
3828 // fully initialized word, return the offset of in(start).  If there
3829 // are no following full-word stores, or if something is fishy, return
3830 // a negative value.
3831 intptr_t InitializeNode::find_next_fullword_store(uint start, PhaseGVN* phase) {
3832   int       int_map = 0;
3833   intptr_t  int_map_off = 0;
3834   const int FULL_MAP = right_n_bits(BytesPerInt);  // the int_map we hope for
3835 
3836   for (uint i = start, limit = req(); i &lt; limit; i++) {
3837     Node* st = in(i);
3838 
3839     intptr_t st_off = get_store_offset(st, phase);
3840     if (st_off &lt; 0)  break;  // return conservative answer
3841 
3842     int st_size = st-&gt;as_Store()-&gt;memory_size();
3843     if (st_size &gt;= BytesPerInt &amp;&amp; (st_off % BytesPerInt) == 0) {
3844       return st_off;            // we found a complete word init
3845     }
3846 
3847     // update the map:
3848 
3849     intptr_t this_int_off = align_size_down(st_off, BytesPerInt);
3850     if (this_int_off != int_map_off) {
3851       // reset the map:
3852       int_map = 0;
3853       int_map_off = this_int_off;
3854     }
3855 
3856     int subword_off = st_off - this_int_off;
3857     int_map |= right_n_bits(st_size) &lt;&lt; subword_off;
3858     if ((int_map &amp; FULL_MAP) == FULL_MAP) {
3859       return this_int_off;      // we found a complete word init
3860     }
3861 
3862     // Did this store hit or cross the word boundary?
3863     intptr_t next_int_off = align_size_down(st_off + st_size, BytesPerInt);
3864     if (next_int_off == this_int_off + BytesPerInt) {
3865       // We passed the current int, without fully initializing it.
3866       int_map_off = next_int_off;
3867       int_map &gt;&gt;= BytesPerInt;
3868     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
3869       // We passed the current and next int.
3870       return this_int_off + BytesPerInt;
3871     }
3872   }
3873 
3874   return -1;
3875 }
3876 
3877 
3878 // Called when the associated AllocateNode is expanded into CFG.
3879 // At this point, we may perform additional optimizations.
3880 // Linearize the stores by ascending offset, to make memory
3881 // activity as coherent as possible.
3882 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
3883                                       intptr_t header_size,
3884                                       Node* size_in_bytes,
3885                                       PhaseGVN* phase) {
3886   assert(!is_complete(), "not already complete");
3887   assert(stores_are_sane(phase), "");
3888   assert(allocation() != NULL, "must be present");
3889 
3890   remove_extra_zeroes();
3891 
3892   if (ReduceFieldZeroing || ReduceBulkZeroing)
3893     // reduce instruction count for common initialization patterns
3894     coalesce_subword_stores(header_size, size_in_bytes, phase);
3895 
3896   Node* zmem = zero_memory();   // initially zero memory state
3897   Node* inits = zmem;           // accumulating a linearized chain of inits
3898   #ifdef ASSERT
3899   intptr_t first_offset = allocation()-&gt;minimum_header_size();
3900   intptr_t last_init_off = first_offset;  // previous init offset
3901   intptr_t last_init_end = first_offset;  // previous init offset+size
3902   intptr_t last_tile_end = first_offset;  // previous tile offset+size
3903   #endif
3904   intptr_t zeroes_done = header_size;
3905 
3906   bool do_zeroing = true;       // we might give up if inits are very sparse
3907   int  big_init_gaps = 0;       // how many large gaps have we seen?
3908 
3909   if (ZeroTLAB)  do_zeroing = false;
3910   if (!ReduceFieldZeroing &amp;&amp; !ReduceBulkZeroing)  do_zeroing = false;
3911 
3912   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3913     Node* st = in(i);
3914     intptr_t st_off = get_store_offset(st, phase);
3915     if (st_off &lt; 0)
3916       break;                    // unknown junk in the inits
3917     if (st-&gt;in(MemNode::Memory) != zmem)
3918       break;                    // complicated store chains somehow in list
3919 
3920     int st_size = st-&gt;as_Store()-&gt;memory_size();
3921     intptr_t next_init_off = st_off + st_size;
3922 
3923     if (do_zeroing &amp;&amp; zeroes_done &lt; next_init_off) {
3924       // See if this store needs a zero before it or under it.
3925       intptr_t zeroes_needed = st_off;
3926 
3927       if (st_size &lt; BytesPerInt) {
3928         // Look for subword stores which only partially initialize words.
3929         // If we find some, we must lay down some word-level zeroes first,
3930         // underneath the subword stores.
3931         //
3932         // Examples:
3933         //   byte[] a = { p,q,r,s }  =&gt;  a[0]=p,a[1]=q,a[2]=r,a[3]=s
3934         //   byte[] a = { x,y,0,0 }  =&gt;  a[0..3] = 0, a[0]=x,a[1]=y
3935         //   byte[] a = { 0,0,z,0 }  =&gt;  a[0..3] = 0, a[2]=z
3936         //
3937         // Note:  coalesce_subword_stores may have already done this,
3938         // if it was prompted by constant non-zero subword initializers.
3939         // But this case can still arise with non-constant stores.
3940 
3941         intptr_t next_full_store = find_next_fullword_store(i, phase);
3942 
3943         // In the examples above:
3944         //   in(i)          p   q   r   s     x   y     z
3945         //   st_off        12  13  14  15    12  13    14
3946         //   st_size        1   1   1   1     1   1     1
3947         //   next_full_s.  12  16  16  16    16  16    16
3948         //   z's_done      12  16  16  16    12  16    12
3949         //   z's_needed    12  16  16  16    16  16    16
3950         //   zsize          0   0   0   0     4   0     4
3951         if (next_full_store &lt; 0) {
3952           // Conservative tack:  Zero to end of current word.
3953           zeroes_needed = align_size_up(zeroes_needed, BytesPerInt);
3954         } else {
3955           // Zero to beginning of next fully initialized word.
3956           // Or, don't zero at all, if we are already in that word.
3957           assert(next_full_store &gt;= zeroes_needed, "must go forward");
3958           assert((next_full_store &amp; (BytesPerInt-1)) == 0, "even boundary");
3959           zeroes_needed = next_full_store;
3960         }
3961       }
3962 
3963       if (zeroes_needed &gt; zeroes_done) {
3964         intptr_t zsize = zeroes_needed - zeroes_done;
3965         // Do some incremental zeroing on rawmem, in parallel with inits.
3966         zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3967         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3968                                               zeroes_done, zeroes_needed,
3969                                               phase);
3970         zeroes_done = zeroes_needed;
3971         if (zsize &gt; Matcher::init_array_short_size &amp;&amp; ++big_init_gaps &gt; 2)
3972           do_zeroing = false;   // leave the hole, next time
3973       }
3974     }
3975 
3976     // Collect the store and move on:
3977     st-&gt;set_req(MemNode::Memory, inits);
3978     inits = st;                 // put it on the linearized chain
3979     set_req(i, zmem);           // unhook from previous position
3980 
3981     if (zeroes_done == st_off)
3982       zeroes_done = next_init_off;
3983 
3984     assert(!do_zeroing || zeroes_done &gt;= next_init_off, "don't miss any");
3985 
3986     #ifdef ASSERT
3987     // Various order invariants.  Weaker than stores_are_sane because
3988     // a large constant tile can be filled in by smaller non-constant stores.
3989     assert(st_off &gt;= last_init_off, "inits do not reverse");
3990     last_init_off = st_off;
3991     const Type* val = NULL;
3992     if (st_size &gt;= BytesPerInt &amp;&amp;
3993         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
3994         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
3995       assert(st_off &gt;= last_tile_end, "tiles do not overlap");
3996       assert(st_off &gt;= last_init_end, "tiles do not overwrite inits");
3997       last_tile_end = MAX2(last_tile_end, next_init_off);
3998     } else {
3999       intptr_t st_tile_end = align_size_up(next_init_off, BytesPerLong);
4000       assert(st_tile_end &gt;= last_tile_end, "inits stay with tiles");
4001       assert(st_off      &gt;= last_init_end, "inits do not overlap");
4002       last_init_end = next_init_off;  // it's a non-tile
4003     }
4004     #endif //ASSERT
4005   }
4006 
4007   remove_extra_zeroes();        // clear out all the zmems left over
4008   add_req(inits);
4009 
4010   if (!ZeroTLAB) {
4011     // If anything remains to be zeroed, zero it all now.
4012     zeroes_done = align_size_down(zeroes_done, BytesPerInt);
4013     // if it is the last unused 4 bytes of an instance, forget about it
4014     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4015     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4016       assert(allocation() != NULL, "");
4017       if (allocation()-&gt;Opcode() == Op_Allocate) {
4018         Node* klass_node = allocation()-&gt;in(AllocateNode::KlassNode);
4019         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4020         if (zeroes_done == k-&gt;layout_helper())
4021           zeroes_done = size_limit;
4022       }
4023     }
4024     if (zeroes_done &lt; size_limit) {
4025       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
4026                                             zeroes_done, size_in_bytes, phase);
4027     }
4028   }
4029 
4030   set_complete(phase);
4031   return rawmem;
4032 }
4033 
4034 
4035 #ifdef ASSERT
4036 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4037   if (is_complete())
4038     return true;                // stores could be anything at this point
4039   assert(allocation() != NULL, "must be present");
4040   intptr_t last_off = allocation()-&gt;minimum_header_size();
4041   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4042     Node* st = in(i);
4043     intptr_t st_off = get_store_offset(st, phase);
4044     if (st_off &lt; 0)  continue;  // ignore dead garbage
4045     if (last_off &gt; st_off) {
4046       tty-&gt;print_cr("*** bad store offset at %d: " INTX_FORMAT " &gt; " INTX_FORMAT, i, last_off, st_off);
4047       this-&gt;dump(2);
4048       assert(false, "ascending store offsets");
4049       return false;
4050     }
4051     last_off = st_off + st-&gt;as_Store()-&gt;memory_size();
4052   }
4053   return true;
4054 }
4055 #endif //ASSERT
4056 
4057 
4058 
4059 
4060 //============================MergeMemNode=====================================
4061 //
4062 // SEMANTICS OF MEMORY MERGES:  A MergeMem is a memory state assembled from several
4063 // contributing store or call operations.  Each contributor provides the memory
4064 // state for a particular "alias type" (see Compile::alias_type).  For example,
4065 // if a MergeMem has an input X for alias category #6, then any memory reference
4066 // to alias category #6 may use X as its memory state input, as an exact equivalent
4067 // to using the MergeMem as a whole.
4068 //   Load&lt;6&gt;( MergeMem(&lt;6&gt;: X, ...), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4069 //
4070 // (Here, the &lt;N&gt; notation gives the index of the relevant adr_type.)
4071 //
4072 // In one special case (and more cases in the future), alias categories overlap.
4073 // The special alias category "Bot" (Compile::AliasIdxBot) includes all memory
4074 // states.  Therefore, if a MergeMem has only one contributing input W for Bot,
4075 // it is exactly equivalent to that state W:
4076 //   MergeMem(&lt;Bot&gt;: W) &lt;==&gt; W
4077 //
4078 // Usually, the merge has more than one input.  In that case, where inputs
4079 // overlap (i.e., one is Bot), the narrower alias type determines the memory
4080 // state for that type, and the wider alias type (Bot) fills in everywhere else:
4081 //   Load&lt;5&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;5&gt;(W,p)
4082 //   Load&lt;6&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4083 //
4084 // A merge can take a "wide" memory state as one of its narrow inputs.
4085 // This simply means that the merge observes out only the relevant parts of
4086 // the wide input.  That is, wide memory states arriving at narrow merge inputs
4087 // are implicitly "filtered" or "sliced" as necessary.  (This is rare.)
4088 //
4089 // These rules imply that MergeMem nodes may cascade (via their &lt;Bot&gt; links),
4090 // and that memory slices "leak through":
4091 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)) &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)
4092 //
4093 // But, in such a cascade, repeated memory slices can "block the leak":
4094 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y), &lt;7&gt;: Y') &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y')
4095 //
4096 // In the last example, Y is not part of the combined memory state of the
4097 // outermost MergeMem.  The system must, of course, prevent unschedulable
4098 // memory states from arising, so you can be sure that the state Y is somehow
4099 // a precursor to state Y'.
4100 //
4101 //
4102 // REPRESENTATION OF MEMORY MERGES: The indexes used to address the Node::in array
4103 // of each MergeMemNode array are exactly the numerical alias indexes, including
4104 // but not limited to AliasIdxTop, AliasIdxBot, and AliasIdxRaw.  The functions
4105 // Compile::alias_type (and kin) produce and manage these indexes.
4106 //
4107 // By convention, the value of in(AliasIdxTop) (i.e., in(1)) is always the top node.
4108 // (Note that this provides quick access to the top node inside MergeMem methods,
4109 // without the need to reach out via TLS to Compile::current.)
4110 //
4111 // As a consequence of what was just described, a MergeMem that represents a full
4112 // memory state has an edge in(AliasIdxBot) which is a "wide" memory state,
4113 // containing all alias categories.
4114 //
4115 // MergeMem nodes never (?) have control inputs, so in(0) is NULL.
4116 //
4117 // All other edges in(N) (including in(AliasIdxRaw), which is in(3)) are either
4118 // a memory state for the alias type &lt;N&gt;, or else the top node, meaning that
4119 // there is no particular input for that alias type.  Note that the length of
4120 // a MergeMem is variable, and may be extended at any time to accommodate new
4121 // memory states at larger alias indexes.  When merges grow, they are of course
4122 // filled with "top" in the unused in() positions.
4123 //
4124 // This use of top is named "empty_memory()", or "empty_mem" (no-memory) as a variable.
4125 // (Top was chosen because it works smoothly with passes like GCM.)
4126 //
4127 // For convenience, we hardwire the alias index for TypeRawPtr::BOTTOM.  (It is
4128 // the type of random VM bits like TLS references.)  Since it is always the
4129 // first non-Bot memory slice, some low-level loops use it to initialize an
4130 // index variable:  for (i = AliasIdxRaw; i &lt; req(); i++).
4131 //
4132 //
4133 // ACCESSORS:  There is a special accessor MergeMemNode::base_memory which returns
4134 // the distinguished "wide" state.  The accessor MergeMemNode::memory_at(N) returns
4135 // the memory state for alias type &lt;N&gt;, or (if there is no particular slice at &lt;N&gt;,
4136 // it returns the base memory.  To prevent bugs, memory_at does not accept &lt;Top&gt;
4137 // or &lt;Bot&gt; indexes.  The iterator MergeMemStream provides robust iteration over
4138 // MergeMem nodes or pairs of such nodes, ensuring that the non-top edges are visited.
4139 //
4140 // %%%% We may get rid of base_memory as a separate accessor at some point; it isn't
4141 // really that different from the other memory inputs.  An abbreviation called
4142 // "bot_memory()" for "memory_at(AliasIdxBot)" would keep code tidy.
4143 //
4144 //
4145 // PARTIAL MEMORY STATES:  During optimization, MergeMem nodes may arise that represent
4146 // partial memory states.  When a Phi splits through a MergeMem, the copy of the Phi
4147 // that "emerges though" the base memory will be marked as excluding the alias types
4148 // of the other (narrow-memory) copies which "emerged through" the narrow edges:
4149 //
4150 //   Phi&lt;Bot&gt;(U, MergeMem(&lt;Bot&gt;: W, &lt;8&gt;: Y))
4151 //     ==Ideal=&gt;  MergeMem(&lt;Bot&gt;: Phi&lt;Bot-8&gt;(U, W), Phi&lt;8&gt;(U, Y))
4152 //
4153 // This strange "subtraction" effect is necessary to ensure IGVN convergence.
4154 // (It is currently unimplemented.)  As you can see, the resulting merge is
4155 // actually a disjoint union of memory states, rather than an overlay.
4156 //
4157 
4158 //------------------------------MergeMemNode-----------------------------------
4159 Node* MergeMemNode::make_empty_memory() {
4160   Node* empty_memory = (Node*) Compile::current()-&gt;top();
4161   assert(empty_memory-&gt;is_top(), "correct sentinel identity");
4162   return empty_memory;
4163 }
4164 
4165 MergeMemNode::MergeMemNode(Node *new_base) : Node(1+Compile::AliasIdxRaw) {
4166   init_class_id(Class_MergeMem);
4167   // all inputs are nullified in Node::Node(int)
4168   // set_input(0, NULL);  // no control input
4169 
4170   // Initialize the edges uniformly to top, for starters.
4171   Node* empty_mem = make_empty_memory();
4172   for (uint i = Compile::AliasIdxTop; i &lt; req(); i++) {
4173     init_req(i,empty_mem);
4174   }
4175   assert(empty_memory() == empty_mem, "");
4176 
4177   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4178     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4179     assert(mdef-&gt;empty_memory() == empty_mem, "consistent sentinels");
4180     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4181       mms.set_memory(mms.memory2());
4182     }
4183     assert(base_memory() == mdef-&gt;base_memory(), "");
4184   } else {
4185     set_base_memory(new_base);
4186   }
4187 }
4188 
4189 // Make a new, untransformed MergeMem with the same base as 'mem'.
4190 // If mem is itself a MergeMem, populate the result with the same edges.
4191 MergeMemNode* MergeMemNode::make(Compile* C, Node* mem) {
4192   return new(C) MergeMemNode(mem);
4193 }
4194 
4195 //------------------------------cmp--------------------------------------------
4196 uint MergeMemNode::hash() const { return NO_HASH; }
4197 uint MergeMemNode::cmp( const Node &amp;n ) const {
4198   return (&amp;n == this);          // Always fail except on self
4199 }
4200 
4201 //------------------------------Identity---------------------------------------
4202 Node* MergeMemNode::Identity(PhaseTransform *phase) {
4203   // Identity if this merge point does not record any interesting memory
4204   // disambiguations.
4205   Node* base_mem = base_memory();
4206   Node* empty_mem = empty_memory();
4207   if (base_mem != empty_mem) {  // Memory path is not dead?
4208     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4209       Node* mem = in(i);
4210       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4211         return this;            // Many memory splits; no change
4212       }
4213     }
4214   }
4215   return base_mem;              // No memory splits; ID on the one true input
4216 }
4217 
4218 //------------------------------Ideal------------------------------------------
4219 // This method is invoked recursively on chains of MergeMem nodes
4220 Node *MergeMemNode::Ideal(PhaseGVN *phase, bool can_reshape) {
4221   // Remove chain'd MergeMems
4222   //
4223   // This is delicate, because the each "in(i)" (i &gt;= Raw) is interpreted
4224   // relative to the "in(Bot)".  Since we are patching both at the same time,
4225   // we have to be careful to read each "in(i)" relative to the old "in(Bot)",
4226   // but rewrite each "in(i)" relative to the new "in(Bot)".
4227   Node *progress = NULL;
4228 
4229 
4230   Node* old_base = base_memory();
4231   Node* empty_mem = empty_memory();
4232   if (old_base == empty_mem)
4233     return NULL; // Dead memory path.
4234 
4235   MergeMemNode* old_mbase;
4236   if (old_base != NULL &amp;&amp; old_base-&gt;is_MergeMem())
4237     old_mbase = old_base-&gt;as_MergeMem();
4238   else
4239     old_mbase = NULL;
4240   Node* new_base = old_base;
4241 
4242   // simplify stacked MergeMems in base memory
4243   if (old_mbase)  new_base = old_mbase-&gt;base_memory();
4244 
4245   // the base memory might contribute new slices beyond my req()
4246   if (old_mbase)  grow_to_match(old_mbase);
4247 
4248   // Look carefully at the base node if it is a phi.
4249   PhiNode* phi_base;
4250   if (new_base != NULL &amp;&amp; new_base-&gt;is_Phi())
4251     phi_base = new_base-&gt;as_Phi();
4252   else
4253     phi_base = NULL;
4254 
4255   Node*    phi_reg = NULL;
4256   uint     phi_len = (uint)-1;
4257   if (phi_base != NULL &amp;&amp; !phi_base-&gt;is_copy()) {
4258     // do not examine phi if degraded to a copy
4259     phi_reg = phi_base-&gt;region();
4260     phi_len = phi_base-&gt;req();
4261     // see if the phi is unfinished
4262     for (uint i = 1; i &lt; phi_len; i++) {
4263       if (phi_base-&gt;in(i) == NULL) {
4264         // incomplete phi; do not look at it yet!
4265         phi_reg = NULL;
4266         phi_len = (uint)-1;
4267         break;
4268       }
4269     }
4270   }
4271 
4272   // Note:  We do not call verify_sparse on entry, because inputs
4273   // can normalize to the base_memory via subsume_node or similar
4274   // mechanisms.  This method repairs that damage.
4275 
4276   assert(!old_mbase || old_mbase-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4277 
4278   // Look at each slice.
4279   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4280     Node* old_in = in(i);
4281     // calculate the old memory value
4282     Node* old_mem = old_in;
4283     if (old_mem == empty_mem)  old_mem = old_base;
4284     assert(old_mem == memory_at(i), "");
4285 
4286     // maybe update (reslice) the old memory value
4287 
4288     // simplify stacked MergeMems
4289     Node* new_mem = old_mem;
4290     MergeMemNode* old_mmem;
4291     if (old_mem != NULL &amp;&amp; old_mem-&gt;is_MergeMem())
4292       old_mmem = old_mem-&gt;as_MergeMem();
4293     else
4294       old_mmem = NULL;
4295     if (old_mmem == this) {
4296       // This can happen if loops break up and safepoints disappear.
4297       // A merge of BotPtr (default) with a RawPtr memory derived from a
4298       // safepoint can be rewritten to a merge of the same BotPtr with
4299       // the BotPtr phi coming into the loop.  If that phi disappears
4300       // also, we can end up with a self-loop of the mergemem.
4301       // In general, if loops degenerate and memory effects disappear,
4302       // a mergemem can be left looking at itself.  This simply means
4303       // that the mergemem's default should be used, since there is
4304       // no longer any apparent effect on this slice.
4305       // Note: If a memory slice is a MergeMem cycle, it is unreachable
4306       //       from start.  Update the input to TOP.
4307       new_mem = (new_base == this || new_base == empty_mem)? empty_mem : new_base;
4308     }
4309     else if (old_mmem != NULL) {
4310       new_mem = old_mmem-&gt;memory_at(i);
4311     }
4312     // else preceding memory was not a MergeMem
4313 
4314     // replace equivalent phis (unfortunately, they do not GVN together)
4315     if (new_mem != NULL &amp;&amp; new_mem != new_base &amp;&amp;
4316         new_mem-&gt;req() == phi_len &amp;&amp; new_mem-&gt;in(0) == phi_reg) {
4317       if (new_mem-&gt;is_Phi()) {
4318         PhiNode* phi_mem = new_mem-&gt;as_Phi();
4319         for (uint i = 1; i &lt; phi_len; i++) {
4320           if (phi_base-&gt;in(i) != phi_mem-&gt;in(i)) {
4321             phi_mem = NULL;
4322             break;
4323           }
4324         }
4325         if (phi_mem != NULL) {
4326           // equivalent phi nodes; revert to the def
4327           new_mem = new_base;
4328         }
4329       }
4330     }
4331 
4332     // maybe store down a new value
4333     Node* new_in = new_mem;
4334     if (new_in == new_base)  new_in = empty_mem;
4335 
4336     if (new_in != old_in) {
4337       // Warning:  Do not combine this "if" with the previous "if"
4338       // A memory slice might have be be rewritten even if it is semantically
4339       // unchanged, if the base_memory value has changed.
4340       set_req(i, new_in);
4341       progress = this;          // Report progress
4342     }
4343   }
4344 
4345   if (new_base != old_base) {
4346     set_req(Compile::AliasIdxBot, new_base);
4347     // Don't use set_base_memory(new_base), because we need to update du.
4348     assert(base_memory() == new_base, "");
4349     progress = this;
4350   }
4351 
4352   if( base_memory() == this ) {
4353     // a self cycle indicates this memory path is dead
4354     set_req(Compile::AliasIdxBot, empty_mem);
4355   }
4356 
4357   // Resolve external cycles by calling Ideal on a MergeMem base_memory
4358   // Recursion must occur after the self cycle check above
4359   if( base_memory()-&gt;is_MergeMem() ) {
4360     MergeMemNode *new_mbase = base_memory()-&gt;as_MergeMem();
4361     Node *m = phase-&gt;transform(new_mbase);  // Rollup any cycles
4362     if( m != NULL &amp;&amp; (m-&gt;is_top() ||
4363         m-&gt;is_MergeMem() &amp;&amp; m-&gt;as_MergeMem()-&gt;base_memory() == empty_mem) ) {
4364       // propagate rollup of dead cycle to self
4365       set_req(Compile::AliasIdxBot, empty_mem);
4366     }
4367   }
4368 
4369   if( base_memory() == empty_mem ) {
4370     progress = this;
4371     // Cut inputs during Parse phase only.
4372     // During Optimize phase a dead MergeMem node will be subsumed by Top.
4373     if( !can_reshape ) {
4374       for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4375         if( in(i) != empty_mem ) { set_req(i, empty_mem); }
4376       }
4377     }
4378   }
4379 
4380   if( !progress &amp;&amp; base_memory()-&gt;is_Phi() &amp;&amp; can_reshape ) {
4381     // Check if PhiNode::Ideal's "Split phis through memory merges"
4382     // transform should be attempted. Look for this-&gt;phi-&gt;this cycle.
4383     uint merge_width = req();
4384     if (merge_width &gt; Compile::AliasIdxRaw) {
4385       PhiNode* phi = base_memory()-&gt;as_Phi();
4386       for( uint i = 1; i &lt; phi-&gt;req(); ++i ) {// For all paths in
4387         if (phi-&gt;in(i) == this) {
4388           phase-&gt;is_IterGVN()-&gt;_worklist.push(phi);
4389           break;
4390         }
4391       }
4392     }
4393   }
4394 
4395   assert(progress || verify_sparse(), "please, no dups of base");
4396   return progress;
4397 }
4398 
4399 //-------------------------set_base_memory-------------------------------------
4400 void MergeMemNode::set_base_memory(Node *new_base) {
4401   Node* empty_mem = empty_memory();
4402   set_req(Compile::AliasIdxBot, new_base);
4403   assert(memory_at(req()) == new_base, "must set default memory");
4404   // Clear out other occurrences of new_base:
4405   if (new_base != empty_mem) {
4406     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4407       if (in(i) == new_base)  set_req(i, empty_mem);
4408     }
4409   }
4410 }
4411 
4412 //------------------------------out_RegMask------------------------------------
4413 const RegMask &amp;MergeMemNode::out_RegMask() const {
4414   return RegMask::Empty;
4415 }
4416 
4417 //------------------------------dump_spec--------------------------------------
4418 #ifndef PRODUCT
4419 void MergeMemNode::dump_spec(outputStream *st) const {
4420   st-&gt;print(" {");
4421   Node* base_mem = base_memory();
4422   for( uint i = Compile::AliasIdxRaw; i &lt; req(); i++ ) {
4423     Node* mem = memory_at(i);
4424     if (mem == base_mem) { st-&gt;print(" -"); continue; }
4425     st-&gt;print( " N%d:", mem-&gt;_idx );
4426     Compile::current()-&gt;get_adr_type(i)-&gt;dump_on(st);
4427   }
4428   st-&gt;print(" }");
4429 }
4430 #endif // !PRODUCT
4431 
4432 
4433 #ifdef ASSERT
4434 static bool might_be_same(Node* a, Node* b) {
4435   if (a == b)  return true;
4436   if (!(a-&gt;is_Phi() || b-&gt;is_Phi()))  return false;
4437   // phis shift around during optimization
4438   return true;  // pretty stupid...
4439 }
4440 
4441 // verify a narrow slice (either incoming or outgoing)
4442 static void verify_memory_slice(const MergeMemNode* m, int alias_idx, Node* n) {
4443   if (!VerifyAliases)       return;  // don't bother to verify unless requested
4444   if (is_error_reported())  return;  // muzzle asserts when debugging an error
4445   if (Node::in_dump())      return;  // muzzle asserts when printing
4446   assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not disturb base_memory or sentinel");
4447   assert(n != NULL, "");
4448   // Elide intervening MergeMem's
4449   while (n-&gt;is_MergeMem()) {
4450     n = n-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
4451   }
4452   Compile* C = Compile::current();
4453   const TypePtr* n_adr_type = n-&gt;adr_type();
4454   if (n == m-&gt;empty_memory()) {
4455     // Implicit copy of base_memory()
4456   } else if (n_adr_type != TypePtr::BOTTOM) {
4457     assert(n_adr_type != NULL, "new memory must have a well-defined adr_type");
4458     assert(C-&gt;must_alias(n_adr_type, alias_idx), "new memory must match selected slice");
4459   } else {
4460     // A few places like make_runtime_call "know" that VM calls are narrow,
4461     // and can be used to update only the VM bits stored as TypeRawPtr::BOTTOM.
4462     bool expected_wide_mem = false;
4463     if (n == m-&gt;base_memory()) {
4464       expected_wide_mem = true;
4465     } else if (alias_idx == Compile::AliasIdxRaw ||
4466                n == m-&gt;memory_at(Compile::AliasIdxRaw)) {
4467       expected_wide_mem = true;
4468     } else if (!C-&gt;alias_type(alias_idx)-&gt;is_rewritable()) {
4469       // memory can "leak through" calls on channels that
4470       // are write-once.  Allow this also.
4471       expected_wide_mem = true;
4472     }
4473     assert(expected_wide_mem, "expected narrow slice replacement");
4474   }
4475 }
4476 #else // !ASSERT
4477 #define verify_memory_slice(m,i,n) (void)(0)  // PRODUCT version is no-op
4478 #endif
4479 
4480 
4481 //-----------------------------memory_at---------------------------------------
4482 Node* MergeMemNode::memory_at(uint alias_idx) const {
4483   assert(alias_idx &gt;= Compile::AliasIdxRaw ||
4484          alias_idx == Compile::AliasIdxBot &amp;&amp; Compile::current()-&gt;AliasLevel() == 0,
4485          "must avoid base_memory and AliasIdxTop");
4486 
4487   // Otherwise, it is a narrow slice.
4488   Node* n = alias_idx &lt; req() ? in(alias_idx) : empty_memory();
4489   Compile *C = Compile::current();
4490   if (is_empty_memory(n)) {
4491     // the array is sparse; empty slots are the "top" node
4492     n = base_memory();
4493     assert(Node::in_dump()
4494            || n == NULL || n-&gt;bottom_type() == Type::TOP
4495            || n-&gt;adr_type() == NULL // address is TOP
4496            || n-&gt;adr_type() == TypePtr::BOTTOM
4497            || n-&gt;adr_type() == TypeRawPtr::BOTTOM
4498            || Compile::current()-&gt;AliasLevel() == 0,
4499            "must be a wide memory");
4500     // AliasLevel == 0 if we are organizing the memory states manually.
4501     // See verify_memory_slice for comments on TypeRawPtr::BOTTOM.
4502   } else {
4503     // make sure the stored slice is sane
4504     #ifdef ASSERT
4505     if (is_error_reported() || Node::in_dump()) {
4506     } else if (might_be_same(n, base_memory())) {
4507       // Give it a pass:  It is a mostly harmless repetition of the base.
4508       // This can arise normally from node subsumption during optimization.
4509     } else {
4510       verify_memory_slice(this, alias_idx, n);
4511     }
4512     #endif
4513   }
4514   return n;
4515 }
4516 
4517 //---------------------------set_memory_at-------------------------------------
4518 void MergeMemNode::set_memory_at(uint alias_idx, Node *n) {
4519   verify_memory_slice(this, alias_idx, n);
4520   Node* empty_mem = empty_memory();
4521   if (n == base_memory())  n = empty_mem;  // collapse default
4522   uint need_req = alias_idx+1;
4523   if (req() &lt; need_req) {
4524     if (n == empty_mem)  return;  // already the default, so do not grow me
4525     // grow the sparse array
4526     do {
4527       add_req(empty_mem);
4528     } while (req() &lt; need_req);
4529   }
4530   set_req( alias_idx, n );
4531 }
4532 
4533 
4534 
4535 //--------------------------iteration_setup------------------------------------
4536 void MergeMemNode::iteration_setup(const MergeMemNode* other) {
4537   if (other != NULL) {
4538     grow_to_match(other);
4539     // invariant:  the finite support of mm2 is within mm-&gt;req()
4540     #ifdef ASSERT
4541     for (uint i = req(); i &lt; other-&gt;req(); i++) {
4542       assert(other-&gt;is_empty_memory(other-&gt;in(i)), "slice left uncovered");
4543     }
4544     #endif
4545   }
4546   // Replace spurious copies of base_memory by top.
4547   Node* base_mem = base_memory();
4548   if (base_mem != NULL &amp;&amp; !base_mem-&gt;is_top()) {
4549     for (uint i = Compile::AliasIdxBot+1, imax = req(); i &lt; imax; i++) {
4550       if (in(i) == base_mem)
4551         set_req(i, empty_memory());
4552     }
4553   }
4554 }
4555 
4556 //---------------------------grow_to_match-------------------------------------
4557 void MergeMemNode::grow_to_match(const MergeMemNode* other) {
4558   Node* empty_mem = empty_memory();
4559   assert(other-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4560   // look for the finite support of the other memory
4561   for (uint i = other-&gt;req(); --i &gt;= req(); ) {
4562     if (other-&gt;in(i) != empty_mem) {
4563       uint new_len = i+1;
4564       while (req() &lt; new_len)  add_req(empty_mem);
4565       break;
4566     }
4567   }
4568 }
4569 
4570 //---------------------------verify_sparse-------------------------------------
4571 #ifndef PRODUCT
4572 bool MergeMemNode::verify_sparse() const {
4573   assert(is_empty_memory(make_empty_memory()), "sane sentinel");
4574   Node* base_mem = base_memory();
4575   // The following can happen in degenerate cases, since empty==top.
4576   if (is_empty_memory(base_mem))  return true;
4577   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4578     assert(in(i) != NULL, "sane slice");
4579     if (in(i) == base_mem)  return false;  // should have been the sentinel value!
4580   }
4581   return true;
4582 }
4583 
4584 bool MergeMemStream::match_memory(Node* mem, const MergeMemNode* mm, int idx) {
4585   Node* n;
4586   n = mm-&gt;in(idx);
4587   if (mem == n)  return true;  // might be empty_memory()
4588   n = (idx == Compile::AliasIdxBot)? mm-&gt;base_memory(): mm-&gt;memory_at(idx);
4589   if (mem == n)  return true;
4590   while (n-&gt;is_Phi() &amp;&amp; (n = n-&gt;as_Phi()-&gt;is_copy()) != NULL) {
4591     if (mem == n)  return true;
4592     if (n == NULL)  break;
4593   }
4594   return false;
4595 }
4596 #endif // !PRODUCT
</pre></body></html>
