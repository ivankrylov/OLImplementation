<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "classfile/vmSymbols.hpp"
  28 #include "compiler/compileBroker.hpp"
  29 #include "compiler/compileLog.hpp"
  30 #include "oops/objArrayKlass.hpp"
  31 #include "opto/addnode.hpp"
  32 #include "opto/callGenerator.hpp"
  33 #include "opto/cfgnode.hpp"
  34 #include "opto/idealKit.hpp"
  35 #include "opto/mathexactnode.hpp"
  36 #include "opto/mulnode.hpp"
  37 #include "opto/parse.hpp"
  38 #include "opto/runtime.hpp"
  39 #include "opto/subnode.hpp"
  40 #include "prims/nativeLookup.hpp"
  41 #include "runtime/sharedRuntime.hpp"
  42 #include "trace/traceMacros.hpp"
  43 
  44 class LibraryIntrinsic : public InlineCallGenerator {
  45   // Extend the set of intrinsics known to the runtime:
  46  public:
  47  private:
  48   bool             _is_virtual;
  49   bool             _does_virtual_dispatch;
  50   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  51   int8_t           _last_predicate; // Last generated predicate
  52   vmIntrinsics::ID _intrinsic_id;
  53 
  54  public:
  55   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  56     : InlineCallGenerator(m),
  57       _is_virtual(is_virtual),
  58       _does_virtual_dispatch(does_virtual_dispatch),
  59       _predicates_count((int8_t)predicates_count),
  60       _last_predicate((int8_t)-1),
  61       _intrinsic_id(id)
  62   {
  63   }
  64   virtual bool is_intrinsic() const { return true; }
  65   virtual bool is_virtual()   const { return _is_virtual; }
  66   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  67   virtual int  predicates_count() const { return _predicates_count; }
  68   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  69   virtual JVMState* generate(JVMState* jvms);
  70   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  71   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  72 };
  73 
  74 
  75 // Local helper class for LibraryIntrinsic:
  76 class LibraryCallKit : public GraphKit {
  77  private:
  78   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  79   Node*             _result;        // the result node, if any
  80   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  81 
  82   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  83 
  84  public:
  85   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  86     : GraphKit(jvms),
  87       _intrinsic(intrinsic),
  88       _result(NULL)
  89   {
  90     // Check if this is a root compile.  In that case we don't have a caller.
  91     if (!jvms-&gt;has_method()) {
  92       _reexecute_sp = sp();
  93     } else {
  94       // Find out how many arguments the interpreter needs when deoptimizing
  95       // and save the stack pointer value so it can used by uncommon_trap.
  96       // We find the argument count by looking at the declared signature.
  97       bool ignored_will_link;
  98       ciSignature* declared_signature = NULL;
  99       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 100       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 101       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 102     }
 103   }
 104 
 105   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 106 
 107   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 108   int               bci()       const    { return jvms()-&gt;bci(); }
 109   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 110   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 111   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 112 
 113   bool  try_to_inline(int predicate);
 114   Node* try_to_predicate(int predicate);
 115 
 116   void push_result() {
 117     // Push the result onto the stack.
 118     if (!stopped() &amp;&amp; result() != NULL) {
 119       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 120       push_node(bt, result());
 121     }
 122   }
 123 
 124  private:
 125   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 126     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 127   }
 128 
 129   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 130   void  set_result(RegionNode* region, PhiNode* value);
 131   Node*     result() { return _result; }
 132 
 133   virtual int reexecute_sp() { return _reexecute_sp; }
 134 
 135   // Helper functions to inline natives
 136   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 137   Node* generate_slow_guard(Node* test, RegionNode* region);
 138   Node* generate_fair_guard(Node* test, RegionNode* region);
 139   Node* generate_negative_guard(Node* index, RegionNode* region,
 140                                 // resulting CastII of index:
 141                                 Node* *pos_index = NULL);
 142   Node* generate_nonpositive_guard(Node* index, bool never_negative,
 143                                    // resulting CastII of index:
 144                                    Node* *pos_index = NULL);
 145   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 146                              Node* array_length,
 147                              RegionNode* region);
 148   Node* generate_current_thread(Node* &amp;tls_output);
 149   address basictype2arraycopy(BasicType t, Node *src_offset, Node *dest_offset,
 150                               bool disjoint_bases, const char* &amp;name, bool dest_uninitialized);
 151   Node* load_mirror_from_klass(Node* klass);
 152   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 153                                       RegionNode* region, int null_path,
 154                                       int offset);
 155   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 156                                RegionNode* region, int null_path) {
 157     int offset = java_lang_Class::klass_offset_in_bytes();
 158     return load_klass_from_mirror_common(mirror, never_see_null,
 159                                          region, null_path,
 160                                          offset);
 161   }
 162   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 163                                      RegionNode* region, int null_path) {
 164     int offset = java_lang_Class::array_klass_offset_in_bytes();
 165     return load_klass_from_mirror_common(mirror, never_see_null,
 166                                          region, null_path,
 167                                          offset);
 168   }
 169   Node* generate_access_flags_guard(Node* kls,
 170                                     int modifier_mask, int modifier_bits,
 171                                     RegionNode* region);
 172   Node* generate_interface_guard(Node* kls, RegionNode* region);
 173   Node* generate_array_guard(Node* kls, RegionNode* region) {
 174     return generate_array_guard_common(kls, region, false, false);
 175   }
 176   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 177     return generate_array_guard_common(kls, region, false, true);
 178   }
 179   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 180     return generate_array_guard_common(kls, region, true, false);
 181   }
 182   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 183     return generate_array_guard_common(kls, region, true, true);
 184   }
 185   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 186                                     bool obj_array, bool not_array);
 187   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 188   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 189                                      bool is_virtual = false, bool is_static = false);
 190   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 191     return generate_method_call(method_id, false, true);
 192   }
 193   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 194     return generate_method_call(method_id, true, false);
 195   }
 196   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static);
 197 
 198   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 199   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 200   bool inline_string_compareTo();
 201   bool inline_string_indexOf();
 202   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 203   bool inline_string_equals();
 204   Node* round_double_node(Node* n);
 205   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 206   bool inline_math_native(vmIntrinsics::ID id);
 207   bool inline_trig(vmIntrinsics::ID id);
 208   bool inline_math(vmIntrinsics::ID id);
 209   template &lt;typename OverflowOp&gt;
 210   bool inline_math_overflow(Node* arg1, Node* arg2);
 211   void inline_math_mathExact(Node* math, Node* test);
 212   bool inline_math_addExactI(bool is_increment);
 213   bool inline_math_addExactL(bool is_increment);
 214   bool inline_math_multiplyExactI();
 215   bool inline_math_multiplyExactL();
 216   bool inline_math_negateExactI();
 217   bool inline_math_negateExactL();
 218   bool inline_math_subtractExactI(bool is_decrement);
 219   bool inline_math_subtractExactL(bool is_decrement);
 220   bool inline_exp();
 221   bool inline_pow();
 222   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 223   bool inline_min_max(vmIntrinsics::ID id);
 224   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 225   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 226   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 227   Node* make_unsafe_address(Node* base, Node* offset);
 228   // Helper for inline_unsafe_access.
 229   // Generates the guards that check whether the result of
 230   // Unsafe.getObject should be recorded in an SATB log buffer.
 231   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 232   bool inline_unsafe_deriveContainedObjectAtOffset();
 233   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 234   bool inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static);
 235   static bool klass_needs_init_guard(Node* kls);
 236   bool inline_unsafe_allocate();
 237   bool inline_unsafe_copyMemory();
 238   bool inline_native_currentThread();
 239 #ifdef TRACE_HAVE_INTRINSICS
 240   bool inline_native_classID();
 241   bool inline_native_threadID();
 242 #endif
 243   bool inline_native_time_funcs(address method, const char* funcName);
 244   bool inline_native_isInterrupted();
 245   bool inline_native_Class_query(vmIntrinsics::ID id);
 246   bool inline_native_subtype_check();
 247 
 248   bool inline_native_newArray();
 249   bool inline_native_getLength();
 250   bool inline_array_copyOf(bool is_copyOfRange);
 251   bool inline_array_equals();
 252   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 253   bool inline_native_clone(bool is_virtual);
 254   bool inline_native_Reflection_getCallerClass();
 255   // Helper function for inlining native object hash method
 256   bool inline_native_hashcode(bool is_virtual, bool is_static);
 257   bool inline_native_getClass();
 258 
 259   // Helper functions for inlining arraycopy
 260   bool inline_arraycopy();
 261   void generate_arraycopy(const TypePtr* adr_type,
 262                           BasicType basic_elem_type,
 263                           Node* src,  Node* src_offset,
 264                           Node* dest, Node* dest_offset,
 265                           Node* copy_length,
 266                           bool disjoint_bases = false,
 267                           bool length_never_negative = false,
 268                           RegionNode* slow_region = NULL);
 269   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 270                                                 RegionNode* slow_region);
 271   void generate_clear_array(const TypePtr* adr_type,
 272                             Node* dest,
 273                             BasicType basic_elem_type,
 274                             Node* slice_off,
 275                             Node* slice_len,
 276                             Node* slice_end);
 277   bool generate_block_arraycopy(const TypePtr* adr_type,
 278                                 BasicType basic_elem_type,
 279                                 AllocateNode* alloc,
 280                                 Node* src,  Node* src_offset,
 281                                 Node* dest, Node* dest_offset,
 282                                 Node* dest_size, bool dest_uninitialized);
 283   void generate_slow_arraycopy(const TypePtr* adr_type,
 284                                Node* src,  Node* src_offset,
 285                                Node* dest, Node* dest_offset,
 286                                Node* copy_length, bool dest_uninitialized);
 287   Node* generate_checkcast_arraycopy(const TypePtr* adr_type,
 288                                      Node* dest_elem_klass,
 289                                      Node* src,  Node* src_offset,
 290                                      Node* dest, Node* dest_offset,
 291                                      Node* copy_length, bool dest_uninitialized);
 292   Node* generate_generic_arraycopy(const TypePtr* adr_type,
 293                                    Node* src,  Node* src_offset,
 294                                    Node* dest, Node* dest_offset,
 295                                    Node* copy_length, bool dest_uninitialized);
 296   void generate_unchecked_arraycopy(const TypePtr* adr_type,
 297                                     BasicType basic_elem_type,
 298                                     bool disjoint_bases,
 299                                     Node* src,  Node* src_offset,
 300                                     Node* dest, Node* dest_offset,
 301                                     Node* copy_length, bool dest_uninitialized);
 302   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 303   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 304   bool inline_unsafe_ordered_store(BasicType type);
 305   bool inline_unsafe_fence(vmIntrinsics::ID id);
 306   bool inline_fp_conversions(vmIntrinsics::ID id);
 307   bool inline_number_methods(vmIntrinsics::ID id);
 308   bool inline_reference_get();
 309   bool inline_derive_contained_object();
 310   bool inline_asa_get();
 311   Node* load_container_class(Node* ctrObj);
 312   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 313   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 314   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 315   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 316   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 317   bool inline_sha_implCompress(vmIntrinsics::ID id);
 318   bool inline_digestBase_implCompressMB(int predicate);
 319   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 320                                  bool long_state, address stubAddr, const char *stubName,
 321                                  Node* src_start, Node* ofs, Node* limit);
 322   Node* get_state_from_sha_object(Node *sha_object);
 323   Node* get_state_from_sha5_object(Node *sha_object);
 324   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 325   bool inline_encodeISOArray();
 326   bool inline_updateCRC32();
 327   bool inline_updateBytesCRC32();
 328   bool inline_updateByteBufferCRC32();
 329   bool inline_multiplyToLen();
 330 };
 331 
 332 
 333 //---------------------------make_vm_intrinsic----------------------------
 334 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 335   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 336   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 337 
 338   ccstr disable_intr = NULL;
 339 
 340   if ((DisableIntrinsic[0] != '\0'
 341        &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) ||
 342       (method_has_option_value("DisableIntrinsic", disable_intr)
 343        &amp;&amp; strstr(disable_intr, vmIntrinsics::name_at(id)) != NULL)) {
 344     // disabled by a user request on the command line:
 345     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 346     return NULL;
 347   }
 348 
 349   if (!m-&gt;is_loaded()) {
 350     // do not attempt to inline unloaded methods
 351     return NULL;
 352   }
 353 
 354   // Only a few intrinsics implement a virtual dispatch.
 355   // They are expensive calls which are also frequently overridden.
 356   if (is_virtual) {
 357     switch (id) {
 358     case vmIntrinsics::_hashCode:
 359     case vmIntrinsics::_clone:
 360       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 361       break;
 362     default:
 363       return NULL;
 364     }
 365   }
 366 
 367   // -XX:-InlineNatives disables nearly all intrinsics:
 368   if (!InlineNatives) {
 369     switch (id) {
 370     case vmIntrinsics::_indexOf:
 371     case vmIntrinsics::_compareTo:
 372     case vmIntrinsics::_equals:
 373     case vmIntrinsics::_equalsC:
 374     case vmIntrinsics::_getAndAddInt:
 375     case vmIntrinsics::_getAndAddLong:
 376     case vmIntrinsics::_getAndSetInt:
 377     case vmIntrinsics::_getAndSetLong:
 378     case vmIntrinsics::_getAndSetObject:
 379     case vmIntrinsics::_loadFence:
 380     case vmIntrinsics::_storeFence:
 381     case vmIntrinsics::_fullFence:
 382       break;  // InlineNatives does not control String.compareTo
 383     case vmIntrinsics::_Reference_get:
 384       break;  // InlineNatives does not control Reference.get
 385     default:
 386       return NULL;
 387     }
 388   }
 389 
 390   int predicates = 0;
 391   bool does_virtual_dispatch = false;
 392 
 393   switch (id) {
 394   case vmIntrinsics::_compareTo:
 395     if (!SpecialStringCompareTo)  return NULL;
 396     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 397     break;
 398   case vmIntrinsics::_indexOf:
 399     if (!SpecialStringIndexOf)  return NULL;
 400     break;
 401   case vmIntrinsics::_equals:
 402     if (!SpecialStringEquals)  return NULL;
 403     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 404     break;
 405   case vmIntrinsics::_equalsC:
 406     if (!SpecialArraysEquals)  return NULL;
 407     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 408     break;
 409   case vmIntrinsics::_arraycopy:
 410     if (!InlineArrayCopy)  return NULL;
 411     break;
 412   case vmIntrinsics::_copyMemory:
 413     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 414     if (!InlineArrayCopy)  return NULL;
 415     break;
 416   case vmIntrinsics::_hashCode:
 417     if (!InlineObjectHash)  return NULL;
 418     does_virtual_dispatch = true;
 419     break;
 420   case vmIntrinsics::_clone:
 421     does_virtual_dispatch = true;
 422   case vmIntrinsics::_copyOf:
 423   case vmIntrinsics::_copyOfRange:
 424     if (!InlineObjectCopy)  return NULL;
 425     // These also use the arraycopy intrinsic mechanism:
 426     if (!InlineArrayCopy)  return NULL;
 427     break;
 428   case vmIntrinsics::_encodeISOArray:
 429     if (!SpecialEncodeISOArray)  return NULL;
 430     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 431     break;
 432   case vmIntrinsics::_checkIndex:
 433     // We do not intrinsify this.  The optimizer does fine with it.
 434     return NULL;
 435 
 436   case vmIntrinsics::_getCallerClass:
 437     if (!UseNewReflection)  return NULL;
 438     if (!InlineReflectionGetCallerClass)  return NULL;
 439     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 440     break;
 441 
 442   case vmIntrinsics::_bitCount_i:
 443     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 444     break;
 445 
 446   case vmIntrinsics::_bitCount_l:
 447     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 448     break;
 449 
 450   case vmIntrinsics::_numberOfLeadingZeros_i:
 451     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 452     break;
 453 
 454   case vmIntrinsics::_numberOfLeadingZeros_l:
 455     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 456     break;
 457 
 458   case vmIntrinsics::_numberOfTrailingZeros_i:
 459     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 460     break;
 461 
 462   case vmIntrinsics::_numberOfTrailingZeros_l:
 463     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 464     break;
 465 
 466   case vmIntrinsics::_reverseBytes_c:
 467     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 468     break;
 469   case vmIntrinsics::_reverseBytes_s:
 470     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 471     break;
 472   case vmIntrinsics::_reverseBytes_i:
 473     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 474     break;
 475   case vmIntrinsics::_reverseBytes_l:
 476     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 477     break;
 478 
 479   case vmIntrinsics::_Reference_get:
 480     // Use the intrinsic version of Reference.get() so that the value in
 481     // the referent field can be registered by the G1 pre-barrier code.
 482     // Also add memory barrier to prevent commoning reads from this field
 483     // across safepoint since GC can change it value.
 484     break;
 485 
 486   case vmIntrinsics::_compareAndSwapObject:
 487 #ifdef _LP64
 488     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 489 #endif
 490     break;
 491 
 492   case vmIntrinsics::_compareAndSwapLong:
 493     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 494     break;
 495 
 496   case vmIntrinsics::_getAndAddInt:
 497     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 498     break;
 499 
 500   case vmIntrinsics::_getAndAddLong:
 501     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 502     break;
 503 
 504   case vmIntrinsics::_getAndSetInt:
 505     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 506     break;
 507 
 508   case vmIntrinsics::_getAndSetLong:
 509     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 510     break;
 511 
 512   case vmIntrinsics::_getAndSetObject:
 513 #ifdef _LP64
 514     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 515     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 516     break;
 517 #else
 518     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 519     break;
 520 #endif
 521 
 522   case vmIntrinsics::_deriveContainedObjectAtOffset:
 523     if (!UseObjectLayoutIntrinsics) return NULL;
 524     break;
 525 
 526   case vmIntrinsics::_aescrypt_encryptBlock:
 527   case vmIntrinsics::_aescrypt_decryptBlock:
 528     if (!UseAESIntrinsics) return NULL;
 529     break;
 530 
 531   case vmIntrinsics::_multiplyToLen:
 532     if (!UseMultiplyToLenIntrinsic) return NULL;
 533     break;
 534 
 535   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 536   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 537     if (!UseAESIntrinsics) return NULL;
 538     // these two require the predicated logic
 539     predicates = 1;
 540     break;
 541 
 542   case vmIntrinsics::_sha_implCompress:
 543     if (!UseSHA1Intrinsics) return NULL;
 544     break;
 545 
 546   case vmIntrinsics::_sha2_implCompress:
 547     if (!UseSHA256Intrinsics) return NULL;
 548     break;
 549 
 550   case vmIntrinsics::_sha5_implCompress:
 551     if (!UseSHA512Intrinsics) return NULL;
 552     break;
 553 
 554   case vmIntrinsics::_digestBase_implCompressMB:
 555     if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) return NULL;
 556     predicates = 3;
 557     break;
 558 
 559   case vmIntrinsics::_updateCRC32:
 560   case vmIntrinsics::_updateBytesCRC32:
 561   case vmIntrinsics::_updateByteBufferCRC32:
 562     if (!UseCRC32Intrinsics) return NULL;
 563     break;
 564 
 565   case vmIntrinsics::_incrementExactI:
 566   case vmIntrinsics::_addExactI:
 567     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 568     break;
 569   case vmIntrinsics::_incrementExactL:
 570   case vmIntrinsics::_addExactL:
 571     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 572     break;
 573   case vmIntrinsics::_decrementExactI:
 574   case vmIntrinsics::_subtractExactI:
 575     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 576     break;
 577   case vmIntrinsics::_decrementExactL:
 578   case vmIntrinsics::_subtractExactL:
 579     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 580     break;
 581   case vmIntrinsics::_negateExactI:
 582     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 583     break;
 584   case vmIntrinsics::_negateExactL:
 585     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 586     break;
 587   case vmIntrinsics::_multiplyExactI:
 588     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 589     break;
 590   case vmIntrinsics::_multiplyExactL:
 591     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 592     break;
 593 
 594  default:
 595     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 596     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 597     break;
 598   }
 599 
 600   // -XX:-InlineClassNatives disables natives from the Class class.
 601   // The flag applies to all reflective calls, notably Array.newArray
 602   // (visible to Java programmers as Array.newInstance).
 603   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 604       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 605     if (!InlineClassNatives)  return NULL;
 606   }
 607 
 608   // -XX:-InlineThreadNatives disables natives from the Thread class.
 609   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 610     if (!InlineThreadNatives)  return NULL;
 611   }
 612 
 613   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 614   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 615       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 616       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 617     if (!InlineMathNatives)  return NULL;
 618   }
 619 
 620   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 621   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 622     if (!InlineUnsafeOps)  return NULL;
 623   }
 624 
 625   return new LibraryIntrinsic(m, is_virtual, predicates, does_virtual_dispatch, (vmIntrinsics::ID) id);
 626 }
 627 
 628 //----------------------register_library_intrinsics-----------------------
 629 // Initialize this file's data structures, for each Compile instance.
 630 void Compile::register_library_intrinsics() {
 631   // Nothing to do here.
 632 }
 633 
 634 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 635   LibraryCallKit kit(jvms, this);
 636   Compile* C = kit.C;
 637   int nodes = C-&gt;unique();
 638 #ifndef PRODUCT
 639   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 640     char buf[1000];
 641     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 642     tty-&gt;print_cr("Intrinsic %s", str);
 643   }
 644 #endif
 645   ciMethod* callee = kit.callee();
 646   const int bci    = kit.bci();
 647 
 648   // Try to inline the intrinsic.
 649   if (kit.try_to_inline(_last_predicate)) {
 650     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 651       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 652     }
 653     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 654     if (C-&gt;log()) {
 655       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 656                      vmIntrinsics::name_at(intrinsic_id()),
 657                      (is_virtual() ? " virtual='1'" : ""),
 658                      C-&gt;unique() - nodes);
 659     }
 660     // Push the result from the inlined method onto the stack.
 661     kit.push_result();
 662     return kit.transfer_exceptions_into_jvms();
 663   }
 664 
 665   // The intrinsic bailed out
 666   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 667     if (jvms-&gt;has_method()) {
 668       // Not a root compile.
 669       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 670       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 671     } else {
 672       // Root compile
 673       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 674                vmIntrinsics::name_at(intrinsic_id()),
 675                (is_virtual() ? " (virtual)" : ""), bci);
 676     }
 677   }
 678   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 679   return NULL;
 680 }
 681 
 682 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 683   LibraryCallKit kit(jvms, this);
 684   Compile* C = kit.C;
 685   int nodes = C-&gt;unique();
 686   _last_predicate = predicate;
 687 #ifndef PRODUCT
 688   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 689   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 690     char buf[1000];
 691     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 692     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 693   }
 694 #endif
 695   ciMethod* callee = kit.callee();
 696   const int bci    = kit.bci();
 697 
 698   Node* slow_ctl = kit.try_to_predicate(predicate);
 699   if (!kit.failing()) {
 700     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 701       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 702     }
 703     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 704     if (C-&gt;log()) {
 705       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 706                      vmIntrinsics::name_at(intrinsic_id()),
 707                      (is_virtual() ? " virtual='1'" : ""),
 708                      C-&gt;unique() - nodes);
 709     }
 710     return slow_ctl; // Could be NULL if the check folds.
 711   }
 712 
 713   // The intrinsic bailed out
 714   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 715     if (jvms-&gt;has_method()) {
 716       // Not a root compile.
 717       const char* msg = "failed to generate predicate for intrinsic";
 718       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 719     } else {
 720       // Root compile
 721       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 722                                         vmIntrinsics::name_at(intrinsic_id()),
 723                                         (is_virtual() ? " (virtual)" : ""), bci);
 724     }
 725   }
 726   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 727   return NULL;
 728 }
 729 
 730 bool LibraryCallKit::try_to_inline(int predicate) {
 731   // Handle symbolic names for otherwise undistinguished boolean switches:
 732   const bool is_store       = true;
 733   const bool is_native_ptr  = true;
 734   const bool is_static      = true;
 735   const bool is_volatile    = true;
 736 
 737   if (!jvms()-&gt;has_method()) {
 738     // Root JVMState has a null method.
 739     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 740     // Insert the memory aliasing node
 741     set_all_memory(reset_memory());
 742   }
 743   assert(merged_memory(), "");
 744 
 745 
 746   switch (intrinsic_id()) {
 747   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 748   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 749   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 750 
 751   case vmIntrinsics::_dsin:
 752   case vmIntrinsics::_dcos:
 753   case vmIntrinsics::_dtan:
 754   case vmIntrinsics::_dabs:
 755   case vmIntrinsics::_datan2:
 756   case vmIntrinsics::_dsqrt:
 757   case vmIntrinsics::_dexp:
 758   case vmIntrinsics::_dlog:
 759   case vmIntrinsics::_dlog10:
 760   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 761 
 762   case vmIntrinsics::_min:
 763   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 764 
 765   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 766   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 767   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 768   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 769   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 770   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 771   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 772   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 773   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 774   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 775   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 776   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 777 
 778   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 779 
 780   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 781   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 782   case vmIntrinsics::_equals:                   return inline_string_equals();
 783 
 784   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 785   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 786   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 787   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 788   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 789   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 790   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 791   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 792   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 793 
 794   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 795   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 796   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 797   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 798   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 799   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 800   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 801   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 802   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 803 
 804   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 805   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 806   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 807   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 808   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 809   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 810   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 811   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 812 
 813   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 814   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 815   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 816   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 817   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 818   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 819   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 820   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 821 
 822   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 823   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 824   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 825   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 826   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 827   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 828   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 829   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 830   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 831 
 832   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 833   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 834   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 835   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 836   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 837   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 838   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 839   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 840   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 841 
 842   case vmIntrinsics::_prefetchRead:             return inline_unsafe_prefetch(!is_native_ptr, !is_store, !is_static);
 843   case vmIntrinsics::_prefetchWrite:            return inline_unsafe_prefetch(!is_native_ptr,  is_store, !is_static);
 844   case vmIntrinsics::_prefetchReadStatic:       return inline_unsafe_prefetch(!is_native_ptr, !is_store,  is_static);
 845   case vmIntrinsics::_prefetchWriteStatic:      return inline_unsafe_prefetch(!is_native_ptr,  is_store,  is_static);
 846 
 847   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 848   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 849   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 850 
 851   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 852   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 853   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 854 
 855   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 856   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 857   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 858   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 859   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 860 
 861   case vmIntrinsics::_loadFence:
 862   case vmIntrinsics::_storeFence:
 863   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 864 
 865   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 866   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 867 
 868 #ifdef TRACE_HAVE_INTRINSICS
 869   case vmIntrinsics::_classID:                  return inline_native_classID();
 870   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 871   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 872 #endif
 873   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 874   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 875   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 876   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 877   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 878   case vmIntrinsics::_getLength:                return inline_native_getLength();
 879   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 880   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 881   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 882   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 883 
 884   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 885 
 886   case vmIntrinsics::_isInstance:
 887   case vmIntrinsics::_getModifiers:
 888   case vmIntrinsics::_isInterface:
 889   case vmIntrinsics::_isArray:
 890   case vmIntrinsics::_isPrimitive:
 891   case vmIntrinsics::_getSuperclass:
 892   case vmIntrinsics::_getComponentType:
 893   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 894 
 895   case vmIntrinsics::_floatToRawIntBits:
 896   case vmIntrinsics::_floatToIntBits:
 897   case vmIntrinsics::_intBitsToFloat:
 898   case vmIntrinsics::_doubleToRawLongBits:
 899   case vmIntrinsics::_doubleToLongBits:
 900   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 901 
 902   case vmIntrinsics::_numberOfLeadingZeros_i:
 903   case vmIntrinsics::_numberOfLeadingZeros_l:
 904   case vmIntrinsics::_numberOfTrailingZeros_i:
 905   case vmIntrinsics::_numberOfTrailingZeros_l:
 906   case vmIntrinsics::_bitCount_i:
 907   case vmIntrinsics::_bitCount_l:
 908   case vmIntrinsics::_reverseBytes_i:
 909   case vmIntrinsics::_reverseBytes_l:
 910   case vmIntrinsics::_reverseBytes_s:
 911   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 912 
 913   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 914 
 915   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 916 
 917   case vmIntrinsics::_deriveContainedObjectAtOffset:
 918                                                 return inline_unsafe_deriveContainedObjectAtOffset();
 919 
 920   case vmIntrinsics::_aescrypt_encryptBlock:
 921   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 922 
 923   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 924   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 925     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 926 
 927   case vmIntrinsics::_sha_implCompress:
 928   case vmIntrinsics::_sha2_implCompress:
 929   case vmIntrinsics::_sha5_implCompress:
 930     return inline_sha_implCompress(intrinsic_id());
 931 
 932   case vmIntrinsics::_digestBase_implCompressMB:
 933     return inline_digestBase_implCompressMB(predicate);
 934 
 935   case vmIntrinsics::_multiplyToLen:
 936     return inline_multiplyToLen();
 937 
 938   case vmIntrinsics::_encodeISOArray:
 939     return inline_encodeISOArray();
 940 
 941   case vmIntrinsics::_updateCRC32:
 942     return inline_updateCRC32();
 943   case vmIntrinsics::_updateBytesCRC32:
 944     return inline_updateBytesCRC32();
 945   case vmIntrinsics::_updateByteBufferCRC32:
 946     return inline_updateByteBufferCRC32();
 947 
 948   default:
 949     // If you get here, it may be that someone has added a new intrinsic
 950     // to the list in vmSymbols.hpp without implementing it here.
 951 #ifndef PRODUCT
 952     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 953       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 954                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 955     }
 956 #endif
 957     return false;
 958   }
 959 }
 960 
 961 Node* LibraryCallKit::try_to_predicate(int predicate) {
 962   if (!jvms()-&gt;has_method()) {
 963     // Root JVMState has a null method.
 964     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 965     // Insert the memory aliasing node
 966     set_all_memory(reset_memory());
 967   }
 968   assert(merged_memory(), "");
 969 
 970   switch (intrinsic_id()) {
 971   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 972     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 973   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 974     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 975   case vmIntrinsics::_digestBase_implCompressMB:
 976     return inline_digestBase_implCompressMB_predicate(predicate);
 977 
 978   default:
 979     // If you get here, it may be that someone has added a new intrinsic
 980     // to the list in vmSymbols.hpp without implementing it here.
 981 #ifndef PRODUCT
 982     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 983       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 984                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 985     }
 986 #endif
 987     Node* slow_ctl = control();
 988     set_control(top()); // No fast path instrinsic
 989     return slow_ctl;
 990   }
 991 }
 992 
 993 //------------------------------set_result-------------------------------
 994 // Helper function for finishing intrinsics.
 995 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 996   record_for_igvn(region);
 997   set_control(_gvn.transform(region));
 998   set_result( _gvn.transform(value));
 999   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
1000 }
1001 
1002 //------------------------------generate_guard---------------------------
1003 // Helper function for generating guarded fast-slow graph structures.
1004 // The given 'test', if true, guards a slow path.  If the test fails
1005 // then a fast path can be taken.  (We generally hope it fails.)
1006 // In all cases, GraphKit::control() is updated to the fast path.
1007 // The returned value represents the control for the slow path.
1008 // The return value is never 'top'; it is either a valid control
1009 // or NULL if it is obvious that the slow path can never be taken.
1010 // Also, if region and the slow control are not NULL, the slow edge
1011 // is appended to the region.
1012 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
1013   if (stopped()) {
1014     // Already short circuited.
1015     return NULL;
1016   }
1017 
1018   // Build an if node and its projections.
1019   // If test is true we take the slow path, which we assume is uncommon.
1020   if (_gvn.type(test) == TypeInt::ZERO) {
1021     // The slow branch is never taken.  No need to build this guard.
1022     return NULL;
1023   }
1024 
1025   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
1026 
1027   Node* if_slow = _gvn.transform(new (C) IfTrueNode(iff));
1028   if (if_slow == top()) {
1029     // The slow branch is never taken.  No need to build this guard.
1030     return NULL;
1031   }
1032 
1033   if (region != NULL)
1034     region-&gt;add_req(if_slow);
1035 
1036   Node* if_fast = _gvn.transform(new (C) IfFalseNode(iff));
1037   set_control(if_fast);
1038 
1039   return if_slow;
1040 }
1041 
1042 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
1043   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
1044 }
1045 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
1046   return generate_guard(test, region, PROB_FAIR);
1047 }
1048 
1049 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
1050                                                      Node* *pos_index) {
1051   if (stopped())
1052     return NULL;                // already stopped
1053   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
1054     return NULL;                // index is already adequately typed
1055   Node* cmp_lt = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1056   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1057   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1058   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1059     // Emulate effect of Parse::adjust_map_after_if.
1060     Node* ccast = new (C) CastIINode(index, TypeInt::POS);
1061     ccast-&gt;set_req(0, control());
1062     (*pos_index) = _gvn.transform(ccast);
1063   }
1064   return is_neg;
1065 }
1066 
1067 inline Node* LibraryCallKit::generate_nonpositive_guard(Node* index, bool never_negative,
1068                                                         Node* *pos_index) {
1069   if (stopped())
1070     return NULL;                // already stopped
1071   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS1)) // [1,maxint]
1072     return NULL;                // index is already adequately typed
1073   Node* cmp_le = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1074   BoolTest::mask le_or_eq = (never_negative ? BoolTest::eq : BoolTest::le);
1075   Node* bol_le = _gvn.transform(new (C) BoolNode(cmp_le, le_or_eq));
1076   Node* is_notp = generate_guard(bol_le, NULL, PROB_MIN);
1077   if (is_notp != NULL &amp;&amp; pos_index != NULL) {
1078     // Emulate effect of Parse::adjust_map_after_if.
1079     Node* ccast = new (C) CastIINode(index, TypeInt::POS1);
1080     ccast-&gt;set_req(0, control());
1081     (*pos_index) = _gvn.transform(ccast);
1082   }
1083   return is_notp;
1084 }
1085 
1086 // Make sure that 'position' is a valid limit index, in [0..length].
1087 // There are two equivalent plans for checking this:
1088 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1089 //   B. offset  &lt;=  (arrayLength - copyLength)
1090 // We require that all of the values above, except for the sum and
1091 // difference, are already known to be non-negative.
1092 // Plan A is robust in the face of overflow, if offset and copyLength
1093 // are both hugely positive.
1094 //
1095 // Plan B is less direct and intuitive, but it does not overflow at
1096 // all, since the difference of two non-negatives is always
1097 // representable.  Whenever Java methods must perform the equivalent
1098 // check they generally use Plan B instead of Plan A.
1099 // For the moment we use Plan A.
1100 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1101                                                   Node* subseq_length,
1102                                                   Node* array_length,
1103                                                   RegionNode* region) {
1104   if (stopped())
1105     return NULL;                // already stopped
1106   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1107   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1108     return NULL;                // common case of whole-array copy
1109   Node* last = subseq_length;
1110   if (!zero_offset)             // last += offset
1111     last = _gvn.transform(new (C) AddINode(last, offset));
1112   Node* cmp_lt = _gvn.transform(new (C) CmpUNode(array_length, last));
1113   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1114   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1115   return is_over;
1116 }
1117 
1118 
1119 //--------------------------generate_current_thread--------------------
1120 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1121   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1122   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1123   Node* thread = _gvn.transform(new (C) ThreadLocalNode());
1124   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1125   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1126   tls_output = thread;
1127   return threadObj;
1128 }
1129 
1130 
1131 //------------------------------make_string_method_node------------------------
1132 // Helper method for String intrinsic functions. This version is called
1133 // with str1 and str2 pointing to String object nodes.
1134 //
1135 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1136   Node* no_ctrl = NULL;
1137 
1138   // Get start addr of string
1139   Node* str1_value   = load_String_value(no_ctrl, str1);
1140   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1141   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1142 
1143   // Get length of string 1
1144   Node* str1_len  = load_String_length(no_ctrl, str1);
1145 
1146   Node* str2_value   = load_String_value(no_ctrl, str2);
1147   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1148   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1149 
1150   Node* str2_len = NULL;
1151   Node* result = NULL;
1152 
1153   switch (opcode) {
1154   case Op_StrIndexOf:
1155     // Get length of string 2
1156     str2_len = load_String_length(no_ctrl, str2);
1157 
1158     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1159                                  str1_start, str1_len, str2_start, str2_len);
1160     break;
1161   case Op_StrComp:
1162     // Get length of string 2
1163     str2_len = load_String_length(no_ctrl, str2);
1164 
1165     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1166                                  str1_start, str1_len, str2_start, str2_len);
1167     break;
1168   case Op_StrEquals:
1169     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1170                                str1_start, str2_start, str1_len);
1171     break;
1172   default:
1173     ShouldNotReachHere();
1174     return NULL;
1175   }
1176 
1177   // All these intrinsics have checks.
1178   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1179 
1180   return _gvn.transform(result);
1181 }
1182 
1183 // Helper method for String intrinsic functions. This version is called
1184 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1185 // to Int nodes containing the lenghts of str1 and str2.
1186 //
1187 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1188   Node* result = NULL;
1189   switch (opcode) {
1190   case Op_StrIndexOf:
1191     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1192                                  str1_start, cnt1, str2_start, cnt2);
1193     break;
1194   case Op_StrComp:
1195     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1196                                  str1_start, cnt1, str2_start, cnt2);
1197     break;
1198   case Op_StrEquals:
1199     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1200                                  str1_start, str2_start, cnt1);
1201     break;
1202   default:
1203     ShouldNotReachHere();
1204     return NULL;
1205   }
1206 
1207   // All these intrinsics have checks.
1208   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1209 
1210   return _gvn.transform(result);
1211 }
1212 
1213 //------------------------------inline_string_compareTo------------------------
1214 // public int java.lang.String.compareTo(String anotherString);
1215 bool LibraryCallKit::inline_string_compareTo() {
1216   Node* receiver = null_check(argument(0));
1217   Node* arg      = null_check(argument(1));
1218   if (stopped()) {
1219     return true;
1220   }
1221   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1222   return true;
1223 }
1224 
1225 //------------------------------inline_string_equals------------------------
1226 bool LibraryCallKit::inline_string_equals() {
1227   Node* receiver = null_check_receiver();
1228   // NOTE: Do not null check argument for String.equals() because spec
1229   // allows to specify NULL as argument.
1230   Node* argument = this-&gt;argument(1);
1231   if (stopped()) {
1232     return true;
1233   }
1234 
1235   // paths (plus control) merge
1236   RegionNode* region = new (C) RegionNode(5);
1237   Node* phi = new (C) PhiNode(region, TypeInt::BOOL);
1238 
1239   // does source == target string?
1240   Node* cmp = _gvn.transform(new (C) CmpPNode(receiver, argument));
1241   Node* bol = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
1242 
1243   Node* if_eq = generate_slow_guard(bol, NULL);
1244   if (if_eq != NULL) {
1245     // receiver == argument
1246     phi-&gt;init_req(2, intcon(1));
1247     region-&gt;init_req(2, if_eq);
1248   }
1249 
1250   // get String klass for instanceOf
1251   ciInstanceKlass* klass = env()-&gt;String_klass();
1252 
1253   if (!stopped()) {
1254     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1255     Node* cmp  = _gvn.transform(new (C) CmpINode(inst, intcon(1)));
1256     Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
1257 
1258     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1259     //instanceOf == true, fallthrough
1260 
1261     if (inst_false != NULL) {
1262       phi-&gt;init_req(3, intcon(0));
1263       region-&gt;init_req(3, inst_false);
1264     }
1265   }
1266 
1267   if (!stopped()) {
1268     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1269 
1270     // Properly cast the argument to String
1271     argument = _gvn.transform(new (C) CheckCastPPNode(control(), argument, string_type));
1272     // This path is taken only when argument's type is String:NotNull.
1273     argument = cast_not_null(argument, false);
1274 
1275     Node* no_ctrl = NULL;
1276 
1277     // Get start addr of receiver
1278     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1279     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1280     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1281 
1282     // Get length of receiver
1283     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1284 
1285     // Get start addr of argument
1286     Node* argument_val    = load_String_value(no_ctrl, argument);
1287     Node* argument_offset = load_String_offset(no_ctrl, argument);
1288     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1289 
1290     // Get length of argument
1291     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1292 
1293     // Check for receiver count != argument count
1294     Node* cmp = _gvn.transform(new(C) CmpINode(receiver_cnt, argument_cnt));
1295     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::ne));
1296     Node* if_ne = generate_slow_guard(bol, NULL);
1297     if (if_ne != NULL) {
1298       phi-&gt;init_req(4, intcon(0));
1299       region-&gt;init_req(4, if_ne);
1300     }
1301 
1302     // Check for count == 0 is done by assembler code for StrEquals.
1303 
1304     if (!stopped()) {
1305       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1306       phi-&gt;init_req(1, equals);
1307       region-&gt;init_req(1, control());
1308     }
1309   }
1310 
1311   // post merge
1312   set_control(_gvn.transform(region));
1313   record_for_igvn(region);
1314 
1315   set_result(_gvn.transform(phi));
1316   return true;
1317 }
1318 
1319 //------------------------------inline_array_equals----------------------------
1320 bool LibraryCallKit::inline_array_equals() {
1321   Node* arg1 = argument(0);
1322   Node* arg2 = argument(1);
1323   set_result(_gvn.transform(new (C) AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1324   return true;
1325 }
1326 
1327 // Java version of String.indexOf(constant string)
1328 // class StringDecl {
1329 //   StringDecl(char[] ca) {
1330 //     offset = 0;
1331 //     count = ca.length;
1332 //     value = ca;
1333 //   }
1334 //   int offset;
1335 //   int count;
1336 //   char[] value;
1337 // }
1338 //
1339 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1340 //                             int targetOffset, int cache_i, int md2) {
1341 //   int cache = cache_i;
1342 //   int sourceOffset = string_object.offset;
1343 //   int sourceCount = string_object.count;
1344 //   int targetCount = target_object.length;
1345 //
1346 //   int targetCountLess1 = targetCount - 1;
1347 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1348 //
1349 //   char[] source = string_object.value;
1350 //   char[] target = target_object;
1351 //   int lastChar = target[targetCountLess1];
1352 //
1353 //  outer_loop:
1354 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1355 //     int src = source[i + targetCountLess1];
1356 //     if (src == lastChar) {
1357 //       // With random strings and a 4-character alphabet,
1358 //       // reverse matching at this point sets up 0.8% fewer
1359 //       // frames, but (paradoxically) makes 0.3% more probes.
1360 //       // Since those probes are nearer the lastChar probe,
1361 //       // there is may be a net D$ win with reverse matching.
1362 //       // But, reversing loop inhibits unroll of inner loop
1363 //       // for unknown reason.  So, does running outer loop from
1364 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1365 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1366 //         if (target[targetOffset + j] != source[i+j]) {
1367 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1368 //             if (md2 &lt; j+1) {
1369 //               i += j+1;
1370 //               continue outer_loop;
1371 //             }
1372 //           }
1373 //           i += md2;
1374 //           continue outer_loop;
1375 //         }
1376 //       }
1377 //       return i - sourceOffset;
1378 //     }
1379 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1380 //       i += targetCountLess1;
1381 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1382 //     i++;
1383 //   }
1384 //   return -1;
1385 // }
1386 
1387 //------------------------------string_indexOf------------------------
1388 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1389                                      jint cache_i, jint md2_i) {
1390 
1391   Node* no_ctrl  = NULL;
1392   float likely   = PROB_LIKELY(0.9);
1393   float unlikely = PROB_UNLIKELY(0.9);
1394 
1395   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1396 
1397   Node* source        = load_String_value(no_ctrl, string_object);
1398   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1399   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1400 
1401   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1402   jint target_length = target_array-&gt;length();
1403   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1404   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1405 
1406   // String.value field is known to be @Stable.
1407   if (UseImplicitStableValues) {
1408     target = cast_array_to_stable(target, target_type);
1409   }
1410 
1411   IdealKit kit(this, false, true);
1412 #define __ kit.
1413   Node* zero             = __ ConI(0);
1414   Node* one              = __ ConI(1);
1415   Node* cache            = __ ConI(cache_i);
1416   Node* md2              = __ ConI(md2_i);
1417   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1418   Node* targetCount      = __ ConI(target_length);
1419   Node* targetCountLess1 = __ ConI(target_length - 1);
1420   Node* targetOffset     = __ ConI(targetOffset_i);
1421   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1422 
1423   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1424   Node* outer_loop = __ make_label(2 /* goto */);
1425   Node* return_    = __ make_label(1);
1426 
1427   __ set(rtn,__ ConI(-1));
1428   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1429        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1430        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1431        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1432        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1433          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1434               Node* tpj = __ AddI(targetOffset, __ value(j));
1435               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1436               Node* ipj  = __ AddI(__ value(i), __ value(j));
1437               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1438               __ if_then(targ, BoolTest::ne, src2); {
1439                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1440                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1441                     __ increment(i, __ AddI(__ value(j), one));
1442                     __ goto_(outer_loop);
1443                   } __ end_if(); __ dead(j);
1444                 }__ end_if(); __ dead(j);
1445                 __ increment(i, md2);
1446                 __ goto_(outer_loop);
1447               }__ end_if();
1448               __ increment(j, one);
1449          }__ end_loop(); __ dead(j);
1450          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1451          __ goto_(return_);
1452        }__ end_if();
1453        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1454          __ increment(i, targetCountLess1);
1455        }__ end_if();
1456        __ increment(i, one);
1457        __ bind(outer_loop);
1458   }__ end_loop(); __ dead(i);
1459   __ bind(return_);
1460 
1461   // Final sync IdealKit and GraphKit.
1462   final_sync(kit);
1463   Node* result = __ value(rtn);
1464 #undef __
1465   C-&gt;set_has_loops(true);
1466   return result;
1467 }
1468 
1469 //------------------------------inline_string_indexOf------------------------
1470 bool LibraryCallKit::inline_string_indexOf() {
1471   Node* receiver = argument(0);
1472   Node* arg      = argument(1);
1473 
1474   Node* result;
1475   // Disable the use of pcmpestri until it can be guaranteed that
1476   // the load doesn't cross into the uncommited space.
1477   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1478       UseSSE42Intrinsics) {
1479     // Generate SSE4.2 version of indexOf
1480     // We currently only have match rules that use SSE4.2
1481 
1482     receiver = null_check(receiver);
1483     arg      = null_check(arg);
1484     if (stopped()) {
1485       return true;
1486     }
1487 
1488     ciInstanceKlass* str_klass = env()-&gt;String_klass();
1489     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(str_klass);
1490 
1491     // Make the merge point
1492     RegionNode* result_rgn = new (C) RegionNode(4);
1493     Node*       result_phi = new (C) PhiNode(result_rgn, TypeInt::INT);
1494     Node* no_ctrl  = NULL;
1495 
1496     // Get start addr of source string
1497     Node* source = load_String_value(no_ctrl, receiver);
1498     Node* source_offset = load_String_offset(no_ctrl, receiver);
1499     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1500 
1501     // Get length of source string
1502     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1503 
1504     // Get start addr of substring
1505     Node* substr = load_String_value(no_ctrl, arg);
1506     Node* substr_offset = load_String_offset(no_ctrl, arg);
1507     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1508 
1509     // Get length of source string
1510     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1511 
1512     // Check for substr count &gt; string count
1513     Node* cmp = _gvn.transform(new(C) CmpINode(substr_cnt, source_cnt));
1514     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::gt));
1515     Node* if_gt = generate_slow_guard(bol, NULL);
1516     if (if_gt != NULL) {
1517       result_phi-&gt;init_req(2, intcon(-1));
1518       result_rgn-&gt;init_req(2, if_gt);
1519     }
1520 
1521     if (!stopped()) {
1522       // Check for substr count == 0
1523       cmp = _gvn.transform(new(C) CmpINode(substr_cnt, intcon(0)));
1524       bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
1525       Node* if_zero = generate_slow_guard(bol, NULL);
1526       if (if_zero != NULL) {
1527         result_phi-&gt;init_req(3, intcon(0));
1528         result_rgn-&gt;init_req(3, if_zero);
1529       }
1530     }
1531 
1532     if (!stopped()) {
1533       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1534       result_phi-&gt;init_req(1, result);
1535       result_rgn-&gt;init_req(1, control());
1536     }
1537     set_control(_gvn.transform(result_rgn));
1538     record_for_igvn(result_rgn);
1539     result = _gvn.transform(result_phi);
1540 
1541   } else { // Use LibraryCallKit::string_indexOf
1542     // don't intrinsify if argument isn't a constant string.
1543     if (!arg-&gt;is_Con()) {
1544      return false;
1545     }
1546     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1547     if (str_type == NULL) {
1548       return false;
1549     }
1550     ciInstanceKlass* klass = env()-&gt;String_klass();
1551     ciObject* str_const = str_type-&gt;const_oop();
1552     if (str_const == NULL || str_const-&gt;klass() != klass) {
1553       return false;
1554     }
1555     ciInstance* str = str_const-&gt;as_instance();
1556     assert(str != NULL, "must be instance");
1557 
1558     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1559     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1560 
1561     int o;
1562     int c;
1563     if (java_lang_String::has_offset_field()) {
1564       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1565       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1566     } else {
1567       o = 0;
1568       c = pat-&gt;length();
1569     }
1570 
1571     // constant strings have no offset and count == length which
1572     // simplifies the resulting code somewhat so lets optimize for that.
1573     if (o != 0 || c != pat-&gt;length()) {
1574      return false;
1575     }
1576 
1577     receiver = null_check(receiver, T_OBJECT);
1578     // NOTE: No null check on the argument is needed since it's a constant String oop.
1579     if (stopped()) {
1580       return true;
1581     }
1582 
1583     // The null string as a pattern always returns 0 (match at beginning of string)
1584     if (c == 0) {
1585       set_result(intcon(0));
1586       return true;
1587     }
1588 
1589     // Generate default indexOf
1590     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1591     int cache = 0;
1592     int i;
1593     for (i = 0; i &lt; c - 1; i++) {
1594       assert(i &lt; pat-&gt;length(), "out of range");
1595       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1596     }
1597 
1598     int md2 = c;
1599     for (i = 0; i &lt; c - 1; i++) {
1600       assert(i &lt; pat-&gt;length(), "out of range");
1601       if (pat-&gt;char_at(o + i) == lastChar) {
1602         md2 = (c - 1) - i;
1603       }
1604     }
1605 
1606     result = string_indexOf(receiver, pat, o, cache, md2);
1607   }
1608   set_result(result);
1609   return true;
1610 }
1611 
1612 //--------------------------round_double_node--------------------------------
1613 // Round a double node if necessary.
1614 Node* LibraryCallKit::round_double_node(Node* n) {
1615   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1616     n = _gvn.transform(new (C) RoundDoubleNode(0, n));
1617   return n;
1618 }
1619 
1620 //------------------------------inline_math-----------------------------------
1621 // public static double Math.abs(double)
1622 // public static double Math.sqrt(double)
1623 // public static double Math.log(double)
1624 // public static double Math.log10(double)
1625 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1626   Node* arg = round_double_node(argument(0));
1627   Node* n;
1628   switch (id) {
1629   case vmIntrinsics::_dabs:   n = new (C) AbsDNode(                arg);  break;
1630   case vmIntrinsics::_dsqrt:  n = new (C) SqrtDNode(C, control(),  arg);  break;
1631   case vmIntrinsics::_dlog:   n = new (C) LogDNode(C, control(),   arg);  break;
1632   case vmIntrinsics::_dlog10: n = new (C) Log10DNode(C, control(), arg);  break;
1633   default:  fatal_unexpected_iid(id);  break;
1634   }
1635   set_result(_gvn.transform(n));
1636   return true;
1637 }
1638 
1639 //------------------------------inline_trig----------------------------------
1640 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1641 // argument reduction which will turn into a fast/slow diamond.
1642 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1643   Node* arg = round_double_node(argument(0));
1644   Node* n = NULL;
1645 
1646   switch (id) {
1647   case vmIntrinsics::_dsin:  n = new (C) SinDNode(C, control(), arg);  break;
1648   case vmIntrinsics::_dcos:  n = new (C) CosDNode(C, control(), arg);  break;
1649   case vmIntrinsics::_dtan:  n = new (C) TanDNode(C, control(), arg);  break;
1650   default:  fatal_unexpected_iid(id);  break;
1651   }
1652   n = _gvn.transform(n);
1653 
1654   // Rounding required?  Check for argument reduction!
1655   if (Matcher::strict_fp_requires_explicit_rounding) {
1656     static const double     pi_4 =  0.7853981633974483;
1657     static const double neg_pi_4 = -0.7853981633974483;
1658     // pi/2 in 80-bit extended precision
1659     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1660     // -pi/2 in 80-bit extended precision
1661     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1662     // Cutoff value for using this argument reduction technique
1663     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1664     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1665 
1666     // Pseudocode for sin:
1667     // if (x &lt;= Math.PI / 4.0) {
1668     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1669     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1670     // } else {
1671     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1672     // }
1673     // return StrictMath.sin(x);
1674 
1675     // Pseudocode for cos:
1676     // if (x &lt;= Math.PI / 4.0) {
1677     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1678     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1679     // } else {
1680     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1681     // }
1682     // return StrictMath.cos(x);
1683 
1684     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1685     // requires a special machine instruction to load it.  Instead we'll try
1686     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1687     // probably do the math inside the SIN encoding.
1688 
1689     // Make the merge point
1690     RegionNode* r = new (C) RegionNode(3);
1691     Node* phi = new (C) PhiNode(r, Type::DOUBLE);
1692 
1693     // Flatten arg so we need only 1 test
1694     Node *abs = _gvn.transform(new (C) AbsDNode(arg));
1695     // Node for PI/4 constant
1696     Node *pi4 = makecon(TypeD::make(pi_4));
1697     // Check PI/4 : abs(arg)
1698     Node *cmp = _gvn.transform(new (C) CmpDNode(pi4,abs));
1699     // Check: If PI/4 &lt; abs(arg) then go slow
1700     Node *bol = _gvn.transform(new (C) BoolNode( cmp, BoolTest::lt ));
1701     // Branch either way
1702     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1703     set_control(opt_iff(r,iff));
1704 
1705     // Set fast path result
1706     phi-&gt;init_req(2, n);
1707 
1708     // Slow path - non-blocking leaf call
1709     Node* call = NULL;
1710     switch (id) {
1711     case vmIntrinsics::_dsin:
1712       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1713                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1714                                "Sin", NULL, arg, top());
1715       break;
1716     case vmIntrinsics::_dcos:
1717       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1718                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1719                                "Cos", NULL, arg, top());
1720       break;
1721     case vmIntrinsics::_dtan:
1722       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1723                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1724                                "Tan", NULL, arg, top());
1725       break;
1726     }
1727     assert(control()-&gt;in(0) == call, "");
1728     Node* slow_result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
1729     r-&gt;init_req(1, control());
1730     phi-&gt;init_req(1, slow_result);
1731 
1732     // Post-merge
1733     set_control(_gvn.transform(r));
1734     record_for_igvn(r);
1735     n = _gvn.transform(phi);
1736 
1737     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1738   }
1739   set_result(n);
1740   return true;
1741 }
1742 
1743 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1744   //-------------------
1745   //result=(result.isNaN())? funcAddr():result;
1746   // Check: If isNaN() by checking result!=result? then either trap
1747   // or go to runtime
1748   Node* cmpisnan = _gvn.transform(new (C) CmpDNode(result, result));
1749   // Build the boolean node
1750   Node* bolisnum = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::eq));
1751 
1752   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1753     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1754       // The pow or exp intrinsic returned a NaN, which requires a call
1755       // to the runtime.  Recompile with the runtime call.
1756       uncommon_trap(Deoptimization::Reason_intrinsic,
1757                     Deoptimization::Action_make_not_entrant);
1758     }
1759     return result;
1760   } else {
1761     // If this inlining ever returned NaN in the past, we compile a call
1762     // to the runtime to properly handle corner cases
1763 
1764     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1765     Node* if_slow = _gvn.transform(new (C) IfFalseNode(iff));
1766     Node* if_fast = _gvn.transform(new (C) IfTrueNode(iff));
1767 
1768     if (!if_slow-&gt;is_top()) {
1769       RegionNode* result_region = new (C) RegionNode(3);
1770       PhiNode*    result_val = new (C) PhiNode(result_region, Type::DOUBLE);
1771 
1772       result_region-&gt;init_req(1, if_fast);
1773       result_val-&gt;init_req(1, result);
1774 
1775       set_control(if_slow);
1776 
1777       const TypePtr* no_memory_effects = NULL;
1778       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1779                                    no_memory_effects,
1780                                    x, top(), y, y ? top() : NULL);
1781       Node* value = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+0));
1782 #ifdef ASSERT
1783       Node* value_top = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+1));
1784       assert(value_top == top(), "second value must be top");
1785 #endif
1786 
1787       result_region-&gt;init_req(2, control());
1788       result_val-&gt;init_req(2, value);
1789       set_control(_gvn.transform(result_region));
1790       return _gvn.transform(result_val);
1791     } else {
1792       return result;
1793     }
1794   }
1795 }
1796 
1797 //------------------------------inline_exp-------------------------------------
1798 // Inline exp instructions, if possible.  The Intel hardware only misses
1799 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1800 bool LibraryCallKit::inline_exp() {
1801   Node* arg = round_double_node(argument(0));
1802   Node* n   = _gvn.transform(new (C) ExpDNode(C, control(), arg));
1803 
1804   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1805   set_result(n);
1806 
1807   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1808   return true;
1809 }
1810 
1811 //------------------------------inline_pow-------------------------------------
1812 // Inline power instructions, if possible.
1813 bool LibraryCallKit::inline_pow() {
1814   // Pseudocode for pow
1815   // if (y == 2) {
1816   //   return x * x;
1817   // } else {
1818   //   if (x &lt;= 0.0) {
1819   //     long longy = (long)y;
1820   //     if ((double)longy == y) { // if y is long
1821   //       if (y + 1 == y) longy = 0; // huge number: even
1822   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1823   //     } else {
1824   //       result = NaN;
1825   //     }
1826   //   } else {
1827   //     result = DPow(x,y);
1828   //   }
1829   //   if (result != result)?  {
1830   //     result = uncommon_trap() or runtime_call();
1831   //   }
1832   //   return result;
1833   // }
1834 
1835   Node* x = round_double_node(argument(0));
1836   Node* y = round_double_node(argument(2));
1837 
1838   Node* result = NULL;
1839 
1840   Node*   const_two_node = makecon(TypeD::make(2.0));
1841   Node*   cmp_node       = _gvn.transform(new (C) CmpDNode(y, const_two_node));
1842   Node*   bool_node      = _gvn.transform(new (C) BoolNode(cmp_node, BoolTest::eq));
1843   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1844   Node*   if_true        = _gvn.transform(new (C) IfTrueNode(if_node));
1845   Node*   if_false       = _gvn.transform(new (C) IfFalseNode(if_node));
1846 
1847   RegionNode* region_node = new (C) RegionNode(3);
1848   region_node-&gt;init_req(1, if_true);
1849 
1850   Node* phi_node = new (C) PhiNode(region_node, Type::DOUBLE);
1851   // special case for x^y where y == 2, we can convert it to x * x
1852   phi_node-&gt;init_req(1, _gvn.transform(new (C) MulDNode(x, x)));
1853 
1854   // set control to if_false since we will now process the false branch
1855   set_control(if_false);
1856 
1857   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1858     // Short form: skip the fancy tests and just check for NaN result.
1859     result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1860   } else {
1861     // If this inlining ever returned NaN in the past, include all
1862     // checks + call to the runtime.
1863 
1864     // Set the merge point for If node with condition of (x &lt;= 0.0)
1865     // There are four possible paths to region node and phi node
1866     RegionNode *r = new (C) RegionNode(4);
1867     Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1868 
1869     // Build the first if node: if (x &lt;= 0.0)
1870     // Node for 0 constant
1871     Node *zeronode = makecon(TypeD::ZERO);
1872     // Check x:0
1873     Node *cmp = _gvn.transform(new (C) CmpDNode(x, zeronode));
1874     // Check: If (x&lt;=0) then go complex path
1875     Node *bol1 = _gvn.transform(new (C) BoolNode( cmp, BoolTest::le ));
1876     // Branch either way
1877     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1878     // Fast path taken; set region slot 3
1879     Node *fast_taken = _gvn.transform(new (C) IfFalseNode(if1));
1880     r-&gt;init_req(3,fast_taken); // Capture fast-control
1881 
1882     // Fast path not-taken, i.e. slow path
1883     Node *complex_path = _gvn.transform(new (C) IfTrueNode(if1));
1884 
1885     // Set fast path result
1886     Node *fast_result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1887     phi-&gt;init_req(3, fast_result);
1888 
1889     // Complex path
1890     // Build the second if node (if y is long)
1891     // Node for (long)y
1892     Node *longy = _gvn.transform(new (C) ConvD2LNode(y));
1893     // Node for (double)((long) y)
1894     Node *doublelongy= _gvn.transform(new (C) ConvL2DNode(longy));
1895     // Check (double)((long) y) : y
1896     Node *cmplongy= _gvn.transform(new (C) CmpDNode(doublelongy, y));
1897     // Check if (y isn't long) then go to slow path
1898 
1899     Node *bol2 = _gvn.transform(new (C) BoolNode( cmplongy, BoolTest::ne ));
1900     // Branch either way
1901     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1902     Node* ylong_path = _gvn.transform(new (C) IfFalseNode(if2));
1903 
1904     Node *slow_path = _gvn.transform(new (C) IfTrueNode(if2));
1905 
1906     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1907     // Node for constant 1
1908     Node *conone = longcon(1);
1909     // 1&amp; (long)y
1910     Node *signnode= _gvn.transform(new (C) AndLNode(conone, longy));
1911 
1912     // A huge number is always even. Detect a huge number by checking
1913     // if y + 1 == y and set integer to be tested for parity to 0.
1914     // Required for corner case:
1915     // (long)9.223372036854776E18 = max_jlong
1916     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1917     // max_jlong is odd but 9.223372036854776E18 is even
1918     Node* yplus1 = _gvn.transform(new (C) AddDNode(y, makecon(TypeD::make(1))));
1919     Node *cmpyplus1= _gvn.transform(new (C) CmpDNode(yplus1, y));
1920     Node *bolyplus1 = _gvn.transform(new (C) BoolNode( cmpyplus1, BoolTest::eq ));
1921     Node* correctedsign = NULL;
1922     if (ConditionalMoveLimit != 0) {
1923       correctedsign = _gvn.transform( CMoveNode::make(C, NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1924     } else {
1925       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1926       RegionNode *r = new (C) RegionNode(3);
1927       Node *phi = new (C) PhiNode(r, TypeLong::LONG);
1928       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyplus1)));
1929       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyplus1)));
1930       phi-&gt;init_req(1, signnode);
1931       phi-&gt;init_req(2, longcon(0));
1932       correctedsign = _gvn.transform(phi);
1933       ylong_path = _gvn.transform(r);
1934       record_for_igvn(r);
1935     }
1936 
1937     // zero node
1938     Node *conzero = longcon(0);
1939     // Check (1&amp;(long)y)==0?
1940     Node *cmpeq1 = _gvn.transform(new (C) CmpLNode(correctedsign, conzero));
1941     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1942     Node *bol3 = _gvn.transform(new (C) BoolNode( cmpeq1, BoolTest::ne ));
1943     // abs(x)
1944     Node *absx=_gvn.transform(new (C) AbsDNode(x));
1945     // abs(x)^y
1946     Node *absxpowy = _gvn.transform(new (C) PowDNode(C, control(), absx, y));
1947     // -abs(x)^y
1948     Node *negabsxpowy = _gvn.transform(new (C) NegDNode (absxpowy));
1949     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1950     Node *signresult = NULL;
1951     if (ConditionalMoveLimit != 0) {
1952       signresult = _gvn.transform( CMoveNode::make(C, NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1953     } else {
1954       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1955       RegionNode *r = new (C) RegionNode(3);
1956       Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1957       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyeven)));
1958       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyeven)));
1959       phi-&gt;init_req(1, absxpowy);
1960       phi-&gt;init_req(2, negabsxpowy);
1961       signresult = _gvn.transform(phi);
1962       ylong_path = _gvn.transform(r);
1963       record_for_igvn(r);
1964     }
1965     // Set complex path fast result
1966     r-&gt;init_req(2, ylong_path);
1967     phi-&gt;init_req(2, signresult);
1968 
1969     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1970     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1971     r-&gt;init_req(1,slow_path);
1972     phi-&gt;init_req(1,slow_result);
1973 
1974     // Post merge
1975     set_control(_gvn.transform(r));
1976     record_for_igvn(r);
1977     result = _gvn.transform(phi);
1978   }
1979 
1980   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1981 
1982   // control from finish_pow_exp is now input to the region node
1983   region_node-&gt;set_req(2, control());
1984   // the result from finish_pow_exp is now input to the phi node
1985   phi_node-&gt;init_req(2, result);
1986   set_control(_gvn.transform(region_node));
1987   record_for_igvn(region_node);
1988   set_result(_gvn.transform(phi_node));
1989 
1990   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1991   return true;
1992 }
1993 
1994 //------------------------------runtime_math-----------------------------
1995 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1996   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1997          "must be (DD)D or (D)D type");
1998 
1999   // Inputs
2000   Node* a = round_double_node(argument(0));
2001   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
2002 
2003   const TypePtr* no_memory_effects = NULL;
2004   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
2005                                  no_memory_effects,
2006                                  a, top(), b, b ? top() : NULL);
2007   Node* value = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+0));
2008 #ifdef ASSERT
2009   Node* value_top = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+1));
2010   assert(value_top == top(), "second value must be top");
2011 #endif
2012 
2013   set_result(value);
2014   return true;
2015 }
2016 
2017 //------------------------------inline_math_native-----------------------------
2018 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
2019 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
2020   switch (id) {
2021     // These intrinsics are not properly supported on all hardware
2022   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
2023     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
2024   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
2025     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
2026   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
2027     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
2028 
2029   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
2030     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
2031   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
2032     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
2033 
2034     // These intrinsics are supported on all hardware
2035   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
2036   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
2037 
2038   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
2039     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
2040   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
2041     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
2042 #undef FN_PTR
2043 
2044    // These intrinsics are not yet correctly implemented
2045   case vmIntrinsics::_datan2:
2046     return false;
2047 
2048   default:
2049     fatal_unexpected_iid(id);
2050     return false;
2051   }
2052 }
2053 
2054 static bool is_simple_name(Node* n) {
2055   return (n-&gt;req() == 1         // constant
2056           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
2057           || n-&gt;is_Proj()       // parameter or return value
2058           || n-&gt;is_Phi()        // local of some sort
2059           );
2060 }
2061 
2062 //----------------------------inline_min_max-----------------------------------
2063 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
2064   set_result(generate_min_max(id, argument(0), argument(1)));
2065   return true;
2066 }
2067 
2068 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
2069   Node* bol = _gvn.transform( new (C) BoolNode(test, BoolTest::overflow) );
2070   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
2071   Node* fast_path = _gvn.transform( new (C) IfFalseNode(check));
2072   Node* slow_path = _gvn.transform( new (C) IfTrueNode(check) );
2073 
2074   {
2075     PreserveJVMState pjvms(this);
2076     PreserveReexecuteState preexecs(this);
2077     jvms()-&gt;set_should_reexecute(true);
2078 
2079     set_control(slow_path);
2080     set_i_o(i_o());
2081 
2082     uncommon_trap(Deoptimization::Reason_intrinsic,
2083                   Deoptimization::Action_none);
2084   }
2085 
2086   set_control(fast_path);
2087   set_result(math);
2088 }
2089 
2090 template &lt;typename OverflowOp&gt;
2091 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2092   typedef typename OverflowOp::MathOp MathOp;
2093 
2094   MathOp* mathOp = new(C) MathOp(arg1, arg2);
2095   Node* operation = _gvn.transform( mathOp );
2096   Node* ofcheck = _gvn.transform( new(C) OverflowOp(arg1, arg2) );
2097   inline_math_mathExact(operation, ofcheck);
2098   return true;
2099 }
2100 
2101 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2102   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2103 }
2104 
2105 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2106   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2107 }
2108 
2109 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2110   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2111 }
2112 
2113 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2114   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2115 }
2116 
2117 bool LibraryCallKit::inline_math_negateExactI() {
2118   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2119 }
2120 
2121 bool LibraryCallKit::inline_math_negateExactL() {
2122   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2123 }
2124 
2125 bool LibraryCallKit::inline_math_multiplyExactI() {
2126   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2127 }
2128 
2129 bool LibraryCallKit::inline_math_multiplyExactL() {
2130   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2131 }
2132 
2133 Node*
2134 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2135   // These are the candidate return value:
2136   Node* xvalue = x0;
2137   Node* yvalue = y0;
2138 
2139   if (xvalue == yvalue) {
2140     return xvalue;
2141   }
2142 
2143   bool want_max = (id == vmIntrinsics::_max);
2144 
2145   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2146   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2147   if (txvalue == NULL || tyvalue == NULL)  return top();
2148   // This is not really necessary, but it is consistent with a
2149   // hypothetical MaxINode::Value method:
2150   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2151 
2152   // %%% This folding logic should (ideally) be in a different place.
2153   // Some should be inside IfNode, and there to be a more reliable
2154   // transformation of ?: style patterns into cmoves.  We also want
2155   // more powerful optimizations around cmove and min/max.
2156 
2157   // Try to find a dominating comparison of these guys.
2158   // It can simplify the index computation for Arrays.copyOf
2159   // and similar uses of System.arraycopy.
2160   // First, compute the normalized version of CmpI(x, y).
2161   int   cmp_op = Op_CmpI;
2162   Node* xkey = xvalue;
2163   Node* ykey = yvalue;
2164   Node* ideal_cmpxy = _gvn.transform(new(C) CmpINode(xkey, ykey));
2165   if (ideal_cmpxy-&gt;is_Cmp()) {
2166     // E.g., if we have CmpI(length - offset, count),
2167     // it might idealize to CmpI(length, count + offset)
2168     cmp_op = ideal_cmpxy-&gt;Opcode();
2169     xkey = ideal_cmpxy-&gt;in(1);
2170     ykey = ideal_cmpxy-&gt;in(2);
2171   }
2172 
2173   // Start by locating any relevant comparisons.
2174   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2175   Node* cmpxy = NULL;
2176   Node* cmpyx = NULL;
2177   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2178     Node* cmp = start_from-&gt;fast_out(k);
2179     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2180         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2181         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2182       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2183       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2184     }
2185   }
2186 
2187   const int NCMPS = 2;
2188   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2189   int cmpn;
2190   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2191     if (cmps[cmpn] != NULL)  break;     // find a result
2192   }
2193   if (cmpn &lt; NCMPS) {
2194     // Look for a dominating test that tells us the min and max.
2195     int depth = 0;                // Limit search depth for speed
2196     Node* dom = control();
2197     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2198       if (++depth &gt;= 100)  break;
2199       Node* ifproj = dom;
2200       if (!ifproj-&gt;is_Proj())  continue;
2201       Node* iff = ifproj-&gt;in(0);
2202       if (!iff-&gt;is_If())  continue;
2203       Node* bol = iff-&gt;in(1);
2204       if (!bol-&gt;is_Bool())  continue;
2205       Node* cmp = bol-&gt;in(1);
2206       if (cmp == NULL)  continue;
2207       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2208         if (cmps[cmpn] == cmp)  break;
2209       if (cmpn == NCMPS)  continue;
2210       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2211       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2212       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2213       // At this point, we know that 'x btest y' is true.
2214       switch (btest) {
2215       case BoolTest::eq:
2216         // They are proven equal, so we can collapse the min/max.
2217         // Either value is the answer.  Choose the simpler.
2218         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2219           return yvalue;
2220         return xvalue;
2221       case BoolTest::lt:          // x &lt; y
2222       case BoolTest::le:          // x &lt;= y
2223         return (want_max ? yvalue : xvalue);
2224       case BoolTest::gt:          // x &gt; y
2225       case BoolTest::ge:          // x &gt;= y
2226         return (want_max ? xvalue : yvalue);
2227       }
2228     }
2229   }
2230 
2231   // We failed to find a dominating test.
2232   // Let's pick a test that might GVN with prior tests.
2233   Node*          best_bol   = NULL;
2234   BoolTest::mask best_btest = BoolTest::illegal;
2235   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2236     Node* cmp = cmps[cmpn];
2237     if (cmp == NULL)  continue;
2238     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2239       Node* bol = cmp-&gt;fast_out(j);
2240       if (!bol-&gt;is_Bool())  continue;
2241       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2242       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2243       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2244       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2245         best_bol   = bol-&gt;as_Bool();
2246         best_btest = btest;
2247       }
2248     }
2249   }
2250 
2251   Node* answer_if_true  = NULL;
2252   Node* answer_if_false = NULL;
2253   switch (best_btest) {
2254   default:
2255     if (cmpxy == NULL)
2256       cmpxy = ideal_cmpxy;
2257     best_bol = _gvn.transform(new(C) BoolNode(cmpxy, BoolTest::lt));
2258     // and fall through:
2259   case BoolTest::lt:          // x &lt; y
2260   case BoolTest::le:          // x &lt;= y
2261     answer_if_true  = (want_max ? yvalue : xvalue);
2262     answer_if_false = (want_max ? xvalue : yvalue);
2263     break;
2264   case BoolTest::gt:          // x &gt; y
2265   case BoolTest::ge:          // x &gt;= y
2266     answer_if_true  = (want_max ? xvalue : yvalue);
2267     answer_if_false = (want_max ? yvalue : xvalue);
2268     break;
2269   }
2270 
2271   jint hi, lo;
2272   if (want_max) {
2273     // We can sharpen the minimum.
2274     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2275     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2276   } else {
2277     // We can sharpen the maximum.
2278     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2279     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2280   }
2281 
2282   // Use a flow-free graph structure, to avoid creating excess control edges
2283   // which could hinder other optimizations.
2284   // Since Math.min/max is often used with arraycopy, we want
2285   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2286   Node* cmov = CMoveNode::make(C, NULL, best_bol,
2287                                answer_if_false, answer_if_true,
2288                                TypeInt::make(lo, hi, widen));
2289 
2290   return _gvn.transform(cmov);
2291 
2292   /*
2293   // This is not as desirable as it may seem, since Min and Max
2294   // nodes do not have a full set of optimizations.
2295   // And they would interfere, anyway, with 'if' optimizations
2296   // and with CMoveI canonical forms.
2297   switch (id) {
2298   case vmIntrinsics::_min:
2299     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2300   case vmIntrinsics::_max:
2301     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2302   default:
2303     ShouldNotReachHere();
2304   }
2305   */
2306 }
2307 
2308 inline int
2309 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2310   const TypePtr* base_type = TypePtr::NULL_PTR;
2311   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2312   if (base_type == NULL) {
2313     // Unknown type.
2314     return Type::AnyPtr;
2315   } else if (base_type == TypePtr::NULL_PTR) {
2316     // Since this is a NULL+long form, we have to switch to a rawptr.
2317     base   = _gvn.transform(new (C) CastX2PNode(offset));
2318     offset = MakeConX(0);
2319     return Type::RawPtr;
2320   } else if (base_type-&gt;base() == Type::RawPtr) {
2321     return Type::RawPtr;
2322   } else if (base_type-&gt;isa_oopptr()) {
2323     // Base is never null =&gt; always a heap address.
2324     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2325       return Type::OopPtr;
2326     }
2327     // Offset is small =&gt; always a heap address.
2328     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2329     if (offset_type != NULL &amp;&amp;
2330         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2331         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2332         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2333       return Type::OopPtr;
2334     }
2335     // Otherwise, it might either be oop+off or NULL+addr.
2336     return Type::AnyPtr;
2337   } else {
2338     // No information:
2339     return Type::AnyPtr;
2340   }
2341 }
2342 
2343 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2344   int kind = classify_unsafe_addr(base, offset);
2345   if (kind == Type::RawPtr) {
2346     return basic_plus_adr(top(), base, offset);
2347   } else {
2348     return basic_plus_adr(base, offset);
2349   }
2350 }
2351 
2352 //--------------------------inline_number_methods-----------------------------
2353 // inline int     Integer.numberOfLeadingZeros(int)
2354 // inline int        Long.numberOfLeadingZeros(long)
2355 //
2356 // inline int     Integer.numberOfTrailingZeros(int)
2357 // inline int        Long.numberOfTrailingZeros(long)
2358 //
2359 // inline int     Integer.bitCount(int)
2360 // inline int        Long.bitCount(long)
2361 //
2362 // inline char  Character.reverseBytes(char)
2363 // inline short     Short.reverseBytes(short)
2364 // inline int     Integer.reverseBytes(int)
2365 // inline long       Long.reverseBytes(long)
2366 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2367   Node* arg = argument(0);
2368   Node* n;
2369   switch (id) {
2370   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new (C) CountLeadingZerosINode( arg);  break;
2371   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new (C) CountLeadingZerosLNode( arg);  break;
2372   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new (C) CountTrailingZerosINode(arg);  break;
2373   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new (C) CountTrailingZerosLNode(arg);  break;
2374   case vmIntrinsics::_bitCount_i:               n = new (C) PopCountINode(          arg);  break;
2375   case vmIntrinsics::_bitCount_l:               n = new (C) PopCountLNode(          arg);  break;
2376   case vmIntrinsics::_reverseBytes_c:           n = new (C) ReverseBytesUSNode(0,   arg);  break;
2377   case vmIntrinsics::_reverseBytes_s:           n = new (C) ReverseBytesSNode( 0,   arg);  break;
2378   case vmIntrinsics::_reverseBytes_i:           n = new (C) ReverseBytesINode( 0,   arg);  break;
2379   case vmIntrinsics::_reverseBytes_l:           n = new (C) ReverseBytesLNode( 0,   arg);  break;
2380   default:  fatal_unexpected_iid(id);  break;
2381   }
2382   set_result(_gvn.transform(n));
2383   return true;
2384 }
2385 
2386 //------------------inline_unsafe_deriveContainedObjectAtOffset---------------
2387 
2388 bool LibraryCallKit::inline_unsafe_deriveContainedObjectAtOffset() {
2389   Node* receiver = argument(0); // the unsafe instance
2390   Node* base     = argument(1);
2391   Node* offset   = argument(2);
2392 
2393   // null check unsafe: must have capability
2394   receiver = null_check(receiver);
2395   if (stopped()) {
2396     return true;
2397   }
2398 
2399   // null check base
2400   base = null_check(base);
2401   if (stopped()) {
2402     return true;
2403   }
2404 
2405   // if (!is_size_aligned((size_t) offset, HeapWordSize))
2406   //   throw new IllegalArgumentException();
2407   // TODO
2408 
2409   Node* adr = basic_plus_adr(top(), base, offset); // don't want to keep base-derived relationship here
2410   Node* cast = new (C) CastDerivedNode(adr, TypeInstPtr::NOTNULL); // assuming a non-null Object
2411   cast = _gvn.transform(cast);
2412   set_result(cast);
2413   return true;
2414 }
2415 
2416 //----------------------------inline_unsafe_access----------------------------
2417 
2418 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2419 
2420 // Helper that guards and inserts a pre-barrier.
2421 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2422                                         Node* pre_val, bool need_mem_bar) {
2423   // We could be accessing the referent field of a reference object. If so, when G1
2424   // is enabled, we need to log the value in the referent field in an SATB buffer.
2425   // This routine performs some compile time filters and generates suitable
2426   // runtime filters that guard the pre-barrier code.
2427   // Also add memory barrier for non volatile load from the referent field
2428   // to prevent commoning of loads across safepoint.
2429   if (!UseG1GC &amp;&amp; !need_mem_bar)
2430     return;
2431 
2432   // Some compile time checks.
2433 
2434   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2435   const TypeX* otype = offset-&gt;find_intptr_t_type();
2436   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2437       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2438     // Constant offset but not the reference_offset so just return
2439     return;
2440   }
2441 
2442   // We only need to generate the runtime guards for instances.
2443   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2444   if (btype != NULL) {
2445     if (btype-&gt;isa_aryptr()) {
2446       // Array type so nothing to do
2447       return;
2448     }
2449 
2450     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2451     if (itype != NULL) {
2452       // Can the klass of base_oop be statically determined to be
2453       // _not_ a sub-class of Reference and _not_ Object?
2454       ciKlass* klass = itype-&gt;klass();
2455       if ( klass-&gt;is_loaded() &amp;&amp;
2456           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2457           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2458         return;
2459       }
2460     }
2461   }
2462 
2463   // The compile time filters did not reject base_oop/offset so
2464   // we need to generate the following runtime filters
2465   //
2466   // if (offset == java_lang_ref_Reference::_reference_offset) {
2467   //   if (instance_of(base, java.lang.ref.Reference)) {
2468   //     pre_barrier(_, pre_val, ...);
2469   //   }
2470   // }
2471 
2472   float likely   = PROB_LIKELY(  0.999);
2473   float unlikely = PROB_UNLIKELY(0.999);
2474 
2475   IdealKit ideal(this);
2476 #define __ ideal.
2477 
2478   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2479 
2480   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2481       // Update graphKit memory and control from IdealKit.
2482       sync_kit(ideal);
2483 
2484       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2485       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2486 
2487       // Update IdealKit memory and control from graphKit.
2488       __ sync_kit(this);
2489 
2490       Node* one = __ ConI(1);
2491       // is_instof == 0 if base_oop == NULL
2492       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2493 
2494         // Update graphKit from IdeakKit.
2495         sync_kit(ideal);
2496 
2497         // Use the pre-barrier to record the value in the referent field
2498         pre_barrier(false /* do_load */,
2499                     __ ctrl(),
2500                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2501                     pre_val /* pre_val */,
2502                     T_OBJECT);
2503         if (need_mem_bar) {
2504           // Add memory barrier to prevent commoning reads from this field
2505           // across safepoint since GC can change its value.
2506           insert_mem_bar(Op_MemBarCPUOrder);
2507         }
2508         // Update IdealKit from graphKit.
2509         __ sync_kit(this);
2510 
2511       } __ end_if(); // _ref_type != ref_none
2512   } __ end_if(); // offset == referent_offset
2513 
2514   // Final sync IdealKit and GraphKit.
2515   final_sync(ideal);
2516 #undef __
2517 }
2518 
2519 
2520 // Interpret Unsafe.fieldOffset cookies correctly:
2521 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2522 
2523 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2524   // Attempt to infer a sharper value type from the offset and base type.
2525   ciKlass* sharpened_klass = NULL;
2526 
2527   // See if it is an instance field, with an object type.
2528   if (alias_type-&gt;field() != NULL) {
2529     assert(!is_native_ptr, "native pointer op cannot use a java address");
2530     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2531       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2532     }
2533   }
2534 
2535   // See if it is a narrow oop array.
2536   if (adr_type-&gt;isa_aryptr()) {
2537     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2538       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2539       if (elem_type != NULL) {
2540         sharpened_klass = elem_type-&gt;klass();
2541       }
2542     }
2543   }
2544 
2545   // The sharpened class might be unloaded if there is no class loader
2546   // contraint in place.
2547   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2548     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2549 
2550 #ifndef PRODUCT
2551     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2552       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2553       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2554     }
2555 #endif
2556     // Sharpen the value type.
2557     return tjp;
2558   }
2559   return NULL;
2560 }
2561 
2562 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2563   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2564 
2565 #ifndef PRODUCT
2566   {
2567     ResourceMark rm;
2568     // Check the signatures.
2569     ciSignature* sig = callee()-&gt;signature();
2570 #ifdef ASSERT
2571     if (!is_store) {
2572       // Object getObject(Object base, int/long offset), etc.
2573       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2574       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2575           rtype = T_ADDRESS;  // it is really a C void*
2576       assert(rtype == type, "getter must return the expected value");
2577       if (!is_native_ptr) {
2578         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2579         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2580         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2581       } else {
2582         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2583         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2584       }
2585     } else {
2586       // void putObject(Object base, int/long offset, Object x), etc.
2587       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2588       if (!is_native_ptr) {
2589         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2590         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2591         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2592       } else {
2593         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2594         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2595       }
2596       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2597       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2598         vtype = T_ADDRESS;  // it is really a C void*
2599       assert(vtype == type, "putter must accept the expected value");
2600     }
2601 #endif // ASSERT
2602  }
2603 #endif //PRODUCT
2604 
2605   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2606 
2607   Node* receiver = argument(0);  // type: oop
2608 
2609   // Build address expression.  See the code in inline_unsafe_prefetch.
2610   Node* adr;
2611   Node* heap_base_oop = top();
2612   Node* offset = top();
2613   Node* val;
2614 
2615   if (!is_native_ptr) {
2616     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2617     Node* base = argument(1);  // type: oop
2618     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2619     offset = argument(2);  // type: long
2620     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2621     // to be plain byte offsets, which are also the same as those accepted
2622     // by oopDesc::field_base.
2623     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2624            "fieldOffset must be byte-scaled");
2625     // 32-bit machines ignore the high half!
2626     offset = ConvL2X(offset);
2627     adr = make_unsafe_address(base, offset);
2628     heap_base_oop = base;
2629     val = is_store ? argument(4) : NULL;
2630   } else {
2631     Node* ptr = argument(1);  // type: long
2632     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2633     adr = make_unsafe_address(NULL, ptr);
2634     val = is_store ? argument(3) : NULL;
2635   }
2636 
2637   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2638 
2639   // First guess at the value type.
2640   const Type *value_type = Type::get_const_basic_type(type);
2641 
2642   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2643   // there was not enough information to nail it down.
2644   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2645   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2646 
2647   // We will need memory barriers unless we can determine a unique
2648   // alias category for this reference.  (Note:  If for some reason
2649   // the barriers get omitted and the unsafe reference begins to "pollute"
2650   // the alias analysis of the rest of the graph, either Compile::can_alias
2651   // or Compile::must_alias will throw a diagnostic assert.)
2652   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2653 
2654   // If we are reading the value of the referent field of a Reference
2655   // object (either by using Unsafe directly or through reflection)
2656   // then, if G1 is enabled, we need to record the referent in an
2657   // SATB log buffer using the pre-barrier mechanism.
2658   // Also we need to add memory barrier to prevent commoning reads
2659   // from this field across safepoint since GC can change its value.
2660   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2661                            offset != top() &amp;&amp; heap_base_oop != top();
2662 
2663   if (!is_store &amp;&amp; type == T_OBJECT) {
2664     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2665     if (tjp != NULL) {
2666       value_type = tjp;
2667     }
2668   }
2669 
2670   receiver = null_check(receiver);
2671   if (stopped()) {
2672     return true;
2673   }
2674   // Heap pointers get a null-check from the interpreter,
2675   // as a courtesy.  However, this is not guaranteed by Unsafe,
2676   // and it is not possible to fully distinguish unintended nulls
2677   // from intended ones in this API.
2678 
2679   if (is_volatile) {
2680     // We need to emit leading and trailing CPU membars (see below) in
2681     // addition to memory membars when is_volatile. This is a little
2682     // too strong, but avoids the need to insert per-alias-type
2683     // volatile membars (for stores; compare Parse::do_put_xxx), which
2684     // we cannot do effectively here because we probably only have a
2685     // rough approximation of type.
2686     need_mem_bar = true;
2687     // For Stores, place a memory ordering barrier now.
2688     if (is_store) {
2689       insert_mem_bar(Op_MemBarRelease);
2690     } else {
2691       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2692         insert_mem_bar(Op_MemBarVolatile);
2693       }
2694     }
2695   }
2696 
2697   // Memory barrier to prevent normal and 'unsafe' accesses from
2698   // bypassing each other.  Happens after null checks, so the
2699   // exception paths do not take memory state from the memory barrier,
2700   // so there's no problems making a strong assert about mixing users
2701   // of safe &amp; unsafe memory.  Otherwise fails in a CTW of rt.jar
2702   // around 5701, class sun/reflect/UnsafeBooleanFieldAccessorImpl.
2703   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2704 
2705   if (!is_store) {
2706     MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2707     Node* p = make_load(control(), adr, value_type, type, adr_type, mo, is_volatile);
2708     // load value
2709     switch (type) {
2710     case T_BOOLEAN:
2711     case T_CHAR:
2712     case T_BYTE:
2713     case T_SHORT:
2714     case T_INT:
2715     case T_LONG:
2716     case T_FLOAT:
2717     case T_DOUBLE:
2718       break;
2719     case T_OBJECT:
2720       if (need_read_barrier) {
2721         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2722       }
2723       break;
2724     case T_ADDRESS:
2725       // Cast to an int type.
2726       p = _gvn.transform(new (C) CastP2XNode(NULL, p));
2727       p = ConvX2UL(p);
2728       break;
2729     default:
2730       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2731       break;
2732     }
2733     // The load node has the control of the preceding MemBarCPUOrder.  All
2734     // following nodes will have the control of the MemBarCPUOrder inserted at
2735     // the end of this method.  So, pushing the load onto the stack at a later
2736     // point is fine.
2737     set_result(p);
2738   } else {
2739     // place effect of store into memory
2740     switch (type) {
2741     case T_DOUBLE:
2742       val = dstore_rounding(val);
2743       break;
2744     case T_ADDRESS:
2745       // Repackage the long as a pointer.
2746       val = ConvL2X(val);
2747       val = _gvn.transform(new (C) CastX2PNode(val));
2748       break;
2749     }
2750 
2751     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2752     if (type != T_OBJECT ) {
2753       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2754     } else {
2755       // Possibly an oop being stored to Java heap or native memory
2756       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2757         // oop to Java heap.
2758         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2759       } else {
2760         // We can't tell at compile time if we are storing in the Java heap or outside
2761         // of it. So we need to emit code to conditionally do the proper type of
2762         // store.
2763 
2764         IdealKit ideal(this);
2765 #define __ ideal.
2766         // QQQ who knows what probability is here??
2767         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2768           // Sync IdealKit and graphKit.
2769           sync_kit(ideal);
2770           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2771           // Update IdealKit memory.
2772           __ sync_kit(this);
2773         } __ else_(); {
2774           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2775         } __ end_if();
2776         // Final sync IdealKit and GraphKit.
2777         final_sync(ideal);
2778 #undef __
2779       }
2780     }
2781   }
2782 
2783   if (is_volatile) {
2784     if (!is_store) {
2785       insert_mem_bar(Op_MemBarAcquire);
2786     } else {
2787       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2788         insert_mem_bar(Op_MemBarVolatile);
2789       }
2790     }
2791   }
2792 
2793   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2794 
2795   return true;
2796 }
2797 
2798 //----------------------------inline_unsafe_prefetch----------------------------
2799 
2800 bool LibraryCallKit::inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static) {
2801 #ifndef PRODUCT
2802   {
2803     ResourceMark rm;
2804     // Check the signatures.
2805     ciSignature* sig = callee()-&gt;signature();
2806 #ifdef ASSERT
2807     // Object getObject(Object base, int/long offset), etc.
2808     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2809     if (!is_native_ptr) {
2810       assert(sig-&gt;count() == 2, "oop prefetch has 2 arguments");
2811       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "prefetch base is object");
2812       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "prefetcha offset is correct");
2813     } else {
2814       assert(sig-&gt;count() == 1, "native prefetch has 1 argument");
2815       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "prefetch base is long");
2816     }
2817 #endif // ASSERT
2818   }
2819 #endif // !PRODUCT
2820 
2821   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2822 
2823   const int idx = is_static ? 0 : 1;
2824   if (!is_static) {
2825     null_check_receiver();
2826     if (stopped()) {
2827       return true;
2828     }
2829   }
2830 
2831   // Build address expression.  See the code in inline_unsafe_access.
2832   Node *adr;
2833   if (!is_native_ptr) {
2834     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2835     Node* base   = argument(idx + 0);  // type: oop
2836     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2837     Node* offset = argument(idx + 1);  // type: long
2838     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2839     // to be plain byte offsets, which are also the same as those accepted
2840     // by oopDesc::field_base.
2841     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2842            "fieldOffset must be byte-scaled");
2843     // 32-bit machines ignore the high half!
2844     offset = ConvL2X(offset);
2845     adr = make_unsafe_address(base, offset);
2846   } else {
2847     Node* ptr = argument(idx + 0);  // type: long
2848     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2849     adr = make_unsafe_address(NULL, ptr);
2850   }
2851 
2852   // Generate the read or write prefetch
2853   Node *prefetch;
2854   if (is_store) {
2855     prefetch = new (C) PrefetchWriteNode(i_o(), adr);
2856   } else {
2857     prefetch = new (C) PrefetchReadNode(i_o(), adr);
2858   }
2859   prefetch-&gt;init_req(0, control());
2860   set_i_o(_gvn.transform(prefetch));
2861 
2862   return true;
2863 }
2864 
2865 //----------------------------inline_unsafe_load_store----------------------------
2866 // This method serves a couple of different customers (depending on LoadStoreKind):
2867 //
2868 // LS_cmpxchg:
2869 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2870 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2871 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2872 //
2873 // LS_xadd:
2874 //   public int  getAndAddInt( Object o, long offset, int  delta)
2875 //   public long getAndAddLong(Object o, long offset, long delta)
2876 //
2877 // LS_xchg:
2878 //   int    getAndSet(Object o, long offset, int    newValue)
2879 //   long   getAndSet(Object o, long offset, long   newValue)
2880 //   Object getAndSet(Object o, long offset, Object newValue)
2881 //
2882 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2883   // This basic scheme here is the same as inline_unsafe_access, but
2884   // differs in enough details that combining them would make the code
2885   // overly confusing.  (This is a true fact! I originally combined
2886   // them, but even I was confused by it!) As much code/comments as
2887   // possible are retained from inline_unsafe_access though to make
2888   // the correspondences clearer. - dl
2889 
2890   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2891 
2892 #ifndef PRODUCT
2893   BasicType rtype;
2894   {
2895     ResourceMark rm;
2896     // Check the signatures.
2897     ciSignature* sig = callee()-&gt;signature();
2898     rtype = sig-&gt;return_type()-&gt;basic_type();
2899     if (kind == LS_xadd || kind == LS_xchg) {
2900       // Check the signatures.
2901 #ifdef ASSERT
2902       assert(rtype == type, "get and set must return the expected type");
2903       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2904       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2905       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2906       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2907 #endif // ASSERT
2908     } else if (kind == LS_cmpxchg) {
2909       // Check the signatures.
2910 #ifdef ASSERT
2911       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2912       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2913       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2914       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2915 #endif // ASSERT
2916     } else {
2917       ShouldNotReachHere();
2918     }
2919   }
2920 #endif //PRODUCT
2921 
2922   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2923 
2924   // Get arguments:
2925   Node* receiver = NULL;
2926   Node* base     = NULL;
2927   Node* offset   = NULL;
2928   Node* oldval   = NULL;
2929   Node* newval   = NULL;
2930   if (kind == LS_cmpxchg) {
2931     const bool two_slot_type = type2size[type] == 2;
2932     receiver = argument(0);  // type: oop
2933     base     = argument(1);  // type: oop
2934     offset   = argument(2);  // type: long
2935     oldval   = argument(4);  // type: oop, int, or long
2936     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2937   } else if (kind == LS_xadd || kind == LS_xchg){
2938     receiver = argument(0);  // type: oop
2939     base     = argument(1);  // type: oop
2940     offset   = argument(2);  // type: long
2941     oldval   = NULL;
2942     newval   = argument(4);  // type: oop, int, or long
2943   }
2944 
2945   // Null check receiver.
2946   receiver = null_check(receiver);
2947   if (stopped()) {
2948     return true;
2949   }
2950 
2951   // Build field offset expression.
2952   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2953   // to be plain byte offsets, which are also the same as those accepted
2954   // by oopDesc::field_base.
2955   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2956   // 32-bit machines ignore the high half of long offsets
2957   offset = ConvL2X(offset);
2958   Node* adr = make_unsafe_address(base, offset);
2959   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2960 
2961   // For CAS, unlike inline_unsafe_access, there seems no point in
2962   // trying to refine types. Just use the coarse types here.
2963   const Type *value_type = Type::get_const_basic_type(type);
2964   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2965   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2966 
2967   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2968     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2969     if (tjp != NULL) {
2970       value_type = tjp;
2971     }
2972   }
2973 
2974   int alias_idx = C-&gt;get_alias_index(adr_type);
2975 
2976   // Memory-model-wise, a LoadStore acts like a little synchronized
2977   // block, so needs barriers on each side.  These don't translate
2978   // into actual barriers on most machines, but we still need rest of
2979   // compiler to respect ordering.
2980 
2981   insert_mem_bar(Op_MemBarRelease);
2982   insert_mem_bar(Op_MemBarCPUOrder);
2983 
2984   // 4984716: MemBars must be inserted before this
2985   //          memory node in order to avoid a false
2986   //          dependency which will confuse the scheduler.
2987   Node *mem = memory(alias_idx);
2988 
2989   // For now, we handle only those cases that actually exist: ints,
2990   // longs, and Object. Adding others should be straightforward.
2991   Node* load_store;
2992   switch(type) {
2993   case T_INT:
2994     if (kind == LS_xadd) {
2995       load_store = _gvn.transform(new (C) GetAndAddINode(control(), mem, adr, newval, adr_type));
2996     } else if (kind == LS_xchg) {
2997       load_store = _gvn.transform(new (C) GetAndSetINode(control(), mem, adr, newval, adr_type));
2998     } else if (kind == LS_cmpxchg) {
2999       load_store = _gvn.transform(new (C) CompareAndSwapINode(control(), mem, adr, newval, oldval));
3000     } else {
3001       ShouldNotReachHere();
3002     }
3003     break;
3004   case T_LONG:
3005     if (kind == LS_xadd) {
3006       load_store = _gvn.transform(new (C) GetAndAddLNode(control(), mem, adr, newval, adr_type));
3007     } else if (kind == LS_xchg) {
3008       load_store = _gvn.transform(new (C) GetAndSetLNode(control(), mem, adr, newval, adr_type));
3009     } else if (kind == LS_cmpxchg) {
3010       load_store = _gvn.transform(new (C) CompareAndSwapLNode(control(), mem, adr, newval, oldval));
3011     } else {
3012       ShouldNotReachHere();
3013     }
3014     break;
3015   case T_OBJECT:
3016     // Transformation of a value which could be NULL pointer (CastPP #NULL)
3017     // could be delayed during Parse (for example, in adjust_map_after_if()).
3018     // Execute transformation here to avoid barrier generation in such case.
3019     if (_gvn.type(newval) == TypePtr::NULL_PTR)
3020       newval = _gvn.makecon(TypePtr::NULL_PTR);
3021 
3022     // Reference stores need a store barrier.
3023     if (kind == LS_xchg) {
3024       // If pre-barrier must execute before the oop store, old value will require do_load here.
3025       if (!can_move_pre_barrier()) {
3026         pre_barrier(true /* do_load*/,
3027                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
3028                     NULL /* pre_val*/,
3029                     T_OBJECT);
3030       } // Else move pre_barrier to use load_store value, see below.
3031     } else if (kind == LS_cmpxchg) {
3032       // Same as for newval above:
3033       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
3034         oldval = _gvn.makecon(TypePtr::NULL_PTR);
3035       }
3036       // The only known value which might get overwritten is oldval.
3037       pre_barrier(false /* do_load */,
3038                   control(), NULL, NULL, max_juint, NULL, NULL,
3039                   oldval /* pre_val */,
3040                   T_OBJECT);
3041     } else {
3042       ShouldNotReachHere();
3043     }
3044 
3045 #ifdef _LP64
3046     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3047       Node *newval_enc = _gvn.transform(new (C) EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
3048       if (kind == LS_xchg) {
3049         load_store = _gvn.transform(new (C) GetAndSetNNode(control(), mem, adr,
3050                                                            newval_enc, adr_type, value_type-&gt;make_narrowoop()));
3051       } else {
3052         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
3053         Node *oldval_enc = _gvn.transform(new (C) EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
3054         load_store = _gvn.transform(new (C) CompareAndSwapNNode(control(), mem, adr,
3055                                                                 newval_enc, oldval_enc));
3056       }
3057     } else
3058 #endif
3059     {
3060       if (kind == LS_xchg) {
3061         load_store = _gvn.transform(new (C) GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
3062       } else {
3063         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
3064         load_store = _gvn.transform(new (C) CompareAndSwapPNode(control(), mem, adr, newval, oldval));
3065       }
3066     }
3067     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
3068     break;
3069   default:
3070     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
3071     break;
3072   }
3073 
3074   // SCMemProjNodes represent the memory state of a LoadStore. Their
3075   // main role is to prevent LoadStore nodes from being optimized away
3076   // when their results aren't used.
3077   Node* proj = _gvn.transform(new (C) SCMemProjNode(load_store));
3078   set_memory(proj, alias_idx);
3079 
3080   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
3081 #ifdef _LP64
3082     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3083       load_store = _gvn.transform(new (C) DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
3084     }
3085 #endif
3086     if (can_move_pre_barrier()) {
3087       // Don't need to load pre_val. The old value is returned by load_store.
3088       // The pre_barrier can execute after the xchg as long as no safepoint
3089       // gets inserted between them.
3090       pre_barrier(false /* do_load */,
3091                   control(), NULL, NULL, max_juint, NULL, NULL,
3092                   load_store /* pre_val */,
3093                   T_OBJECT);
3094     }
3095   }
3096 
3097   // Add the trailing membar surrounding the access
3098   insert_mem_bar(Op_MemBarCPUOrder);
3099   insert_mem_bar(Op_MemBarAcquire);
3100 
3101   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3102   set_result(load_store);
3103   return true;
3104 }
3105 
3106 //----------------------------inline_unsafe_ordered_store----------------------
3107 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
3108 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
3109 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
3110 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
3111   // This is another variant of inline_unsafe_access, differing in
3112   // that it always issues store-store ("release") barrier and ensures
3113   // store-atomicity (which only matters for "long").
3114 
3115   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3116 
3117 #ifndef PRODUCT
3118   {
3119     ResourceMark rm;
3120     // Check the signatures.
3121     ciSignature* sig = callee()-&gt;signature();
3122 #ifdef ASSERT
3123     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3124     assert(rtype == T_VOID, "must return void");
3125     assert(sig-&gt;count() == 3, "has 3 arguments");
3126     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3127     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3128 #endif // ASSERT
3129   }
3130 #endif //PRODUCT
3131 
3132   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3133 
3134   // Get arguments:
3135   Node* receiver = argument(0);  // type: oop
3136   Node* base     = argument(1);  // type: oop
3137   Node* offset   = argument(2);  // type: long
3138   Node* val      = argument(4);  // type: oop, int, or long
3139 
3140   // Null check receiver.
3141   receiver = null_check(receiver);
3142   if (stopped()) {
3143     return true;
3144   }
3145 
3146   // Build field offset expression.
3147   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3148   // 32-bit machines ignore the high half of long offsets
3149   offset = ConvL2X(offset);
3150   Node* adr = make_unsafe_address(base, offset);
3151   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3152   const Type *value_type = Type::get_const_basic_type(type);
3153   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3154 
3155   insert_mem_bar(Op_MemBarRelease);
3156   insert_mem_bar(Op_MemBarCPUOrder);
3157   // Ensure that the store is atomic for longs:
3158   const bool require_atomic_access = true;
3159   Node* store;
3160   if (type == T_OBJECT) // reference stores need a store barrier.
3161     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3162   else {
3163     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3164   }
3165   insert_mem_bar(Op_MemBarCPUOrder);
3166   return true;
3167 }
3168 
3169 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3170   // Regardless of form, don't allow previous ld/st to move down,
3171   // then issue acquire, release, or volatile mem_bar.
3172   insert_mem_bar(Op_MemBarCPUOrder);
3173   switch(id) {
3174     case vmIntrinsics::_loadFence:
3175       insert_mem_bar(Op_LoadFence);
3176       return true;
3177     case vmIntrinsics::_storeFence:
3178       insert_mem_bar(Op_StoreFence);
3179       return true;
3180     case vmIntrinsics::_fullFence:
3181       insert_mem_bar(Op_MemBarVolatile);
3182       return true;
3183     default:
3184       fatal_unexpected_iid(id);
3185       return false;
3186   }
3187 }
3188 
3189 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3190   if (!kls-&gt;is_Con()) {
3191     return true;
3192   }
3193   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3194   if (klsptr == NULL) {
3195     return true;
3196   }
3197   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3198   // don't need a guard for a klass that is already initialized
3199   return !ik-&gt;is_initialized();
3200 }
3201 
3202 //----------------------------inline_unsafe_allocate---------------------------
3203 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3204 bool LibraryCallKit::inline_unsafe_allocate() {
3205   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3206 
3207   null_check_receiver();  // null-check, then ignore
3208   Node* cls = null_check(argument(1));
3209   if (stopped())  return true;
3210 
3211   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3212   kls = null_check(kls);
3213   if (stopped())  return true;  // argument was like int.class
3214 
3215   Node* test = NULL;
3216   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3217     // Note:  The argument might still be an illegal value like
3218     // Serializable.class or Object[].class.   The runtime will handle it.
3219     // But we must make an explicit check for initialization.
3220     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3221     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3222     // can generate code to load it as unsigned byte.
3223     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3224     Node* bits = intcon(InstanceKlass::fully_initialized);
3225     test = _gvn.transform(new (C) SubINode(inst, bits));
3226     // The 'test' is non-zero if we need to take a slow path.
3227   }
3228 
3229   Node* obj = new_instance(kls, test);
3230   set_result(obj);
3231   return true;
3232 }
3233 
3234 #ifdef TRACE_HAVE_INTRINSICS
3235 /*
3236  * oop -&gt; myklass
3237  * myklass-&gt;trace_id |= USED
3238  * return myklass-&gt;trace_id &amp; ~0x3
3239  */
3240 bool LibraryCallKit::inline_native_classID() {
3241   null_check_receiver();  // null-check, then ignore
3242   Node* cls = null_check(argument(1), T_OBJECT);
3243   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3244   kls = null_check(kls, T_OBJECT);
3245   ByteSize offset = TRACE_ID_OFFSET;
3246   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3247   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3248   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3249   Node* andl = _gvn.transform(new (C) AndLNode(tvalue, bits));
3250   Node* clsused = longcon(0x01l); // set the class bit
3251   Node* orl = _gvn.transform(new (C) OrLNode(tvalue, clsused));
3252 
3253   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3254   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3255   set_result(andl);
3256   return true;
3257 }
3258 
3259 bool LibraryCallKit::inline_native_threadID() {
3260   Node* tls_ptr = NULL;
3261   Node* cur_thr = generate_current_thread(tls_ptr);
3262   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3263   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3264   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3265 
3266   Node* threadid = NULL;
3267   size_t thread_id_size = OSThread::thread_id_size();
3268   if (thread_id_size == (size_t) BytesPerLong) {
3269     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3270   } else if (thread_id_size == (size_t) BytesPerInt) {
3271     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3272   } else {
3273     ShouldNotReachHere();
3274   }
3275   set_result(threadid);
3276   return true;
3277 }
3278 #endif
3279 
3280 //------------------------inline_native_time_funcs--------------
3281 // inline code for System.currentTimeMillis() and System.nanoTime()
3282 // these have the same type and signature
3283 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3284   const TypeFunc* tf = OptoRuntime::void_long_Type();
3285   const TypePtr* no_memory_effects = NULL;
3286   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3287   Node* value = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+0));
3288 #ifdef ASSERT
3289   Node* value_top = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+1));
3290   assert(value_top == top(), "second value must be top");
3291 #endif
3292   set_result(value);
3293   return true;
3294 }
3295 
3296 //------------------------inline_native_currentThread------------------
3297 bool LibraryCallKit::inline_native_currentThread() {
3298   Node* junk = NULL;
3299   set_result(generate_current_thread(junk));
3300   return true;
3301 }
3302 
3303 //------------------------inline_native_isInterrupted------------------
3304 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3305 bool LibraryCallKit::inline_native_isInterrupted() {
3306   // Add a fast path to t.isInterrupted(clear_int):
3307   //   (t == Thread.current() &amp;&amp;
3308   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3309   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3310   // So, in the common case that the interrupt bit is false,
3311   // we avoid making a call into the VM.  Even if the interrupt bit
3312   // is true, if the clear_int argument is false, we avoid the VM call.
3313   // However, if the receiver is not currentThread, we must call the VM,
3314   // because there must be some locking done around the operation.
3315 
3316   // We only go to the fast case code if we pass two guards.
3317   // Paths which do not pass are accumulated in the slow_region.
3318 
3319   enum {
3320     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3321     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3322     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3323     PATH_LIMIT
3324   };
3325 
3326   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3327   // out of the function.
3328   insert_mem_bar(Op_MemBarCPUOrder);
3329 
3330   RegionNode* result_rgn = new (C) RegionNode(PATH_LIMIT);
3331   PhiNode*    result_val = new (C) PhiNode(result_rgn, TypeInt::BOOL);
3332 
3333   RegionNode* slow_region = new (C) RegionNode(1);
3334   record_for_igvn(slow_region);
3335 
3336   // (a) Receiving thread must be the current thread.
3337   Node* rec_thr = argument(0);
3338   Node* tls_ptr = NULL;
3339   Node* cur_thr = generate_current_thread(tls_ptr);
3340   Node* cmp_thr = _gvn.transform(new (C) CmpPNode(cur_thr, rec_thr));
3341   Node* bol_thr = _gvn.transform(new (C) BoolNode(cmp_thr, BoolTest::ne));
3342 
3343   generate_slow_guard(bol_thr, slow_region);
3344 
3345   // (b) Interrupt bit on TLS must be false.
3346   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3347   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3348   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3349 
3350   // Set the control input on the field _interrupted read to prevent it floating up.
3351   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3352   Node* cmp_bit = _gvn.transform(new (C) CmpINode(int_bit, intcon(0)));
3353   Node* bol_bit = _gvn.transform(new (C) BoolNode(cmp_bit, BoolTest::ne));
3354 
3355   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3356 
3357   // First fast path:  if (!TLS._interrupted) return false;
3358   Node* false_bit = _gvn.transform(new (C) IfFalseNode(iff_bit));
3359   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3360   result_val-&gt;init_req(no_int_result_path, intcon(0));
3361 
3362   // drop through to next case
3363   set_control( _gvn.transform(new (C) IfTrueNode(iff_bit)));
3364 
3365 #ifndef TARGET_OS_FAMILY_windows
3366   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3367   Node* clr_arg = argument(1);
3368   Node* cmp_arg = _gvn.transform(new (C) CmpINode(clr_arg, intcon(0)));
3369   Node* bol_arg = _gvn.transform(new (C) BoolNode(cmp_arg, BoolTest::ne));
3370   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3371 
3372   // Second fast path:  ... else if (!clear_int) return true;
3373   Node* false_arg = _gvn.transform(new (C) IfFalseNode(iff_arg));
3374   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3375   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3376 
3377   // drop through to next case
3378   set_control( _gvn.transform(new (C) IfTrueNode(iff_arg)));
3379 #else
3380   // To return true on Windows you must read the _interrupted field
3381   // and check the the event state i.e. take the slow path.
3382 #endif // TARGET_OS_FAMILY_windows
3383 
3384   // (d) Otherwise, go to the slow path.
3385   slow_region-&gt;add_req(control());
3386   set_control( _gvn.transform(slow_region));
3387 
3388   if (stopped()) {
3389     // There is no slow path.
3390     result_rgn-&gt;init_req(slow_result_path, top());
3391     result_val-&gt;init_req(slow_result_path, top());
3392   } else {
3393     // non-virtual because it is a private non-static
3394     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3395 
3396     Node* slow_val = set_results_for_java_call(slow_call);
3397     // this-&gt;control() comes from set_results_for_java_call
3398 
3399     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3400     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3401 
3402     // These two phis are pre-filled with copies of of the fast IO and Memory
3403     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3404     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3405 
3406     result_rgn-&gt;init_req(slow_result_path, control());
3407     result_io -&gt;init_req(slow_result_path, i_o());
3408     result_mem-&gt;init_req(slow_result_path, reset_memory());
3409     result_val-&gt;init_req(slow_result_path, slow_val);
3410 
3411     set_all_memory(_gvn.transform(result_mem));
3412     set_i_o(       _gvn.transform(result_io));
3413   }
3414 
3415   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3416   set_result(result_rgn, result_val);
3417   return true;
3418 }
3419 
3420 //---------------------------load_mirror_from_klass----------------------------
3421 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3422 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3423   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3424   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3425 }
3426 
3427 //-----------------------load_klass_from_mirror_common-------------------------
3428 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3429 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3430 // and branch to the given path on the region.
3431 // If never_see_null, take an uncommon trap on null, so we can optimistically
3432 // compile for the non-null case.
3433 // If the region is NULL, force never_see_null = true.
3434 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3435                                                     bool never_see_null,
3436                                                     RegionNode* region,
3437                                                     int null_path,
3438                                                     int offset) {
3439   if (region == NULL)  never_see_null = true;
3440   Node* p = basic_plus_adr(mirror, offset);
3441   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3442   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3443   Node* null_ctl = top();
3444   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3445   if (region != NULL) {
3446     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3447     region-&gt;init_req(null_path, null_ctl);
3448   } else {
3449     assert(null_ctl == top(), "no loose ends");
3450   }
3451   return kls;
3452 }
3453 
3454 //--------------------(inline_native_Class_query helpers)---------------------
3455 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3456 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3457 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3458   // Branch around if the given klass has the given modifier bit set.
3459   // Like generate_guard, adds a new path onto the region.
3460   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3461   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3462   Node* mask = intcon(modifier_mask);
3463   Node* bits = intcon(modifier_bits);
3464   Node* mbit = _gvn.transform(new (C) AndINode(mods, mask));
3465   Node* cmp  = _gvn.transform(new (C) CmpINode(mbit, bits));
3466   Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
3467   return generate_fair_guard(bol, region);
3468 }
3469 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3470   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3471 }
3472 
3473 //-------------------------inline_native_Class_query-------------------
3474 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3475   const Type* return_type = TypeInt::BOOL;
3476   Node* prim_return_value = top();  // what happens if it's a primitive class?
3477   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3478   bool expect_prim = false;     // most of these guys expect to work on refs
3479 
3480   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3481 
3482   Node* mirror = argument(0);
3483   Node* obj    = top();
3484 
3485   switch (id) {
3486   case vmIntrinsics::_isInstance:
3487     // nothing is an instance of a primitive type
3488     prim_return_value = intcon(0);
3489     obj = argument(1);
3490     break;
3491   case vmIntrinsics::_getModifiers:
3492     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3493     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3494     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3495     break;
3496   case vmIntrinsics::_isInterface:
3497     prim_return_value = intcon(0);
3498     break;
3499   case vmIntrinsics::_isArray:
3500     prim_return_value = intcon(0);
3501     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3502     break;
3503   case vmIntrinsics::_isPrimitive:
3504     prim_return_value = intcon(1);
3505     expect_prim = true;  // obviously
3506     break;
3507   case vmIntrinsics::_getSuperclass:
3508     prim_return_value = null();
3509     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3510     break;
3511   case vmIntrinsics::_getComponentType:
3512     prim_return_value = null();
3513     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3514     break;
3515   case vmIntrinsics::_getClassAccessFlags:
3516     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3517     return_type = TypeInt::INT;  // not bool!  6297094
3518     break;
3519   default:
3520     fatal_unexpected_iid(id);
3521     break;
3522   }
3523 
3524   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3525   if (mirror_con == NULL)  return false;  // cannot happen?
3526 
3527 #ifndef PRODUCT
3528   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3529     ciType* k = mirror_con-&gt;java_mirror_type();
3530     if (k) {
3531       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3532       k-&gt;print_name();
3533       tty-&gt;cr();
3534     }
3535   }
3536 #endif
3537 
3538   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3539   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3540   record_for_igvn(region);
3541   PhiNode* phi = new (C) PhiNode(region, return_type);
3542 
3543   // The mirror will never be null of Reflection.getClassAccessFlags, however
3544   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3545   // if it is. See bug 4774291.
3546 
3547   // For Reflection.getClassAccessFlags(), the null check occurs in
3548   // the wrong place; see inline_unsafe_access(), above, for a similar
3549   // situation.
3550   mirror = null_check(mirror);
3551   // If mirror or obj is dead, only null-path is taken.
3552   if (stopped())  return true;
3553 
3554   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3555 
3556   // Now load the mirror's klass metaobject, and null-check it.
3557   // Side-effects region with the control path if the klass is null.
3558   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3559   // If kls is null, we have a primitive mirror.
3560   phi-&gt;init_req(_prim_path, prim_return_value);
3561   if (stopped()) { set_result(region, phi); return true; }
3562   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3563 
3564   Node* p;  // handy temp
3565   Node* null_ctl;
3566 
3567   // Now that we have the non-null klass, we can perform the real query.
3568   // For constant classes, the query will constant-fold in LoadNode::Value.
3569   Node* query_value = top();
3570   switch (id) {
3571   case vmIntrinsics::_isInstance:
3572     // nothing is an instance of a primitive type
3573     query_value = gen_instanceof(obj, kls, safe_for_replace);
3574     break;
3575 
3576   case vmIntrinsics::_getModifiers:
3577     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3578     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3579     break;
3580 
3581   case vmIntrinsics::_isInterface:
3582     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3583     if (generate_interface_guard(kls, region) != NULL)
3584       // A guard was added.  If the guard is taken, it was an interface.
3585       phi-&gt;add_req(intcon(1));
3586     // If we fall through, it's a plain class.
3587     query_value = intcon(0);
3588     break;
3589 
3590   case vmIntrinsics::_isArray:
3591     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3592     if (generate_array_guard(kls, region) != NULL)
3593       // A guard was added.  If the guard is taken, it was an array.
3594       phi-&gt;add_req(intcon(1));
3595     // If we fall through, it's a plain class.
3596     query_value = intcon(0);
3597     break;
3598 
3599   case vmIntrinsics::_isPrimitive:
3600     query_value = intcon(0); // "normal" path produces false
3601     break;
3602 
3603   case vmIntrinsics::_getSuperclass:
3604     // The rules here are somewhat unfortunate, but we can still do better
3605     // with random logic than with a JNI call.
3606     // Interfaces store null or Object as _super, but must report null.
3607     // Arrays store an intermediate super as _super, but must report Object.
3608     // Other types can report the actual _super.
3609     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3610     if (generate_interface_guard(kls, region) != NULL)
3611       // A guard was added.  If the guard is taken, it was an interface.
3612       phi-&gt;add_req(null());
3613     if (generate_array_guard(kls, region) != NULL)
3614       // A guard was added.  If the guard is taken, it was an array.
3615       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3616     // If we fall through, it's a plain class.  Get its _super.
3617     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3618     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3619     null_ctl = top();
3620     kls = null_check_oop(kls, &amp;null_ctl);
3621     if (null_ctl != top()) {
3622       // If the guard is taken, Object.superClass is null (both klass and mirror).
3623       region-&gt;add_req(null_ctl);
3624       phi   -&gt;add_req(null());
3625     }
3626     if (!stopped()) {
3627       query_value = load_mirror_from_klass(kls);
3628     }
3629     break;
3630 
3631   case vmIntrinsics::_getComponentType:
3632     if (generate_array_guard(kls, region) != NULL) {
3633       // Be sure to pin the oop load to the guard edge just created:
3634       Node* is_array_ctrl = region-&gt;in(region-&gt;req()-1);
3635       Node* cma = basic_plus_adr(kls, in_bytes(ArrayKlass::component_mirror_offset()));
3636       Node* cmo = make_load(is_array_ctrl, cma, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3637       phi-&gt;add_req(cmo);
3638     }
3639     query_value = null();  // non-array case is null
3640     break;
3641 
3642   case vmIntrinsics::_getClassAccessFlags:
3643     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3644     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3645     break;
3646 
3647   default:
3648     fatal_unexpected_iid(id);
3649     break;
3650   }
3651 
3652   // Fall-through is the normal case of a query to a real class.
3653   phi-&gt;init_req(1, query_value);
3654   region-&gt;init_req(1, control());
3655 
3656   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3657   set_result(region, phi);
3658   return true;
3659 }
3660 
3661 //--------------------------inline_native_subtype_check------------------------
3662 // This intrinsic takes the JNI calls out of the heart of
3663 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3664 bool LibraryCallKit::inline_native_subtype_check() {
3665   // Pull both arguments off the stack.
3666   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3667   args[0] = argument(0);
3668   args[1] = argument(1);
3669   Node* klasses[2];             // corresponding Klasses: superk, subk
3670   klasses[0] = klasses[1] = top();
3671 
3672   enum {
3673     // A full decision tree on {superc is prim, subc is prim}:
3674     _prim_0_path = 1,           // {P,N} =&gt; false
3675                                 // {P,P} &amp; superc!=subc =&gt; false
3676     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3677     _prim_1_path,               // {N,P} =&gt; false
3678     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3679     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3680     PATH_LIMIT
3681   };
3682 
3683   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3684   Node*       phi    = new (C) PhiNode(region, TypeInt::BOOL);
3685   record_for_igvn(region);
3686 
3687   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3688   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3689   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3690 
3691   // First null-check both mirrors and load each mirror's klass metaobject.
3692   int which_arg;
3693   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3694     Node* arg = args[which_arg];
3695     arg = null_check(arg);
3696     if (stopped())  break;
3697     args[which_arg] = arg;
3698 
3699     Node* p = basic_plus_adr(arg, class_klass_offset);
3700     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3701     klasses[which_arg] = _gvn.transform(kls);
3702   }
3703 
3704   // Having loaded both klasses, test each for null.
3705   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3706   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3707     Node* kls = klasses[which_arg];
3708     Node* null_ctl = top();
3709     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3710     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3711     region-&gt;init_req(prim_path, null_ctl);
3712     if (stopped())  break;
3713     klasses[which_arg] = kls;
3714   }
3715 
3716   if (!stopped()) {
3717     // now we have two reference types, in klasses[0..1]
3718     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3719     Node* superk = klasses[0];  // the receiver
3720     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3721     // now we have a successful reference subtype check
3722     region-&gt;set_req(_ref_subtype_path, control());
3723   }
3724 
3725   // If both operands are primitive (both klasses null), then
3726   // we must return true when they are identical primitives.
3727   // It is convenient to test this after the first null klass check.
3728   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3729   if (!stopped()) {
3730     // Since superc is primitive, make a guard for the superc==subc case.
3731     Node* cmp_eq = _gvn.transform(new (C) CmpPNode(args[0], args[1]));
3732     Node* bol_eq = _gvn.transform(new (C) BoolNode(cmp_eq, BoolTest::eq));
3733     generate_guard(bol_eq, region, PROB_FAIR);
3734     if (region-&gt;req() == PATH_LIMIT+1) {
3735       // A guard was added.  If the added guard is taken, superc==subc.
3736       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3737       region-&gt;del_req(PATH_LIMIT);
3738     }
3739     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3740   }
3741 
3742   // these are the only paths that produce 'true':
3743   phi-&gt;set_req(_prim_same_path,   intcon(1));
3744   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3745 
3746   // pull together the cases:
3747   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3748   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3749     Node* ctl = region-&gt;in(i);
3750     if (ctl == NULL || ctl == top()) {
3751       region-&gt;set_req(i, top());
3752       phi   -&gt;set_req(i, top());
3753     } else if (phi-&gt;in(i) == NULL) {
3754       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3755     }
3756   }
3757 
3758   set_control(_gvn.transform(region));
3759   set_result(_gvn.transform(phi));
3760   return true;
3761 }
3762 
3763 //---------------------generate_array_guard_common------------------------
3764 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3765                                                   bool obj_array, bool not_array) {
3766   // If obj_array/non_array==false/false:
3767   // Branch around if the given klass is in fact an array (either obj or prim).
3768   // If obj_array/non_array==false/true:
3769   // Branch around if the given klass is not an array klass of any kind.
3770   // If obj_array/non_array==true/true:
3771   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3772   // If obj_array/non_array==true/false:
3773   // Branch around if the kls is an oop array (Object[] or subtype)
3774   //
3775   // Like generate_guard, adds a new path onto the region.
3776   jint  layout_con = 0;
3777   Node* layout_val = get_layout_helper(kls, layout_con);
3778   if (layout_val == NULL) {
3779     bool query = (obj_array
3780                   ? Klass::layout_helper_is_objArray(layout_con)
3781                   : Klass::layout_helper_is_array(layout_con));
3782     if (query == not_array) {
3783       return NULL;                       // never a branch
3784     } else {                             // always a branch
3785       Node* always_branch = control();
3786       if (region != NULL)
3787         region-&gt;add_req(always_branch);
3788       set_control(top());
3789       return always_branch;
3790     }
3791   }
3792   // Now test the correct condition.
3793   jint  nval = (obj_array
3794                 ? ((jint)Klass::_lh_array_tag_type_value
3795                    &lt;&lt;    Klass::_lh_array_tag_shift)
3796                 : Klass::_lh_neutral_value);
3797   Node* cmp = _gvn.transform(new(C) CmpINode(layout_val, intcon(nval)));
3798   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3799   // invert the test if we are looking for a non-array
3800   if (not_array)  btest = BoolTest(btest).negate();
3801   Node* bol = _gvn.transform(new(C) BoolNode(cmp, btest));
3802   return generate_fair_guard(bol, region);
3803 }
3804 
3805 
3806 //-----------------------inline_native_newArray--------------------------
3807 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3808 bool LibraryCallKit::inline_native_newArray() {
3809   Node* mirror    = argument(0);
3810   Node* count_val = argument(1);
3811 
3812   mirror = null_check(mirror);
3813   // If mirror or obj is dead, only null-path is taken.
3814   if (stopped())  return true;
3815 
3816   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3817   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3818   PhiNode*    result_val = new(C) PhiNode(result_reg,
3819                                           TypeInstPtr::NOTNULL);
3820   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3821   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3822                                           TypePtr::BOTTOM);
3823 
3824   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3825   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3826                                                   result_reg, _slow_path);
3827   Node* normal_ctl   = control();
3828   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3829 
3830   // Generate code for the slow case.  We make a call to newArray().
3831   set_control(no_array_ctl);
3832   if (!stopped()) {
3833     // Either the input type is void.class, or else the
3834     // array klass has not yet been cached.  Either the
3835     // ensuing call will throw an exception, or else it
3836     // will cache the array klass for next time.
3837     PreserveJVMState pjvms(this);
3838     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3839     Node* slow_result = set_results_for_java_call(slow_call);
3840     // this-&gt;control() comes from set_results_for_java_call
3841     result_reg-&gt;set_req(_slow_path, control());
3842     result_val-&gt;set_req(_slow_path, slow_result);
3843     result_io -&gt;set_req(_slow_path, i_o());
3844     result_mem-&gt;set_req(_slow_path, reset_memory());
3845   }
3846 
3847   set_control(normal_ctl);
3848   if (!stopped()) {
3849     // Normal case:  The array type has been cached in the java.lang.Class.
3850     // The following call works fine even if the array type is polymorphic.
3851     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3852     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3853     result_reg-&gt;init_req(_normal_path, control());
3854     result_val-&gt;init_req(_normal_path, obj);
3855     result_io -&gt;init_req(_normal_path, i_o());
3856     result_mem-&gt;init_req(_normal_path, reset_memory());
3857   }
3858 
3859   // Return the combined state.
3860   set_i_o(        _gvn.transform(result_io)  );
3861   set_all_memory( _gvn.transform(result_mem));
3862 
3863   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3864   set_result(result_reg, result_val);
3865   return true;
3866 }
3867 
3868 //----------------------inline_native_getLength--------------------------
3869 // public static native int java.lang.reflect.Array.getLength(Object array);
3870 bool LibraryCallKit::inline_native_getLength() {
3871   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3872 
3873   Node* array = null_check(argument(0));
3874   // If array is dead, only null-path is taken.
3875   if (stopped())  return true;
3876 
3877   // Deoptimize if it is a non-array.
3878   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3879 
3880   if (non_array != NULL) {
3881     PreserveJVMState pjvms(this);
3882     set_control(non_array);
3883     uncommon_trap(Deoptimization::Reason_intrinsic,
3884                   Deoptimization::Action_maybe_recompile);
3885   }
3886 
3887   // If control is dead, only non-array-path is taken.
3888   if (stopped())  return true;
3889 
3890   // The works fine even if the array type is polymorphic.
3891   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3892   Node* result = load_array_length(array);
3893 
3894   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3895   set_result(result);
3896   return true;
3897 }
3898 
3899 //------------------------inline_array_copyOf----------------------------
3900 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3901 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3902 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3903   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3904 
3905   // Get the arguments.
3906   Node* original          = argument(0);
3907   Node* start             = is_copyOfRange? argument(1): intcon(0);
3908   Node* end               = is_copyOfRange? argument(2): argument(1);
3909   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3910 
3911   Node* newcopy;
3912 
3913   // Set the original stack and the reexecute bit for the interpreter to reexecute
3914   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3915   { PreserveReexecuteState preexecs(this);
3916     jvms()-&gt;set_should_reexecute(true);
3917 
3918     array_type_mirror = null_check(array_type_mirror);
3919     original          = null_check(original);
3920 
3921     // Check if a null path was taken unconditionally.
3922     if (stopped())  return true;
3923 
3924     Node* orig_length = load_array_length(original);
3925 
3926     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3927     klass_node = null_check(klass_node);
3928 
3929     RegionNode* bailout = new (C) RegionNode(1);
3930     record_for_igvn(bailout);
3931 
3932     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3933     // Bail out if that is so.
3934     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3935     if (not_objArray != NULL) {
3936       // Improve the klass node's type from the new optimistic assumption:
3937       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3938       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3939       Node* cast = new (C) CastPPNode(klass_node, akls);
3940       cast-&gt;init_req(0, control());
3941       klass_node = _gvn.transform(cast);
3942     }
3943 
3944     // Bail out if either start or end is negative.
3945     generate_negative_guard(start, bailout, &amp;start);
3946     generate_negative_guard(end,   bailout, &amp;end);
3947 
3948     Node* length = end;
3949     if (_gvn.type(start) != TypeInt::ZERO) {
3950       length = _gvn.transform(new (C) SubINode(end, start));
3951     }
3952 
3953     // Bail out if length is negative.
3954     // Without this the new_array would throw
3955     // NegativeArraySizeException but IllegalArgumentException is what
3956     // should be thrown
3957     generate_negative_guard(length, bailout, &amp;length);
3958 
3959     if (bailout-&gt;req() &gt; 1) {
3960       PreserveJVMState pjvms(this);
3961       set_control(_gvn.transform(bailout));
3962       uncommon_trap(Deoptimization::Reason_intrinsic,
3963                     Deoptimization::Action_maybe_recompile);
3964     }
3965 
3966     if (!stopped()) {
3967       // How many elements will we copy from the original?
3968       // The answer is MinI(orig_length - start, length).
3969       Node* orig_tail = _gvn.transform(new (C) SubINode(orig_length, start));
3970       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3971 
3972       newcopy = new_array(klass_node, length, 0);  // no argments to push
3973 
3974       // Generate a direct call to the right arraycopy function(s).
3975       // We know the copy is disjoint but we might not know if the
3976       // oop stores need checking.
3977       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3978       // This will fail a store-check if x contains any non-nulls.
3979       bool disjoint_bases = true;
3980       // if start &gt; orig_length then the length of the copy may be
3981       // negative.
3982       bool length_never_negative = !is_copyOfRange;
3983       generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
3984                          original, start, newcopy, intcon(0), moved,
3985                          disjoint_bases, length_never_negative);
3986     }
3987   } // original reexecute is set back here
3988 
3989   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3990   if (!stopped()) {
3991     set_result(newcopy);
3992   }
3993   return true;
3994 }
3995 
3996 
3997 //----------------------generate_virtual_guard---------------------------
3998 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
3999 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
4000                                              RegionNode* slow_region) {
4001   ciMethod* method = callee();
4002   int vtable_index = method-&gt;vtable_index();
4003   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4004          err_msg_res("bad index %d", vtable_index));
4005   // Get the Method* out of the appropriate vtable entry.
4006   int entry_offset  = (InstanceKlass::vtable_start_offset() +
4007                      vtable_index*vtableEntry::size()) * wordSize +
4008                      vtableEntry::method_offset_in_bytes();
4009   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
4010   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4011 
4012   // Compare the target method with the expected method (e.g., Object.hashCode).
4013   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
4014 
4015   Node* native_call = makecon(native_call_addr);
4016   Node* chk_native  = _gvn.transform(new(C) CmpPNode(target_call, native_call));
4017   Node* test_native = _gvn.transform(new(C) BoolNode(chk_native, BoolTest::ne));
4018 
4019   return generate_slow_guard(test_native, slow_region);
4020 }
4021 
4022 //-----------------------generate_method_call----------------------------
4023 // Use generate_method_call to make a slow-call to the real
4024 // method if the fast path fails.  An alternative would be to
4025 // use a stub like OptoRuntime::slow_arraycopy_Java.
4026 // This only works for expanding the current library call,
4027 // not another intrinsic.  (E.g., don't use this for making an
4028 // arraycopy call inside of the copyOf intrinsic.)
4029 CallJavaNode*
4030 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
4031   // When compiling the intrinsic method itself, do not use this technique.
4032   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
4033 
4034   ciMethod* method = callee();
4035   // ensure the JVMS we have will be correct for this call
4036   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4037 
4038   const TypeFunc* tf = TypeFunc::make(method);
4039   CallJavaNode* slow_call;
4040   if (is_static) {
4041     assert(!is_virtual, "");
4042     slow_call = new(C) CallStaticJavaNode(C, tf,
4043                            SharedRuntime::get_resolve_static_call_stub(),
4044                            method, bci());
4045   } else if (is_virtual) {
4046     null_check_receiver();
4047     int vtable_index = Method::invalid_vtable_index;
4048     if (UseInlineCaches) {
4049       // Suppress the vtable call
4050     } else {
4051       // hashCode and clone are not a miranda methods,
4052       // so the vtable index is fixed.
4053       // No need to use the linkResolver to get it.
4054        vtable_index = method-&gt;vtable_index();
4055        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4056               err_msg_res("bad index %d", vtable_index));
4057     }
4058     slow_call = new(C) CallDynamicJavaNode(tf,
4059                           SharedRuntime::get_resolve_virtual_call_stub(),
4060                           method, vtable_index, bci());
4061   } else {  // neither virtual nor static:  opt_virtual
4062     null_check_receiver();
4063     slow_call = new(C) CallStaticJavaNode(C, tf,
4064                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4065                                 method, bci());
4066     slow_call-&gt;set_optimized_virtual(true);
4067   }
4068   set_arguments_for_java_call(slow_call);
4069   set_edges_for_java_call(slow_call);
4070   return slow_call;
4071 }
4072 
4073 
4074 /**
4075  * Build special case code for calls to hashCode on an object. This call may
4076  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4077  * slightly different code.
4078  */
4079 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4080   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4081   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4082 
4083   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4084 
4085   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4086   PhiNode*    result_val = new(C) PhiNode(result_reg, TypeInt::INT);
4087   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
4088   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4089   Node* obj = NULL;
4090   if (!is_static) {
4091     // Check for hashing null object
4092     obj = null_check_receiver();
4093     if (stopped())  return true;        // unconditionally null
4094     result_reg-&gt;init_req(_null_path, top());
4095     result_val-&gt;init_req(_null_path, top());
4096   } else {
4097     // Do a null check, and return zero if null.
4098     // System.identityHashCode(null) == 0
4099     obj = argument(0);
4100     Node* null_ctl = top();
4101     obj = null_check_oop(obj, &amp;null_ctl);
4102     result_reg-&gt;init_req(_null_path, null_ctl);
4103     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4104   }
4105 
4106   // Unconditionally null?  Then return right away.
4107   if (stopped()) {
4108     set_control( result_reg-&gt;in(_null_path));
4109     if (!stopped())
4110       set_result(result_val-&gt;in(_null_path));
4111     return true;
4112   }
4113 
4114   // We only go to the fast case code if we pass a number of guards.  The
4115   // paths which do not pass are accumulated in the slow_region.
4116   RegionNode* slow_region = new (C) RegionNode(1);
4117   record_for_igvn(slow_region);
4118 
4119   // If this is a virtual call, we generate a funny guard.  We pull out
4120   // the vtable entry corresponding to hashCode() from the target object.
4121   // If the target method which we are calling happens to be the native
4122   // Object hashCode() method, we pass the guard.  We do not need this
4123   // guard for non-virtual calls -- the caller is known to be the native
4124   // Object hashCode().
4125   if (is_virtual) {
4126     // After null check, get the object's klass.
4127     Node* obj_klass = load_object_klass(obj);
4128     generate_virtual_guard(obj_klass, slow_region);
4129   }
4130 
4131   // Get the header out of the object, use LoadMarkNode when available
4132   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4133   // The control of the load must be NULL. Otherwise, the load can move before
4134   // the null check after castPP removal.
4135   Node* no_ctrl = NULL;
4136   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4137 
4138   // Test the header to see if it is unlocked.
4139   Node* lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4140   Node* lmasked_header = _gvn.transform(new (C) AndXNode(header, lock_mask));
4141   Node* unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4142   Node* chk_unlocked   = _gvn.transform(new (C) CmpXNode( lmasked_header, unlocked_val));
4143   Node* test_unlocked  = _gvn.transform(new (C) BoolNode( chk_unlocked, BoolTest::ne));
4144 
4145   generate_slow_guard(test_unlocked, slow_region);
4146 
4147   // Get the hash value and check to see that it has been properly assigned.
4148   // We depend on hash_mask being at most 32 bits and avoid the use of
4149   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4150   // vm: see markOop.hpp.
4151   Node* hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4152   Node* hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4153   Node* hshifted_header= _gvn.transform(new (C) URShiftXNode(header, hash_shift));
4154   // This hack lets the hash bits live anywhere in the mark object now, as long
4155   // as the shift drops the relevant bits into the low 32 bits.  Note that
4156   // Java spec says that HashCode is an int so there's no point in capturing
4157   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4158   hshifted_header      = ConvX2I(hshifted_header);
4159   Node* hash_val       = _gvn.transform(new (C) AndINode(hshifted_header, hash_mask));
4160 
4161   Node* no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4162   Node* chk_assigned   = _gvn.transform(new (C) CmpINode( hash_val, no_hash_val));
4163   Node* test_assigned  = _gvn.transform(new (C) BoolNode( chk_assigned, BoolTest::eq));
4164 
4165   generate_slow_guard(test_assigned, slow_region);
4166 
4167   Node* init_mem = reset_memory();
4168   // fill in the rest of the null path:
4169   result_io -&gt;init_req(_null_path, i_o());
4170   result_mem-&gt;init_req(_null_path, init_mem);
4171 
4172   result_val-&gt;init_req(_fast_path, hash_val);
4173   result_reg-&gt;init_req(_fast_path, control());
4174   result_io -&gt;init_req(_fast_path, i_o());
4175   result_mem-&gt;init_req(_fast_path, init_mem);
4176 
4177   // Generate code for the slow case.  We make a call to hashCode().
4178   set_control(_gvn.transform(slow_region));
4179   if (!stopped()) {
4180     // No need for PreserveJVMState, because we're using up the present state.
4181     set_all_memory(init_mem);
4182     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4183     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4184     Node* slow_result = set_results_for_java_call(slow_call);
4185     // this-&gt;control() comes from set_results_for_java_call
4186     result_reg-&gt;init_req(_slow_path, control());
4187     result_val-&gt;init_req(_slow_path, slow_result);
4188     result_io  -&gt;set_req(_slow_path, i_o());
4189     result_mem -&gt;set_req(_slow_path, reset_memory());
4190   }
4191 
4192   // Return the combined state.
4193   set_i_o(        _gvn.transform(result_io)  );
4194   set_all_memory( _gvn.transform(result_mem));
4195 
4196   set_result(result_reg, result_val);
4197   return true;
4198 }
4199 
4200 //---------------------------inline_native_getClass----------------------------
4201 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4202 //
4203 // Build special case code for calls to getClass on an object.
4204 bool LibraryCallKit::inline_native_getClass() {
4205   Node* obj = null_check_receiver();
4206   if (stopped())  return true;
4207   set_result(load_mirror_from_klass(load_object_klass(obj)));
4208   return true;
4209 }
4210 
4211 //-----------------inline_native_Reflection_getCallerClass---------------------
4212 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4213 //
4214 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4215 //
4216 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4217 // in that it must skip particular security frames and checks for
4218 // caller sensitive methods.
4219 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4220 #ifndef PRODUCT
4221   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4222     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4223   }
4224 #endif
4225 
4226   if (!jvms()-&gt;has_method()) {
4227 #ifndef PRODUCT
4228     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4229       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4230     }
4231 #endif
4232     return false;
4233   }
4234 
4235   // Walk back up the JVM state to find the caller at the required
4236   // depth.
4237   JVMState* caller_jvms = jvms();
4238 
4239   // Cf. JVM_GetCallerClass
4240   // NOTE: Start the loop at depth 1 because the current JVM state does
4241   // not include the Reflection.getCallerClass() frame.
4242   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4243     ciMethod* m = caller_jvms-&gt;method();
4244     switch (n) {
4245     case 0:
4246       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4247       break;
4248     case 1:
4249       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4250       if (!m-&gt;caller_sensitive()) {
4251 #ifndef PRODUCT
4252         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4253           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4254         }
4255 #endif
4256         return false;  // bail-out; let JVM_GetCallerClass do the work
4257       }
4258       break;
4259     default:
4260       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4261         // We have reached the desired frame; return the holder class.
4262         // Acquire method holder as java.lang.Class and push as constant.
4263         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4264         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4265         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4266 
4267 #ifndef PRODUCT
4268         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4269           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4270           tty-&gt;print_cr("  JVM state at this point:");
4271           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4272             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4273             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4274           }
4275         }
4276 #endif
4277         return true;
4278       }
4279       break;
4280     }
4281   }
4282 
4283 #ifndef PRODUCT
4284   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4285     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4286     tty-&gt;print_cr("  JVM state at this point:");
4287     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4288       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4289       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4290     }
4291   }
4292 #endif
4293 
4294   return false;  // bail-out; let JVM_GetCallerClass do the work
4295 }
4296 
4297 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4298   Node* arg = argument(0);
4299   Node* result;
4300 
4301   switch (id) {
4302   case vmIntrinsics::_floatToRawIntBits:    result = new (C) MoveF2INode(arg);  break;
4303   case vmIntrinsics::_intBitsToFloat:       result = new (C) MoveI2FNode(arg);  break;
4304   case vmIntrinsics::_doubleToRawLongBits:  result = new (C) MoveD2LNode(arg);  break;
4305   case vmIntrinsics::_longBitsToDouble:     result = new (C) MoveL2DNode(arg);  break;
4306 
4307   case vmIntrinsics::_doubleToLongBits: {
4308     // two paths (plus control) merge in a wood
4309     RegionNode *r = new (C) RegionNode(3);
4310     Node *phi = new (C) PhiNode(r, TypeLong::LONG);
4311 
4312     Node *cmpisnan = _gvn.transform(new (C) CmpDNode(arg, arg));
4313     // Build the boolean node
4314     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4315 
4316     // Branch either way.
4317     // NaN case is less traveled, which makes all the difference.
4318     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4319     Node *opt_isnan = _gvn.transform(ifisnan);
4320     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4321     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4322     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4323 
4324     set_control(iftrue);
4325 
4326     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4327     Node *slow_result = longcon(nan_bits); // return NaN
4328     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4329     r-&gt;init_req(1, iftrue);
4330 
4331     // Else fall through
4332     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4333     set_control(iffalse);
4334 
4335     phi-&gt;init_req(2, _gvn.transform(new (C) MoveD2LNode(arg)));
4336     r-&gt;init_req(2, iffalse);
4337 
4338     // Post merge
4339     set_control(_gvn.transform(r));
4340     record_for_igvn(r);
4341 
4342     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4343     result = phi;
4344     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4345     break;
4346   }
4347 
4348   case vmIntrinsics::_floatToIntBits: {
4349     // two paths (plus control) merge in a wood
4350     RegionNode *r = new (C) RegionNode(3);
4351     Node *phi = new (C) PhiNode(r, TypeInt::INT);
4352 
4353     Node *cmpisnan = _gvn.transform(new (C) CmpFNode(arg, arg));
4354     // Build the boolean node
4355     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4356 
4357     // Branch either way.
4358     // NaN case is less traveled, which makes all the difference.
4359     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4360     Node *opt_isnan = _gvn.transform(ifisnan);
4361     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4362     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4363     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4364 
4365     set_control(iftrue);
4366 
4367     static const jint nan_bits = 0x7fc00000;
4368     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4369     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4370     r-&gt;init_req(1, iftrue);
4371 
4372     // Else fall through
4373     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4374     set_control(iffalse);
4375 
4376     phi-&gt;init_req(2, _gvn.transform(new (C) MoveF2INode(arg)));
4377     r-&gt;init_req(2, iffalse);
4378 
4379     // Post merge
4380     set_control(_gvn.transform(r));
4381     record_for_igvn(r);
4382 
4383     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4384     result = phi;
4385     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4386     break;
4387   }
4388 
4389   default:
4390     fatal_unexpected_iid(id);
4391     break;
4392   }
4393   set_result(_gvn.transform(result));
4394   return true;
4395 }
4396 
4397 #ifdef _LP64
4398 #define XTOP ,top() /*additional argument*/
4399 #else  //_LP64
4400 #define XTOP        /*no additional argument*/
4401 #endif //_LP64
4402 
4403 //----------------------inline_unsafe_copyMemory-------------------------
4404 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4405 bool LibraryCallKit::inline_unsafe_copyMemory() {
4406   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4407   null_check_receiver();  // null-check receiver
4408   if (stopped())  return true;
4409 
4410   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4411 
4412   Node* src_ptr =         argument(1);   // type: oop
4413   Node* src_off = ConvL2X(argument(2));  // type: long
4414   Node* dst_ptr =         argument(4);   // type: oop
4415   Node* dst_off = ConvL2X(argument(5));  // type: long
4416   Node* size    = ConvL2X(argument(7));  // type: long
4417 
4418   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4419          "fieldOffset must be byte-scaled");
4420 
4421   Node* src = make_unsafe_address(src_ptr, src_off);
4422   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4423 
4424   // Conservatively insert a memory barrier on all memory slices.
4425   // Do not let writes of the copy source or destination float below the copy.
4426   insert_mem_bar(Op_MemBarCPUOrder);
4427 
4428   // Call it.  Note that the length argument is not scaled.
4429   make_runtime_call(RC_LEAF|RC_NO_FP,
4430                     OptoRuntime::fast_arraycopy_Type(),
4431                     StubRoutines::unsafe_arraycopy(),
4432                     "unsafe_arraycopy",
4433                     TypeRawPtr::BOTTOM,
4434                     src, dst, size XTOP);
4435 
4436   // Do not let reads of the copy destination float above the copy.
4437   insert_mem_bar(Op_MemBarCPUOrder);
4438 
4439   return true;
4440 }
4441 
4442 //------------------------clone_coping-----------------------------------
4443 // Helper function for inline_native_clone.
4444 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4445   assert(obj_size != NULL, "");
4446   Node* raw_obj = alloc_obj-&gt;in(1);
4447   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4448 
4449   AllocateNode* alloc = NULL;
4450   if (ReduceBulkZeroing) {
4451     // We will be completely responsible for initializing this object -
4452     // mark Initialize node as complete.
4453     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4454     // The object was just allocated - there should be no any stores!
4455     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4456     // Mark as complete_with_arraycopy so that on AllocateNode
4457     // expansion, we know this AllocateNode is initialized by an array
4458     // copy and a StoreStore barrier exists after the array copy.
4459     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4460   }
4461 
4462   // Copy the fastest available way.
4463   // TODO: generate fields copies for small objects instead.
4464   Node* src  = obj;
4465   Node* dest = alloc_obj;
4466   Node* size = _gvn.transform(obj_size);
4467 
4468   // Exclude the header but include array length to copy by 8 bytes words.
4469   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4470   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4471                             instanceOopDesc::base_offset_in_bytes();
4472   // base_off:
4473   // 8  - 32-bit VM
4474   // 12 - 64-bit VM, compressed klass
4475   // 16 - 64-bit VM, normal klass
4476   if (base_off % BytesPerLong != 0) {
4477     assert(UseCompressedClassPointers, "");
4478     if (is_array) {
4479       // Exclude length to copy by 8 bytes words.
4480       base_off += sizeof(int);
4481     } else {
4482       // Include klass to copy by 8 bytes words.
4483       base_off = instanceOopDesc::klass_offset_in_bytes();
4484     }
4485     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4486   }
4487   src  = basic_plus_adr(src,  base_off);
4488   dest = basic_plus_adr(dest, base_off);
4489 
4490   // Compute the length also, if needed:
4491   Node* countx = size;
4492   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(base_off)));
4493   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong) ));
4494 
4495   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4496   bool disjoint_bases = true;
4497   generate_unchecked_arraycopy(raw_adr_type, T_LONG, disjoint_bases,
4498                                src, NULL, dest, NULL, countx,
4499                                /*dest_uninitialized*/true);
4500 
4501   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4502   if (card_mark) {
4503     assert(!is_array, "");
4504     // Put in store barrier for any and all oops we are sticking
4505     // into this object.  (We could avoid this if we could prove
4506     // that the object type contains no oop fields at all.)
4507     Node* no_particular_value = NULL;
4508     Node* no_particular_field = NULL;
4509     int raw_adr_idx = Compile::AliasIdxRaw;
4510     post_barrier(control(),
4511                  memory(raw_adr_type),
4512                  alloc_obj,
4513                  no_particular_field,
4514                  raw_adr_idx,
4515                  no_particular_value,
4516                  T_OBJECT,
4517                  false);
4518   }
4519 
4520   // Do not let reads from the cloned object float above the arraycopy.
4521   if (alloc != NULL) {
4522     // Do not let stores that initialize this object be reordered with
4523     // a subsequent store that would make this object accessible by
4524     // other threads.
4525     // Record what AllocateNode this StoreStore protects so that
4526     // escape analysis can go from the MemBarStoreStoreNode to the
4527     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4528     // based on the escape status of the AllocateNode.
4529     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4530   } else {
4531     insert_mem_bar(Op_MemBarCPUOrder);
4532   }
4533 }
4534 
4535 //------------------------inline_native_clone----------------------------
4536 // protected native Object java.lang.Object.clone();
4537 //
4538 // Here are the simple edge cases:
4539 //  null receiver =&gt; normal trap
4540 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4541 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4542 //
4543 // The general case has two steps, allocation and copying.
4544 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4545 //
4546 // Copying also has two cases, oop arrays and everything else.
4547 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4548 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4549 //
4550 // These steps fold up nicely if and when the cloned object's klass
4551 // can be sharply typed as an object array, a type array, or an instance.
4552 //
4553 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4554   PhiNode* result_val;
4555 
4556   // Set the reexecute bit for the interpreter to reexecute
4557   // the bytecode that invokes Object.clone if deoptimization happens.
4558   { PreserveReexecuteState preexecs(this);
4559     jvms()-&gt;set_should_reexecute(true);
4560 
4561     Node* obj = null_check_receiver();
4562     if (stopped())  return true;
4563 
4564     Node* obj_klass = load_object_klass(obj);
4565     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4566     const TypeOopPtr*   toop   = ((tklass != NULL)
4567                                 ? tklass-&gt;as_instance_type()
4568                                 : TypeInstPtr::NOTNULL);
4569 
4570     // Conservatively insert a memory barrier on all memory slices.
4571     // Do not let writes into the original float below the clone.
4572     insert_mem_bar(Op_MemBarCPUOrder);
4573 
4574     // paths into result_reg:
4575     enum {
4576       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4577       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4578       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4579       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4580       PATH_LIMIT
4581     };
4582     RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4583     result_val             = new(C) PhiNode(result_reg,
4584                                             TypeInstPtr::NOTNULL);
4585     PhiNode*    result_i_o = new(C) PhiNode(result_reg, Type::ABIO);
4586     PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
4587                                             TypePtr::BOTTOM);
4588     record_for_igvn(result_reg);
4589 
4590     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4591     int raw_adr_idx = Compile::AliasIdxRaw;
4592 
4593     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4594     if (array_ctl != NULL) {
4595       // It's an array.
4596       PreserveJVMState pjvms(this);
4597       set_control(array_ctl);
4598       Node* obj_length = load_array_length(obj);
4599       Node* obj_size  = NULL;
4600       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4601 
4602       if (!use_ReduceInitialCardMarks()) {
4603         // If it is an oop array, it requires very special treatment,
4604         // because card marking is required on each card of the array.
4605         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4606         if (is_obja != NULL) {
4607           PreserveJVMState pjvms2(this);
4608           set_control(is_obja);
4609           // Generate a direct call to the right arraycopy function(s).
4610           bool disjoint_bases = true;
4611           bool length_never_negative = true;
4612           generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
4613                              obj, intcon(0), alloc_obj, intcon(0),
4614                              obj_length,
4615                              disjoint_bases, length_never_negative);
4616           result_reg-&gt;init_req(_objArray_path, control());
4617           result_val-&gt;init_req(_objArray_path, alloc_obj);
4618           result_i_o -&gt;set_req(_objArray_path, i_o());
4619           result_mem -&gt;set_req(_objArray_path, reset_memory());
4620         }
4621       }
4622       // Otherwise, there are no card marks to worry about.
4623       // (We can dispense with card marks if we know the allocation
4624       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4625       //  causes the non-eden paths to take compensating steps to
4626       //  simulate a fresh allocation, so that no further
4627       //  card marks are required in compiled code to initialize
4628       //  the object.)
4629 
4630       if (!stopped()) {
4631         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4632 
4633         // Present the results of the copy.
4634         result_reg-&gt;init_req(_array_path, control());
4635         result_val-&gt;init_req(_array_path, alloc_obj);
4636         result_i_o -&gt;set_req(_array_path, i_o());
4637         result_mem -&gt;set_req(_array_path, reset_memory());
4638       }
4639     }
4640 
4641     // We only go to the instance fast case code if we pass a number of guards.
4642     // The paths which do not pass are accumulated in the slow_region.
4643     RegionNode* slow_region = new (C) RegionNode(1);
4644     record_for_igvn(slow_region);
4645     if (!stopped()) {
4646       // It's an instance (we did array above).  Make the slow-path tests.
4647       // If this is a virtual call, we generate a funny guard.  We grab
4648       // the vtable entry corresponding to clone() from the target object.
4649       // If the target method which we are calling happens to be the
4650       // Object clone() method, we pass the guard.  We do not need this
4651       // guard for non-virtual calls; the caller is known to be the native
4652       // Object clone().
4653       if (is_virtual) {
4654         generate_virtual_guard(obj_klass, slow_region);
4655       }
4656 
4657       // The object must be cloneable and must not have a finalizer.
4658       // Both of these conditions may be checked in a single test.
4659       // We could optimize the cloneable test further, but we don't care.
4660       generate_access_flags_guard(obj_klass,
4661                                   // Test both conditions:
4662                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4663                                   // Must be cloneable but not finalizer:
4664                                   JVM_ACC_IS_CLONEABLE,
4665                                   slow_region);
4666     }
4667 
4668     if (!stopped()) {
4669       // It's an instance, and it passed the slow-path tests.
4670       PreserveJVMState pjvms(this);
4671       Node* obj_size  = NULL;
4672       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4673       // is reexecuted if deoptimization occurs and there could be problems when merging
4674       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4675       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4676 
4677       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4678 
4679       // Present the results of the slow call.
4680       result_reg-&gt;init_req(_instance_path, control());
4681       result_val-&gt;init_req(_instance_path, alloc_obj);
4682       result_i_o -&gt;set_req(_instance_path, i_o());
4683       result_mem -&gt;set_req(_instance_path, reset_memory());
4684     }
4685 
4686     // Generate code for the slow case.  We make a call to clone().
4687     set_control(_gvn.transform(slow_region));
4688     if (!stopped()) {
4689       PreserveJVMState pjvms(this);
4690       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4691       Node* slow_result = set_results_for_java_call(slow_call);
4692       // this-&gt;control() comes from set_results_for_java_call
4693       result_reg-&gt;init_req(_slow_path, control());
4694       result_val-&gt;init_req(_slow_path, slow_result);
4695       result_i_o -&gt;set_req(_slow_path, i_o());
4696       result_mem -&gt;set_req(_slow_path, reset_memory());
4697     }
4698 
4699     // Return the combined state.
4700     set_control(    _gvn.transform(result_reg));
4701     set_i_o(        _gvn.transform(result_i_o));
4702     set_all_memory( _gvn.transform(result_mem));
4703   } // original reexecute is set back here
4704 
4705   set_result(_gvn.transform(result_val));
4706   return true;
4707 }
4708 
4709 //------------------------------basictype2arraycopy----------------------------
4710 address LibraryCallKit::basictype2arraycopy(BasicType t,
4711                                             Node* src_offset,
4712                                             Node* dest_offset,
4713                                             bool disjoint_bases,
4714                                             const char* &amp;name,
4715                                             bool dest_uninitialized) {
4716   const TypeInt* src_offset_inttype  = gvn().find_int_type(src_offset);;
4717   const TypeInt* dest_offset_inttype = gvn().find_int_type(dest_offset);;
4718 
4719   bool aligned = false;
4720   bool disjoint = disjoint_bases;
4721 
4722   // if the offsets are the same, we can treat the memory regions as
4723   // disjoint, because either the memory regions are in different arrays,
4724   // or they are identical (which we can treat as disjoint.)  We can also
4725   // treat a copy with a destination index  less that the source index
4726   // as disjoint since a low-&gt;high copy will work correctly in this case.
4727   if (src_offset_inttype != NULL &amp;&amp; src_offset_inttype-&gt;is_con() &amp;&amp;
4728       dest_offset_inttype != NULL &amp;&amp; dest_offset_inttype-&gt;is_con()) {
4729     // both indices are constants
4730     int s_offs = src_offset_inttype-&gt;get_con();
4731     int d_offs = dest_offset_inttype-&gt;get_con();
4732     int element_size = type2aelembytes(t);
4733     aligned = ((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &amp;&amp;
4734               ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0);
4735     if (s_offs &gt;= d_offs)  disjoint = true;
4736   } else if (src_offset == dest_offset &amp;&amp; src_offset != NULL) {
4737     // This can occur if the offsets are identical non-constants.
4738     disjoint = true;
4739   }
4740 
4741   return StubRoutines::select_arraycopy_function(t, aligned, disjoint, name, dest_uninitialized);
4742 }
4743 
4744 
4745 //------------------------------inline_arraycopy-----------------------
4746 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4747 //                                                      Object dest, int destPos,
4748 //                                                      int length);
4749 bool LibraryCallKit::inline_arraycopy() {
4750   // Get the arguments.
4751   Node* src         = argument(0);  // type: oop
4752   Node* src_offset  = argument(1);  // type: int
4753   Node* dest        = argument(2);  // type: oop
4754   Node* dest_offset = argument(3);  // type: int
4755   Node* length      = argument(4);  // type: int
4756 
4757   // Compile time checks.  If any of these checks cannot be verified at compile time,
4758   // we do not make a fast path for this call.  Instead, we let the call remain as it
4759   // is.  The checks we choose to mandate at compile time are:
4760   //
4761   // (1) src and dest are arrays.
4762   const Type* src_type  = src-&gt;Value(&amp;_gvn);
4763   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
4764   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4765   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4766 
4767   // Do we have the type of src?
4768   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4769   // Do we have the type of dest?
4770   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4771   // Is the type for src from speculation?
4772   bool src_spec = false;
4773   // Is the type for dest from speculation?
4774   bool dest_spec = false;
4775 
4776   if (!has_src || !has_dest) {
4777     // We don't have sufficient type information, let's see if
4778     // speculative types can help. We need to have types for both src
4779     // and dest so that it pays off.
4780 
4781     // Do we already have or could we have type information for src
4782     bool could_have_src = has_src;
4783     // Do we already have or could we have type information for dest
4784     bool could_have_dest = has_dest;
4785 
4786     ciKlass* src_k = NULL;
4787     if (!has_src) {
4788       src_k = src_type-&gt;speculative_type();
4789       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4790         could_have_src = true;
4791       }
4792     }
4793 
4794     ciKlass* dest_k = NULL;
4795     if (!has_dest) {
4796       dest_k = dest_type-&gt;speculative_type();
4797       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4798         could_have_dest = true;
4799       }
4800     }
4801 
4802     if (could_have_src &amp;&amp; could_have_dest) {
4803       // This is going to pay off so emit the required guards
4804       if (!has_src) {
4805         src = maybe_cast_profiled_obj(src, src_k);
4806         src_type  = _gvn.type(src);
4807         top_src  = src_type-&gt;isa_aryptr();
4808         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4809         src_spec = true;
4810       }
4811       if (!has_dest) {
4812         dest = maybe_cast_profiled_obj(dest, dest_k);
4813         dest_type  = _gvn.type(dest);
4814         top_dest  = dest_type-&gt;isa_aryptr();
4815         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4816         dest_spec = true;
4817       }
4818     }
4819   }
4820 
4821   if (!has_src || !has_dest) {
4822     // Conservatively insert a memory barrier on all memory slices.
4823     // Do not let writes into the source float below the arraycopy.
4824     insert_mem_bar(Op_MemBarCPUOrder);
4825 
4826     // Call StubRoutines::generic_arraycopy stub.
4827     generate_arraycopy(TypeRawPtr::BOTTOM, T_CONFLICT,
4828                        src, src_offset, dest, dest_offset, length);
4829 
4830     // Do not let reads from the destination float above the arraycopy.
4831     // Since we cannot type the arrays, we don't know which slices
4832     // might be affected.  We could restrict this barrier only to those
4833     // memory slices which pertain to array elements--but don't bother.
4834     if (!InsertMemBarAfterArraycopy)
4835       // (If InsertMemBarAfterArraycopy, there is already one in place.)
4836       insert_mem_bar(Op_MemBarCPUOrder);
4837     return true;
4838   }
4839 
4840   // (2) src and dest arrays must have elements of the same BasicType
4841   // Figure out the size and type of the elements we will be copying.
4842   BasicType src_elem  =  top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4843   BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4844   if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4845   if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4846 
4847   if (src_elem != dest_elem || dest_elem == T_VOID) {
4848     // The component types are not the same or are not recognized.  Punt.
4849     // (But, avoid the native method wrapper to JVM_ArrayCopy.)
4850     generate_slow_arraycopy(TypePtr::BOTTOM,
4851                             src, src_offset, dest, dest_offset, length,
4852                             /*dest_uninitialized*/false);
4853     return true;
4854   }
4855 
4856   if (src_elem == T_OBJECT) {
4857     // If both arrays are object arrays then having the exact types
4858     // for both will remove the need for a subtype check at runtime
4859     // before the call and may make it possible to pick a faster copy
4860     // routine (without a subtype check on every element)
4861     // Do we have the exact type of src?
4862     bool could_have_src = src_spec;
4863     // Do we have the exact type of dest?
4864     bool could_have_dest = dest_spec;
4865     ciKlass* src_k = top_src-&gt;klass();
4866     ciKlass* dest_k = top_dest-&gt;klass();
4867     if (!src_spec) {
4868       src_k = src_type-&gt;speculative_type();
4869       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4870           could_have_src = true;
4871       }
4872     }
4873     if (!dest_spec) {
4874       dest_k = dest_type-&gt;speculative_type();
4875       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4876         could_have_dest = true;
4877       }
4878     }
4879     if (could_have_src &amp;&amp; could_have_dest) {
4880       // If we can have both exact types, emit the missing guards
4881       if (could_have_src &amp;&amp; !src_spec) {
4882         src = maybe_cast_profiled_obj(src, src_k);
4883       }
4884       if (could_have_dest &amp;&amp; !dest_spec) {
4885         dest = maybe_cast_profiled_obj(dest, dest_k);
4886       }
4887     }
4888   }
4889 
4890   //---------------------------------------------------------------------------
4891   // We will make a fast path for this call to arraycopy.
4892 
4893   // We have the following tests left to perform:
4894   //
4895   // (3) src and dest must not be null.
4896   // (4) src_offset must not be negative.
4897   // (5) dest_offset must not be negative.
4898   // (6) length must not be negative.
4899   // (7) src_offset + length must not exceed length of src.
4900   // (8) dest_offset + length must not exceed length of dest.
4901   // (9) each element of an oop array must be assignable
4902 
4903   RegionNode* slow_region = new (C) RegionNode(1);
4904   record_for_igvn(slow_region);
4905 
4906   // (3) operands must not be null
4907   // We currently perform our null checks with the null_check routine.
4908   // This means that the null exceptions will be reported in the caller
4909   // rather than (correctly) reported inside of the native arraycopy call.
4910   // This should be corrected, given time.  We do our null check with the
4911   // stack pointer restored.
4912   src  = null_check(src,  T_ARRAY);
4913   dest = null_check(dest, T_ARRAY);
4914 
4915   // (4) src_offset must not be negative.
4916   generate_negative_guard(src_offset, slow_region);
4917 
4918   // (5) dest_offset must not be negative.
4919   generate_negative_guard(dest_offset, slow_region);
4920 
4921   // (6) length must not be negative (moved to generate_arraycopy()).
4922   // generate_negative_guard(length, slow_region);
4923 
4924   // (7) src_offset + length must not exceed length of src.
4925   generate_limit_guard(src_offset, length,
4926                        load_array_length(src),
4927                        slow_region);
4928 
4929   // (8) dest_offset + length must not exceed length of dest.
4930   generate_limit_guard(dest_offset, length,
4931                        load_array_length(dest),
4932                        slow_region);
4933 
4934   // (9) each element of an oop array must be assignable
4935   // The generate_arraycopy subroutine checks this.
4936 
4937   // This is where the memory effects are placed:
4938   const TypePtr* adr_type = TypeAryPtr::get_array_body_type(dest_elem);
4939   generate_arraycopy(adr_type, dest_elem,
4940                      src, src_offset, dest, dest_offset, length,
4941                      false, false, slow_region);
4942 
4943   return true;
4944 }
4945 
4946 //-----------------------------generate_arraycopy----------------------
4947 // Generate an optimized call to arraycopy.
4948 // Caller must guard against non-arrays.
4949 // Caller must determine a common array basic-type for both arrays.
4950 // Caller must validate offsets against array bounds.
4951 // The slow_region has already collected guard failure paths
4952 // (such as out of bounds length or non-conformable array types).
4953 // The generated code has this shape, in general:
4954 //
4955 //     if (length == 0)  return   // via zero_path
4956 //     slowval = -1
4957 //     if (types unknown) {
4958 //       slowval = call generic copy loop
4959 //       if (slowval == 0)  return  // via checked_path
4960 //     } else if (indexes in bounds) {
4961 //       if ((is object array) &amp;&amp; !(array type check)) {
4962 //         slowval = call checked copy loop
4963 //         if (slowval == 0)  return  // via checked_path
4964 //       } else {
4965 //         call bulk copy loop
4966 //         return  // via fast_path
4967 //       }
4968 //     }
4969 //     // adjust params for remaining work:
4970 //     if (slowval != -1) {
4971 //       n = -1^slowval; src_offset += n; dest_offset += n; length -= n
4972 //     }
4973 //   slow_region:
4974 //     call slow arraycopy(src, src_offset, dest, dest_offset, length)
4975 //     return  // via slow_call_path
4976 //
4977 // This routine is used from several intrinsics:  System.arraycopy,
4978 // Object.clone (the array subcase), and Arrays.copyOf[Range].
4979 //
4980 void
4981 LibraryCallKit::generate_arraycopy(const TypePtr* adr_type,
4982                                    BasicType basic_elem_type,
4983                                    Node* src,  Node* src_offset,
4984                                    Node* dest, Node* dest_offset,
4985                                    Node* copy_length,
4986                                    bool disjoint_bases,
4987                                    bool length_never_negative,
4988                                    RegionNode* slow_region) {
4989 
4990   if (slow_region == NULL) {
4991     slow_region = new(C) RegionNode(1);
4992     record_for_igvn(slow_region);
4993   }
4994 
4995   Node* original_dest      = dest;
4996   AllocateArrayNode* alloc = NULL;  // used for zeroing, if needed
4997   bool  dest_uninitialized = false;
4998 
4999   // See if this is the initialization of a newly-allocated array.
5000   // If so, we will take responsibility here for initializing it to zero.
5001   // (Note:  Because tightly_coupled_allocation performs checks on the
5002   // out-edges of the dest, we need to avoid making derived pointers
5003   // from it until we have checked its uses.)
5004   if (ReduceBulkZeroing
5005       &amp;&amp; !ZeroTLAB              // pointless if already zeroed
5006       &amp;&amp; basic_elem_type != T_CONFLICT // avoid corner case
5007       &amp;&amp; !src-&gt;eqv_uncast(dest)
5008       &amp;&amp; ((alloc = tightly_coupled_allocation(dest, slow_region))
5009           != NULL)
5010       &amp;&amp; _gvn.find_int_con(alloc-&gt;in(AllocateNode::ALength), 1) &gt; 0
5011       &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn)) {
5012     // "You break it, you buy it."
5013     InitializeNode* init = alloc-&gt;initialization();
5014     assert(init-&gt;is_complete(), "we just did this");
5015     init-&gt;set_complete_with_arraycopy();
5016     assert(dest-&gt;is_CheckCastPP(), "sanity");
5017     assert(dest-&gt;in(0)-&gt;in(0) == init, "dest pinned");
5018     adr_type = TypeRawPtr::BOTTOM;  // all initializations are into raw memory
5019     // From this point on, every exit path is responsible for
5020     // initializing any non-copied parts of the object to zero.
5021     // Also, if this flag is set we make sure that arraycopy interacts properly
5022     // with G1, eliding pre-barriers. See CR 6627983.
5023     dest_uninitialized = true;
5024   } else {
5025     // No zeroing elimination here.
5026     alloc             = NULL;
5027     //original_dest   = dest;
5028     //dest_uninitialized = false;
5029   }
5030 
5031   // Results are placed here:
5032   enum { fast_path        = 1,  // normal void-returning assembly stub
5033          checked_path     = 2,  // special assembly stub with cleanup
5034          slow_call_path   = 3,  // something went wrong; call the VM
5035          zero_path        = 4,  // bypass when length of copy is zero
5036          bcopy_path       = 5,  // copy primitive array by 64-bit blocks
5037          PATH_LIMIT       = 6
5038   };
5039   RegionNode* result_region = new(C) RegionNode(PATH_LIMIT);
5040   PhiNode*    result_i_o    = new(C) PhiNode(result_region, Type::ABIO);
5041   PhiNode*    result_memory = new(C) PhiNode(result_region, Type::MEMORY, adr_type);
5042   record_for_igvn(result_region);
5043   _gvn.set_type_bottom(result_i_o);
5044   _gvn.set_type_bottom(result_memory);
5045   assert(adr_type != TypePtr::BOTTOM, "must be RawMem or a T[] slice");
5046 
5047   // The slow_control path:
5048   Node* slow_control;
5049   Node* slow_i_o = i_o();
5050   Node* slow_mem = memory(adr_type);
5051   debug_only(slow_control = (Node*) badAddress);
5052 
5053   // Checked control path:
5054   Node* checked_control = top();
5055   Node* checked_mem     = NULL;
5056   Node* checked_i_o     = NULL;
5057   Node* checked_value   = NULL;
5058 
5059   if (basic_elem_type == T_CONFLICT) {
5060     assert(!dest_uninitialized, "");
5061     Node* cv = generate_generic_arraycopy(adr_type,
5062                                           src, src_offset, dest, dest_offset,
5063                                           copy_length, dest_uninitialized);
5064     if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5065     checked_control = control();
5066     checked_i_o     = i_o();
5067     checked_mem     = memory(adr_type);
5068     checked_value   = cv;
5069     set_control(top());         // no fast path
5070   }
5071 
5072   Node* not_pos = generate_nonpositive_guard(copy_length, length_never_negative);
5073   if (not_pos != NULL) {
5074     PreserveJVMState pjvms(this);
5075     set_control(not_pos);
5076 
5077     // (6) length must not be negative.
5078     if (!length_never_negative) {
5079       generate_negative_guard(copy_length, slow_region);
5080     }
5081 
5082     // copy_length is 0.
5083     if (!stopped() &amp;&amp; dest_uninitialized) {
5084       Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
5085       if (copy_length-&gt;eqv_uncast(dest_length)
5086           || _gvn.find_int_con(dest_length, 1) &lt;= 0) {
5087         // There is no zeroing to do. No need for a secondary raw memory barrier.
5088       } else {
5089         // Clear the whole thing since there are no source elements to copy.
5090         generate_clear_array(adr_type, dest, basic_elem_type,
5091                              intcon(0), NULL,
5092                              alloc-&gt;in(AllocateNode::AllocSize));
5093         // Use a secondary InitializeNode as raw memory barrier.
5094         // Currently it is needed only on this path since other
5095         // paths have stub or runtime calls as raw memory barriers.
5096         InitializeNode* init = insert_mem_bar_volatile(Op_Initialize,
5097                                                        Compile::AliasIdxRaw,
5098                                                        top())-&gt;as_Initialize();
5099         init-&gt;set_complete(&amp;_gvn);  // (there is no corresponding AllocateNode)
5100       }
5101     }
5102 
5103     // Present the results of the fast call.
5104     result_region-&gt;init_req(zero_path, control());
5105     result_i_o   -&gt;init_req(zero_path, i_o());
5106     result_memory-&gt;init_req(zero_path, memory(adr_type));
5107   }
5108 
5109   if (!stopped() &amp;&amp; dest_uninitialized) {
5110     // We have to initialize the *uncopied* part of the array to zero.
5111     // The copy destination is the slice dest[off..off+len].  The other slices
5112     // are dest_head = dest[0..off] and dest_tail = dest[off+len..dest.length].
5113     Node* dest_size   = alloc-&gt;in(AllocateNode::AllocSize);
5114     Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
5115     Node* dest_tail   = _gvn.transform(new(C) AddINode(dest_offset,
5116                                                           copy_length));
5117 
5118     // If there is a head section that needs zeroing, do it now.
5119     if (find_int_con(dest_offset, -1) != 0) {
5120       generate_clear_array(adr_type, dest, basic_elem_type,
5121                            intcon(0), dest_offset,
5122                            NULL);
5123     }
5124 
5125     // Next, perform a dynamic check on the tail length.
5126     // It is often zero, and we can win big if we prove this.
5127     // There are two wins:  Avoid generating the ClearArray
5128     // with its attendant messy index arithmetic, and upgrade
5129     // the copy to a more hardware-friendly word size of 64 bits.
5130     Node* tail_ctl = NULL;
5131     if (!stopped() &amp;&amp; !dest_tail-&gt;eqv_uncast(dest_length)) {
5132       Node* cmp_lt   = _gvn.transform(new(C) CmpINode(dest_tail, dest_length));
5133       Node* bol_lt   = _gvn.transform(new(C) BoolNode(cmp_lt, BoolTest::lt));
5134       tail_ctl = generate_slow_guard(bol_lt, NULL);
5135       assert(tail_ctl != NULL || !stopped(), "must be an outcome");
5136     }
5137 
5138     // At this point, let's assume there is no tail.
5139     if (!stopped() &amp;&amp; alloc != NULL &amp;&amp; basic_elem_type != T_OBJECT) {
5140       // There is no tail.  Try an upgrade to a 64-bit copy.
5141       bool didit = false;
5142       { PreserveJVMState pjvms(this);
5143         didit = generate_block_arraycopy(adr_type, basic_elem_type, alloc,
5144                                          src, src_offset, dest, dest_offset,
5145                                          dest_size, dest_uninitialized);
5146         if (didit) {
5147           // Present the results of the block-copying fast call.
5148           result_region-&gt;init_req(bcopy_path, control());
5149           result_i_o   -&gt;init_req(bcopy_path, i_o());
5150           result_memory-&gt;init_req(bcopy_path, memory(adr_type));
5151         }
5152       }
5153       if (didit)
5154         set_control(top());     // no regular fast path
5155     }
5156 
5157     // Clear the tail, if any.
5158     if (tail_ctl != NULL) {
5159       Node* notail_ctl = stopped() ? NULL : control();
5160       set_control(tail_ctl);
5161       if (notail_ctl == NULL) {
5162         generate_clear_array(adr_type, dest, basic_elem_type,
5163                              dest_tail, NULL,
5164                              dest_size);
5165       } else {
5166         // Make a local merge.
5167         Node* done_ctl = new(C) RegionNode(3);
5168         Node* done_mem = new(C) PhiNode(done_ctl, Type::MEMORY, adr_type);
5169         done_ctl-&gt;init_req(1, notail_ctl);
5170         done_mem-&gt;init_req(1, memory(adr_type));
5171         generate_clear_array(adr_type, dest, basic_elem_type,
5172                              dest_tail, NULL,
5173                              dest_size);
5174         done_ctl-&gt;init_req(2, control());
5175         done_mem-&gt;init_req(2, memory(adr_type));
5176         set_control( _gvn.transform(done_ctl));
5177         set_memory(  _gvn.transform(done_mem), adr_type );
5178       }
5179     }
5180   }
5181 
5182   BasicType copy_type = basic_elem_type;
5183   assert(basic_elem_type != T_ARRAY, "caller must fix this");
5184   if (!stopped() &amp;&amp; copy_type == T_OBJECT) {
5185     // If src and dest have compatible element types, we can copy bits.
5186     // Types S[] and D[] are compatible if D is a supertype of S.
5187     //
5188     // If they are not, we will use checked_oop_disjoint_arraycopy,
5189     // which performs a fast optimistic per-oop check, and backs off
5190     // further to JVM_ArrayCopy on the first per-oop check that fails.
5191     // (Actually, we don't move raw bits only; the GC requires card marks.)
5192 
5193     // Get the Klass* for both src and dest
5194     Node* src_klass  = load_object_klass(src);
5195     Node* dest_klass = load_object_klass(dest);
5196 
5197     // Generate the subtype check.
5198     // This might fold up statically, or then again it might not.
5199     //
5200     // Non-static example:  Copying List&lt;String&gt;.elements to a new String[].
5201     // The backing store for a List&lt;String&gt; is always an Object[],
5202     // but its elements are always type String, if the generic types
5203     // are correct at the source level.
5204     //
5205     // Test S[] against D[], not S against D, because (probably)
5206     // the secondary supertype cache is less busy for S[] than S.
5207     // This usually only matters when D is an interface.
5208     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5209     // Plug failing path into checked_oop_disjoint_arraycopy
5210     if (not_subtype_ctrl != top()) {
5211       PreserveJVMState pjvms(this);
5212       set_control(not_subtype_ctrl);
5213       // (At this point we can assume disjoint_bases, since types differ.)
5214       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
5215       Node* p1 = basic_plus_adr(dest_klass, ek_offset);
5216       Node* n1 = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p1, TypeRawPtr::BOTTOM);
5217       Node* dest_elem_klass = _gvn.transform(n1);
5218       Node* cv = generate_checkcast_arraycopy(adr_type,
5219                                               dest_elem_klass,
5220                                               src, src_offset, dest, dest_offset,
5221                                               ConvI2X(copy_length), dest_uninitialized);
5222       if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5223       checked_control = control();
5224       checked_i_o     = i_o();
5225       checked_mem     = memory(adr_type);
5226       checked_value   = cv;
5227     }
5228     // At this point we know we do not need type checks on oop stores.
5229 
5230     // Let's see if we need card marks:
5231     if (alloc != NULL &amp;&amp; use_ReduceInitialCardMarks()) {
5232       // If we do not need card marks, copy using the jint or jlong stub.
5233       copy_type = LP64_ONLY(UseCompressedOops ? T_INT : T_LONG) NOT_LP64(T_INT);
5234       assert(type2aelembytes(basic_elem_type) == type2aelembytes(copy_type),
5235              "sizes agree");
5236     }
5237   }
5238 
5239   if (!stopped()) {
5240     // Generate the fast path, if possible.
5241     PreserveJVMState pjvms(this);
5242     generate_unchecked_arraycopy(adr_type, copy_type, disjoint_bases,
5243                                  src, src_offset, dest, dest_offset,
5244                                  ConvI2X(copy_length), dest_uninitialized);
5245 
5246     // Present the results of the fast call.
5247     result_region-&gt;init_req(fast_path, control());
5248     result_i_o   -&gt;init_req(fast_path, i_o());
5249     result_memory-&gt;init_req(fast_path, memory(adr_type));
5250   }
5251 
5252   // Here are all the slow paths up to this point, in one bundle:
5253   slow_control = top();
5254   if (slow_region != NULL)
5255     slow_control = _gvn.transform(slow_region);
5256   DEBUG_ONLY(slow_region = (RegionNode*)badAddress);
5257 
5258   set_control(checked_control);
5259   if (!stopped()) {
5260     // Clean up after the checked call.
5261     // The returned value is either 0 or -1^K,
5262     // where K = number of partially transferred array elements.
5263     Node* cmp = _gvn.transform(new(C) CmpINode(checked_value, intcon(0)));
5264     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
5265     IfNode* iff = create_and_map_if(control(), bol, PROB_MAX, COUNT_UNKNOWN);
5266 
5267     // If it is 0, we are done, so transfer to the end.
5268     Node* checks_done = _gvn.transform(new(C) IfTrueNode(iff));
5269     result_region-&gt;init_req(checked_path, checks_done);
5270     result_i_o   -&gt;init_req(checked_path, checked_i_o);
5271     result_memory-&gt;init_req(checked_path, checked_mem);
5272 
5273     // If it is not zero, merge into the slow call.
5274     set_control( _gvn.transform(new(C) IfFalseNode(iff) ));
5275     RegionNode* slow_reg2 = new(C) RegionNode(3);
5276     PhiNode*    slow_i_o2 = new(C) PhiNode(slow_reg2, Type::ABIO);
5277     PhiNode*    slow_mem2 = new(C) PhiNode(slow_reg2, Type::MEMORY, adr_type);
5278     record_for_igvn(slow_reg2);
5279     slow_reg2  -&gt;init_req(1, slow_control);
5280     slow_i_o2  -&gt;init_req(1, slow_i_o);
5281     slow_mem2  -&gt;init_req(1, slow_mem);
5282     slow_reg2  -&gt;init_req(2, control());
5283     slow_i_o2  -&gt;init_req(2, checked_i_o);
5284     slow_mem2  -&gt;init_req(2, checked_mem);
5285 
5286     slow_control = _gvn.transform(slow_reg2);
5287     slow_i_o     = _gvn.transform(slow_i_o2);
5288     slow_mem     = _gvn.transform(slow_mem2);
5289 
5290     if (alloc != NULL) {
5291       // We'll restart from the very beginning, after zeroing the whole thing.
5292       // This can cause double writes, but that's OK since dest is brand new.
5293       // So we ignore the low 31 bits of the value returned from the stub.
5294     } else {
5295       // We must continue the copy exactly where it failed, or else
5296       // another thread might see the wrong number of writes to dest.
5297       Node* checked_offset = _gvn.transform(new(C) XorINode(checked_value, intcon(-1)));
5298       Node* slow_offset    = new(C) PhiNode(slow_reg2, TypeInt::INT);
5299       slow_offset-&gt;init_req(1, intcon(0));
5300       slow_offset-&gt;init_req(2, checked_offset);
5301       slow_offset  = _gvn.transform(slow_offset);
5302 
5303       // Adjust the arguments by the conditionally incoming offset.
5304       Node* src_off_plus  = _gvn.transform(new(C) AddINode(src_offset,  slow_offset));
5305       Node* dest_off_plus = _gvn.transform(new(C) AddINode(dest_offset, slow_offset));
5306       Node* length_minus  = _gvn.transform(new(C) SubINode(copy_length, slow_offset));
5307 
5308       // Tweak the node variables to adjust the code produced below:
5309       src_offset  = src_off_plus;
5310       dest_offset = dest_off_plus;
5311       copy_length = length_minus;
5312     }
5313   }
5314 
5315   set_control(slow_control);
5316   if (!stopped()) {
5317     // Generate the slow path, if needed.
5318     PreserveJVMState pjvms(this);   // replace_in_map may trash the map
5319 
5320     set_memory(slow_mem, adr_type);
5321     set_i_o(slow_i_o);
5322 
5323     if (dest_uninitialized) {
5324       generate_clear_array(adr_type, dest, basic_elem_type,
5325                            intcon(0), NULL,
5326                            alloc-&gt;in(AllocateNode::AllocSize));
5327     }
5328 
5329     generate_slow_arraycopy(adr_type,
5330                             src, src_offset, dest, dest_offset,
5331                             copy_length, /*dest_uninitialized*/false);
5332 
5333     result_region-&gt;init_req(slow_call_path, control());
5334     result_i_o   -&gt;init_req(slow_call_path, i_o());
5335     result_memory-&gt;init_req(slow_call_path, memory(adr_type));
5336   }
5337 
5338   // Remove unused edges.
5339   for (uint i = 1; i &lt; result_region-&gt;req(); i++) {
5340     if (result_region-&gt;in(i) == NULL)
5341       result_region-&gt;init_req(i, top());
5342   }
5343 
5344   // Finished; return the combined state.
5345   set_control( _gvn.transform(result_region));
5346   set_i_o(     _gvn.transform(result_i_o)    );
5347   set_memory(  _gvn.transform(result_memory), adr_type );
5348 
5349   // The memory edges above are precise in order to model effects around
5350   // array copies accurately to allow value numbering of field loads around
5351   // arraycopy.  Such field loads, both before and after, are common in Java
5352   // collections and similar classes involving header/array data structures.
5353   //
5354   // But with low number of register or when some registers are used or killed
5355   // by arraycopy calls it causes registers spilling on stack. See 6544710.
5356   // The next memory barrier is added to avoid it. If the arraycopy can be
5357   // optimized away (which it can, sometimes) then we can manually remove
5358   // the membar also.
5359   //
5360   // Do not let reads from the cloned object float above the arraycopy.
5361   if (alloc != NULL) {
5362     // Do not let stores that initialize this object be reordered with
5363     // a subsequent store that would make this object accessible by
5364     // other threads.
5365     // Record what AllocateNode this StoreStore protects so that
5366     // escape analysis can go from the MemBarStoreStoreNode to the
5367     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
5368     // based on the escape status of the AllocateNode.
5369     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
5370   } else if (InsertMemBarAfterArraycopy)
5371     insert_mem_bar(Op_MemBarCPUOrder);
5372 }
5373 
5374 
5375 // Helper function which determines if an arraycopy immediately follows
5376 // an allocation, with no intervening tests or other escapes for the object.
5377 AllocateArrayNode*
5378 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5379                                            RegionNode* slow_region) {
5380   if (stopped())             return NULL;  // no fast path
5381   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5382 
5383   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5384   if (alloc == NULL)  return NULL;
5385 
5386   Node* rawmem = memory(Compile::AliasIdxRaw);
5387   // Is the allocation's memory state untouched?
5388   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5389     // Bail out if there have been raw-memory effects since the allocation.
5390     // (Example:  There might have been a call or safepoint.)
5391     return NULL;
5392   }
5393   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5394   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5395     return NULL;
5396   }
5397 
5398   // There must be no unexpected observers of this allocation.
5399   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5400     Node* obs = ptr-&gt;fast_out(i);
5401     if (obs != this-&gt;map()) {
5402       return NULL;
5403     }
5404   }
5405 
5406   // This arraycopy must unconditionally follow the allocation of the ptr.
5407   Node* alloc_ctl = ptr-&gt;in(0);
5408   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5409 
5410   Node* ctl = control();
5411   while (ctl != alloc_ctl) {
5412     // There may be guards which feed into the slow_region.
5413     // Any other control flow means that we might not get a chance
5414     // to finish initializing the allocated object.
5415     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5416       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5417       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5418       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5419       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5420         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5421         continue;
5422       }
5423       // One more try:  Various low-level checks bottom out in
5424       // uncommon traps.  If the debug-info of the trap omits
5425       // any reference to the allocation, as we've already
5426       // observed, then there can be no objection to the trap.
5427       bool found_trap = false;
5428       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5429         Node* obs = not_ctl-&gt;fast_out(j);
5430         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5431             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5432           found_trap = true; break;
5433         }
5434       }
5435       if (found_trap) {
5436         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5437         continue;
5438       }
5439     }
5440     return NULL;
5441   }
5442 
5443   // If we get this far, we have an allocation which immediately
5444   // precedes the arraycopy, and we can take over zeroing the new object.
5445   // The arraycopy will finish the initialization, and provide
5446   // a new control state to which we will anchor the destination pointer.
5447 
5448   return alloc;
5449 }
5450 
5451 // Helper for initialization of arrays, creating a ClearArray.
5452 // It writes zero bits in [start..end), within the body of an array object.
5453 // The memory effects are all chained onto the 'adr_type' alias category.
5454 //
5455 // Since the object is otherwise uninitialized, we are free
5456 // to put a little "slop" around the edges of the cleared area,
5457 // as long as it does not go back into the array's header,
5458 // or beyond the array end within the heap.
5459 //
5460 // The lower edge can be rounded down to the nearest jint and the
5461 // upper edge can be rounded up to the nearest MinObjAlignmentInBytes.
5462 //
5463 // Arguments:
5464 //   adr_type           memory slice where writes are generated
5465 //   dest               oop of the destination array
5466 //   basic_elem_type    element type of the destination
5467 //   slice_idx          array index of first element to store
5468 //   slice_len          number of elements to store (or NULL)
5469 //   dest_size          total size in bytes of the array object
5470 //
5471 // Exactly one of slice_len or dest_size must be non-NULL.
5472 // If dest_size is non-NULL, zeroing extends to the end of the object.
5473 // If slice_len is non-NULL, the slice_idx value must be a constant.
5474 void
5475 LibraryCallKit::generate_clear_array(const TypePtr* adr_type,
5476                                      Node* dest,
5477                                      BasicType basic_elem_type,
5478                                      Node* slice_idx,
5479                                      Node* slice_len,
5480                                      Node* dest_size) {
5481   // one or the other but not both of slice_len and dest_size:
5482   assert((slice_len != NULL? 1: 0) + (dest_size != NULL? 1: 0) == 1, "");
5483   if (slice_len == NULL)  slice_len = top();
5484   if (dest_size == NULL)  dest_size = top();
5485 
5486   // operate on this memory slice:
5487   Node* mem = memory(adr_type); // memory slice to operate on
5488 
5489   // scaling and rounding of indexes:
5490   int scale = exact_log2(type2aelembytes(basic_elem_type));
5491   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5492   int clear_low = (-1 &lt;&lt; scale) &amp; (BytesPerInt  - 1);
5493   int bump_bit  = (-1 &lt;&lt; scale) &amp; BytesPerInt;
5494 
5495   // determine constant starts and ends
5496   const intptr_t BIG_NEG = -128;
5497   assert(BIG_NEG + 2*abase &lt; 0, "neg enough");
5498   intptr_t slice_idx_con = (intptr_t) find_int_con(slice_idx, BIG_NEG);
5499   intptr_t slice_len_con = (intptr_t) find_int_con(slice_len, BIG_NEG);
5500   if (slice_len_con == 0) {
5501     return;                     // nothing to do here
5502   }
5503   intptr_t start_con = (abase + (slice_idx_con &lt;&lt; scale)) &amp; ~clear_low;
5504   intptr_t end_con   = find_intptr_t_con(dest_size, -1);
5505   if (slice_idx_con &gt;= 0 &amp;&amp; slice_len_con &gt;= 0) {
5506     assert(end_con &lt; 0, "not two cons");
5507     end_con = round_to(abase + ((slice_idx_con + slice_len_con) &lt;&lt; scale),
5508                        BytesPerLong);
5509   }
5510 
5511   if (start_con &gt;= 0 &amp;&amp; end_con &gt;= 0) {
5512     // Constant start and end.  Simple.
5513     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5514                                        start_con, end_con, &amp;_gvn);
5515   } else if (start_con &gt;= 0 &amp;&amp; dest_size != top()) {
5516     // Constant start, pre-rounded end after the tail of the array.
5517     Node* end = dest_size;
5518     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5519                                        start_con, end, &amp;_gvn);
5520   } else if (start_con &gt;= 0 &amp;&amp; slice_len != top()) {
5521     // Constant start, non-constant end.  End needs rounding up.
5522     // End offset = round_up(abase + ((slice_idx_con + slice_len) &lt;&lt; scale), 8)
5523     intptr_t end_base  = abase + (slice_idx_con &lt;&lt; scale);
5524     int      end_round = (-1 &lt;&lt; scale) &amp; (BytesPerLong  - 1);
5525     Node*    end       = ConvI2X(slice_len);
5526     if (scale != 0)
5527       end = _gvn.transform(new(C) LShiftXNode(end, intcon(scale) ));
5528     end_base += end_round;
5529     end = _gvn.transform(new(C) AddXNode(end, MakeConX(end_base)));
5530     end = _gvn.transform(new(C) AndXNode(end, MakeConX(~end_round)));
5531     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5532                                        start_con, end, &amp;_gvn);
5533   } else if (start_con &lt; 0 &amp;&amp; dest_size != top()) {
5534     // Non-constant start, pre-rounded end after the tail of the array.
5535     // This is almost certainly a "round-to-end" operation.
5536     Node* start = slice_idx;
5537     start = ConvI2X(start);
5538     if (scale != 0)
5539       start = _gvn.transform(new(C) LShiftXNode( start, intcon(scale) ));
5540     start = _gvn.transform(new(C) AddXNode(start, MakeConX(abase)));
5541     if ((bump_bit | clear_low) != 0) {
5542       int to_clear = (bump_bit | clear_low);
5543       // Align up mod 8, then store a jint zero unconditionally
5544       // just before the mod-8 boundary.
5545       if (((abase + bump_bit) &amp; ~to_clear) - bump_bit
5546           &lt; arrayOopDesc::length_offset_in_bytes() + BytesPerInt) {
5547         bump_bit = 0;
5548         assert((abase &amp; to_clear) == 0, "array base must be long-aligned");
5549       } else {
5550         // Bump 'start' up to (or past) the next jint boundary:
5551         start = _gvn.transform(new(C) AddXNode(start, MakeConX(bump_bit)));
5552         assert((abase &amp; clear_low) == 0, "array base must be int-aligned");
5553       }
5554       // Round bumped 'start' down to jlong boundary in body of array.
5555       start = _gvn.transform(new(C) AndXNode(start, MakeConX(~to_clear)));
5556       if (bump_bit != 0) {
5557         // Store a zero to the immediately preceding jint:
5558         Node* x1 = _gvn.transform(new(C) AddXNode(start, MakeConX(-bump_bit)));
5559         Node* p1 = basic_plus_adr(dest, x1);
5560         mem = StoreNode::make(_gvn, control(), mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);
5561         mem = _gvn.transform(mem);
5562       }
5563     }
5564     Node* end = dest_size; // pre-rounded
5565     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5566                                        start, end, &amp;_gvn);
5567   } else {
5568     // Non-constant start, unrounded non-constant end.
5569     // (Nobody zeroes a random midsection of an array using this routine.)
5570     ShouldNotReachHere();       // fix caller
5571   }
5572 
5573   // Done.
5574   set_memory(mem, adr_type);
5575 }
5576 
5577 
5578 bool
5579 LibraryCallKit::generate_block_arraycopy(const TypePtr* adr_type,
5580                                          BasicType basic_elem_type,
5581                                          AllocateNode* alloc,
5582                                          Node* src,  Node* src_offset,
5583                                          Node* dest, Node* dest_offset,
5584                                          Node* dest_size, bool dest_uninitialized) {
5585   // See if there is an advantage from block transfer.
5586   int scale = exact_log2(type2aelembytes(basic_elem_type));
5587   if (scale &gt;= LogBytesPerLong)
5588     return false;               // it is already a block transfer
5589 
5590   // Look at the alignment of the starting offsets.
5591   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5592 
5593   intptr_t src_off_con  = (intptr_t) find_int_con(src_offset, -1);
5594   intptr_t dest_off_con = (intptr_t) find_int_con(dest_offset, -1);
5595   if (src_off_con &lt; 0 || dest_off_con &lt; 0)
5596     // At present, we can only understand constants.
5597     return false;
5598 
5599   intptr_t src_off  = abase + (src_off_con  &lt;&lt; scale);
5600   intptr_t dest_off = abase + (dest_off_con &lt;&lt; scale);
5601 
5602   if (((src_off | dest_off) &amp; (BytesPerLong-1)) != 0) {
5603     // Non-aligned; too bad.
5604     // One more chance:  Pick off an initial 32-bit word.
5605     // This is a common case, since abase can be odd mod 8.
5606     if (((src_off | dest_off) &amp; (BytesPerLong-1)) == BytesPerInt &amp;&amp;
5607         ((src_off ^ dest_off) &amp; (BytesPerLong-1)) == 0) {
5608       Node* sptr = basic_plus_adr(src,  src_off);
5609       Node* dptr = basic_plus_adr(dest, dest_off);
5610       Node* sval = make_load(control(), sptr, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
5611       store_to_memory(control(), dptr, sval, T_INT, adr_type, MemNode::unordered);
5612       src_off += BytesPerInt;
5613       dest_off += BytesPerInt;
5614     } else {
5615       return false;
5616     }
5617   }
5618   assert(src_off % BytesPerLong == 0, "");
5619   assert(dest_off % BytesPerLong == 0, "");
5620 
5621   // Do this copy by giant steps.
5622   Node* sptr  = basic_plus_adr(src,  src_off);
5623   Node* dptr  = basic_plus_adr(dest, dest_off);
5624   Node* countx = dest_size;
5625   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(dest_off)));
5626   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong)));
5627 
5628   bool disjoint_bases = true;   // since alloc != NULL
5629   generate_unchecked_arraycopy(adr_type, T_LONG, disjoint_bases,
5630                                sptr, NULL, dptr, NULL, countx, dest_uninitialized);
5631 
5632   return true;
5633 }
5634 
5635 
5636 // Helper function; generates code for the slow case.
5637 // We make a call to a runtime method which emulates the native method,
5638 // but without the native wrapper overhead.
5639 void
5640 LibraryCallKit::generate_slow_arraycopy(const TypePtr* adr_type,
5641                                         Node* src,  Node* src_offset,
5642                                         Node* dest, Node* dest_offset,
5643                                         Node* copy_length, bool dest_uninitialized) {
5644   assert(!dest_uninitialized, "Invariant");
5645   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON,
5646                                  OptoRuntime::slow_arraycopy_Type(),
5647                                  OptoRuntime::slow_arraycopy_Java(),
5648                                  "slow_arraycopy", adr_type,
5649                                  src, src_offset, dest, dest_offset,
5650                                  copy_length);
5651 
5652   // Handle exceptions thrown by this fellow:
5653   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
5654 }
5655 
5656 // Helper function; generates code for cases requiring runtime checks.
5657 Node*
5658 LibraryCallKit::generate_checkcast_arraycopy(const TypePtr* adr_type,
5659                                              Node* dest_elem_klass,
5660                                              Node* src,  Node* src_offset,
5661                                              Node* dest, Node* dest_offset,
5662                                              Node* copy_length, bool dest_uninitialized) {
5663   if (stopped())  return NULL;
5664 
5665   address copyfunc_addr = StubRoutines::checkcast_arraycopy(dest_uninitialized);
5666   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5667     return NULL;
5668   }
5669 
5670   // Pick out the parameters required to perform a store-check
5671   // for the target array.  This is an optimistic check.  It will
5672   // look in each non-null element's class, at the desired klass's
5673   // super_check_offset, for the desired klass.
5674   int sco_offset = in_bytes(Klass::super_check_offset_offset());
5675   Node* p3 = basic_plus_adr(dest_elem_klass, sco_offset);
5676   Node* n3 = new(C) LoadINode(NULL, memory(p3), p3, _gvn.type(p3)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered);
5677   Node* check_offset = ConvI2X(_gvn.transform(n3));
5678   Node* check_value  = dest_elem_klass;
5679 
5680   Node* src_start  = array_element_address(src,  src_offset,  T_OBJECT);
5681   Node* dest_start = array_element_address(dest, dest_offset, T_OBJECT);
5682 
5683   // (We know the arrays are never conjoint, because their types differ.)
5684   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5685                                  OptoRuntime::checkcast_arraycopy_Type(),
5686                                  copyfunc_addr, "checkcast_arraycopy", adr_type,
5687                                  // five arguments, of which two are
5688                                  // intptr_t (jlong in LP64)
5689                                  src_start, dest_start,
5690                                  copy_length XTOP,
5691                                  check_offset XTOP,
5692                                  check_value);
5693 
5694   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5695 }
5696 
5697 
5698 // Helper function; generates code for cases requiring runtime checks.
5699 Node*
5700 LibraryCallKit::generate_generic_arraycopy(const TypePtr* adr_type,
5701                                            Node* src,  Node* src_offset,
5702                                            Node* dest, Node* dest_offset,
5703                                            Node* copy_length, bool dest_uninitialized) {
5704   assert(!dest_uninitialized, "Invariant");
5705   if (stopped())  return NULL;
5706   address copyfunc_addr = StubRoutines::generic_arraycopy();
5707   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5708     return NULL;
5709   }
5710 
5711   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5712                     OptoRuntime::generic_arraycopy_Type(),
5713                     copyfunc_addr, "generic_arraycopy", adr_type,
5714                     src, src_offset, dest, dest_offset, copy_length);
5715 
5716   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5717 }
5718 
5719 // Helper function; generates the fast out-of-line call to an arraycopy stub.
5720 void
5721 LibraryCallKit::generate_unchecked_arraycopy(const TypePtr* adr_type,
5722                                              BasicType basic_elem_type,
5723                                              bool disjoint_bases,
5724                                              Node* src,  Node* src_offset,
5725                                              Node* dest, Node* dest_offset,
5726                                              Node* copy_length, bool dest_uninitialized) {
5727   if (stopped())  return;               // nothing to do
5728 
5729   Node* src_start  = src;
5730   Node* dest_start = dest;
5731   if (src_offset != NULL || dest_offset != NULL) {
5732     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5733     src_start  = array_element_address(src,  src_offset,  basic_elem_type);
5734     dest_start = array_element_address(dest, dest_offset, basic_elem_type);
5735   }
5736 
5737   // Figure out which arraycopy runtime method to call.
5738   const char* copyfunc_name = "arraycopy";
5739   address     copyfunc_addr =
5740       basictype2arraycopy(basic_elem_type, src_offset, dest_offset,
5741                           disjoint_bases, copyfunc_name, dest_uninitialized);
5742 
5743   // Call it.  Note that the count_ix value is not scaled to a byte-size.
5744   make_runtime_call(RC_LEAF|RC_NO_FP,
5745                     OptoRuntime::fast_arraycopy_Type(),
5746                     copyfunc_addr, copyfunc_name, adr_type,
5747                     src_start, dest_start, copy_length XTOP);
5748 }
5749 
5750 //-------------inline_encodeISOArray-----------------------------------
5751 // encode char[] to byte[] in ISO_8859_1
5752 bool LibraryCallKit::inline_encodeISOArray() {
5753   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5754   // no receiver since it is static method
5755   Node *src         = argument(0);
5756   Node *src_offset  = argument(1);
5757   Node *dst         = argument(2);
5758   Node *dst_offset  = argument(3);
5759   Node *length      = argument(4);
5760 
5761   const Type* src_type = src-&gt;Value(&amp;_gvn);
5762   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5763   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5764   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5765   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5766       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5767     // failed array check
5768     return false;
5769   }
5770 
5771   // Figure out the size and type of the elements we will be copying.
5772   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5773   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5774   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5775     return false;
5776   }
5777   Node* src_start = array_element_address(src, src_offset, src_elem);
5778   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5779   // 'src_start' points to src array + scaled offset
5780   // 'dst_start' points to dst array + scaled offset
5781 
5782   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5783   Node* enc = new (C) EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5784   enc = _gvn.transform(enc);
5785   Node* res_mem = _gvn.transform(new (C) SCMemProjNode(enc));
5786   set_memory(res_mem, mtype);
5787   set_result(enc);
5788   return true;
5789 }
5790 
5791 //-------------inline_multiplyToLen-----------------------------------
5792 bool LibraryCallKit::inline_multiplyToLen() {
5793   assert(UseMultiplyToLenIntrinsic, "not implementated on this platform");
5794 
5795   address stubAddr = StubRoutines::multiplyToLen();
5796   if (stubAddr == NULL) {
5797     return false; // Intrinsic's stub is not implemented on this platform
5798   }
5799   const char* stubName = "multiplyToLen";
5800 
5801   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5802 
5803   Node* x    = argument(1);
5804   Node* xlen = argument(2);
5805   Node* y    = argument(3);
5806   Node* ylen = argument(4);
5807   Node* z    = argument(5);
5808 
5809   const Type* x_type = x-&gt;Value(&amp;_gvn);
5810   const Type* y_type = y-&gt;Value(&amp;_gvn);
5811   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5812   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5813   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5814       top_y == NULL || top_y-&gt;klass() == NULL) {
5815     // failed array check
5816     return false;
5817   }
5818 
5819   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5820   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5821   if (x_elem != T_INT || y_elem != T_INT) {
5822     return false;
5823   }
5824 
5825   // Set the original stack and the reexecute bit for the interpreter to reexecute
5826   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5827   // on the return from z array allocation in runtime.
5828   { PreserveReexecuteState preexecs(this);
5829     jvms()-&gt;set_should_reexecute(true);
5830 
5831     Node* x_start = array_element_address(x, intcon(0), x_elem);
5832     Node* y_start = array_element_address(y, intcon(0), y_elem);
5833     // 'x_start' points to x array + scaled xlen
5834     // 'y_start' points to y array + scaled ylen
5835 
5836     // Allocate the result array
5837     Node* zlen = _gvn.transform(new(C) AddINode(xlen, ylen));
5838     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5839     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5840 
5841     IdealKit ideal(this);
5842 
5843 #define __ ideal.
5844      Node* one = __ ConI(1);
5845      Node* zero = __ ConI(0);
5846      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5847      __ set(need_alloc, zero);
5848      __ set(z_alloc, z);
5849      __ if_then(z, BoolTest::eq, null()); {
5850        __ increment (need_alloc, one);
5851      } __ else_(); {
5852        // Update graphKit memory and control from IdealKit.
5853        sync_kit(ideal);
5854        Node* zlen_arg = load_array_length(z);
5855        // Update IdealKit memory and control from graphKit.
5856        __ sync_kit(this);
5857        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5858          __ increment (need_alloc, one);
5859        } __ end_if();
5860      } __ end_if();
5861 
5862      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5863        // Update graphKit memory and control from IdealKit.
5864        sync_kit(ideal);
5865        Node * narr = new_array(klass_node, zlen, 1);
5866        // Update IdealKit memory and control from graphKit.
5867        __ sync_kit(this);
5868        __ set(z_alloc, narr);
5869      } __ end_if();
5870 
5871      sync_kit(ideal);
5872      z = __ value(z_alloc);
5873      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5874      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5875      // Final sync IdealKit and GraphKit.
5876      final_sync(ideal);
5877 #undef __
5878 
5879     Node* z_start = array_element_address(z, intcon(0), T_INT);
5880 
5881     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5882                                    OptoRuntime::multiplyToLen_Type(),
5883                                    stubAddr, stubName, TypePtr::BOTTOM,
5884                                    x_start, xlen, y_start, ylen, z_start, zlen);
5885   } // original reexecute is set back here
5886 
5887   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5888   set_result(z);
5889   return true;
5890 }
5891 
5892 
5893 /**
5894  * Calculate CRC32 for byte.
5895  * int java.util.zip.CRC32.update(int crc, int b)
5896  */
5897 bool LibraryCallKit::inline_updateCRC32() {
5898   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5899   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5900   // no receiver since it is static method
5901   Node* crc  = argument(0); // type: int
5902   Node* b    = argument(1); // type: int
5903 
5904   /*
5905    *    int c = ~ crc;
5906    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5907    *    b = b ^ (c &gt;&gt;&gt; 8);
5908    *    crc = ~b;
5909    */
5910 
5911   Node* M1 = intcon(-1);
5912   crc = _gvn.transform(new (C) XorINode(crc, M1));
5913   Node* result = _gvn.transform(new (C) XorINode(crc, b));
5914   result = _gvn.transform(new (C) AndINode(result, intcon(0xFF)));
5915 
5916   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5917   Node* offset = _gvn.transform(new (C) LShiftINode(result, intcon(0x2)));
5918   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5919   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5920 
5921   crc = _gvn.transform(new (C) URShiftINode(crc, intcon(8)));
5922   result = _gvn.transform(new (C) XorINode(crc, result));
5923   result = _gvn.transform(new (C) XorINode(result, M1));
5924   set_result(result);
5925   return true;
5926 }
5927 
5928 /**
5929  * Calculate CRC32 for byte[] array.
5930  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5931  */
5932 bool LibraryCallKit::inline_updateBytesCRC32() {
5933   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5934   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5935   // no receiver since it is static method
5936   Node* crc     = argument(0); // type: int
5937   Node* src     = argument(1); // type: oop
5938   Node* offset  = argument(2); // type: int
5939   Node* length  = argument(3); // type: int
5940 
5941   const Type* src_type = src-&gt;Value(&amp;_gvn);
5942   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5943   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5944     // failed array check
5945     return false;
5946   }
5947 
5948   // Figure out the size and type of the elements we will be copying.
5949   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5950   if (src_elem != T_BYTE) {
5951     return false;
5952   }
5953 
5954   // 'src_start' points to src array + scaled offset
5955   Node* src_start = array_element_address(src, offset, src_elem);
5956 
5957   // We assume that range check is done by caller.
5958   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5959 
5960   // Call the stub.
5961   address stubAddr = StubRoutines::updateBytesCRC32();
5962   const char *stubName = "updateBytesCRC32";
5963 
5964   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5965                                  stubAddr, stubName, TypePtr::BOTTOM,
5966                                  crc, src_start, length);
5967   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5968   set_result(result);
5969   return true;
5970 }
5971 
5972 /**
5973  * Calculate CRC32 for ByteBuffer.
5974  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5975  */
5976 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5977   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5978   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5979   // no receiver since it is static method
5980   Node* crc     = argument(0); // type: int
5981   Node* src     = argument(1); // type: long
5982   Node* offset  = argument(3); // type: int
5983   Node* length  = argument(4); // type: int
5984 
5985   src = ConvL2X(src);  // adjust Java long to machine word
5986   Node* base = _gvn.transform(new (C) CastX2PNode(src));
5987   offset = ConvI2X(offset);
5988 
5989   // 'src_start' points to src array + scaled offset
5990   Node* src_start = basic_plus_adr(top(), base, offset);
5991 
5992   // Call the stub.
5993   address stubAddr = StubRoutines::updateBytesCRC32();
5994   const char *stubName = "updateBytesCRC32";
5995 
5996   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5997                                  stubAddr, stubName, TypePtr::BOTTOM,
5998                                  crc, src_start, length);
5999   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6000   set_result(result);
6001   return true;
6002 }
6003 
6004 //----------------------------inline_reference_get----------------------------
6005 // public T java.lang.ref.Reference.get();
6006 bool LibraryCallKit::inline_reference_get() {
6007   const int referent_offset = java_lang_ref_Reference::referent_offset;
6008   guarantee(referent_offset &gt; 0, "should have already been set");
6009 
6010   // Get the argument:
6011   Node* reference_obj = null_check_receiver();
6012   if (stopped()) return true;
6013 
6014   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
6015 
6016   ciInstanceKlass* klass = env()-&gt;Object_klass();
6017   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
6018 
6019   Node* no_ctrl = NULL;
6020   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
6021 
6022   // Use the pre-barrier to record the value in the referent field
6023   pre_barrier(false /* do_load */,
6024               control(),
6025               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
6026               result /* pre_val */,
6027               T_OBJECT);
6028 
6029   // Add memory barrier to prevent commoning reads from this field
6030   // across safepoint since GC can change its value.
6031   insert_mem_bar(Op_MemBarCPUOrder);
6032 
6033   set_result(result);
6034   return true;
6035 }
6036 
6037 
6038 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
6039                                               bool is_exact=true, bool is_static=false) {
6040 
6041   const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
6042   assert(tinst != NULL, "obj is null");
6043   assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
6044   assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
6045 
6046   ciField* field = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_name(ciSymbol::make(fieldName),
6047                                                                           ciSymbol::make(fieldTypeString),
6048                                                                           is_static);
6049   if (field == NULL) return (Node *) NULL;
6050   assert (field != NULL, "undefined field");
6051 
6052   // Next code  copied from Parse::do_get_xxx():
6053 
6054   // Compute address and memory type.
6055   int offset  = field-&gt;offset_in_bytes();
6056   bool is_vol = field-&gt;is_volatile();
6057   ciType* field_klass = field-&gt;type();
6058   assert(field_klass-&gt;is_loaded(), "should be loaded");
6059   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
6060   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
6061   BasicType bt = field-&gt;layout_type();
6062 
6063   // Build the resultant type of the load
6064   const Type *type;
6065   if (bt == T_OBJECT) {
6066     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
6067   } else {
6068     type = Type::get_const_basic_type(bt);
6069   }
6070 
6071   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
6072     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
6073   }
6074   // Build the load.
6075   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
6076   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, is_vol);
6077   // If reference is volatile, prevent following memory ops from
6078   // floating up past the volatile read.  Also prevents commoning
6079   // another volatile read.
6080   if (is_vol) {
6081     // Memory barrier includes bogus read of value to force load BEFORE membar
6082     insert_mem_bar(Op_MemBarAcquire, loadedField);
6083   }
6084   return loadedField;
6085 }
6086 
6087 Node * LibraryCallKit::load_container_class(Node* ctrObj) {
6088 
6089   const TypeInstPtr* tinst = _gvn.type(ctrObj)-&gt;isa_instptr();
6090   assert(tinst != NULL, "obj is null");
6091   assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
6092 
6093   ciField* field = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_name(
6094           ciSymbol::make("elementClass"), ciSymbol::make("Ljava/lang/Class;"), false);
6095   if (field == NULL) return (Node *) NULL;
6096   assert (field != NULL, "undefined field");
6097 
6098   ciType* field_klass = field-&gt;type();
6099   assert(field_klass-&gt;is_loaded(), "should be loaded");
6100   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
6101   int offset  = field-&gt;offset_in_bytes();
6102   Node* adr = basic_plus_adr(ctrObj, ctrObj, offset);
6103   BasicType bt = field-&gt;layout_type();
6104   assert(bt == T_OBJECT, "");
6105 
6106   const Type* type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
6107   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, MemNode::unordered, false);
6108   return loadedField;
6109 }
6110 
6111 bool LibraryCallKit::inline_asa_get() {
6112     assert(UseObjectLayoutIntrinsics, "not implemented on this platform");
6113 
6114 #ifndef PRODUCT
6115   tty-&gt;print_cr("Attempting to inline org.ObjectLayout.AbstractStructuredArray.get(long) ...");
6116   {
6117     ResourceMark rm;
6118     // Check the signature
6119     ciSignature* sig = callee()-&gt;signature();
6120     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
6121     assert(rtype == T_OBJECT, "return value is object");
6122     assert(sig-&gt;count() == 1, "1 arguments");
6123     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG,   "sanity");
6124   }
6125 #endif // PRODUCT
6126   Node* receiver = argument(0);
6127   Node* index    = argument(1);
6128 
6129   receiver = null_check_receiver();
6130   if (stopped()) return true;
6131 
6132   Node* bodySize = NULL;
6133   Node* elemtSize = NULL;
6134   Node* padSize = NULL;
6135 
6136   int len_off = org_ObjectLayout_AbstractStructuredArray::length_offset();
6137   int bs_off  = org_ObjectLayout_AbstractStructuredArray::bodySize_offset();
6138   int es_off  = org_ObjectLayout_AbstractStructuredArray::elementSize_offset();
6139   int ps_off  = org_ObjectLayout_AbstractStructuredArray::paddingSize_offset();
6140 
6141   Node* lnp = basic_plus_adr(top(), receiver, len_off);
6142   if (lnp == NULL) return false; // cannot happen?
6143   Node* lnv = make_load(NULL, lnp, TypeLong::LONG, T_LONG, MemNode::unordered);
6144 
6145   Node* bsp = basic_plus_adr(top(), receiver, bs_off);
6146   if (bsp == NULL) return false; // cannot happen?
6147   Node* bsv = make_load(NULL, bsp, TypeInt::INT, T_INT, MemNode::unordered);
6148 
6149   Node* esp = basic_plus_adr(top(), receiver, es_off);
6150   if (esp == NULL) return false; // cannot happen?
6151   Node* esv = make_load(NULL, esp, TypeLong::LONG, T_LONG, MemNode::unordered);
6152 
6153   Node* psp = basic_plus_adr(top(), receiver, ps_off);
6154   if (psp == NULL) return false; // cannot happen?
6155   Node* psv = make_load(NULL, psp, TypeLong::LONG, T_LONG, MemNode::unordered);
6156 
6157   /* long offset = getBodySize() + index*getElementSize() + getPaddingSize(); */
6158 
6159   Node* bs_ps_sum = _gvn.transform(new (C) AddLNode(bsv, psv));
6160   Node* idx_es_mul = _gvn.transform(new (C) MulLNode(index, esv));
6161   Node* offset = _gvn.transform(new (C) AddLNode(bs_ps_sum, idx_es_mul));
6162 
6163   const Type* t = TypeOopPtr::BOTTOM; // FIXME
6164 
6165   Node* result = make_load(NULL, offset, t, T_OBJECT, MemNode::unordered);
6166 
6167   _gvn.set_type(result, t);
6168 
6169   set_result(result);
6170 #ifndef PRODUCT
6171   tty-&gt;print_cr("Done.");
6172 #endif
6173   return false;
6174 }
6175 
6176 /*
6177  * Derive contained object at offset.
6178  * Object deriveContainedObjectAtOffset(Object container, long index)
6179  */
6180 bool LibraryCallKit::inline_derive_contained_object() {
6181     assert(UseObjectLayoutIntrinsics, "not implemented on this platform");
6182 
6183 #ifndef PRODUCT
6184   tty-&gt;print_cr("Attempting to inline sun.misc.Unsafe.deriveContainedObjectAtOffset(Object,long) ...");
6185   {
6186     ResourceMark rm;
6187     // Check the signature
6188     ciSignature* sig = callee()-&gt;signature();
6189     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
6190     assert(rtype == T_OBJECT, "return value is object");
6191     assert(sig-&gt;count() == 2, "2 arguments");
6192     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "sanity");
6193     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG,   "sanity");
6194   }
6195 #endif // PRODUCT
6196 
6197   Node* receiver = argument(0); // type: oop
6198   Node* container = argument(1); // type: oop
6199   Node* offset = argument(2); // type: long
6200 
6201   receiver  = null_check(receiver);
6202   container = null_check(container);
6203   if (stopped()) {
6204     return true;
6205   }
6206 
6207   Node* result = basic_plus_adr(container, container, offset);
6208 
6209   const TypePtr *adr_type = _gvn.type(result)-&gt;isa_ptr();
6210   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
6211   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
6212 
6213   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM); // it's true
6214   
6215   Node* heap_base_oop = container;
6216   bool need_read_barrier = offset != top() &amp;&amp; heap_base_oop != top(); // it's true
6217   
6218   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
6219 
6220   if (need_read_barrier) {
6221     insert_pre_barrier(heap_base_oop, offset, result, !(need_mem_bar));
6222   }
6223 
6224   set_result(result);
6225 
6226   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
6227 #ifndef PRODUCT
6228   tty-&gt;print_cr("Done.");
6229 #endif
6230   return true;
6231 }
6232 
6233 //------------------------------inline_aescrypt_Block-----------------------
6234 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
6235   address stubAddr;
6236   const char *stubName;
6237   assert(UseAES, "need AES instruction support");
6238 
6239   switch(id) {
6240   case vmIntrinsics::_aescrypt_encryptBlock:
6241     stubAddr = StubRoutines::aescrypt_encryptBlock();
6242     stubName = "aescrypt_encryptBlock";
6243     break;
6244   case vmIntrinsics::_aescrypt_decryptBlock:
6245     stubAddr = StubRoutines::aescrypt_decryptBlock();
6246     stubName = "aescrypt_decryptBlock";
6247     break;
6248   }
6249   if (stubAddr == NULL) return false;
6250 
6251   Node* aescrypt_object = argument(0);
6252   Node* src             = argument(1);
6253   Node* src_offset      = argument(2);
6254   Node* dest            = argument(3);
6255   Node* dest_offset     = argument(4);
6256 
6257   // (1) src and dest are arrays.
6258   const Type* src_type = src-&gt;Value(&amp;_gvn);
6259   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6260   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6261   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6262   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6263 
6264   // for the quick and dirty code we will skip all the checks.
6265   // we are just trying to get the call to be generated.
6266   Node* src_start  = src;
6267   Node* dest_start = dest;
6268   if (src_offset != NULL || dest_offset != NULL) {
6269     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6270     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6271     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6272   }
6273 
6274   // now need to get the start of its expanded key array
6275   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6276   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6277   if (k_start == NULL) return false;
6278 
6279   if (Matcher::pass_original_key_for_aes()) {
6280     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6281     // compatibility issues between Java key expansion and SPARC crypto instructions
6282     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6283     if (original_k_start == NULL) return false;
6284 
6285     // Call the stub.
6286     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6287                       stubAddr, stubName, TypePtr::BOTTOM,
6288                       src_start, dest_start, k_start, original_k_start);
6289   } else {
6290     // Call the stub.
6291     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6292                       stubAddr, stubName, TypePtr::BOTTOM,
6293                       src_start, dest_start, k_start);
6294   }
6295 
6296   return true;
6297 }
6298 
6299 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
6300 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
6301   address stubAddr;
6302   const char *stubName;
6303 
6304   assert(UseAES, "need AES instruction support");
6305 
6306   switch(id) {
6307   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
6308     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
6309     stubName = "cipherBlockChaining_encryptAESCrypt";
6310     break;
6311   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
6312     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
6313     stubName = "cipherBlockChaining_decryptAESCrypt";
6314     break;
6315   }
6316   if (stubAddr == NULL) return false;
6317 
6318   Node* cipherBlockChaining_object = argument(0);
6319   Node* src                        = argument(1);
6320   Node* src_offset                 = argument(2);
6321   Node* len                        = argument(3);
6322   Node* dest                       = argument(4);
6323   Node* dest_offset                = argument(5);
6324 
6325   // (1) src and dest are arrays.
6326   const Type* src_type = src-&gt;Value(&amp;_gvn);
6327   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6328   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6329   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6330   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
6331           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6332 
6333   // checks are the responsibility of the caller
6334   Node* src_start  = src;
6335   Node* dest_start = dest;
6336   if (src_offset != NULL || dest_offset != NULL) {
6337     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6338     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6339     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6340   }
6341 
6342   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
6343   // (because of the predicated logic executed earlier).
6344   // so we cast it here safely.
6345   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6346 
6347   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6348   if (embeddedCipherObj == NULL) return false;
6349 
6350   // cast it to what we know it will be at runtime
6351   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
6352   assert(tinst != NULL, "CBC obj is null");
6353   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
6354   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6355   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
6356 
6357   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6358   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
6359   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6360   Node* aescrypt_object = new(C) CheckCastPPNode(control(), embeddedCipherObj, xtype);
6361   aescrypt_object = _gvn.transform(aescrypt_object);
6362 
6363   // we need to get the start of the aescrypt_object's expanded key array
6364   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6365   if (k_start == NULL) return false;
6366 
6367   // similarly, get the start address of the r vector
6368   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
6369   if (objRvec == NULL) return false;
6370   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
6371 
6372   Node* cbcCrypt;
6373   if (Matcher::pass_original_key_for_aes()) {
6374     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6375     // compatibility issues between Java key expansion and SPARC crypto instructions
6376     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6377     if (original_k_start == NULL) return false;
6378 
6379     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
6380     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6381                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6382                                  stubAddr, stubName, TypePtr::BOTTOM,
6383                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6384   } else {
6385     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6386     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6387                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6388                                  stubAddr, stubName, TypePtr::BOTTOM,
6389                                  src_start, dest_start, k_start, r_start, len);
6390   }
6391 
6392   // return cipher length (int)
6393   Node* retvalue = _gvn.transform(new (C) ProjNode(cbcCrypt, TypeFunc::Parms));
6394   set_result(retvalue);
6395   return true;
6396 }
6397 
6398 //------------------------------get_key_start_from_aescrypt_object-----------------------
6399 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6400   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6401   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6402   if (objAESCryptKey == NULL) return (Node *) NULL;
6403 
6404   // now have the array, need to get the start address of the K array
6405   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6406   return k_start;
6407 }
6408 
6409 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6410 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6411   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6412   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6413   if (objAESCryptKey == NULL) return (Node *) NULL;
6414 
6415   // now have the array, need to get the start address of the lastKey array
6416   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6417   return original_k_start;
6418 }
6419 
6420 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6421 // Return node representing slow path of predicate check.
6422 // the pseudo code we want to emulate with this predicate is:
6423 // for encryption:
6424 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6425 // for decryption:
6426 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6427 //    note cipher==plain is more conservative than the original java code but that's OK
6428 //
6429 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6430   // The receiver was checked for NULL already.
6431   Node* objCBC = argument(0);
6432 
6433   // Load embeddedCipher field of CipherBlockChaining object.
6434   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6435 
6436   // get AESCrypt klass for instanceOf check
6437   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6438   // will have same classloader as CipherBlockChaining object
6439   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6440   assert(tinst != NULL, "CBCobj is null");
6441   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6442 
6443   // we want to do an instanceof comparison against the AESCrypt class
6444   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6445   if (!klass_AESCrypt-&gt;is_loaded()) {
6446     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6447     Node* ctrl = control();
6448     set_control(top()); // no regular fast path
6449     return ctrl;
6450   }
6451   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6452 
6453   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6454   Node* cmp_instof  = _gvn.transform(new (C) CmpINode(instof, intcon(1)));
6455   Node* bool_instof  = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6456 
6457   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6458 
6459   // for encryption, we are done
6460   if (!decrypting)
6461     return instof_false;  // even if it is NULL
6462 
6463   // for decryption, we need to add a further check to avoid
6464   // taking the intrinsic path when cipher and plain are the same
6465   // see the original java code for why.
6466   RegionNode* region = new(C) RegionNode(3);
6467   region-&gt;init_req(1, instof_false);
6468   Node* src = argument(1);
6469   Node* dest = argument(4);
6470   Node* cmp_src_dest = _gvn.transform(new (C) CmpPNode(src, dest));
6471   Node* bool_src_dest = _gvn.transform(new (C) BoolNode(cmp_src_dest, BoolTest::eq));
6472   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6473   region-&gt;init_req(2, src_dest_conjoint);
6474 
6475   record_for_igvn(region);
6476   return _gvn.transform(region);
6477 }
6478 
6479 //------------------------------inline_sha_implCompress-----------------------
6480 //
6481 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6482 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6483 //
6484 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6485 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6486 //
6487 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6488 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6489 //
6490 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6491   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6492 
6493   Node* sha_obj = argument(0);
6494   Node* src     = argument(1); // type oop
6495   Node* ofs     = argument(2); // type int
6496 
6497   const Type* src_type = src-&gt;Value(&amp;_gvn);
6498   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6499   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6500     // failed array check
6501     return false;
6502   }
6503   // Figure out the size and type of the elements we will be copying.
6504   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6505   if (src_elem != T_BYTE) {
6506     return false;
6507   }
6508   // 'src_start' points to src array + offset
6509   Node* src_start = array_element_address(src, ofs, src_elem);
6510   Node* state = NULL;
6511   address stubAddr;
6512   const char *stubName;
6513 
6514   switch(id) {
6515   case vmIntrinsics::_sha_implCompress:
6516     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6517     state = get_state_from_sha_object(sha_obj);
6518     stubAddr = StubRoutines::sha1_implCompress();
6519     stubName = "sha1_implCompress";
6520     break;
6521   case vmIntrinsics::_sha2_implCompress:
6522     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6523     state = get_state_from_sha_object(sha_obj);
6524     stubAddr = StubRoutines::sha256_implCompress();
6525     stubName = "sha256_implCompress";
6526     break;
6527   case vmIntrinsics::_sha5_implCompress:
6528     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6529     state = get_state_from_sha5_object(sha_obj);
6530     stubAddr = StubRoutines::sha512_implCompress();
6531     stubName = "sha512_implCompress";
6532     break;
6533   default:
6534     fatal_unexpected_iid(id);
6535     return false;
6536   }
6537   if (state == NULL) return false;
6538 
6539   // Call the stub.
6540   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6541                                  stubAddr, stubName, TypePtr::BOTTOM,
6542                                  src_start, state);
6543 
6544   return true;
6545 }
6546 
6547 //------------------------------inline_digestBase_implCompressMB-----------------------
6548 //
6549 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6550 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6551 //
6552 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6553   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6554          "need SHA1/SHA256/SHA512 instruction support");
6555   assert((uint)predicate &lt; 3, "sanity");
6556   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6557 
6558   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6559   Node* src            = argument(1); // byte[] array
6560   Node* ofs            = argument(2); // type int
6561   Node* limit          = argument(3); // type int
6562 
6563   const Type* src_type = src-&gt;Value(&amp;_gvn);
6564   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6565   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6566     // failed array check
6567     return false;
6568   }
6569   // Figure out the size and type of the elements we will be copying.
6570   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6571   if (src_elem != T_BYTE) {
6572     return false;
6573   }
6574   // 'src_start' points to src array + offset
6575   Node* src_start = array_element_address(src, ofs, src_elem);
6576 
6577   const char* klass_SHA_name = NULL;
6578   const char* stub_name = NULL;
6579   address     stub_addr = NULL;
6580   bool        long_state = false;
6581 
6582   switch (predicate) {
6583   case 0:
6584     if (UseSHA1Intrinsics) {
6585       klass_SHA_name = "sun/security/provider/SHA";
6586       stub_name = "sha1_implCompressMB";
6587       stub_addr = StubRoutines::sha1_implCompressMB();
6588     }
6589     break;
6590   case 1:
6591     if (UseSHA256Intrinsics) {
6592       klass_SHA_name = "sun/security/provider/SHA2";
6593       stub_name = "sha256_implCompressMB";
6594       stub_addr = StubRoutines::sha256_implCompressMB();
6595     }
6596     break;
6597   case 2:
6598     if (UseSHA512Intrinsics) {
6599       klass_SHA_name = "sun/security/provider/SHA5";
6600       stub_name = "sha512_implCompressMB";
6601       stub_addr = StubRoutines::sha512_implCompressMB();
6602       long_state = true;
6603     }
6604     break;
6605   default:
6606     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6607   }
6608   if (klass_SHA_name != NULL) {
6609     // get DigestBase klass to lookup for SHA klass
6610     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6611     assert(tinst != NULL, "digestBase_obj is not instance???");
6612     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6613 
6614     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6615     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6616     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6617     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6618   }
6619   return false;
6620 }
6621 //------------------------------inline_sha_implCompressMB-----------------------
6622 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6623                                                bool long_state, address stubAddr, const char *stubName,
6624                                                Node* src_start, Node* ofs, Node* limit) {
6625   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6626   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6627   Node* sha_obj = new (C) CheckCastPPNode(control(), digestBase_obj, xtype);
6628   sha_obj = _gvn.transform(sha_obj);
6629 
6630   Node* state;
6631   if (long_state) {
6632     state = get_state_from_sha5_object(sha_obj);
6633   } else {
6634     state = get_state_from_sha_object(sha_obj);
6635   }
6636   if (state == NULL) return false;
6637 
6638   // Call the stub.
6639   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6640                                  OptoRuntime::digestBase_implCompressMB_Type(),
6641                                  stubAddr, stubName, TypePtr::BOTTOM,
6642                                  src_start, state, ofs, limit);
6643   // return ofs (int)
6644   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6645   set_result(result);
6646 
6647   return true;
6648 }
6649 
6650 //------------------------------get_state_from_sha_object-----------------------
6651 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6652   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6653   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6654   if (sha_state == NULL) return (Node *) NULL;
6655 
6656   // now have the array, need to get the start address of the state array
6657   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6658   return state;
6659 }
6660 
6661 //------------------------------get_state_from_sha5_object-----------------------
6662 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6663   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6664   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6665   if (sha_state == NULL) return (Node *) NULL;
6666 
6667   // now have the array, need to get the start address of the state array
6668   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6669   return state;
6670 }
6671 
6672 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6673 // Return node representing slow path of predicate check.
6674 // the pseudo code we want to emulate with this predicate is:
6675 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6676 //
6677 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6678   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6679          "need SHA1/SHA256/SHA512 instruction support");
6680   assert((uint)predicate &lt; 3, "sanity");
6681 
6682   // The receiver was checked for NULL already.
6683   Node* digestBaseObj = argument(0);
6684 
6685   // get DigestBase klass for instanceOf check
6686   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6687   assert(tinst != NULL, "digestBaseObj is null");
6688   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6689 
6690   const char* klass_SHA_name = NULL;
6691   switch (predicate) {
6692   case 0:
6693     if (UseSHA1Intrinsics) {
6694       // we want to do an instanceof comparison against the SHA class
6695       klass_SHA_name = "sun/security/provider/SHA";
6696     }
6697     break;
6698   case 1:
6699     if (UseSHA256Intrinsics) {
6700       // we want to do an instanceof comparison against the SHA2 class
6701       klass_SHA_name = "sun/security/provider/SHA2";
6702     }
6703     break;
6704   case 2:
6705     if (UseSHA512Intrinsics) {
6706       // we want to do an instanceof comparison against the SHA5 class
6707       klass_SHA_name = "sun/security/provider/SHA5";
6708     }
6709     break;
6710   default:
6711     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6712   }
6713 
6714   ciKlass* klass_SHA = NULL;
6715   if (klass_SHA_name != NULL) {
6716     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6717   }
6718   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6719     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6720     Node* ctrl = control();
6721     set_control(top()); // no intrinsic path
6722     return ctrl;
6723   }
6724   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6725 
6726   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6727   Node* cmp_instof = _gvn.transform(new (C) CmpINode(instofSHA, intcon(1)));
6728   Node* bool_instof = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6729   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6730 
6731   return instof_false;  // even if it is NULL
6732 }
</pre></body></html>
