<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre></pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "classfile/vmSymbols.hpp"
  28 #include "compiler/compileBroker.hpp"
  29 #include "compiler/compileLog.hpp"
  30 #include "oops/objArrayKlass.hpp"
  31 #include "opto/addnode.hpp"
  32 #include "opto/callGenerator.hpp"
  33 #include "opto/cfgnode.hpp"
<a name="1" id="anc1"></a><span class="removed">  34 #include "opto/connode.hpp"</span>
  35 #include "opto/idealKit.hpp"
  36 #include "opto/mathexactnode.hpp"
  37 #include "opto/mulnode.hpp"
  38 #include "opto/parse.hpp"
  39 #include "opto/runtime.hpp"
  40 #include "opto/subnode.hpp"
  41 #include "prims/nativeLookup.hpp"
  42 #include "runtime/sharedRuntime.hpp"
  43 #include "trace/traceMacros.hpp"
  44 
  45 class LibraryIntrinsic : public InlineCallGenerator {
  46   // Extend the set of intrinsics known to the runtime:
  47  public:
  48  private:
  49   bool             _is_virtual;
  50   bool             _does_virtual_dispatch;
  51   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  52   int8_t           _last_predicate; // Last generated predicate
  53   vmIntrinsics::ID _intrinsic_id;
  54 
  55  public:
  56   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  57     : InlineCallGenerator(m),
  58       _is_virtual(is_virtual),
  59       _does_virtual_dispatch(does_virtual_dispatch),
  60       _predicates_count((int8_t)predicates_count),
  61       _last_predicate((int8_t)-1),
  62       _intrinsic_id(id)
  63   {
  64   }
  65   virtual bool is_intrinsic() const { return true; }
  66   virtual bool is_virtual()   const { return _is_virtual; }
  67   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  68   virtual int  predicates_count() const { return _predicates_count; }
  69   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  70   virtual JVMState* generate(JVMState* jvms);
  71   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  72   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  73 };
  74 
  75 
  76 // Local helper class for LibraryIntrinsic:
  77 class LibraryCallKit : public GraphKit {
  78  private:
  79   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  80   Node*             _result;        // the result node, if any
  81   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  82 
  83   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  84 
  85  public:
  86   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  87     : GraphKit(jvms),
  88       _intrinsic(intrinsic),
  89       _result(NULL)
  90   {
  91     // Check if this is a root compile.  In that case we don't have a caller.
  92     if (!jvms-&gt;has_method()) {
  93       _reexecute_sp = sp();
  94     } else {
  95       // Find out how many arguments the interpreter needs when deoptimizing
  96       // and save the stack pointer value so it can used by uncommon_trap.
  97       // We find the argument count by looking at the declared signature.
  98       bool ignored_will_link;
  99       ciSignature* declared_signature = NULL;
 100       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 101       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 102       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 103     }
 104   }
 105 
 106   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 107 
 108   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 109   int               bci()       const    { return jvms()-&gt;bci(); }
 110   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 111   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 112   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 113 
 114   bool  try_to_inline(int predicate);
 115   Node* try_to_predicate(int predicate);
 116 
 117   void push_result() {
 118     // Push the result onto the stack.
 119     if (!stopped() &amp;&amp; result() != NULL) {
 120       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 121       push_node(bt, result());
 122     }
 123   }
 124 
 125  private:
 126   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 127     fatal(err_msg_res("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
 128   }
 129 
 130   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 131   void  set_result(RegionNode* region, PhiNode* value);
 132   Node*     result() { return _result; }
 133 
 134   virtual int reexecute_sp() { return _reexecute_sp; }
 135 
 136   // Helper functions to inline natives
 137   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 138   Node* generate_slow_guard(Node* test, RegionNode* region);
 139   Node* generate_fair_guard(Node* test, RegionNode* region);
 140   Node* generate_negative_guard(Node* index, RegionNode* region,
 141                                 // resulting CastII of index:
 142                                 Node* *pos_index = NULL);
 143   Node* generate_nonpositive_guard(Node* index, bool never_negative,
 144                                    // resulting CastII of index:
 145                                    Node* *pos_index = NULL);
 146   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 147                              Node* array_length,
 148                              RegionNode* region);
 149   Node* generate_current_thread(Node* &amp;tls_output);
 150   address basictype2arraycopy(BasicType t, Node *src_offset, Node *dest_offset,
 151                               bool disjoint_bases, const char* &amp;name, bool dest_uninitialized);
 152   Node* load_mirror_from_klass(Node* klass);
 153   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 154                                       RegionNode* region, int null_path,
 155                                       int offset);
 156   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 157                                RegionNode* region, int null_path) {
 158     int offset = java_lang_Class::klass_offset_in_bytes();
 159     return load_klass_from_mirror_common(mirror, never_see_null,
 160                                          region, null_path,
 161                                          offset);
 162   }
 163   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 164                                      RegionNode* region, int null_path) {
 165     int offset = java_lang_Class::array_klass_offset_in_bytes();
 166     return load_klass_from_mirror_common(mirror, never_see_null,
 167                                          region, null_path,
 168                                          offset);
 169   }
 170   Node* generate_access_flags_guard(Node* kls,
 171                                     int modifier_mask, int modifier_bits,
 172                                     RegionNode* region);
 173   Node* generate_interface_guard(Node* kls, RegionNode* region);
 174   Node* generate_array_guard(Node* kls, RegionNode* region) {
 175     return generate_array_guard_common(kls, region, false, false);
 176   }
 177   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 178     return generate_array_guard_common(kls, region, false, true);
 179   }
 180   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 181     return generate_array_guard_common(kls, region, true, false);
 182   }
 183   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 184     return generate_array_guard_common(kls, region, true, true);
 185   }
 186   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 187                                     bool obj_array, bool not_array);
 188   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 189   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 190                                      bool is_virtual = false, bool is_static = false);
 191   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 192     return generate_method_call(method_id, false, true);
 193   }
 194   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 195     return generate_method_call(method_id, true, false);
 196   }
 197   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static);
 198 
 199   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2);
 200   Node* make_string_method_node(int opcode, Node* str1, Node* str2);
 201   bool inline_string_compareTo();
 202   bool inline_string_indexOf();
 203   Node* string_indexOf(Node* string_object, ciTypeArray* target_array, jint offset, jint cache_i, jint md2_i);
 204   bool inline_string_equals();
 205   Node* round_double_node(Node* n);
 206   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 207   bool inline_math_native(vmIntrinsics::ID id);
 208   bool inline_trig(vmIntrinsics::ID id);
 209   bool inline_math(vmIntrinsics::ID id);
 210   template &lt;typename OverflowOp&gt;
 211   bool inline_math_overflow(Node* arg1, Node* arg2);
 212   void inline_math_mathExact(Node* math, Node* test);
 213   bool inline_math_addExactI(bool is_increment);
 214   bool inline_math_addExactL(bool is_increment);
 215   bool inline_math_multiplyExactI();
 216   bool inline_math_multiplyExactL();
 217   bool inline_math_negateExactI();
 218   bool inline_math_negateExactL();
 219   bool inline_math_subtractExactI(bool is_decrement);
 220   bool inline_math_subtractExactL(bool is_decrement);
 221   bool inline_exp();
 222   bool inline_pow();
 223   Node* finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName);
 224   bool inline_min_max(vmIntrinsics::ID id);
 225   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 226   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 227   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 228   Node* make_unsafe_address(Node* base, Node* offset);
 229   // Helper for inline_unsafe_access.
 230   // Generates the guards that check whether the result of
 231   // Unsafe.getObject should be recorded in an SATB log buffer.
 232   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
<a name="2" id="anc2"></a>
 233   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
 234   bool inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static);
 235   static bool klass_needs_init_guard(Node* kls);
 236   bool inline_unsafe_allocate();
 237   bool inline_unsafe_copyMemory();
 238   bool inline_native_currentThread();
 239 #ifdef TRACE_HAVE_INTRINSICS
 240   bool inline_native_classID();
 241   bool inline_native_threadID();
 242 #endif
 243   bool inline_native_time_funcs(address method, const char* funcName);
 244   bool inline_native_isInterrupted();
 245   bool inline_native_Class_query(vmIntrinsics::ID id);
 246   bool inline_native_subtype_check();
 247 
 248   bool inline_native_newArray();
 249   bool inline_native_getLength();
 250   bool inline_array_copyOf(bool is_copyOfRange);
 251   bool inline_array_equals();
 252   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 253   bool inline_native_clone(bool is_virtual);
 254   bool inline_native_Reflection_getCallerClass();
 255   // Helper function for inlining native object hash method
 256   bool inline_native_hashcode(bool is_virtual, bool is_static);
 257   bool inline_native_getClass();
 258 
 259   // Helper functions for inlining arraycopy
 260   bool inline_arraycopy();
 261   void generate_arraycopy(const TypePtr* adr_type,
 262                           BasicType basic_elem_type,
 263                           Node* src,  Node* src_offset,
 264                           Node* dest, Node* dest_offset,
 265                           Node* copy_length,
 266                           bool disjoint_bases = false,
 267                           bool length_never_negative = false,
 268                           RegionNode* slow_region = NULL);
 269   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 270                                                 RegionNode* slow_region);
 271   void generate_clear_array(const TypePtr* adr_type,
 272                             Node* dest,
 273                             BasicType basic_elem_type,
 274                             Node* slice_off,
 275                             Node* slice_len,
 276                             Node* slice_end);
 277   bool generate_block_arraycopy(const TypePtr* adr_type,
 278                                 BasicType basic_elem_type,
 279                                 AllocateNode* alloc,
 280                                 Node* src,  Node* src_offset,
 281                                 Node* dest, Node* dest_offset,
 282                                 Node* dest_size, bool dest_uninitialized);
 283   void generate_slow_arraycopy(const TypePtr* adr_type,
 284                                Node* src,  Node* src_offset,
 285                                Node* dest, Node* dest_offset,
 286                                Node* copy_length, bool dest_uninitialized);
 287   Node* generate_checkcast_arraycopy(const TypePtr* adr_type,
 288                                      Node* dest_elem_klass,
 289                                      Node* src,  Node* src_offset,
 290                                      Node* dest, Node* dest_offset,
 291                                      Node* copy_length, bool dest_uninitialized);
 292   Node* generate_generic_arraycopy(const TypePtr* adr_type,
 293                                    Node* src,  Node* src_offset,
 294                                    Node* dest, Node* dest_offset,
 295                                    Node* copy_length, bool dest_uninitialized);
 296   void generate_unchecked_arraycopy(const TypePtr* adr_type,
 297                                     BasicType basic_elem_type,
 298                                     bool disjoint_bases,
 299                                     Node* src,  Node* src_offset,
 300                                     Node* dest, Node* dest_offset,
 301                                     Node* copy_length, bool dest_uninitialized);
 302   typedef enum { LS_xadd, LS_xchg, LS_cmpxchg } LoadStoreKind;
 303   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind);
 304   bool inline_unsafe_ordered_store(BasicType type);
 305   bool inline_unsafe_fence(vmIntrinsics::ID id);
 306   bool inline_fp_conversions(vmIntrinsics::ID id);
 307   bool inline_number_methods(vmIntrinsics::ID id);
 308   bool inline_reference_get();
<a name="3" id="anc3"></a>


 309   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 310   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 311   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 312   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 313   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 314   bool inline_sha_implCompress(vmIntrinsics::ID id);
 315   bool inline_digestBase_implCompressMB(int predicate);
 316   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 317                                  bool long_state, address stubAddr, const char *stubName,
 318                                  Node* src_start, Node* ofs, Node* limit);
 319   Node* get_state_from_sha_object(Node *sha_object);
 320   Node* get_state_from_sha5_object(Node *sha_object);
 321   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 322   bool inline_encodeISOArray();
 323   bool inline_updateCRC32();
 324   bool inline_updateBytesCRC32();
 325   bool inline_updateByteBufferCRC32();
 326   bool inline_multiplyToLen();
<a name="4" id="anc4"></a><span class="removed"> 327 </span>
<span class="removed"> 328   bool inline_profileBoolean();</span>
 329 };
 330 
 331 
 332 //---------------------------make_vm_intrinsic----------------------------
 333 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 334   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 335   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 336 
 337   ccstr disable_intr = NULL;
 338 
 339   if ((DisableIntrinsic[0] != '\0'
 340        &amp;&amp; strstr(DisableIntrinsic, vmIntrinsics::name_at(id)) != NULL) ||
 341       (method_has_option_value("DisableIntrinsic", disable_intr)
 342        &amp;&amp; strstr(disable_intr, vmIntrinsics::name_at(id)) != NULL)) {
 343     // disabled by a user request on the command line:
 344     // example: -XX:DisableIntrinsic=_hashCode,_getClass
 345     return NULL;
 346   }
 347 
 348   if (!m-&gt;is_loaded()) {
 349     // do not attempt to inline unloaded methods
 350     return NULL;
 351   }
 352 
 353   // Only a few intrinsics implement a virtual dispatch.
 354   // They are expensive calls which are also frequently overridden.
 355   if (is_virtual) {
 356     switch (id) {
 357     case vmIntrinsics::_hashCode:
 358     case vmIntrinsics::_clone:
 359       // OK, Object.hashCode and Object.clone intrinsics come in both flavors
 360       break;
 361     default:
 362       return NULL;
 363     }
 364   }
 365 
 366   // -XX:-InlineNatives disables nearly all intrinsics:
 367   if (!InlineNatives) {
 368     switch (id) {
 369     case vmIntrinsics::_indexOf:
 370     case vmIntrinsics::_compareTo:
 371     case vmIntrinsics::_equals:
 372     case vmIntrinsics::_equalsC:
 373     case vmIntrinsics::_getAndAddInt:
 374     case vmIntrinsics::_getAndAddLong:
 375     case vmIntrinsics::_getAndSetInt:
 376     case vmIntrinsics::_getAndSetLong:
 377     case vmIntrinsics::_getAndSetObject:
 378     case vmIntrinsics::_loadFence:
 379     case vmIntrinsics::_storeFence:
 380     case vmIntrinsics::_fullFence:
 381       break;  // InlineNatives does not control String.compareTo
 382     case vmIntrinsics::_Reference_get:
 383       break;  // InlineNatives does not control Reference.get
 384     default:
 385       return NULL;
 386     }
 387   }
 388 
 389   int predicates = 0;
 390   bool does_virtual_dispatch = false;
 391 
 392   switch (id) {
 393   case vmIntrinsics::_compareTo:
 394     if (!SpecialStringCompareTo)  return NULL;
 395     if (!Matcher::match_rule_supported(Op_StrComp))  return NULL;
 396     break;
 397   case vmIntrinsics::_indexOf:
 398     if (!SpecialStringIndexOf)  return NULL;
 399     break;
 400   case vmIntrinsics::_equals:
 401     if (!SpecialStringEquals)  return NULL;
 402     if (!Matcher::match_rule_supported(Op_StrEquals))  return NULL;
 403     break;
 404   case vmIntrinsics::_equalsC:
 405     if (!SpecialArraysEquals)  return NULL;
 406     if (!Matcher::match_rule_supported(Op_AryEq))  return NULL;
 407     break;
 408   case vmIntrinsics::_arraycopy:
 409     if (!InlineArrayCopy)  return NULL;
 410     break;
 411   case vmIntrinsics::_copyMemory:
 412     if (StubRoutines::unsafe_arraycopy() == NULL)  return NULL;
 413     if (!InlineArrayCopy)  return NULL;
 414     break;
 415   case vmIntrinsics::_hashCode:
 416     if (!InlineObjectHash)  return NULL;
 417     does_virtual_dispatch = true;
 418     break;
 419   case vmIntrinsics::_clone:
 420     does_virtual_dispatch = true;
 421   case vmIntrinsics::_copyOf:
 422   case vmIntrinsics::_copyOfRange:
 423     if (!InlineObjectCopy)  return NULL;
 424     // These also use the arraycopy intrinsic mechanism:
 425     if (!InlineArrayCopy)  return NULL;
 426     break;
 427   case vmIntrinsics::_encodeISOArray:
 428     if (!SpecialEncodeISOArray)  return NULL;
 429     if (!Matcher::match_rule_supported(Op_EncodeISOArray))  return NULL;
 430     break;
 431   case vmIntrinsics::_checkIndex:
 432     // We do not intrinsify this.  The optimizer does fine with it.
 433     return NULL;
 434 
 435   case vmIntrinsics::_getCallerClass:
 436     if (!UseNewReflection)  return NULL;
 437     if (!InlineReflectionGetCallerClass)  return NULL;
 438     if (SystemDictionary::reflect_CallerSensitive_klass() == NULL)  return NULL;
 439     break;
 440 
 441   case vmIntrinsics::_bitCount_i:
 442     if (!Matcher::match_rule_supported(Op_PopCountI)) return NULL;
 443     break;
 444 
 445   case vmIntrinsics::_bitCount_l:
 446     if (!Matcher::match_rule_supported(Op_PopCountL)) return NULL;
 447     break;
 448 
 449   case vmIntrinsics::_numberOfLeadingZeros_i:
 450     if (!Matcher::match_rule_supported(Op_CountLeadingZerosI)) return NULL;
 451     break;
 452 
 453   case vmIntrinsics::_numberOfLeadingZeros_l:
 454     if (!Matcher::match_rule_supported(Op_CountLeadingZerosL)) return NULL;
 455     break;
 456 
 457   case vmIntrinsics::_numberOfTrailingZeros_i:
 458     if (!Matcher::match_rule_supported(Op_CountTrailingZerosI)) return NULL;
 459     break;
 460 
 461   case vmIntrinsics::_numberOfTrailingZeros_l:
 462     if (!Matcher::match_rule_supported(Op_CountTrailingZerosL)) return NULL;
 463     break;
 464 
 465   case vmIntrinsics::_reverseBytes_c:
 466     if (!Matcher::match_rule_supported(Op_ReverseBytesUS)) return NULL;
 467     break;
 468   case vmIntrinsics::_reverseBytes_s:
 469     if (!Matcher::match_rule_supported(Op_ReverseBytesS))  return NULL;
 470     break;
 471   case vmIntrinsics::_reverseBytes_i:
 472     if (!Matcher::match_rule_supported(Op_ReverseBytesI))  return NULL;
 473     break;
 474   case vmIntrinsics::_reverseBytes_l:
 475     if (!Matcher::match_rule_supported(Op_ReverseBytesL))  return NULL;
 476     break;
 477 
 478   case vmIntrinsics::_Reference_get:
 479     // Use the intrinsic version of Reference.get() so that the value in
 480     // the referent field can be registered by the G1 pre-barrier code.
 481     // Also add memory barrier to prevent commoning reads from this field
 482     // across safepoint since GC can change it value.
 483     break;
 484 
 485   case vmIntrinsics::_compareAndSwapObject:
 486 #ifdef _LP64
 487     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_CompareAndSwapP)) return NULL;
 488 #endif
 489     break;
 490 
 491   case vmIntrinsics::_compareAndSwapLong:
 492     if (!Matcher::match_rule_supported(Op_CompareAndSwapL)) return NULL;
 493     break;
 494 
 495   case vmIntrinsics::_getAndAddInt:
 496     if (!Matcher::match_rule_supported(Op_GetAndAddI)) return NULL;
 497     break;
 498 
 499   case vmIntrinsics::_getAndAddLong:
 500     if (!Matcher::match_rule_supported(Op_GetAndAddL)) return NULL;
 501     break;
 502 
 503   case vmIntrinsics::_getAndSetInt:
 504     if (!Matcher::match_rule_supported(Op_GetAndSetI)) return NULL;
 505     break;
 506 
 507   case vmIntrinsics::_getAndSetLong:
 508     if (!Matcher::match_rule_supported(Op_GetAndSetL)) return NULL;
 509     break;
 510 
 511   case vmIntrinsics::_getAndSetObject:
 512 #ifdef _LP64
 513     if (!UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 514     if (UseCompressedOops &amp;&amp; !Matcher::match_rule_supported(Op_GetAndSetN)) return NULL;
 515     break;
 516 #else
 517     if (!Matcher::match_rule_supported(Op_GetAndSetP)) return NULL;
 518     break;
 519 #endif
 520 
<a name="5" id="anc5"></a>



 521   case vmIntrinsics::_aescrypt_encryptBlock:
 522   case vmIntrinsics::_aescrypt_decryptBlock:
 523     if (!UseAESIntrinsics) return NULL;
 524     break;
 525 
 526   case vmIntrinsics::_multiplyToLen:
 527     if (!UseMultiplyToLenIntrinsic) return NULL;
 528     break;
 529 
 530   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 531   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 532     if (!UseAESIntrinsics) return NULL;
 533     // these two require the predicated logic
 534     predicates = 1;
 535     break;
 536 
 537   case vmIntrinsics::_sha_implCompress:
 538     if (!UseSHA1Intrinsics) return NULL;
 539     break;
 540 
 541   case vmIntrinsics::_sha2_implCompress:
 542     if (!UseSHA256Intrinsics) return NULL;
 543     break;
 544 
 545   case vmIntrinsics::_sha5_implCompress:
 546     if (!UseSHA512Intrinsics) return NULL;
 547     break;
 548 
 549   case vmIntrinsics::_digestBase_implCompressMB:
 550     if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) return NULL;
 551     predicates = 3;
 552     break;
 553 
 554   case vmIntrinsics::_updateCRC32:
 555   case vmIntrinsics::_updateBytesCRC32:
 556   case vmIntrinsics::_updateByteBufferCRC32:
 557     if (!UseCRC32Intrinsics) return NULL;
 558     break;
 559 
 560   case vmIntrinsics::_incrementExactI:
 561   case vmIntrinsics::_addExactI:
 562     if (!Matcher::match_rule_supported(Op_OverflowAddI) || !UseMathExactIntrinsics) return NULL;
 563     break;
 564   case vmIntrinsics::_incrementExactL:
 565   case vmIntrinsics::_addExactL:
 566     if (!Matcher::match_rule_supported(Op_OverflowAddL) || !UseMathExactIntrinsics) return NULL;
 567     break;
 568   case vmIntrinsics::_decrementExactI:
 569   case vmIntrinsics::_subtractExactI:
 570     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 571     break;
 572   case vmIntrinsics::_decrementExactL:
 573   case vmIntrinsics::_subtractExactL:
 574     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 575     break;
 576   case vmIntrinsics::_negateExactI:
 577     if (!Matcher::match_rule_supported(Op_OverflowSubI) || !UseMathExactIntrinsics) return NULL;
 578     break;
 579   case vmIntrinsics::_negateExactL:
 580     if (!Matcher::match_rule_supported(Op_OverflowSubL) || !UseMathExactIntrinsics) return NULL;
 581     break;
 582   case vmIntrinsics::_multiplyExactI:
 583     if (!Matcher::match_rule_supported(Op_OverflowMulI) || !UseMathExactIntrinsics) return NULL;
 584     break;
 585   case vmIntrinsics::_multiplyExactL:
 586     if (!Matcher::match_rule_supported(Op_OverflowMulL) || !UseMathExactIntrinsics) return NULL;
 587     break;
 588 
 589  default:
 590     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 591     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 592     break;
 593   }
 594 
 595   // -XX:-InlineClassNatives disables natives from the Class class.
 596   // The flag applies to all reflective calls, notably Array.newArray
 597   // (visible to Java programmers as Array.newInstance).
 598   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Class() ||
 599       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_reflect_Array()) {
 600     if (!InlineClassNatives)  return NULL;
 601   }
 602 
 603   // -XX:-InlineThreadNatives disables natives from the Thread class.
 604   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Thread()) {
 605     if (!InlineThreadNatives)  return NULL;
 606   }
 607 
 608   // -XX:-InlineMathNatives disables natives from the Math,Float and Double classes.
 609   if (m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Math() ||
 610       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Float() ||
 611       m-&gt;holder()-&gt;name() == ciSymbol::java_lang_Double()) {
 612     if (!InlineMathNatives)  return NULL;
 613   }
 614 
 615   // -XX:-InlineUnsafeOps disables natives from the Unsafe class.
 616   if (m-&gt;holder()-&gt;name() == ciSymbol::sun_misc_Unsafe()) {
 617     if (!InlineUnsafeOps)  return NULL;
 618   }
 619 
 620   return new LibraryIntrinsic(m, is_virtual, predicates, does_virtual_dispatch, (vmIntrinsics::ID) id);
 621 }
 622 
 623 //----------------------register_library_intrinsics-----------------------
 624 // Initialize this file's data structures, for each Compile instance.
 625 void Compile::register_library_intrinsics() {
 626   // Nothing to do here.
 627 }
 628 
 629 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 630   LibraryCallKit kit(jvms, this);
 631   Compile* C = kit.C;
 632   int nodes = C-&gt;unique();
 633 #ifndef PRODUCT
 634   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 635     char buf[1000];
 636     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 637     tty-&gt;print_cr("Intrinsic %s", str);
 638   }
 639 #endif
 640   ciMethod* callee = kit.callee();
 641   const int bci    = kit.bci();
 642 
 643   // Try to inline the intrinsic.
 644   if (kit.try_to_inline(_last_predicate)) {
 645     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 646       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 647     }
 648     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 649     if (C-&gt;log()) {
 650       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 651                      vmIntrinsics::name_at(intrinsic_id()),
 652                      (is_virtual() ? " virtual='1'" : ""),
 653                      C-&gt;unique() - nodes);
 654     }
 655     // Push the result from the inlined method onto the stack.
 656     kit.push_result();
 657     return kit.transfer_exceptions_into_jvms();
 658   }
 659 
 660   // The intrinsic bailed out
 661   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 662     if (jvms-&gt;has_method()) {
 663       // Not a root compile.
 664       const char* msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 665       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 666     } else {
 667       // Root compile
 668       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 669                vmIntrinsics::name_at(intrinsic_id()),
 670                (is_virtual() ? " (virtual)" : ""), bci);
 671     }
 672   }
 673   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 674   return NULL;
 675 }
 676 
 677 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 678   LibraryCallKit kit(jvms, this);
 679   Compile* C = kit.C;
 680   int nodes = C-&gt;unique();
 681   _last_predicate = predicate;
 682 #ifndef PRODUCT
 683   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 684   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 685     char buf[1000];
 686     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 687     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 688   }
 689 #endif
 690   ciMethod* callee = kit.callee();
 691   const int bci    = kit.bci();
 692 
 693   Node* slow_ctl = kit.try_to_predicate(predicate);
 694   if (!kit.failing()) {
 695     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 696       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 697     }
 698     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 699     if (C-&gt;log()) {
 700       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 701                      vmIntrinsics::name_at(intrinsic_id()),
 702                      (is_virtual() ? " virtual='1'" : ""),
 703                      C-&gt;unique() - nodes);
 704     }
 705     return slow_ctl; // Could be NULL if the check folds.
 706   }
 707 
 708   // The intrinsic bailed out
 709   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 710     if (jvms-&gt;has_method()) {
 711       // Not a root compile.
 712       const char* msg = "failed to generate predicate for intrinsic";
 713       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 714     } else {
 715       // Root compile
 716       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 717                                         vmIntrinsics::name_at(intrinsic_id()),
 718                                         (is_virtual() ? " (virtual)" : ""), bci);
 719     }
 720   }
 721   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 722   return NULL;
 723 }
 724 
 725 bool LibraryCallKit::try_to_inline(int predicate) {
 726   // Handle symbolic names for otherwise undistinguished boolean switches:
 727   const bool is_store       = true;
 728   const bool is_native_ptr  = true;
 729   const bool is_static      = true;
 730   const bool is_volatile    = true;
 731 
 732   if (!jvms()-&gt;has_method()) {
 733     // Root JVMState has a null method.
 734     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 735     // Insert the memory aliasing node
 736     set_all_memory(reset_memory());
 737   }
 738   assert(merged_memory(), "");
 739 
 740 
 741   switch (intrinsic_id()) {
 742   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 743   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 744   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 745 
 746   case vmIntrinsics::_dsin:
 747   case vmIntrinsics::_dcos:
 748   case vmIntrinsics::_dtan:
 749   case vmIntrinsics::_dabs:
 750   case vmIntrinsics::_datan2:
 751   case vmIntrinsics::_dsqrt:
 752   case vmIntrinsics::_dexp:
 753   case vmIntrinsics::_dlog:
 754   case vmIntrinsics::_dlog10:
 755   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 756 
 757   case vmIntrinsics::_min:
 758   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 759 
 760   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 761   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 762   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 763   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 764   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 765   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 766   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 767   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 768   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 769   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 770   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 771   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 772 
 773   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 774 
 775   case vmIntrinsics::_compareTo:                return inline_string_compareTo();
 776   case vmIntrinsics::_indexOf:                  return inline_string_indexOf();
 777   case vmIntrinsics::_equals:                   return inline_string_equals();
 778 
 779   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,  !is_volatile);
 780   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN, !is_volatile);
 781   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 782   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 783   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 784   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,     !is_volatile);
 785   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,    !is_volatile);
 786   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 787   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 788 
 789   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,  !is_volatile);
 790   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN, !is_volatile);
 791   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 792   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 793   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 794   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,     !is_volatile);
 795   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,    !is_volatile);
 796   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 797   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 798 
 799   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,    !is_volatile);
 800   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,   !is_volatile);
 801   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,    !is_volatile);
 802   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,     !is_volatile);
 803   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,    !is_volatile);
 804   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,   !is_volatile);
 805   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,  !is_volatile);
 806   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS, !is_volatile);
 807 
 808   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,    !is_volatile);
 809   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,   !is_volatile);
 810   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,    !is_volatile);
 811   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,     !is_volatile);
 812   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,    !is_volatile);
 813   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,   !is_volatile);
 814   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,  !is_volatile);
 815   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS, !is_volatile);
 816 
 817   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   is_volatile);
 818   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  is_volatile);
 819   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     is_volatile);
 820   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    is_volatile);
 821   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     is_volatile);
 822   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      is_volatile);
 823   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     is_volatile);
 824   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    is_volatile);
 825   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   is_volatile);
 826 
 827   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   is_volatile);
 828   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  is_volatile);
 829   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     is_volatile);
 830   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    is_volatile);
 831   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     is_volatile);
 832   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      is_volatile);
 833   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     is_volatile);
 834   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    is_volatile);
 835   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   is_volatile);
 836 
 837   case vmIntrinsics::_prefetchRead:             return inline_unsafe_prefetch(!is_native_ptr, !is_store, !is_static);
 838   case vmIntrinsics::_prefetchWrite:            return inline_unsafe_prefetch(!is_native_ptr,  is_store, !is_static);
 839   case vmIntrinsics::_prefetchReadStatic:       return inline_unsafe_prefetch(!is_native_ptr, !is_store,  is_static);
 840   case vmIntrinsics::_prefetchWriteStatic:      return inline_unsafe_prefetch(!is_native_ptr,  is_store,  is_static);
 841 
 842   case vmIntrinsics::_compareAndSwapObject:     return inline_unsafe_load_store(T_OBJECT, LS_cmpxchg);
 843   case vmIntrinsics::_compareAndSwapInt:        return inline_unsafe_load_store(T_INT,    LS_cmpxchg);
 844   case vmIntrinsics::_compareAndSwapLong:       return inline_unsafe_load_store(T_LONG,   LS_cmpxchg);
 845 
 846   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_ordered_store(T_OBJECT);
 847   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_ordered_store(T_INT);
 848   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_ordered_store(T_LONG);
 849 
 850   case vmIntrinsics::_getAndAddInt:             return inline_unsafe_load_store(T_INT,    LS_xadd);
 851   case vmIntrinsics::_getAndAddLong:            return inline_unsafe_load_store(T_LONG,   LS_xadd);
 852   case vmIntrinsics::_getAndSetInt:             return inline_unsafe_load_store(T_INT,    LS_xchg);
 853   case vmIntrinsics::_getAndSetLong:            return inline_unsafe_load_store(T_LONG,   LS_xchg);
 854   case vmIntrinsics::_getAndSetObject:          return inline_unsafe_load_store(T_OBJECT, LS_xchg);
 855 
 856   case vmIntrinsics::_loadFence:
 857   case vmIntrinsics::_storeFence:
 858   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 859 
 860   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 861   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 862 
 863 #ifdef TRACE_HAVE_INTRINSICS
 864   case vmIntrinsics::_classID:                  return inline_native_classID();
 865   case vmIntrinsics::_threadID:                 return inline_native_threadID();
 866   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 867 #endif
 868   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 869   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 870   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 871   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 872   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 873   case vmIntrinsics::_getLength:                return inline_native_getLength();
 874   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 875   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 876   case vmIntrinsics::_equalsC:                  return inline_array_equals();
 877   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 878 
 879   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 880 
 881   case vmIntrinsics::_isInstance:
 882   case vmIntrinsics::_getModifiers:
 883   case vmIntrinsics::_isInterface:
 884   case vmIntrinsics::_isArray:
 885   case vmIntrinsics::_isPrimitive:
 886   case vmIntrinsics::_getSuperclass:
 887   case vmIntrinsics::_getComponentType:
 888   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 889 
 890   case vmIntrinsics::_floatToRawIntBits:
 891   case vmIntrinsics::_floatToIntBits:
 892   case vmIntrinsics::_intBitsToFloat:
 893   case vmIntrinsics::_doubleToRawLongBits:
 894   case vmIntrinsics::_doubleToLongBits:
 895   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 896 
 897   case vmIntrinsics::_numberOfLeadingZeros_i:
 898   case vmIntrinsics::_numberOfLeadingZeros_l:
 899   case vmIntrinsics::_numberOfTrailingZeros_i:
 900   case vmIntrinsics::_numberOfTrailingZeros_l:
 901   case vmIntrinsics::_bitCount_i:
 902   case vmIntrinsics::_bitCount_l:
 903   case vmIntrinsics::_reverseBytes_i:
 904   case vmIntrinsics::_reverseBytes_l:
 905   case vmIntrinsics::_reverseBytes_s:
 906   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 907 
 908   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 909 
 910   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 911 
<a name="6" id="anc6"></a>


 912   case vmIntrinsics::_aescrypt_encryptBlock:
 913   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 914 
 915   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 916   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 917     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 918 
 919   case vmIntrinsics::_sha_implCompress:
 920   case vmIntrinsics::_sha2_implCompress:
 921   case vmIntrinsics::_sha5_implCompress:
 922     return inline_sha_implCompress(intrinsic_id());
 923 
 924   case vmIntrinsics::_digestBase_implCompressMB:
 925     return inline_digestBase_implCompressMB(predicate);
 926 
 927   case vmIntrinsics::_multiplyToLen:
 928     return inline_multiplyToLen();
 929 
 930   case vmIntrinsics::_encodeISOArray:
 931     return inline_encodeISOArray();
 932 
 933   case vmIntrinsics::_updateCRC32:
 934     return inline_updateCRC32();
 935   case vmIntrinsics::_updateBytesCRC32:
 936     return inline_updateBytesCRC32();
 937   case vmIntrinsics::_updateByteBufferCRC32:
 938     return inline_updateByteBufferCRC32();
 939 
<a name="7" id="anc7"></a><span class="removed"> 940   case vmIntrinsics::_profileBoolean:</span>
<span class="removed"> 941     return inline_profileBoolean();</span>
<span class="removed"> 942 </span>
 943   default:
 944     // If you get here, it may be that someone has added a new intrinsic
 945     // to the list in vmSymbols.hpp without implementing it here.
 946 #ifndef PRODUCT
 947     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 948       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 949                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 950     }
 951 #endif
 952     return false;
 953   }
 954 }
 955 
 956 Node* LibraryCallKit::try_to_predicate(int predicate) {
 957   if (!jvms()-&gt;has_method()) {
 958     // Root JVMState has a null method.
 959     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 960     // Insert the memory aliasing node
 961     set_all_memory(reset_memory());
 962   }
 963   assert(merged_memory(), "");
 964 
 965   switch (intrinsic_id()) {
 966   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 967     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 968   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 969     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 970   case vmIntrinsics::_digestBase_implCompressMB:
 971     return inline_digestBase_implCompressMB_predicate(predicate);
 972 
 973   default:
 974     // If you get here, it may be that someone has added a new intrinsic
 975     // to the list in vmSymbols.hpp without implementing it here.
 976 #ifndef PRODUCT
 977     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 978       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 979                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 980     }
 981 #endif
 982     Node* slow_ctl = control();
 983     set_control(top()); // No fast path instrinsic
 984     return slow_ctl;
 985   }
 986 }
 987 
 988 //------------------------------set_result-------------------------------
 989 // Helper function for finishing intrinsics.
 990 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 991   record_for_igvn(region);
 992   set_control(_gvn.transform(region));
 993   set_result( _gvn.transform(value));
 994   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 995 }
 996 
 997 //------------------------------generate_guard---------------------------
 998 // Helper function for generating guarded fast-slow graph structures.
 999 // The given 'test', if true, guards a slow path.  If the test fails
1000 // then a fast path can be taken.  (We generally hope it fails.)
1001 // In all cases, GraphKit::control() is updated to the fast path.
1002 // The returned value represents the control for the slow path.
1003 // The return value is never 'top'; it is either a valid control
1004 // or NULL if it is obvious that the slow path can never be taken.
1005 // Also, if region and the slow control are not NULL, the slow edge
1006 // is appended to the region.
1007 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
1008   if (stopped()) {
1009     // Already short circuited.
1010     return NULL;
1011   }
1012 
1013   // Build an if node and its projections.
1014   // If test is true we take the slow path, which we assume is uncommon.
1015   if (_gvn.type(test) == TypeInt::ZERO) {
1016     // The slow branch is never taken.  No need to build this guard.
1017     return NULL;
1018   }
1019 
1020   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
1021 
1022   Node* if_slow = _gvn.transform(new (C) IfTrueNode(iff));
1023   if (if_slow == top()) {
1024     // The slow branch is never taken.  No need to build this guard.
1025     return NULL;
1026   }
1027 
1028   if (region != NULL)
1029     region-&gt;add_req(if_slow);
1030 
1031   Node* if_fast = _gvn.transform(new (C) IfFalseNode(iff));
1032   set_control(if_fast);
1033 
1034   return if_slow;
1035 }
1036 
1037 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
1038   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
1039 }
1040 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
1041   return generate_guard(test, region, PROB_FAIR);
1042 }
1043 
1044 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
1045                                                      Node* *pos_index) {
1046   if (stopped())
1047     return NULL;                // already stopped
1048   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
1049     return NULL;                // index is already adequately typed
1050   Node* cmp_lt = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1051   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1052   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
1053   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
1054     // Emulate effect of Parse::adjust_map_after_if.
1055     Node* ccast = new (C) CastIINode(index, TypeInt::POS);
1056     ccast-&gt;set_req(0, control());
1057     (*pos_index) = _gvn.transform(ccast);
1058   }
1059   return is_neg;
1060 }
1061 
1062 inline Node* LibraryCallKit::generate_nonpositive_guard(Node* index, bool never_negative,
1063                                                         Node* *pos_index) {
1064   if (stopped())
1065     return NULL;                // already stopped
1066   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS1)) // [1,maxint]
1067     return NULL;                // index is already adequately typed
1068   Node* cmp_le = _gvn.transform(new (C) CmpINode(index, intcon(0)));
1069   BoolTest::mask le_or_eq = (never_negative ? BoolTest::eq : BoolTest::le);
1070   Node* bol_le = _gvn.transform(new (C) BoolNode(cmp_le, le_or_eq));
1071   Node* is_notp = generate_guard(bol_le, NULL, PROB_MIN);
1072   if (is_notp != NULL &amp;&amp; pos_index != NULL) {
1073     // Emulate effect of Parse::adjust_map_after_if.
1074     Node* ccast = new (C) CastIINode(index, TypeInt::POS1);
1075     ccast-&gt;set_req(0, control());
1076     (*pos_index) = _gvn.transform(ccast);
1077   }
1078   return is_notp;
1079 }
1080 
1081 // Make sure that 'position' is a valid limit index, in [0..length].
1082 // There are two equivalent plans for checking this:
1083 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
1084 //   B. offset  &lt;=  (arrayLength - copyLength)
1085 // We require that all of the values above, except for the sum and
1086 // difference, are already known to be non-negative.
1087 // Plan A is robust in the face of overflow, if offset and copyLength
1088 // are both hugely positive.
1089 //
1090 // Plan B is less direct and intuitive, but it does not overflow at
1091 // all, since the difference of two non-negatives is always
1092 // representable.  Whenever Java methods must perform the equivalent
1093 // check they generally use Plan B instead of Plan A.
1094 // For the moment we use Plan A.
1095 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
1096                                                   Node* subseq_length,
1097                                                   Node* array_length,
1098                                                   RegionNode* region) {
1099   if (stopped())
1100     return NULL;                // already stopped
1101   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
1102   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
1103     return NULL;                // common case of whole-array copy
1104   Node* last = subseq_length;
1105   if (!zero_offset)             // last += offset
1106     last = _gvn.transform(new (C) AddINode(last, offset));
1107   Node* cmp_lt = _gvn.transform(new (C) CmpUNode(array_length, last));
1108   Node* bol_lt = _gvn.transform(new (C) BoolNode(cmp_lt, BoolTest::lt));
1109   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
1110   return is_over;
1111 }
1112 
1113 
1114 //--------------------------generate_current_thread--------------------
1115 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1116   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1117   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1118   Node* thread = _gvn.transform(new (C) ThreadLocalNode());
1119   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1120   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1121   tls_output = thread;
1122   return threadObj;
1123 }
1124 
1125 
1126 //------------------------------make_string_method_node------------------------
1127 // Helper method for String intrinsic functions. This version is called
1128 // with str1 and str2 pointing to String object nodes.
1129 //
1130 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1, Node* str2) {
1131   Node* no_ctrl = NULL;
1132 
1133   // Get start addr of string
1134   Node* str1_value   = load_String_value(no_ctrl, str1);
1135   Node* str1_offset  = load_String_offset(no_ctrl, str1);
1136   Node* str1_start   = array_element_address(str1_value, str1_offset, T_CHAR);
1137 
1138   // Get length of string 1
1139   Node* str1_len  = load_String_length(no_ctrl, str1);
1140 
1141   Node* str2_value   = load_String_value(no_ctrl, str2);
1142   Node* str2_offset  = load_String_offset(no_ctrl, str2);
1143   Node* str2_start   = array_element_address(str2_value, str2_offset, T_CHAR);
1144 
1145   Node* str2_len = NULL;
1146   Node* result = NULL;
1147 
1148   switch (opcode) {
1149   case Op_StrIndexOf:
1150     // Get length of string 2
1151     str2_len = load_String_length(no_ctrl, str2);
1152 
1153     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1154                                  str1_start, str1_len, str2_start, str2_len);
1155     break;
1156   case Op_StrComp:
1157     // Get length of string 2
1158     str2_len = load_String_length(no_ctrl, str2);
1159 
1160     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1161                                  str1_start, str1_len, str2_start, str2_len);
1162     break;
1163   case Op_StrEquals:
1164     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1165                                str1_start, str2_start, str1_len);
1166     break;
1167   default:
1168     ShouldNotReachHere();
1169     return NULL;
1170   }
1171 
1172   // All these intrinsics have checks.
1173   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1174 
1175   return _gvn.transform(result);
1176 }
1177 
1178 // Helper method for String intrinsic functions. This version is called
1179 // with str1 and str2 pointing to char[] nodes, with cnt1 and cnt2 pointing
1180 // to Int nodes containing the lenghts of str1 and str2.
1181 //
1182 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2) {
1183   Node* result = NULL;
1184   switch (opcode) {
1185   case Op_StrIndexOf:
1186     result = new (C) StrIndexOfNode(control(), memory(TypeAryPtr::CHARS),
1187                                  str1_start, cnt1, str2_start, cnt2);
1188     break;
1189   case Op_StrComp:
1190     result = new (C) StrCompNode(control(), memory(TypeAryPtr::CHARS),
1191                                  str1_start, cnt1, str2_start, cnt2);
1192     break;
1193   case Op_StrEquals:
1194     result = new (C) StrEqualsNode(control(), memory(TypeAryPtr::CHARS),
1195                                  str1_start, str2_start, cnt1);
1196     break;
1197   default:
1198     ShouldNotReachHere();
1199     return NULL;
1200   }
1201 
1202   // All these intrinsics have checks.
1203   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1204 
1205   return _gvn.transform(result);
1206 }
1207 
1208 //------------------------------inline_string_compareTo------------------------
1209 // public int java.lang.String.compareTo(String anotherString);
1210 bool LibraryCallKit::inline_string_compareTo() {
1211   Node* receiver = null_check(argument(0));
1212   Node* arg      = null_check(argument(1));
1213   if (stopped()) {
1214     return true;
1215   }
1216   set_result(make_string_method_node(Op_StrComp, receiver, arg));
1217   return true;
1218 }
1219 
1220 //------------------------------inline_string_equals------------------------
1221 bool LibraryCallKit::inline_string_equals() {
1222   Node* receiver = null_check_receiver();
1223   // NOTE: Do not null check argument for String.equals() because spec
1224   // allows to specify NULL as argument.
1225   Node* argument = this-&gt;argument(1);
1226   if (stopped()) {
1227     return true;
1228   }
1229 
1230   // paths (plus control) merge
1231   RegionNode* region = new (C) RegionNode(5);
1232   Node* phi = new (C) PhiNode(region, TypeInt::BOOL);
1233 
1234   // does source == target string?
1235   Node* cmp = _gvn.transform(new (C) CmpPNode(receiver, argument));
1236   Node* bol = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
1237 
1238   Node* if_eq = generate_slow_guard(bol, NULL);
1239   if (if_eq != NULL) {
1240     // receiver == argument
1241     phi-&gt;init_req(2, intcon(1));
1242     region-&gt;init_req(2, if_eq);
1243   }
1244 
1245   // get String klass for instanceOf
1246   ciInstanceKlass* klass = env()-&gt;String_klass();
1247 
1248   if (!stopped()) {
1249     Node* inst = gen_instanceof(argument, makecon(TypeKlassPtr::make(klass)));
1250     Node* cmp  = _gvn.transform(new (C) CmpINode(inst, intcon(1)));
1251     Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
1252 
1253     Node* inst_false = generate_guard(bol, NULL, PROB_MIN);
1254     //instanceOf == true, fallthrough
1255 
1256     if (inst_false != NULL) {
1257       phi-&gt;init_req(3, intcon(0));
1258       region-&gt;init_req(3, inst_false);
1259     }
1260   }
1261 
1262   if (!stopped()) {
1263     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(klass);
1264 
1265     // Properly cast the argument to String
1266     argument = _gvn.transform(new (C) CheckCastPPNode(control(), argument, string_type));
1267     // This path is taken only when argument's type is String:NotNull.
1268     argument = cast_not_null(argument, false);
1269 
1270     Node* no_ctrl = NULL;
1271 
1272     // Get start addr of receiver
1273     Node* receiver_val    = load_String_value(no_ctrl, receiver);
1274     Node* receiver_offset = load_String_offset(no_ctrl, receiver);
1275     Node* receiver_start = array_element_address(receiver_val, receiver_offset, T_CHAR);
1276 
1277     // Get length of receiver
1278     Node* receiver_cnt  = load_String_length(no_ctrl, receiver);
1279 
1280     // Get start addr of argument
1281     Node* argument_val    = load_String_value(no_ctrl, argument);
1282     Node* argument_offset = load_String_offset(no_ctrl, argument);
1283     Node* argument_start = array_element_address(argument_val, argument_offset, T_CHAR);
1284 
1285     // Get length of argument
1286     Node* argument_cnt  = load_String_length(no_ctrl, argument);
1287 
1288     // Check for receiver count != argument count
1289     Node* cmp = _gvn.transform(new(C) CmpINode(receiver_cnt, argument_cnt));
1290     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::ne));
1291     Node* if_ne = generate_slow_guard(bol, NULL);
1292     if (if_ne != NULL) {
1293       phi-&gt;init_req(4, intcon(0));
1294       region-&gt;init_req(4, if_ne);
1295     }
1296 
1297     // Check for count == 0 is done by assembler code for StrEquals.
1298 
1299     if (!stopped()) {
1300       Node* equals = make_string_method_node(Op_StrEquals, receiver_start, receiver_cnt, argument_start, argument_cnt);
1301       phi-&gt;init_req(1, equals);
1302       region-&gt;init_req(1, control());
1303     }
1304   }
1305 
1306   // post merge
1307   set_control(_gvn.transform(region));
1308   record_for_igvn(region);
1309 
1310   set_result(_gvn.transform(phi));
1311   return true;
1312 }
1313 
1314 //------------------------------inline_array_equals----------------------------
1315 bool LibraryCallKit::inline_array_equals() {
1316   Node* arg1 = argument(0);
1317   Node* arg2 = argument(1);
1318   set_result(_gvn.transform(new (C) AryEqNode(control(), memory(TypeAryPtr::CHARS), arg1, arg2)));
1319   return true;
1320 }
1321 
1322 // Java version of String.indexOf(constant string)
1323 // class StringDecl {
1324 //   StringDecl(char[] ca) {
1325 //     offset = 0;
1326 //     count = ca.length;
1327 //     value = ca;
1328 //   }
1329 //   int offset;
1330 //   int count;
1331 //   char[] value;
1332 // }
1333 //
1334 // static int string_indexOf_J(StringDecl string_object, char[] target_object,
1335 //                             int targetOffset, int cache_i, int md2) {
1336 //   int cache = cache_i;
1337 //   int sourceOffset = string_object.offset;
1338 //   int sourceCount = string_object.count;
1339 //   int targetCount = target_object.length;
1340 //
1341 //   int targetCountLess1 = targetCount - 1;
1342 //   int sourceEnd = sourceOffset + sourceCount - targetCountLess1;
1343 //
1344 //   char[] source = string_object.value;
1345 //   char[] target = target_object;
1346 //   int lastChar = target[targetCountLess1];
1347 //
1348 //  outer_loop:
1349 //   for (int i = sourceOffset; i &lt; sourceEnd; ) {
1350 //     int src = source[i + targetCountLess1];
1351 //     if (src == lastChar) {
1352 //       // With random strings and a 4-character alphabet,
1353 //       // reverse matching at this point sets up 0.8% fewer
1354 //       // frames, but (paradoxically) makes 0.3% more probes.
1355 //       // Since those probes are nearer the lastChar probe,
1356 //       // there is may be a net D$ win with reverse matching.
1357 //       // But, reversing loop inhibits unroll of inner loop
1358 //       // for unknown reason.  So, does running outer loop from
1359 //       // (sourceOffset - targetCountLess1) to (sourceOffset + sourceCount)
1360 //       for (int j = 0; j &lt; targetCountLess1; j++) {
1361 //         if (target[targetOffset + j] != source[i+j]) {
1362 //           if ((cache &amp; (1 &lt;&lt; source[i+j])) == 0) {
1363 //             if (md2 &lt; j+1) {
1364 //               i += j+1;
1365 //               continue outer_loop;
1366 //             }
1367 //           }
1368 //           i += md2;
1369 //           continue outer_loop;
1370 //         }
1371 //       }
1372 //       return i - sourceOffset;
1373 //     }
1374 //     if ((cache &amp; (1 &lt;&lt; src)) == 0) {
1375 //       i += targetCountLess1;
1376 //     } // using "i += targetCount;" and an "else i++;" causes a jump to jump.
1377 //     i++;
1378 //   }
1379 //   return -1;
1380 // }
1381 
1382 //------------------------------string_indexOf------------------------
1383 Node* LibraryCallKit::string_indexOf(Node* string_object, ciTypeArray* target_array, jint targetOffset_i,
1384                                      jint cache_i, jint md2_i) {
1385 
1386   Node* no_ctrl  = NULL;
1387   float likely   = PROB_LIKELY(0.9);
1388   float unlikely = PROB_UNLIKELY(0.9);
1389 
1390   const int nargs = 0; // no arguments to push back for uncommon trap in predicate
1391 
1392   Node* source        = load_String_value(no_ctrl, string_object);
1393   Node* sourceOffset  = load_String_offset(no_ctrl, string_object);
1394   Node* sourceCount   = load_String_length(no_ctrl, string_object);
1395 
1396   Node* target = _gvn.transform( makecon(TypeOopPtr::make_from_constant(target_array, true)));
1397   jint target_length = target_array-&gt;length();
1398   const TypeAry* target_array_type = TypeAry::make(TypeInt::CHAR, TypeInt::make(0, target_length, Type::WidenMin));
1399   const TypeAryPtr* target_type = TypeAryPtr::make(TypePtr::BotPTR, target_array_type, target_array-&gt;klass(), true, Type::OffsetBot);
1400 
1401   // String.value field is known to be @Stable.
1402   if (UseImplicitStableValues) {
1403     target = cast_array_to_stable(target, target_type);
1404   }
1405 
1406   IdealKit kit(this, false, true);
1407 #define __ kit.
1408   Node* zero             = __ ConI(0);
1409   Node* one              = __ ConI(1);
1410   Node* cache            = __ ConI(cache_i);
1411   Node* md2              = __ ConI(md2_i);
1412   Node* lastChar         = __ ConI(target_array-&gt;char_at(target_length - 1));
1413   Node* targetCount      = __ ConI(target_length);
1414   Node* targetCountLess1 = __ ConI(target_length - 1);
1415   Node* targetOffset     = __ ConI(targetOffset_i);
1416   Node* sourceEnd        = __ SubI(__ AddI(sourceOffset, sourceCount), targetCountLess1);
1417 
1418   IdealVariable rtn(kit), i(kit), j(kit); __ declarations_done();
1419   Node* outer_loop = __ make_label(2 /* goto */);
1420   Node* return_    = __ make_label(1);
1421 
1422   __ set(rtn,__ ConI(-1));
1423   __ loop(this, nargs, i, sourceOffset, BoolTest::lt, sourceEnd); {
1424        Node* i2  = __ AddI(__ value(i), targetCountLess1);
1425        // pin to prohibit loading of "next iteration" value which may SEGV (rare)
1426        Node* src = load_array_element(__ ctrl(), source, i2, TypeAryPtr::CHARS);
1427        __ if_then(src, BoolTest::eq, lastChar, unlikely); {
1428          __ loop(this, nargs, j, zero, BoolTest::lt, targetCountLess1); {
1429               Node* tpj = __ AddI(targetOffset, __ value(j));
1430               Node* targ = load_array_element(no_ctrl, target, tpj, target_type);
1431               Node* ipj  = __ AddI(__ value(i), __ value(j));
1432               Node* src2 = load_array_element(no_ctrl, source, ipj, TypeAryPtr::CHARS);
1433               __ if_then(targ, BoolTest::ne, src2); {
1434                 __ if_then(__ AndI(cache, __ LShiftI(one, src2)), BoolTest::eq, zero); {
1435                   __ if_then(md2, BoolTest::lt, __ AddI(__ value(j), one)); {
1436                     __ increment(i, __ AddI(__ value(j), one));
1437                     __ goto_(outer_loop);
1438                   } __ end_if(); __ dead(j);
1439                 }__ end_if(); __ dead(j);
1440                 __ increment(i, md2);
1441                 __ goto_(outer_loop);
1442               }__ end_if();
1443               __ increment(j, one);
1444          }__ end_loop(); __ dead(j);
1445          __ set(rtn, __ SubI(__ value(i), sourceOffset)); __ dead(i);
1446          __ goto_(return_);
1447        }__ end_if();
1448        __ if_then(__ AndI(cache, __ LShiftI(one, src)), BoolTest::eq, zero, likely); {
1449          __ increment(i, targetCountLess1);
1450        }__ end_if();
1451        __ increment(i, one);
1452        __ bind(outer_loop);
1453   }__ end_loop(); __ dead(i);
1454   __ bind(return_);
1455 
1456   // Final sync IdealKit and GraphKit.
1457   final_sync(kit);
1458   Node* result = __ value(rtn);
1459 #undef __
1460   C-&gt;set_has_loops(true);
1461   return result;
1462 }
1463 
1464 //------------------------------inline_string_indexOf------------------------
1465 bool LibraryCallKit::inline_string_indexOf() {
1466   Node* receiver = argument(0);
1467   Node* arg      = argument(1);
1468 
1469   Node* result;
1470   // Disable the use of pcmpestri until it can be guaranteed that
1471   // the load doesn't cross into the uncommited space.
1472   if (Matcher::has_match_rule(Op_StrIndexOf) &amp;&amp;
1473       UseSSE42Intrinsics) {
1474     // Generate SSE4.2 version of indexOf
1475     // We currently only have match rules that use SSE4.2
1476 
1477     receiver = null_check(receiver);
1478     arg      = null_check(arg);
1479     if (stopped()) {
1480       return true;
1481     }
1482 
1483     ciInstanceKlass* str_klass = env()-&gt;String_klass();
1484     const TypeOopPtr* string_type = TypeOopPtr::make_from_klass(str_klass);
1485 
1486     // Make the merge point
1487     RegionNode* result_rgn = new (C) RegionNode(4);
1488     Node*       result_phi = new (C) PhiNode(result_rgn, TypeInt::INT);
1489     Node* no_ctrl  = NULL;
1490 
1491     // Get start addr of source string
1492     Node* source = load_String_value(no_ctrl, receiver);
1493     Node* source_offset = load_String_offset(no_ctrl, receiver);
1494     Node* source_start = array_element_address(source, source_offset, T_CHAR);
1495 
1496     // Get length of source string
1497     Node* source_cnt  = load_String_length(no_ctrl, receiver);
1498 
1499     // Get start addr of substring
1500     Node* substr = load_String_value(no_ctrl, arg);
1501     Node* substr_offset = load_String_offset(no_ctrl, arg);
1502     Node* substr_start = array_element_address(substr, substr_offset, T_CHAR);
1503 
1504     // Get length of source string
1505     Node* substr_cnt  = load_String_length(no_ctrl, arg);
1506 
1507     // Check for substr count &gt; string count
1508     Node* cmp = _gvn.transform(new(C) CmpINode(substr_cnt, source_cnt));
1509     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::gt));
1510     Node* if_gt = generate_slow_guard(bol, NULL);
1511     if (if_gt != NULL) {
1512       result_phi-&gt;init_req(2, intcon(-1));
1513       result_rgn-&gt;init_req(2, if_gt);
1514     }
1515 
1516     if (!stopped()) {
1517       // Check for substr count == 0
1518       cmp = _gvn.transform(new(C) CmpINode(substr_cnt, intcon(0)));
1519       bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
1520       Node* if_zero = generate_slow_guard(bol, NULL);
1521       if (if_zero != NULL) {
1522         result_phi-&gt;init_req(3, intcon(0));
1523         result_rgn-&gt;init_req(3, if_zero);
1524       }
1525     }
1526 
1527     if (!stopped()) {
1528       result = make_string_method_node(Op_StrIndexOf, source_start, source_cnt, substr_start, substr_cnt);
1529       result_phi-&gt;init_req(1, result);
1530       result_rgn-&gt;init_req(1, control());
1531     }
1532     set_control(_gvn.transform(result_rgn));
1533     record_for_igvn(result_rgn);
1534     result = _gvn.transform(result_phi);
1535 
1536   } else { // Use LibraryCallKit::string_indexOf
1537     // don't intrinsify if argument isn't a constant string.
1538     if (!arg-&gt;is_Con()) {
1539      return false;
1540     }
1541     const TypeOopPtr* str_type = _gvn.type(arg)-&gt;isa_oopptr();
1542     if (str_type == NULL) {
1543       return false;
1544     }
1545     ciInstanceKlass* klass = env()-&gt;String_klass();
1546     ciObject* str_const = str_type-&gt;const_oop();
1547     if (str_const == NULL || str_const-&gt;klass() != klass) {
1548       return false;
1549     }
1550     ciInstance* str = str_const-&gt;as_instance();
1551     assert(str != NULL, "must be instance");
1552 
1553     ciObject* v = str-&gt;field_value_by_offset(java_lang_String::value_offset_in_bytes()).as_object();
1554     ciTypeArray* pat = v-&gt;as_type_array(); // pattern (argument) character array
1555 
1556     int o;
1557     int c;
1558     if (java_lang_String::has_offset_field()) {
1559       o = str-&gt;field_value_by_offset(java_lang_String::offset_offset_in_bytes()).as_int();
1560       c = str-&gt;field_value_by_offset(java_lang_String::count_offset_in_bytes()).as_int();
1561     } else {
1562       o = 0;
1563       c = pat-&gt;length();
1564     }
1565 
1566     // constant strings have no offset and count == length which
1567     // simplifies the resulting code somewhat so lets optimize for that.
1568     if (o != 0 || c != pat-&gt;length()) {
1569      return false;
1570     }
1571 
1572     receiver = null_check(receiver, T_OBJECT);
1573     // NOTE: No null check on the argument is needed since it's a constant String oop.
1574     if (stopped()) {
1575       return true;
1576     }
1577 
1578     // The null string as a pattern always returns 0 (match at beginning of string)
1579     if (c == 0) {
1580       set_result(intcon(0));
1581       return true;
1582     }
1583 
1584     // Generate default indexOf
1585     jchar lastChar = pat-&gt;char_at(o + (c - 1));
1586     int cache = 0;
1587     int i;
1588     for (i = 0; i &lt; c - 1; i++) {
1589       assert(i &lt; pat-&gt;length(), "out of range");
1590       cache |= (1 &lt;&lt; (pat-&gt;char_at(o + i) &amp; (sizeof(cache) * BitsPerByte - 1)));
1591     }
1592 
1593     int md2 = c;
1594     for (i = 0; i &lt; c - 1; i++) {
1595       assert(i &lt; pat-&gt;length(), "out of range");
1596       if (pat-&gt;char_at(o + i) == lastChar) {
1597         md2 = (c - 1) - i;
1598       }
1599     }
1600 
1601     result = string_indexOf(receiver, pat, o, cache, md2);
1602   }
1603   set_result(result);
1604   return true;
1605 }
1606 
1607 //--------------------------round_double_node--------------------------------
1608 // Round a double node if necessary.
1609 Node* LibraryCallKit::round_double_node(Node* n) {
1610   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1611     n = _gvn.transform(new (C) RoundDoubleNode(0, n));
1612   return n;
1613 }
1614 
1615 //------------------------------inline_math-----------------------------------
1616 // public static double Math.abs(double)
1617 // public static double Math.sqrt(double)
1618 // public static double Math.log(double)
1619 // public static double Math.log10(double)
1620 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1621   Node* arg = round_double_node(argument(0));
1622   Node* n;
1623   switch (id) {
1624   case vmIntrinsics::_dabs:   n = new (C) AbsDNode(                arg);  break;
1625   case vmIntrinsics::_dsqrt:  n = new (C) SqrtDNode(C, control(),  arg);  break;
1626   case vmIntrinsics::_dlog:   n = new (C) LogDNode(C, control(),   arg);  break;
1627   case vmIntrinsics::_dlog10: n = new (C) Log10DNode(C, control(), arg);  break;
1628   default:  fatal_unexpected_iid(id);  break;
1629   }
1630   set_result(_gvn.transform(n));
1631   return true;
1632 }
1633 
1634 //------------------------------inline_trig----------------------------------
1635 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1636 // argument reduction which will turn into a fast/slow diamond.
1637 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1638   Node* arg = round_double_node(argument(0));
1639   Node* n = NULL;
1640 
1641   switch (id) {
1642   case vmIntrinsics::_dsin:  n = new (C) SinDNode(C, control(), arg);  break;
1643   case vmIntrinsics::_dcos:  n = new (C) CosDNode(C, control(), arg);  break;
1644   case vmIntrinsics::_dtan:  n = new (C) TanDNode(C, control(), arg);  break;
1645   default:  fatal_unexpected_iid(id);  break;
1646   }
1647   n = _gvn.transform(n);
1648 
1649   // Rounding required?  Check for argument reduction!
1650   if (Matcher::strict_fp_requires_explicit_rounding) {
1651     static const double     pi_4 =  0.7853981633974483;
1652     static const double neg_pi_4 = -0.7853981633974483;
1653     // pi/2 in 80-bit extended precision
1654     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1655     // -pi/2 in 80-bit extended precision
1656     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1657     // Cutoff value for using this argument reduction technique
1658     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1659     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1660 
1661     // Pseudocode for sin:
1662     // if (x &lt;= Math.PI / 4.0) {
1663     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1664     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1665     // } else {
1666     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1667     // }
1668     // return StrictMath.sin(x);
1669 
1670     // Pseudocode for cos:
1671     // if (x &lt;= Math.PI / 4.0) {
1672     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1673     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1674     // } else {
1675     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1676     // }
1677     // return StrictMath.cos(x);
1678 
1679     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1680     // requires a special machine instruction to load it.  Instead we'll try
1681     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1682     // probably do the math inside the SIN encoding.
1683 
1684     // Make the merge point
1685     RegionNode* r = new (C) RegionNode(3);
1686     Node* phi = new (C) PhiNode(r, Type::DOUBLE);
1687 
1688     // Flatten arg so we need only 1 test
1689     Node *abs = _gvn.transform(new (C) AbsDNode(arg));
1690     // Node for PI/4 constant
1691     Node *pi4 = makecon(TypeD::make(pi_4));
1692     // Check PI/4 : abs(arg)
1693     Node *cmp = _gvn.transform(new (C) CmpDNode(pi4,abs));
1694     // Check: If PI/4 &lt; abs(arg) then go slow
1695     Node *bol = _gvn.transform(new (C) BoolNode( cmp, BoolTest::lt ));
1696     // Branch either way
1697     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1698     set_control(opt_iff(r,iff));
1699 
1700     // Set fast path result
1701     phi-&gt;init_req(2, n);
1702 
1703     // Slow path - non-blocking leaf call
1704     Node* call = NULL;
1705     switch (id) {
1706     case vmIntrinsics::_dsin:
1707       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1708                                CAST_FROM_FN_PTR(address, SharedRuntime::dsin),
1709                                "Sin", NULL, arg, top());
1710       break;
1711     case vmIntrinsics::_dcos:
1712       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1713                                CAST_FROM_FN_PTR(address, SharedRuntime::dcos),
1714                                "Cos", NULL, arg, top());
1715       break;
1716     case vmIntrinsics::_dtan:
1717       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1718                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1719                                "Tan", NULL, arg, top());
1720       break;
1721     }
1722     assert(control()-&gt;in(0) == call, "");
1723     Node* slow_result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
1724     r-&gt;init_req(1, control());
1725     phi-&gt;init_req(1, slow_result);
1726 
1727     // Post-merge
1728     set_control(_gvn.transform(r));
1729     record_for_igvn(r);
1730     n = _gvn.transform(phi);
1731 
1732     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1733   }
1734   set_result(n);
1735   return true;
1736 }
1737 
1738 Node* LibraryCallKit::finish_pow_exp(Node* result, Node* x, Node* y, const TypeFunc* call_type, address funcAddr, const char* funcName) {
1739   //-------------------
1740   //result=(result.isNaN())? funcAddr():result;
1741   // Check: If isNaN() by checking result!=result? then either trap
1742   // or go to runtime
1743   Node* cmpisnan = _gvn.transform(new (C) CmpDNode(result, result));
1744   // Build the boolean node
1745   Node* bolisnum = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::eq));
1746 
1747   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1748     { BuildCutout unless(this, bolisnum, PROB_STATIC_FREQUENT);
1749       // The pow or exp intrinsic returned a NaN, which requires a call
1750       // to the runtime.  Recompile with the runtime call.
1751       uncommon_trap(Deoptimization::Reason_intrinsic,
1752                     Deoptimization::Action_make_not_entrant);
1753     }
1754     return result;
1755   } else {
1756     // If this inlining ever returned NaN in the past, we compile a call
1757     // to the runtime to properly handle corner cases
1758 
1759     IfNode* iff = create_and_xform_if(control(), bolisnum, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1760     Node* if_slow = _gvn.transform(new (C) IfFalseNode(iff));
1761     Node* if_fast = _gvn.transform(new (C) IfTrueNode(iff));
1762 
1763     if (!if_slow-&gt;is_top()) {
1764       RegionNode* result_region = new (C) RegionNode(3);
1765       PhiNode*    result_val = new (C) PhiNode(result_region, Type::DOUBLE);
1766 
1767       result_region-&gt;init_req(1, if_fast);
1768       result_val-&gt;init_req(1, result);
1769 
1770       set_control(if_slow);
1771 
1772       const TypePtr* no_memory_effects = NULL;
1773       Node* rt = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1774                                    no_memory_effects,
1775                                    x, top(), y, y ? top() : NULL);
1776       Node* value = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+0));
1777 #ifdef ASSERT
1778       Node* value_top = _gvn.transform(new (C) ProjNode(rt, TypeFunc::Parms+1));
1779       assert(value_top == top(), "second value must be top");
1780 #endif
1781 
1782       result_region-&gt;init_req(2, control());
1783       result_val-&gt;init_req(2, value);
1784       set_control(_gvn.transform(result_region));
1785       return _gvn.transform(result_val);
1786     } else {
1787       return result;
1788     }
1789   }
1790 }
1791 
1792 //------------------------------inline_exp-------------------------------------
1793 // Inline exp instructions, if possible.  The Intel hardware only misses
1794 // really odd corner cases (+/- Infinity).  Just uncommon-trap them.
1795 bool LibraryCallKit::inline_exp() {
1796   Node* arg = round_double_node(argument(0));
1797   Node* n   = _gvn.transform(new (C) ExpDNode(C, control(), arg));
1798 
1799   n = finish_pow_exp(n, arg, NULL, OptoRuntime::Math_D_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dexp), "EXP");
1800   set_result(n);
1801 
1802   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1803   return true;
1804 }
1805 
1806 //------------------------------inline_pow-------------------------------------
1807 // Inline power instructions, if possible.
1808 bool LibraryCallKit::inline_pow() {
1809   // Pseudocode for pow
1810   // if (y == 2) {
1811   //   return x * x;
1812   // } else {
1813   //   if (x &lt;= 0.0) {
1814   //     long longy = (long)y;
1815   //     if ((double)longy == y) { // if y is long
1816   //       if (y + 1 == y) longy = 0; // huge number: even
1817   //       result = ((1&amp;longy) == 0)?-DPow(abs(x), y):DPow(abs(x), y);
1818   //     } else {
1819   //       result = NaN;
1820   //     }
1821   //   } else {
1822   //     result = DPow(x,y);
1823   //   }
1824   //   if (result != result)?  {
1825   //     result = uncommon_trap() or runtime_call();
1826   //   }
1827   //   return result;
1828   // }
1829 
1830   Node* x = round_double_node(argument(0));
1831   Node* y = round_double_node(argument(2));
1832 
1833   Node* result = NULL;
1834 
1835   Node*   const_two_node = makecon(TypeD::make(2.0));
1836   Node*   cmp_node       = _gvn.transform(new (C) CmpDNode(y, const_two_node));
1837   Node*   bool_node      = _gvn.transform(new (C) BoolNode(cmp_node, BoolTest::eq));
1838   IfNode* if_node        = create_and_xform_if(control(), bool_node, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1839   Node*   if_true        = _gvn.transform(new (C) IfTrueNode(if_node));
1840   Node*   if_false       = _gvn.transform(new (C) IfFalseNode(if_node));
1841 
1842   RegionNode* region_node = new (C) RegionNode(3);
1843   region_node-&gt;init_req(1, if_true);
1844 
1845   Node* phi_node = new (C) PhiNode(region_node, Type::DOUBLE);
1846   // special case for x^y where y == 2, we can convert it to x * x
1847   phi_node-&gt;init_req(1, _gvn.transform(new (C) MulDNode(x, x)));
1848 
1849   // set control to if_false since we will now process the false branch
1850   set_control(if_false);
1851 
1852   if (!too_many_traps(Deoptimization::Reason_intrinsic)) {
1853     // Short form: skip the fancy tests and just check for NaN result.
1854     result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1855   } else {
1856     // If this inlining ever returned NaN in the past, include all
1857     // checks + call to the runtime.
1858 
1859     // Set the merge point for If node with condition of (x &lt;= 0.0)
1860     // There are four possible paths to region node and phi node
1861     RegionNode *r = new (C) RegionNode(4);
1862     Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1863 
1864     // Build the first if node: if (x &lt;= 0.0)
1865     // Node for 0 constant
1866     Node *zeronode = makecon(TypeD::ZERO);
1867     // Check x:0
1868     Node *cmp = _gvn.transform(new (C) CmpDNode(x, zeronode));
1869     // Check: If (x&lt;=0) then go complex path
1870     Node *bol1 = _gvn.transform(new (C) BoolNode( cmp, BoolTest::le ));
1871     // Branch either way
1872     IfNode *if1 = create_and_xform_if(control(),bol1, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1873     // Fast path taken; set region slot 3
1874     Node *fast_taken = _gvn.transform(new (C) IfFalseNode(if1));
1875     r-&gt;init_req(3,fast_taken); // Capture fast-control
1876 
1877     // Fast path not-taken, i.e. slow path
1878     Node *complex_path = _gvn.transform(new (C) IfTrueNode(if1));
1879 
1880     // Set fast path result
1881     Node *fast_result = _gvn.transform(new (C) PowDNode(C, control(), x, y));
1882     phi-&gt;init_req(3, fast_result);
1883 
1884     // Complex path
1885     // Build the second if node (if y is long)
1886     // Node for (long)y
1887     Node *longy = _gvn.transform(new (C) ConvD2LNode(y));
1888     // Node for (double)((long) y)
1889     Node *doublelongy= _gvn.transform(new (C) ConvL2DNode(longy));
1890     // Check (double)((long) y) : y
1891     Node *cmplongy= _gvn.transform(new (C) CmpDNode(doublelongy, y));
1892     // Check if (y isn't long) then go to slow path
1893 
1894     Node *bol2 = _gvn.transform(new (C) BoolNode( cmplongy, BoolTest::ne ));
1895     // Branch either way
1896     IfNode *if2 = create_and_xform_if(complex_path,bol2, PROB_STATIC_INFREQUENT, COUNT_UNKNOWN);
1897     Node* ylong_path = _gvn.transform(new (C) IfFalseNode(if2));
1898 
1899     Node *slow_path = _gvn.transform(new (C) IfTrueNode(if2));
1900 
1901     // Calculate DPow(abs(x), y)*(1 &amp; (long)y)
1902     // Node for constant 1
1903     Node *conone = longcon(1);
1904     // 1&amp; (long)y
1905     Node *signnode= _gvn.transform(new (C) AndLNode(conone, longy));
1906 
1907     // A huge number is always even. Detect a huge number by checking
1908     // if y + 1 == y and set integer to be tested for parity to 0.
1909     // Required for corner case:
1910     // (long)9.223372036854776E18 = max_jlong
1911     // (double)(long)9.223372036854776E18 = 9.223372036854776E18
1912     // max_jlong is odd but 9.223372036854776E18 is even
1913     Node* yplus1 = _gvn.transform(new (C) AddDNode(y, makecon(TypeD::make(1))));
1914     Node *cmpyplus1= _gvn.transform(new (C) CmpDNode(yplus1, y));
1915     Node *bolyplus1 = _gvn.transform(new (C) BoolNode( cmpyplus1, BoolTest::eq ));
1916     Node* correctedsign = NULL;
1917     if (ConditionalMoveLimit != 0) {
1918       correctedsign = _gvn.transform( CMoveNode::make(C, NULL, bolyplus1, signnode, longcon(0), TypeLong::LONG));
1919     } else {
1920       IfNode *ifyplus1 = create_and_xform_if(ylong_path,bolyplus1, PROB_FAIR, COUNT_UNKNOWN);
1921       RegionNode *r = new (C) RegionNode(3);
1922       Node *phi = new (C) PhiNode(r, TypeLong::LONG);
1923       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyplus1)));
1924       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyplus1)));
1925       phi-&gt;init_req(1, signnode);
1926       phi-&gt;init_req(2, longcon(0));
1927       correctedsign = _gvn.transform(phi);
1928       ylong_path = _gvn.transform(r);
1929       record_for_igvn(r);
1930     }
1931 
1932     // zero node
1933     Node *conzero = longcon(0);
1934     // Check (1&amp;(long)y)==0?
1935     Node *cmpeq1 = _gvn.transform(new (C) CmpLNode(correctedsign, conzero));
1936     // Check if (1&amp;(long)y)!=0?, if so the result is negative
1937     Node *bol3 = _gvn.transform(new (C) BoolNode( cmpeq1, BoolTest::ne ));
1938     // abs(x)
1939     Node *absx=_gvn.transform(new (C) AbsDNode(x));
1940     // abs(x)^y
1941     Node *absxpowy = _gvn.transform(new (C) PowDNode(C, control(), absx, y));
1942     // -abs(x)^y
1943     Node *negabsxpowy = _gvn.transform(new (C) NegDNode (absxpowy));
1944     // (1&amp;(long)y)==1?-DPow(abs(x), y):DPow(abs(x), y)
1945     Node *signresult = NULL;
1946     if (ConditionalMoveLimit != 0) {
1947       signresult = _gvn.transform( CMoveNode::make(C, NULL, bol3, absxpowy, negabsxpowy, Type::DOUBLE));
1948     } else {
1949       IfNode *ifyeven = create_and_xform_if(ylong_path,bol3, PROB_FAIR, COUNT_UNKNOWN);
1950       RegionNode *r = new (C) RegionNode(3);
1951       Node *phi = new (C) PhiNode(r, Type::DOUBLE);
1952       r-&gt;init_req(1, _gvn.transform(new (C) IfFalseNode(ifyeven)));
1953       r-&gt;init_req(2, _gvn.transform(new (C) IfTrueNode(ifyeven)));
1954       phi-&gt;init_req(1, absxpowy);
1955       phi-&gt;init_req(2, negabsxpowy);
1956       signresult = _gvn.transform(phi);
1957       ylong_path = _gvn.transform(r);
1958       record_for_igvn(r);
1959     }
1960     // Set complex path fast result
1961     r-&gt;init_req(2, ylong_path);
1962     phi-&gt;init_req(2, signresult);
1963 
1964     static const jlong nan_bits = CONST64(0x7ff8000000000000);
1965     Node *slow_result = makecon(TypeD::make(*(double*)&amp;nan_bits)); // return NaN
1966     r-&gt;init_req(1,slow_path);
1967     phi-&gt;init_req(1,slow_result);
1968 
1969     // Post merge
1970     set_control(_gvn.transform(r));
1971     record_for_igvn(r);
1972     result = _gvn.transform(phi);
1973   }
1974 
1975   result = finish_pow_exp(result, x, y, OptoRuntime::Math_DD_D_Type(), CAST_FROM_FN_PTR(address, SharedRuntime::dpow), "POW");
1976 
1977   // control from finish_pow_exp is now input to the region node
1978   region_node-&gt;set_req(2, control());
1979   // the result from finish_pow_exp is now input to the phi node
1980   phi_node-&gt;init_req(2, result);
1981   set_control(_gvn.transform(region_node));
1982   record_for_igvn(region_node);
1983   set_result(_gvn.transform(phi_node));
1984 
1985   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1986   return true;
1987 }
1988 
1989 //------------------------------runtime_math-----------------------------
1990 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1991   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1992          "must be (DD)D or (D)D type");
1993 
1994   // Inputs
1995   Node* a = round_double_node(argument(0));
1996   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1997 
1998   const TypePtr* no_memory_effects = NULL;
1999   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
2000                                  no_memory_effects,
2001                                  a, top(), b, b ? top() : NULL);
2002   Node* value = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+0));
2003 #ifdef ASSERT
2004   Node* value_top = _gvn.transform(new (C) ProjNode(trig, TypeFunc::Parms+1));
2005   assert(value_top == top(), "second value must be top");
2006 #endif
2007 
2008   set_result(value);
2009   return true;
2010 }
2011 
2012 //------------------------------inline_math_native-----------------------------
2013 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
2014 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
2015   switch (id) {
2016     // These intrinsics are not properly supported on all hardware
2017   case vmIntrinsics::_dcos:   return Matcher::has_match_rule(Op_CosD)   ? inline_trig(id) :
2018     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
2019   case vmIntrinsics::_dsin:   return Matcher::has_match_rule(Op_SinD)   ? inline_trig(id) :
2020     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
2021   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
2022     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
2023 
2024   case vmIntrinsics::_dlog:   return Matcher::has_match_rule(Op_LogD)   ? inline_math(id) :
2025     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
2026   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
2027     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
2028 
2029     // These intrinsics are supported on all hardware
2030   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
2031   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
2032 
2033   case vmIntrinsics::_dexp:   return Matcher::has_match_rule(Op_ExpD)   ? inline_exp()    :
2034     runtime_math(OptoRuntime::Math_D_D_Type(),  FN_PTR(SharedRuntime::dexp),  "EXP");
2035   case vmIntrinsics::_dpow:   return Matcher::has_match_rule(Op_PowD)   ? inline_pow()    :
2036     runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
2037 #undef FN_PTR
2038 
2039    // These intrinsics are not yet correctly implemented
2040   case vmIntrinsics::_datan2:
2041     return false;
2042 
2043   default:
2044     fatal_unexpected_iid(id);
2045     return false;
2046   }
2047 }
2048 
2049 static bool is_simple_name(Node* n) {
2050   return (n-&gt;req() == 1         // constant
2051           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
2052           || n-&gt;is_Proj()       // parameter or return value
2053           || n-&gt;is_Phi()        // local of some sort
2054           );
2055 }
2056 
2057 //----------------------------inline_min_max-----------------------------------
2058 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
2059   set_result(generate_min_max(id, argument(0), argument(1)));
2060   return true;
2061 }
2062 
2063 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
2064   Node* bol = _gvn.transform( new (C) BoolNode(test, BoolTest::overflow) );
2065   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
2066   Node* fast_path = _gvn.transform( new (C) IfFalseNode(check));
2067   Node* slow_path = _gvn.transform( new (C) IfTrueNode(check) );
2068 
2069   {
2070     PreserveJVMState pjvms(this);
2071     PreserveReexecuteState preexecs(this);
2072     jvms()-&gt;set_should_reexecute(true);
2073 
2074     set_control(slow_path);
2075     set_i_o(i_o());
2076 
2077     uncommon_trap(Deoptimization::Reason_intrinsic,
2078                   Deoptimization::Action_none);
2079   }
2080 
2081   set_control(fast_path);
2082   set_result(math);
2083 }
2084 
2085 template &lt;typename OverflowOp&gt;
2086 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
2087   typedef typename OverflowOp::MathOp MathOp;
2088 
2089   MathOp* mathOp = new(C) MathOp(arg1, arg2);
2090   Node* operation = _gvn.transform( mathOp );
2091   Node* ofcheck = _gvn.transform( new(C) OverflowOp(arg1, arg2) );
2092   inline_math_mathExact(operation, ofcheck);
2093   return true;
2094 }
2095 
2096 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
2097   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
2098 }
2099 
2100 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
2101   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
2102 }
2103 
2104 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
2105   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
2106 }
2107 
2108 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
2109   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
2110 }
2111 
2112 bool LibraryCallKit::inline_math_negateExactI() {
2113   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
2114 }
2115 
2116 bool LibraryCallKit::inline_math_negateExactL() {
2117   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
2118 }
2119 
2120 bool LibraryCallKit::inline_math_multiplyExactI() {
2121   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
2122 }
2123 
2124 bool LibraryCallKit::inline_math_multiplyExactL() {
2125   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
2126 }
2127 
2128 Node*
2129 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
2130   // These are the candidate return value:
2131   Node* xvalue = x0;
2132   Node* yvalue = y0;
2133 
2134   if (xvalue == yvalue) {
2135     return xvalue;
2136   }
2137 
2138   bool want_max = (id == vmIntrinsics::_max);
2139 
2140   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
2141   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
2142   if (txvalue == NULL || tyvalue == NULL)  return top();
2143   // This is not really necessary, but it is consistent with a
2144   // hypothetical MaxINode::Value method:
2145   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
2146 
2147   // %%% This folding logic should (ideally) be in a different place.
2148   // Some should be inside IfNode, and there to be a more reliable
2149   // transformation of ?: style patterns into cmoves.  We also want
2150   // more powerful optimizations around cmove and min/max.
2151 
2152   // Try to find a dominating comparison of these guys.
2153   // It can simplify the index computation for Arrays.copyOf
2154   // and similar uses of System.arraycopy.
2155   // First, compute the normalized version of CmpI(x, y).
2156   int   cmp_op = Op_CmpI;
2157   Node* xkey = xvalue;
2158   Node* ykey = yvalue;
2159   Node* ideal_cmpxy = _gvn.transform(new(C) CmpINode(xkey, ykey));
2160   if (ideal_cmpxy-&gt;is_Cmp()) {
2161     // E.g., if we have CmpI(length - offset, count),
2162     // it might idealize to CmpI(length, count + offset)
2163     cmp_op = ideal_cmpxy-&gt;Opcode();
2164     xkey = ideal_cmpxy-&gt;in(1);
2165     ykey = ideal_cmpxy-&gt;in(2);
2166   }
2167 
2168   // Start by locating any relevant comparisons.
2169   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
2170   Node* cmpxy = NULL;
2171   Node* cmpyx = NULL;
2172   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
2173     Node* cmp = start_from-&gt;fast_out(k);
2174     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
2175         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
2176         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
2177       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
2178       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2179     }
2180   }
2181 
2182   const int NCMPS = 2;
2183   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2184   int cmpn;
2185   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2186     if (cmps[cmpn] != NULL)  break;     // find a result
2187   }
2188   if (cmpn &lt; NCMPS) {
2189     // Look for a dominating test that tells us the min and max.
2190     int depth = 0;                // Limit search depth for speed
2191     Node* dom = control();
2192     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2193       if (++depth &gt;= 100)  break;
2194       Node* ifproj = dom;
2195       if (!ifproj-&gt;is_Proj())  continue;
2196       Node* iff = ifproj-&gt;in(0);
2197       if (!iff-&gt;is_If())  continue;
2198       Node* bol = iff-&gt;in(1);
2199       if (!bol-&gt;is_Bool())  continue;
2200       Node* cmp = bol-&gt;in(1);
2201       if (cmp == NULL)  continue;
2202       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2203         if (cmps[cmpn] == cmp)  break;
2204       if (cmpn == NCMPS)  continue;
2205       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2206       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2207       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2208       // At this point, we know that 'x btest y' is true.
2209       switch (btest) {
2210       case BoolTest::eq:
2211         // They are proven equal, so we can collapse the min/max.
2212         // Either value is the answer.  Choose the simpler.
2213         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2214           return yvalue;
2215         return xvalue;
2216       case BoolTest::lt:          // x &lt; y
2217       case BoolTest::le:          // x &lt;= y
2218         return (want_max ? yvalue : xvalue);
2219       case BoolTest::gt:          // x &gt; y
2220       case BoolTest::ge:          // x &gt;= y
2221         return (want_max ? xvalue : yvalue);
2222       }
2223     }
2224   }
2225 
2226   // We failed to find a dominating test.
2227   // Let's pick a test that might GVN with prior tests.
2228   Node*          best_bol   = NULL;
2229   BoolTest::mask best_btest = BoolTest::illegal;
2230   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2231     Node* cmp = cmps[cmpn];
2232     if (cmp == NULL)  continue;
2233     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2234       Node* bol = cmp-&gt;fast_out(j);
2235       if (!bol-&gt;is_Bool())  continue;
2236       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2237       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2238       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2239       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2240         best_bol   = bol-&gt;as_Bool();
2241         best_btest = btest;
2242       }
2243     }
2244   }
2245 
2246   Node* answer_if_true  = NULL;
2247   Node* answer_if_false = NULL;
2248   switch (best_btest) {
2249   default:
2250     if (cmpxy == NULL)
2251       cmpxy = ideal_cmpxy;
2252     best_bol = _gvn.transform(new(C) BoolNode(cmpxy, BoolTest::lt));
2253     // and fall through:
2254   case BoolTest::lt:          // x &lt; y
2255   case BoolTest::le:          // x &lt;= y
2256     answer_if_true  = (want_max ? yvalue : xvalue);
2257     answer_if_false = (want_max ? xvalue : yvalue);
2258     break;
2259   case BoolTest::gt:          // x &gt; y
2260   case BoolTest::ge:          // x &gt;= y
2261     answer_if_true  = (want_max ? xvalue : yvalue);
2262     answer_if_false = (want_max ? yvalue : xvalue);
2263     break;
2264   }
2265 
2266   jint hi, lo;
2267   if (want_max) {
2268     // We can sharpen the minimum.
2269     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2270     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2271   } else {
2272     // We can sharpen the maximum.
2273     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2274     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2275   }
2276 
2277   // Use a flow-free graph structure, to avoid creating excess control edges
2278   // which could hinder other optimizations.
2279   // Since Math.min/max is often used with arraycopy, we want
2280   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2281   Node* cmov = CMoveNode::make(C, NULL, best_bol,
2282                                answer_if_false, answer_if_true,
2283                                TypeInt::make(lo, hi, widen));
2284 
2285   return _gvn.transform(cmov);
2286 
2287   /*
2288   // This is not as desirable as it may seem, since Min and Max
2289   // nodes do not have a full set of optimizations.
2290   // And they would interfere, anyway, with 'if' optimizations
2291   // and with CMoveI canonical forms.
2292   switch (id) {
2293   case vmIntrinsics::_min:
2294     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2295   case vmIntrinsics::_max:
2296     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2297   default:
2298     ShouldNotReachHere();
2299   }
2300   */
2301 }
2302 
2303 inline int
2304 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2305   const TypePtr* base_type = TypePtr::NULL_PTR;
2306   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2307   if (base_type == NULL) {
2308     // Unknown type.
2309     return Type::AnyPtr;
2310   } else if (base_type == TypePtr::NULL_PTR) {
2311     // Since this is a NULL+long form, we have to switch to a rawptr.
2312     base   = _gvn.transform(new (C) CastX2PNode(offset));
2313     offset = MakeConX(0);
2314     return Type::RawPtr;
2315   } else if (base_type-&gt;base() == Type::RawPtr) {
2316     return Type::RawPtr;
2317   } else if (base_type-&gt;isa_oopptr()) {
2318     // Base is never null =&gt; always a heap address.
2319     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2320       return Type::OopPtr;
2321     }
2322     // Offset is small =&gt; always a heap address.
2323     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2324     if (offset_type != NULL &amp;&amp;
2325         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2326         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2327         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2328       return Type::OopPtr;
2329     }
2330     // Otherwise, it might either be oop+off or NULL+addr.
2331     return Type::AnyPtr;
2332   } else {
2333     // No information:
2334     return Type::AnyPtr;
2335   }
2336 }
2337 
2338 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2339   int kind = classify_unsafe_addr(base, offset);
2340   if (kind == Type::RawPtr) {
2341     return basic_plus_adr(top(), base, offset);
2342   } else {
2343     return basic_plus_adr(base, offset);
2344   }
2345 }
2346 
2347 //--------------------------inline_number_methods-----------------------------
2348 // inline int     Integer.numberOfLeadingZeros(int)
2349 // inline int        Long.numberOfLeadingZeros(long)
2350 //
2351 // inline int     Integer.numberOfTrailingZeros(int)
2352 // inline int        Long.numberOfTrailingZeros(long)
2353 //
2354 // inline int     Integer.bitCount(int)
2355 // inline int        Long.bitCount(long)
2356 //
2357 // inline char  Character.reverseBytes(char)
2358 // inline short     Short.reverseBytes(short)
2359 // inline int     Integer.reverseBytes(int)
2360 // inline long       Long.reverseBytes(long)
2361 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2362   Node* arg = argument(0);
2363   Node* n;
2364   switch (id) {
2365   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new (C) CountLeadingZerosINode( arg);  break;
2366   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new (C) CountLeadingZerosLNode( arg);  break;
2367   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new (C) CountTrailingZerosINode(arg);  break;
2368   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new (C) CountTrailingZerosLNode(arg);  break;
2369   case vmIntrinsics::_bitCount_i:               n = new (C) PopCountINode(          arg);  break;
2370   case vmIntrinsics::_bitCount_l:               n = new (C) PopCountLNode(          arg);  break;
2371   case vmIntrinsics::_reverseBytes_c:           n = new (C) ReverseBytesUSNode(0,   arg);  break;
2372   case vmIntrinsics::_reverseBytes_s:           n = new (C) ReverseBytesSNode( 0,   arg);  break;
2373   case vmIntrinsics::_reverseBytes_i:           n = new (C) ReverseBytesINode( 0,   arg);  break;
2374   case vmIntrinsics::_reverseBytes_l:           n = new (C) ReverseBytesLNode( 0,   arg);  break;
2375   default:  fatal_unexpected_iid(id);  break;
2376   }
2377   set_result(_gvn.transform(n));
2378   return true;
2379 }
2380 
<a name="8" id="anc8"></a>





























2381 //----------------------------inline_unsafe_access----------------------------
2382 
2383 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2384 
2385 // Helper that guards and inserts a pre-barrier.
2386 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2387                                         Node* pre_val, bool need_mem_bar) {
2388   // We could be accessing the referent field of a reference object. If so, when G1
2389   // is enabled, we need to log the value in the referent field in an SATB buffer.
2390   // This routine performs some compile time filters and generates suitable
2391   // runtime filters that guard the pre-barrier code.
2392   // Also add memory barrier for non volatile load from the referent field
2393   // to prevent commoning of loads across safepoint.
2394   if (!UseG1GC &amp;&amp; !need_mem_bar)
2395     return;
2396 
2397   // Some compile time checks.
2398 
2399   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2400   const TypeX* otype = offset-&gt;find_intptr_t_type();
2401   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2402       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2403     // Constant offset but not the reference_offset so just return
2404     return;
2405   }
2406 
2407   // We only need to generate the runtime guards for instances.
2408   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2409   if (btype != NULL) {
2410     if (btype-&gt;isa_aryptr()) {
2411       // Array type so nothing to do
2412       return;
2413     }
2414 
2415     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2416     if (itype != NULL) {
2417       // Can the klass of base_oop be statically determined to be
2418       // _not_ a sub-class of Reference and _not_ Object?
2419       ciKlass* klass = itype-&gt;klass();
2420       if ( klass-&gt;is_loaded() &amp;&amp;
2421           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2422           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2423         return;
2424       }
2425     }
2426   }
2427 
2428   // The compile time filters did not reject base_oop/offset so
2429   // we need to generate the following runtime filters
2430   //
2431   // if (offset == java_lang_ref_Reference::_reference_offset) {
2432   //   if (instance_of(base, java.lang.ref.Reference)) {
2433   //     pre_barrier(_, pre_val, ...);
2434   //   }
2435   // }
2436 
2437   float likely   = PROB_LIKELY(  0.999);
2438   float unlikely = PROB_UNLIKELY(0.999);
2439 
2440   IdealKit ideal(this);
2441 #define __ ideal.
2442 
2443   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2444 
2445   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2446       // Update graphKit memory and control from IdealKit.
2447       sync_kit(ideal);
2448 
2449       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2450       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2451 
2452       // Update IdealKit memory and control from graphKit.
2453       __ sync_kit(this);
2454 
2455       Node* one = __ ConI(1);
2456       // is_instof == 0 if base_oop == NULL
2457       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2458 
2459         // Update graphKit from IdeakKit.
2460         sync_kit(ideal);
2461 
2462         // Use the pre-barrier to record the value in the referent field
2463         pre_barrier(false /* do_load */,
2464                     __ ctrl(),
2465                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2466                     pre_val /* pre_val */,
2467                     T_OBJECT);
2468         if (need_mem_bar) {
2469           // Add memory barrier to prevent commoning reads from this field
2470           // across safepoint since GC can change its value.
2471           insert_mem_bar(Op_MemBarCPUOrder);
2472         }
2473         // Update IdealKit from graphKit.
2474         __ sync_kit(this);
2475 
2476       } __ end_if(); // _ref_type != ref_none
2477   } __ end_if(); // offset == referent_offset
2478 
2479   // Final sync IdealKit and GraphKit.
2480   final_sync(ideal);
2481 #undef __
2482 }
2483 
2484 
2485 // Interpret Unsafe.fieldOffset cookies correctly:
2486 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2487 
2488 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2489   // Attempt to infer a sharper value type from the offset and base type.
2490   ciKlass* sharpened_klass = NULL;
2491 
2492   // See if it is an instance field, with an object type.
2493   if (alias_type-&gt;field() != NULL) {
2494     assert(!is_native_ptr, "native pointer op cannot use a java address");
2495     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2496       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2497     }
2498   }
2499 
2500   // See if it is a narrow oop array.
2501   if (adr_type-&gt;isa_aryptr()) {
2502     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2503       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2504       if (elem_type != NULL) {
2505         sharpened_klass = elem_type-&gt;klass();
2506       }
2507     }
2508   }
2509 
2510   // The sharpened class might be unloaded if there is no class loader
2511   // contraint in place.
2512   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2513     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2514 
2515 #ifndef PRODUCT
2516     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2517       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2518       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2519     }
2520 #endif
2521     // Sharpen the value type.
2522     return tjp;
2523   }
2524   return NULL;
2525 }
2526 
2527 bool LibraryCallKit::inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile) {
2528   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2529 
2530 #ifndef PRODUCT
2531   {
2532     ResourceMark rm;
2533     // Check the signatures.
2534     ciSignature* sig = callee()-&gt;signature();
2535 #ifdef ASSERT
2536     if (!is_store) {
2537       // Object getObject(Object base, int/long offset), etc.
2538       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2539       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2540           rtype = T_ADDRESS;  // it is really a C void*
2541       assert(rtype == type, "getter must return the expected value");
2542       if (!is_native_ptr) {
2543         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2544         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2545         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2546       } else {
2547         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2548         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2549       }
2550     } else {
2551       // void putObject(Object base, int/long offset, Object x), etc.
2552       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2553       if (!is_native_ptr) {
2554         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2555         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2556         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2557       } else {
2558         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2559         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2560       }
2561       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2562       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2563         vtype = T_ADDRESS;  // it is really a C void*
2564       assert(vtype == type, "putter must accept the expected value");
2565     }
2566 #endif // ASSERT
2567  }
2568 #endif //PRODUCT
2569 
2570   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2571 
2572   Node* receiver = argument(0);  // type: oop
2573 
2574   // Build address expression.  See the code in inline_unsafe_prefetch.
2575   Node* adr;
2576   Node* heap_base_oop = top();
2577   Node* offset = top();
2578   Node* val;
2579 
2580   if (!is_native_ptr) {
2581     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2582     Node* base = argument(1);  // type: oop
2583     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2584     offset = argument(2);  // type: long
2585     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2586     // to be plain byte offsets, which are also the same as those accepted
2587     // by oopDesc::field_base.
2588     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2589            "fieldOffset must be byte-scaled");
2590     // 32-bit machines ignore the high half!
2591     offset = ConvL2X(offset);
2592     adr = make_unsafe_address(base, offset);
2593     heap_base_oop = base;
2594     val = is_store ? argument(4) : NULL;
2595   } else {
2596     Node* ptr = argument(1);  // type: long
2597     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2598     adr = make_unsafe_address(NULL, ptr);
2599     val = is_store ? argument(3) : NULL;
2600   }
2601 
2602   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2603 
2604   // First guess at the value type.
2605   const Type *value_type = Type::get_const_basic_type(type);
2606 
2607   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2608   // there was not enough information to nail it down.
2609   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2610   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2611 
2612   // We will need memory barriers unless we can determine a unique
2613   // alias category for this reference.  (Note:  If for some reason
2614   // the barriers get omitted and the unsafe reference begins to "pollute"
2615   // the alias analysis of the rest of the graph, either Compile::can_alias
2616   // or Compile::must_alias will throw a diagnostic assert.)
2617   bool need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2618 
2619   // If we are reading the value of the referent field of a Reference
2620   // object (either by using Unsafe directly or through reflection)
2621   // then, if G1 is enabled, we need to record the referent in an
2622   // SATB log buffer using the pre-barrier mechanism.
2623   // Also we need to add memory barrier to prevent commoning reads
2624   // from this field across safepoint since GC can change its value.
2625   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2626                            offset != top() &amp;&amp; heap_base_oop != top();
2627 
2628   if (!is_store &amp;&amp; type == T_OBJECT) {
2629     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2630     if (tjp != NULL) {
2631       value_type = tjp;
2632     }
2633   }
2634 
2635   receiver = null_check(receiver);
2636   if (stopped()) {
2637     return true;
2638   }
2639   // Heap pointers get a null-check from the interpreter,
2640   // as a courtesy.  However, this is not guaranteed by Unsafe,
2641   // and it is not possible to fully distinguish unintended nulls
2642   // from intended ones in this API.
2643 
2644   if (is_volatile) {
2645     // We need to emit leading and trailing CPU membars (see below) in
2646     // addition to memory membars when is_volatile. This is a little
2647     // too strong, but avoids the need to insert per-alias-type
2648     // volatile membars (for stores; compare Parse::do_put_xxx), which
2649     // we cannot do effectively here because we probably only have a
2650     // rough approximation of type.
2651     need_mem_bar = true;
2652     // For Stores, place a memory ordering barrier now.
2653     if (is_store) {
2654       insert_mem_bar(Op_MemBarRelease);
2655     } else {
2656       if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2657         insert_mem_bar(Op_MemBarVolatile);
2658       }
2659     }
2660   }
2661 
2662   // Memory barrier to prevent normal and 'unsafe' accesses from
2663   // bypassing each other.  Happens after null checks, so the
2664   // exception paths do not take memory state from the memory barrier,
2665   // so there's no problems making a strong assert about mixing users
2666   // of safe &amp; unsafe memory.  Otherwise fails in a CTW of rt.jar
2667   // around 5701, class sun/reflect/UnsafeBooleanFieldAccessorImpl.
2668   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2669 
2670   if (!is_store) {
2671     MemNode::MemOrd mo = is_volatile ? MemNode::acquire : MemNode::unordered;
2672     Node* p = make_load(control(), adr, value_type, type, adr_type, mo, is_volatile);
2673     // load value
2674     switch (type) {
2675     case T_BOOLEAN:
2676     case T_CHAR:
2677     case T_BYTE:
2678     case T_SHORT:
2679     case T_INT:
2680     case T_LONG:
2681     case T_FLOAT:
2682     case T_DOUBLE:
2683       break;
2684     case T_OBJECT:
2685       if (need_read_barrier) {
2686         insert_pre_barrier(heap_base_oop, offset, p, !(is_volatile || need_mem_bar));
2687       }
2688       break;
2689     case T_ADDRESS:
2690       // Cast to an int type.
2691       p = _gvn.transform(new (C) CastP2XNode(NULL, p));
2692       p = ConvX2UL(p);
2693       break;
2694     default:
2695       fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
2696       break;
2697     }
2698     // The load node has the control of the preceding MemBarCPUOrder.  All
2699     // following nodes will have the control of the MemBarCPUOrder inserted at
2700     // the end of this method.  So, pushing the load onto the stack at a later
2701     // point is fine.
2702     set_result(p);
2703   } else {
2704     // place effect of store into memory
2705     switch (type) {
2706     case T_DOUBLE:
2707       val = dstore_rounding(val);
2708       break;
2709     case T_ADDRESS:
2710       // Repackage the long as a pointer.
2711       val = ConvL2X(val);
2712       val = _gvn.transform(new (C) CastX2PNode(val));
2713       break;
2714     }
2715 
2716     MemNode::MemOrd mo = is_volatile ? MemNode::release : MemNode::unordered;
2717     if (type != T_OBJECT ) {
2718       (void) store_to_memory(control(), adr, val, type, adr_type, mo, is_volatile);
2719     } else {
2720       // Possibly an oop being stored to Java heap or native memory
2721       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2722         // oop to Java heap.
2723         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2724       } else {
2725         // We can't tell at compile time if we are storing in the Java heap or outside
2726         // of it. So we need to emit code to conditionally do the proper type of
2727         // store.
2728 
2729         IdealKit ideal(this);
2730 #define __ ideal.
2731         // QQQ who knows what probability is here??
2732         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2733           // Sync IdealKit and graphKit.
2734           sync_kit(ideal);
2735           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo);
2736           // Update IdealKit memory.
2737           __ sync_kit(this);
2738         } __ else_(); {
2739           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, is_volatile);
2740         } __ end_if();
2741         // Final sync IdealKit and GraphKit.
2742         final_sync(ideal);
2743 #undef __
2744       }
2745     }
2746   }
2747 
2748   if (is_volatile) {
2749     if (!is_store) {
2750       insert_mem_bar(Op_MemBarAcquire);
2751     } else {
2752       if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2753         insert_mem_bar(Op_MemBarVolatile);
2754       }
2755     }
2756   }
2757 
2758   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2759 
2760   return true;
2761 }
2762 
2763 //----------------------------inline_unsafe_prefetch----------------------------
2764 
2765 bool LibraryCallKit::inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static) {
2766 #ifndef PRODUCT
2767   {
2768     ResourceMark rm;
2769     // Check the signatures.
2770     ciSignature* sig = callee()-&gt;signature();
2771 #ifdef ASSERT
2772     // Object getObject(Object base, int/long offset), etc.
2773     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2774     if (!is_native_ptr) {
2775       assert(sig-&gt;count() == 2, "oop prefetch has 2 arguments");
2776       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "prefetch base is object");
2777       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "prefetcha offset is correct");
2778     } else {
2779       assert(sig-&gt;count() == 1, "native prefetch has 1 argument");
2780       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "prefetch base is long");
2781     }
2782 #endif // ASSERT
2783   }
2784 #endif // !PRODUCT
2785 
2786   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2787 
2788   const int idx = is_static ? 0 : 1;
2789   if (!is_static) {
2790     null_check_receiver();
2791     if (stopped()) {
2792       return true;
2793     }
2794   }
2795 
2796   // Build address expression.  See the code in inline_unsafe_access.
2797   Node *adr;
2798   if (!is_native_ptr) {
2799     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2800     Node* base   = argument(idx + 0);  // type: oop
2801     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2802     Node* offset = argument(idx + 1);  // type: long
2803     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2804     // to be plain byte offsets, which are also the same as those accepted
2805     // by oopDesc::field_base.
2806     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2807            "fieldOffset must be byte-scaled");
2808     // 32-bit machines ignore the high half!
2809     offset = ConvL2X(offset);
2810     adr = make_unsafe_address(base, offset);
2811   } else {
2812     Node* ptr = argument(idx + 0);  // type: long
2813     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2814     adr = make_unsafe_address(NULL, ptr);
2815   }
2816 
2817   // Generate the read or write prefetch
2818   Node *prefetch;
2819   if (is_store) {
2820     prefetch = new (C) PrefetchWriteNode(i_o(), adr);
2821   } else {
2822     prefetch = new (C) PrefetchReadNode(i_o(), adr);
2823   }
2824   prefetch-&gt;init_req(0, control());
2825   set_i_o(_gvn.transform(prefetch));
2826 
2827   return true;
2828 }
2829 
2830 //----------------------------inline_unsafe_load_store----------------------------
2831 // This method serves a couple of different customers (depending on LoadStoreKind):
2832 //
2833 // LS_cmpxchg:
2834 //   public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2835 //   public final native boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2836 //   public final native boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2837 //
2838 // LS_xadd:
2839 //   public int  getAndAddInt( Object o, long offset, int  delta)
2840 //   public long getAndAddLong(Object o, long offset, long delta)
2841 //
2842 // LS_xchg:
2843 //   int    getAndSet(Object o, long offset, int    newValue)
2844 //   long   getAndSet(Object o, long offset, long   newValue)
2845 //   Object getAndSet(Object o, long offset, Object newValue)
2846 //
2847 bool LibraryCallKit::inline_unsafe_load_store(BasicType type, LoadStoreKind kind) {
2848   // This basic scheme here is the same as inline_unsafe_access, but
2849   // differs in enough details that combining them would make the code
2850   // overly confusing.  (This is a true fact! I originally combined
2851   // them, but even I was confused by it!) As much code/comments as
2852   // possible are retained from inline_unsafe_access though to make
2853   // the correspondences clearer. - dl
2854 
2855   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2856 
2857 #ifndef PRODUCT
2858   BasicType rtype;
2859   {
2860     ResourceMark rm;
2861     // Check the signatures.
2862     ciSignature* sig = callee()-&gt;signature();
2863     rtype = sig-&gt;return_type()-&gt;basic_type();
2864     if (kind == LS_xadd || kind == LS_xchg) {
2865       // Check the signatures.
2866 #ifdef ASSERT
2867       assert(rtype == type, "get and set must return the expected type");
2868       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2869       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2870       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2871       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2872 #endif // ASSERT
2873     } else if (kind == LS_cmpxchg) {
2874       // Check the signatures.
2875 #ifdef ASSERT
2876       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2877       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2878       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2879       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2880 #endif // ASSERT
2881     } else {
2882       ShouldNotReachHere();
2883     }
2884   }
2885 #endif //PRODUCT
2886 
2887   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2888 
2889   // Get arguments:
2890   Node* receiver = NULL;
2891   Node* base     = NULL;
2892   Node* offset   = NULL;
2893   Node* oldval   = NULL;
2894   Node* newval   = NULL;
2895   if (kind == LS_cmpxchg) {
2896     const bool two_slot_type = type2size[type] == 2;
2897     receiver = argument(0);  // type: oop
2898     base     = argument(1);  // type: oop
2899     offset   = argument(2);  // type: long
2900     oldval   = argument(4);  // type: oop, int, or long
2901     newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2902   } else if (kind == LS_xadd || kind == LS_xchg){
2903     receiver = argument(0);  // type: oop
2904     base     = argument(1);  // type: oop
2905     offset   = argument(2);  // type: long
2906     oldval   = NULL;
2907     newval   = argument(4);  // type: oop, int, or long
2908   }
2909 
2910   // Null check receiver.
2911   receiver = null_check(receiver);
2912   if (stopped()) {
2913     return true;
2914   }
2915 
2916   // Build field offset expression.
2917   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2918   // to be plain byte offsets, which are also the same as those accepted
2919   // by oopDesc::field_base.
2920   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2921   // 32-bit machines ignore the high half of long offsets
2922   offset = ConvL2X(offset);
2923   Node* adr = make_unsafe_address(base, offset);
2924   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2925 
2926   // For CAS, unlike inline_unsafe_access, there seems no point in
2927   // trying to refine types. Just use the coarse types here.
2928   const Type *value_type = Type::get_const_basic_type(type);
2929   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2930   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2931 
2932   if (kind == LS_xchg &amp;&amp; type == T_OBJECT) {
2933     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2934     if (tjp != NULL) {
2935       value_type = tjp;
2936     }
2937   }
2938 
2939   int alias_idx = C-&gt;get_alias_index(adr_type);
2940 
2941   // Memory-model-wise, a LoadStore acts like a little synchronized
2942   // block, so needs barriers on each side.  These don't translate
2943   // into actual barriers on most machines, but we still need rest of
2944   // compiler to respect ordering.
2945 
2946   insert_mem_bar(Op_MemBarRelease);
2947   insert_mem_bar(Op_MemBarCPUOrder);
2948 
2949   // 4984716: MemBars must be inserted before this
2950   //          memory node in order to avoid a false
2951   //          dependency which will confuse the scheduler.
2952   Node *mem = memory(alias_idx);
2953 
2954   // For now, we handle only those cases that actually exist: ints,
2955   // longs, and Object. Adding others should be straightforward.
2956   Node* load_store;
2957   switch(type) {
2958   case T_INT:
2959     if (kind == LS_xadd) {
2960       load_store = _gvn.transform(new (C) GetAndAddINode(control(), mem, adr, newval, adr_type));
2961     } else if (kind == LS_xchg) {
2962       load_store = _gvn.transform(new (C) GetAndSetINode(control(), mem, adr, newval, adr_type));
2963     } else if (kind == LS_cmpxchg) {
2964       load_store = _gvn.transform(new (C) CompareAndSwapINode(control(), mem, adr, newval, oldval));
2965     } else {
2966       ShouldNotReachHere();
2967     }
2968     break;
2969   case T_LONG:
2970     if (kind == LS_xadd) {
2971       load_store = _gvn.transform(new (C) GetAndAddLNode(control(), mem, adr, newval, adr_type));
2972     } else if (kind == LS_xchg) {
2973       load_store = _gvn.transform(new (C) GetAndSetLNode(control(), mem, adr, newval, adr_type));
2974     } else if (kind == LS_cmpxchg) {
2975       load_store = _gvn.transform(new (C) CompareAndSwapLNode(control(), mem, adr, newval, oldval));
2976     } else {
2977       ShouldNotReachHere();
2978     }
2979     break;
2980   case T_OBJECT:
2981     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2982     // could be delayed during Parse (for example, in adjust_map_after_if()).
2983     // Execute transformation here to avoid barrier generation in such case.
2984     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2985       newval = _gvn.makecon(TypePtr::NULL_PTR);
2986 
2987     // Reference stores need a store barrier.
2988     if (kind == LS_xchg) {
2989       // If pre-barrier must execute before the oop store, old value will require do_load here.
2990       if (!can_move_pre_barrier()) {
2991         pre_barrier(true /* do_load*/,
2992                     control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2993                     NULL /* pre_val*/,
2994                     T_OBJECT);
2995       } // Else move pre_barrier to use load_store value, see below.
2996     } else if (kind == LS_cmpxchg) {
2997       // Same as for newval above:
2998       if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2999         oldval = _gvn.makecon(TypePtr::NULL_PTR);
3000       }
3001       // The only known value which might get overwritten is oldval.
3002       pre_barrier(false /* do_load */,
3003                   control(), NULL, NULL, max_juint, NULL, NULL,
3004                   oldval /* pre_val */,
3005                   T_OBJECT);
3006     } else {
3007       ShouldNotReachHere();
3008     }
3009 
3010 #ifdef _LP64
3011     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3012       Node *newval_enc = _gvn.transform(new (C) EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
3013       if (kind == LS_xchg) {
3014         load_store = _gvn.transform(new (C) GetAndSetNNode(control(), mem, adr,
3015                                                            newval_enc, adr_type, value_type-&gt;make_narrowoop()));
3016       } else {
3017         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
3018         Node *oldval_enc = _gvn.transform(new (C) EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
3019         load_store = _gvn.transform(new (C) CompareAndSwapNNode(control(), mem, adr,
3020                                                                 newval_enc, oldval_enc));
3021       }
3022     } else
3023 #endif
3024     {
3025       if (kind == LS_xchg) {
3026         load_store = _gvn.transform(new (C) GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
3027       } else {
3028         assert(kind == LS_cmpxchg, "wrong LoadStore operation");
3029         load_store = _gvn.transform(new (C) CompareAndSwapPNode(control(), mem, adr, newval, oldval));
3030       }
3031     }
3032     post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
3033     break;
3034   default:
3035     fatal(err_msg_res("unexpected type %d: %s", type, type2name(type)));
3036     break;
3037   }
3038 
3039   // SCMemProjNodes represent the memory state of a LoadStore. Their
3040   // main role is to prevent LoadStore nodes from being optimized away
3041   // when their results aren't used.
3042   Node* proj = _gvn.transform(new (C) SCMemProjNode(load_store));
3043   set_memory(proj, alias_idx);
3044 
3045   if (type == T_OBJECT &amp;&amp; kind == LS_xchg) {
3046 #ifdef _LP64
3047     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3048       load_store = _gvn.transform(new (C) DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
3049     }
3050 #endif
3051     if (can_move_pre_barrier()) {
3052       // Don't need to load pre_val. The old value is returned by load_store.
3053       // The pre_barrier can execute after the xchg as long as no safepoint
3054       // gets inserted between them.
3055       pre_barrier(false /* do_load */,
3056                   control(), NULL, NULL, max_juint, NULL, NULL,
3057                   load_store /* pre_val */,
3058                   T_OBJECT);
3059     }
3060   }
3061 
3062   // Add the trailing membar surrounding the access
3063   insert_mem_bar(Op_MemBarCPUOrder);
3064   insert_mem_bar(Op_MemBarAcquire);
3065 
3066   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3067   set_result(load_store);
3068   return true;
3069 }
3070 
3071 //----------------------------inline_unsafe_ordered_store----------------------
3072 // public native void sun.misc.Unsafe.putOrderedObject(Object o, long offset, Object x);
3073 // public native void sun.misc.Unsafe.putOrderedInt(Object o, long offset, int x);
3074 // public native void sun.misc.Unsafe.putOrderedLong(Object o, long offset, long x);
3075 bool LibraryCallKit::inline_unsafe_ordered_store(BasicType type) {
3076   // This is another variant of inline_unsafe_access, differing in
3077   // that it always issues store-store ("release") barrier and ensures
3078   // store-atomicity (which only matters for "long").
3079 
3080   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3081 
3082 #ifndef PRODUCT
3083   {
3084     ResourceMark rm;
3085     // Check the signatures.
3086     ciSignature* sig = callee()-&gt;signature();
3087 #ifdef ASSERT
3088     BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
3089     assert(rtype == T_VOID, "must return void");
3090     assert(sig-&gt;count() == 3, "has 3 arguments");
3091     assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "base is object");
3092     assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "offset is long");
3093 #endif // ASSERT
3094   }
3095 #endif //PRODUCT
3096 
3097   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
3098 
3099   // Get arguments:
3100   Node* receiver = argument(0);  // type: oop
3101   Node* base     = argument(1);  // type: oop
3102   Node* offset   = argument(2);  // type: long
3103   Node* val      = argument(4);  // type: oop, int, or long
3104 
3105   // Null check receiver.
3106   receiver = null_check(receiver);
3107   if (stopped()) {
3108     return true;
3109   }
3110 
3111   // Build field offset expression.
3112   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
3113   // 32-bit machines ignore the high half of long offsets
3114   offset = ConvL2X(offset);
3115   Node* adr = make_unsafe_address(base, offset);
3116   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
3117   const Type *value_type = Type::get_const_basic_type(type);
3118   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
3119 
3120   insert_mem_bar(Op_MemBarRelease);
3121   insert_mem_bar(Op_MemBarCPUOrder);
3122   // Ensure that the store is atomic for longs:
3123   const bool require_atomic_access = true;
3124   Node* store;
3125   if (type == T_OBJECT) // reference stores need a store barrier.
3126     store = store_oop_to_unknown(control(), base, adr, adr_type, val, type, MemNode::release);
3127   else {
3128     store = store_to_memory(control(), adr, val, type, adr_type, MemNode::release, require_atomic_access);
3129   }
3130   insert_mem_bar(Op_MemBarCPUOrder);
3131   return true;
3132 }
3133 
3134 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3135   // Regardless of form, don't allow previous ld/st to move down,
3136   // then issue acquire, release, or volatile mem_bar.
3137   insert_mem_bar(Op_MemBarCPUOrder);
3138   switch(id) {
3139     case vmIntrinsics::_loadFence:
3140       insert_mem_bar(Op_LoadFence);
3141       return true;
3142     case vmIntrinsics::_storeFence:
3143       insert_mem_bar(Op_StoreFence);
3144       return true;
3145     case vmIntrinsics::_fullFence:
3146       insert_mem_bar(Op_MemBarVolatile);
3147       return true;
3148     default:
3149       fatal_unexpected_iid(id);
3150       return false;
3151   }
3152 }
3153 
3154 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3155   if (!kls-&gt;is_Con()) {
3156     return true;
3157   }
3158   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3159   if (klsptr == NULL) {
3160     return true;
3161   }
3162   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3163   // don't need a guard for a klass that is already initialized
3164   return !ik-&gt;is_initialized();
3165 }
3166 
3167 //----------------------------inline_unsafe_allocate---------------------------
3168 // public native Object sun.misc.Unsafe.allocateInstance(Class&lt;?&gt; cls);
3169 bool LibraryCallKit::inline_unsafe_allocate() {
3170   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3171 
3172   null_check_receiver();  // null-check, then ignore
3173   Node* cls = null_check(argument(1));
3174   if (stopped())  return true;
3175 
3176   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3177   kls = null_check(kls);
3178   if (stopped())  return true;  // argument was like int.class
3179 
3180   Node* test = NULL;
3181   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3182     // Note:  The argument might still be an illegal value like
3183     // Serializable.class or Object[].class.   The runtime will handle it.
3184     // But we must make an explicit check for initialization.
3185     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3186     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3187     // can generate code to load it as unsigned byte.
3188     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3189     Node* bits = intcon(InstanceKlass::fully_initialized);
3190     test = _gvn.transform(new (C) SubINode(inst, bits));
3191     // The 'test' is non-zero if we need to take a slow path.
3192   }
3193 
3194   Node* obj = new_instance(kls, test);
3195   set_result(obj);
3196   return true;
3197 }
3198 
3199 #ifdef TRACE_HAVE_INTRINSICS
3200 /*
3201  * oop -&gt; myklass
3202  * myklass-&gt;trace_id |= USED
3203  * return myklass-&gt;trace_id &amp; ~0x3
3204  */
3205 bool LibraryCallKit::inline_native_classID() {
3206   null_check_receiver();  // null-check, then ignore
3207   Node* cls = null_check(argument(1), T_OBJECT);
3208   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3209   kls = null_check(kls, T_OBJECT);
3210   ByteSize offset = TRACE_ID_OFFSET;
3211   Node* insp = basic_plus_adr(kls, in_bytes(offset));
3212   Node* tvalue = make_load(NULL, insp, TypeLong::LONG, T_LONG, MemNode::unordered);
3213   Node* bits = longcon(~0x03l); // ignore bit 0 &amp; 1
3214   Node* andl = _gvn.transform(new (C) AndLNode(tvalue, bits));
3215   Node* clsused = longcon(0x01l); // set the class bit
3216   Node* orl = _gvn.transform(new (C) OrLNode(tvalue, clsused));
3217 
3218   const TypePtr *adr_type = _gvn.type(insp)-&gt;isa_ptr();
3219   store_to_memory(control(), insp, orl, T_LONG, adr_type, MemNode::unordered);
3220   set_result(andl);
3221   return true;
3222 }
3223 
3224 bool LibraryCallKit::inline_native_threadID() {
3225   Node* tls_ptr = NULL;
3226   Node* cur_thr = generate_current_thread(tls_ptr);
3227   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3228   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3229   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::thread_id_offset()));
3230 
3231   Node* threadid = NULL;
3232   size_t thread_id_size = OSThread::thread_id_size();
3233   if (thread_id_size == (size_t) BytesPerLong) {
3234     threadid = ConvL2I(make_load(control(), p, TypeLong::LONG, T_LONG, MemNode::unordered));
3235   } else if (thread_id_size == (size_t) BytesPerInt) {
3236     threadid = make_load(control(), p, TypeInt::INT, T_INT, MemNode::unordered);
3237   } else {
3238     ShouldNotReachHere();
3239   }
3240   set_result(threadid);
3241   return true;
3242 }
3243 #endif
3244 
3245 //------------------------inline_native_time_funcs--------------
3246 // inline code for System.currentTimeMillis() and System.nanoTime()
3247 // these have the same type and signature
3248 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3249   const TypeFunc* tf = OptoRuntime::void_long_Type();
3250   const TypePtr* no_memory_effects = NULL;
3251   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3252   Node* value = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+0));
3253 #ifdef ASSERT
3254   Node* value_top = _gvn.transform(new (C) ProjNode(time, TypeFunc::Parms+1));
3255   assert(value_top == top(), "second value must be top");
3256 #endif
3257   set_result(value);
3258   return true;
3259 }
3260 
3261 //------------------------inline_native_currentThread------------------
3262 bool LibraryCallKit::inline_native_currentThread() {
3263   Node* junk = NULL;
3264   set_result(generate_current_thread(junk));
3265   return true;
3266 }
3267 
3268 //------------------------inline_native_isInterrupted------------------
3269 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3270 bool LibraryCallKit::inline_native_isInterrupted() {
3271   // Add a fast path to t.isInterrupted(clear_int):
3272   //   (t == Thread.current() &amp;&amp;
3273   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3274   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3275   // So, in the common case that the interrupt bit is false,
3276   // we avoid making a call into the VM.  Even if the interrupt bit
3277   // is true, if the clear_int argument is false, we avoid the VM call.
3278   // However, if the receiver is not currentThread, we must call the VM,
3279   // because there must be some locking done around the operation.
3280 
3281   // We only go to the fast case code if we pass two guards.
3282   // Paths which do not pass are accumulated in the slow_region.
3283 
3284   enum {
3285     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3286     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3287     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3288     PATH_LIMIT
3289   };
3290 
3291   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3292   // out of the function.
3293   insert_mem_bar(Op_MemBarCPUOrder);
3294 
3295   RegionNode* result_rgn = new (C) RegionNode(PATH_LIMIT);
3296   PhiNode*    result_val = new (C) PhiNode(result_rgn, TypeInt::BOOL);
3297 
3298   RegionNode* slow_region = new (C) RegionNode(1);
3299   record_for_igvn(slow_region);
3300 
3301   // (a) Receiving thread must be the current thread.
3302   Node* rec_thr = argument(0);
3303   Node* tls_ptr = NULL;
3304   Node* cur_thr = generate_current_thread(tls_ptr);
3305   Node* cmp_thr = _gvn.transform(new (C) CmpPNode(cur_thr, rec_thr));
3306   Node* bol_thr = _gvn.transform(new (C) BoolNode(cmp_thr, BoolTest::ne));
3307 
3308   generate_slow_guard(bol_thr, slow_region);
3309 
3310   // (b) Interrupt bit on TLS must be false.
3311   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3312   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3313   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3314 
3315   // Set the control input on the field _interrupted read to prevent it floating up.
3316   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3317   Node* cmp_bit = _gvn.transform(new (C) CmpINode(int_bit, intcon(0)));
3318   Node* bol_bit = _gvn.transform(new (C) BoolNode(cmp_bit, BoolTest::ne));
3319 
3320   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3321 
3322   // First fast path:  if (!TLS._interrupted) return false;
3323   Node* false_bit = _gvn.transform(new (C) IfFalseNode(iff_bit));
3324   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3325   result_val-&gt;init_req(no_int_result_path, intcon(0));
3326 
3327   // drop through to next case
3328   set_control( _gvn.transform(new (C) IfTrueNode(iff_bit)));
3329 
3330 #ifndef TARGET_OS_FAMILY_windows
3331   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3332   Node* clr_arg = argument(1);
3333   Node* cmp_arg = _gvn.transform(new (C) CmpINode(clr_arg, intcon(0)));
3334   Node* bol_arg = _gvn.transform(new (C) BoolNode(cmp_arg, BoolTest::ne));
3335   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3336 
3337   // Second fast path:  ... else if (!clear_int) return true;
3338   Node* false_arg = _gvn.transform(new (C) IfFalseNode(iff_arg));
3339   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3340   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3341 
3342   // drop through to next case
3343   set_control( _gvn.transform(new (C) IfTrueNode(iff_arg)));
3344 #else
3345   // To return true on Windows you must read the _interrupted field
3346   // and check the the event state i.e. take the slow path.
3347 #endif // TARGET_OS_FAMILY_windows
3348 
3349   // (d) Otherwise, go to the slow path.
3350   slow_region-&gt;add_req(control());
3351   set_control( _gvn.transform(slow_region));
3352 
3353   if (stopped()) {
3354     // There is no slow path.
3355     result_rgn-&gt;init_req(slow_result_path, top());
3356     result_val-&gt;init_req(slow_result_path, top());
3357   } else {
3358     // non-virtual because it is a private non-static
3359     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3360 
3361     Node* slow_val = set_results_for_java_call(slow_call);
3362     // this-&gt;control() comes from set_results_for_java_call
3363 
3364     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3365     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3366 
3367     // These two phis are pre-filled with copies of of the fast IO and Memory
3368     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3369     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3370 
3371     result_rgn-&gt;init_req(slow_result_path, control());
3372     result_io -&gt;init_req(slow_result_path, i_o());
3373     result_mem-&gt;init_req(slow_result_path, reset_memory());
3374     result_val-&gt;init_req(slow_result_path, slow_val);
3375 
3376     set_all_memory(_gvn.transform(result_mem));
3377     set_i_o(       _gvn.transform(result_io));
3378   }
3379 
3380   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3381   set_result(result_rgn, result_val);
3382   return true;
3383 }
3384 
3385 //---------------------------load_mirror_from_klass----------------------------
3386 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3387 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3388   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3389   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3390 }
3391 
3392 //-----------------------load_klass_from_mirror_common-------------------------
3393 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3394 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3395 // and branch to the given path on the region.
3396 // If never_see_null, take an uncommon trap on null, so we can optimistically
3397 // compile for the non-null case.
3398 // If the region is NULL, force never_see_null = true.
3399 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3400                                                     bool never_see_null,
3401                                                     RegionNode* region,
3402                                                     int null_path,
3403                                                     int offset) {
3404   if (region == NULL)  never_see_null = true;
3405   Node* p = basic_plus_adr(mirror, offset);
3406   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3407   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3408   Node* null_ctl = top();
3409   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3410   if (region != NULL) {
3411     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3412     region-&gt;init_req(null_path, null_ctl);
3413   } else {
3414     assert(null_ctl == top(), "no loose ends");
3415   }
3416   return kls;
3417 }
3418 
3419 //--------------------(inline_native_Class_query helpers)---------------------
3420 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE, JVM_ACC_HAS_FINALIZER.
3421 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3422 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3423   // Branch around if the given klass has the given modifier bit set.
3424   // Like generate_guard, adds a new path onto the region.
3425   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3426   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3427   Node* mask = intcon(modifier_mask);
3428   Node* bits = intcon(modifier_bits);
3429   Node* mbit = _gvn.transform(new (C) AndINode(mods, mask));
3430   Node* cmp  = _gvn.transform(new (C) CmpINode(mbit, bits));
3431   Node* bol  = _gvn.transform(new (C) BoolNode(cmp, BoolTest::ne));
3432   return generate_fair_guard(bol, region);
3433 }
3434 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3435   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3436 }
3437 
3438 //-------------------------inline_native_Class_query-------------------
3439 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3440   const Type* return_type = TypeInt::BOOL;
3441   Node* prim_return_value = top();  // what happens if it's a primitive class?
3442   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3443   bool expect_prim = false;     // most of these guys expect to work on refs
3444 
3445   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3446 
3447   Node* mirror = argument(0);
3448   Node* obj    = top();
3449 
3450   switch (id) {
3451   case vmIntrinsics::_isInstance:
3452     // nothing is an instance of a primitive type
3453     prim_return_value = intcon(0);
3454     obj = argument(1);
3455     break;
3456   case vmIntrinsics::_getModifiers:
3457     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3458     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3459     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3460     break;
3461   case vmIntrinsics::_isInterface:
3462     prim_return_value = intcon(0);
3463     break;
3464   case vmIntrinsics::_isArray:
3465     prim_return_value = intcon(0);
3466     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3467     break;
3468   case vmIntrinsics::_isPrimitive:
3469     prim_return_value = intcon(1);
3470     expect_prim = true;  // obviously
3471     break;
3472   case vmIntrinsics::_getSuperclass:
3473     prim_return_value = null();
3474     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3475     break;
3476   case vmIntrinsics::_getComponentType:
3477     prim_return_value = null();
3478     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3479     break;
3480   case vmIntrinsics::_getClassAccessFlags:
3481     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3482     return_type = TypeInt::INT;  // not bool!  6297094
3483     break;
3484   default:
3485     fatal_unexpected_iid(id);
3486     break;
3487   }
3488 
3489   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3490   if (mirror_con == NULL)  return false;  // cannot happen?
3491 
3492 #ifndef PRODUCT
3493   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3494     ciType* k = mirror_con-&gt;java_mirror_type();
3495     if (k) {
3496       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3497       k-&gt;print_name();
3498       tty-&gt;cr();
3499     }
3500   }
3501 #endif
3502 
3503   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3504   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3505   record_for_igvn(region);
3506   PhiNode* phi = new (C) PhiNode(region, return_type);
3507 
3508   // The mirror will never be null of Reflection.getClassAccessFlags, however
3509   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3510   // if it is. See bug 4774291.
3511 
3512   // For Reflection.getClassAccessFlags(), the null check occurs in
3513   // the wrong place; see inline_unsafe_access(), above, for a similar
3514   // situation.
3515   mirror = null_check(mirror);
3516   // If mirror or obj is dead, only null-path is taken.
3517   if (stopped())  return true;
3518 
3519   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3520 
3521   // Now load the mirror's klass metaobject, and null-check it.
3522   // Side-effects region with the control path if the klass is null.
3523   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3524   // If kls is null, we have a primitive mirror.
3525   phi-&gt;init_req(_prim_path, prim_return_value);
3526   if (stopped()) { set_result(region, phi); return true; }
3527   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3528 
3529   Node* p;  // handy temp
3530   Node* null_ctl;
3531 
3532   // Now that we have the non-null klass, we can perform the real query.
3533   // For constant classes, the query will constant-fold in LoadNode::Value.
3534   Node* query_value = top();
3535   switch (id) {
3536   case vmIntrinsics::_isInstance:
3537     // nothing is an instance of a primitive type
3538     query_value = gen_instanceof(obj, kls, safe_for_replace);
3539     break;
3540 
3541   case vmIntrinsics::_getModifiers:
3542     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3543     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3544     break;
3545 
3546   case vmIntrinsics::_isInterface:
3547     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3548     if (generate_interface_guard(kls, region) != NULL)
3549       // A guard was added.  If the guard is taken, it was an interface.
3550       phi-&gt;add_req(intcon(1));
3551     // If we fall through, it's a plain class.
3552     query_value = intcon(0);
3553     break;
3554 
3555   case vmIntrinsics::_isArray:
3556     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3557     if (generate_array_guard(kls, region) != NULL)
3558       // A guard was added.  If the guard is taken, it was an array.
3559       phi-&gt;add_req(intcon(1));
3560     // If we fall through, it's a plain class.
3561     query_value = intcon(0);
3562     break;
3563 
3564   case vmIntrinsics::_isPrimitive:
3565     query_value = intcon(0); // "normal" path produces false
3566     break;
3567 
3568   case vmIntrinsics::_getSuperclass:
3569     // The rules here are somewhat unfortunate, but we can still do better
3570     // with random logic than with a JNI call.
3571     // Interfaces store null or Object as _super, but must report null.
3572     // Arrays store an intermediate super as _super, but must report Object.
3573     // Other types can report the actual _super.
3574     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3575     if (generate_interface_guard(kls, region) != NULL)
3576       // A guard was added.  If the guard is taken, it was an interface.
3577       phi-&gt;add_req(null());
3578     if (generate_array_guard(kls, region) != NULL)
3579       // A guard was added.  If the guard is taken, it was an array.
3580       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3581     // If we fall through, it's a plain class.  Get its _super.
3582     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3583     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3584     null_ctl = top();
3585     kls = null_check_oop(kls, &amp;null_ctl);
3586     if (null_ctl != top()) {
3587       // If the guard is taken, Object.superClass is null (both klass and mirror).
3588       region-&gt;add_req(null_ctl);
3589       phi   -&gt;add_req(null());
3590     }
3591     if (!stopped()) {
3592       query_value = load_mirror_from_klass(kls);
3593     }
3594     break;
3595 
3596   case vmIntrinsics::_getComponentType:
3597     if (generate_array_guard(kls, region) != NULL) {
3598       // Be sure to pin the oop load to the guard edge just created:
3599       Node* is_array_ctrl = region-&gt;in(region-&gt;req()-1);
3600       Node* cma = basic_plus_adr(kls, in_bytes(ArrayKlass::component_mirror_offset()));
3601       Node* cmo = make_load(is_array_ctrl, cma, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3602       phi-&gt;add_req(cmo);
3603     }
3604     query_value = null();  // non-array case is null
3605     break;
3606 
3607   case vmIntrinsics::_getClassAccessFlags:
3608     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3609     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3610     break;
3611 
3612   default:
3613     fatal_unexpected_iid(id);
3614     break;
3615   }
3616 
3617   // Fall-through is the normal case of a query to a real class.
3618   phi-&gt;init_req(1, query_value);
3619   region-&gt;init_req(1, control());
3620 
3621   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3622   set_result(region, phi);
3623   return true;
3624 }
3625 
3626 //--------------------------inline_native_subtype_check------------------------
3627 // This intrinsic takes the JNI calls out of the heart of
3628 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3629 bool LibraryCallKit::inline_native_subtype_check() {
3630   // Pull both arguments off the stack.
3631   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3632   args[0] = argument(0);
3633   args[1] = argument(1);
3634   Node* klasses[2];             // corresponding Klasses: superk, subk
3635   klasses[0] = klasses[1] = top();
3636 
3637   enum {
3638     // A full decision tree on {superc is prim, subc is prim}:
3639     _prim_0_path = 1,           // {P,N} =&gt; false
3640                                 // {P,P} &amp; superc!=subc =&gt; false
3641     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3642     _prim_1_path,               // {N,P} =&gt; false
3643     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3644     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3645     PATH_LIMIT
3646   };
3647 
3648   RegionNode* region = new (C) RegionNode(PATH_LIMIT);
3649   Node*       phi    = new (C) PhiNode(region, TypeInt::BOOL);
3650   record_for_igvn(region);
3651 
3652   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3653   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3654   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3655 
3656   // First null-check both mirrors and load each mirror's klass metaobject.
3657   int which_arg;
3658   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3659     Node* arg = args[which_arg];
3660     arg = null_check(arg);
3661     if (stopped())  break;
3662     args[which_arg] = arg;
3663 
3664     Node* p = basic_plus_adr(arg, class_klass_offset);
3665     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3666     klasses[which_arg] = _gvn.transform(kls);
3667   }
3668 
3669   // Having loaded both klasses, test each for null.
3670   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3671   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3672     Node* kls = klasses[which_arg];
3673     Node* null_ctl = top();
3674     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3675     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3676     region-&gt;init_req(prim_path, null_ctl);
3677     if (stopped())  break;
3678     klasses[which_arg] = kls;
3679   }
3680 
3681   if (!stopped()) {
3682     // now we have two reference types, in klasses[0..1]
3683     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3684     Node* superk = klasses[0];  // the receiver
3685     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3686     // now we have a successful reference subtype check
3687     region-&gt;set_req(_ref_subtype_path, control());
3688   }
3689 
3690   // If both operands are primitive (both klasses null), then
3691   // we must return true when they are identical primitives.
3692   // It is convenient to test this after the first null klass check.
3693   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3694   if (!stopped()) {
3695     // Since superc is primitive, make a guard for the superc==subc case.
3696     Node* cmp_eq = _gvn.transform(new (C) CmpPNode(args[0], args[1]));
3697     Node* bol_eq = _gvn.transform(new (C) BoolNode(cmp_eq, BoolTest::eq));
3698     generate_guard(bol_eq, region, PROB_FAIR);
3699     if (region-&gt;req() == PATH_LIMIT+1) {
3700       // A guard was added.  If the added guard is taken, superc==subc.
3701       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3702       region-&gt;del_req(PATH_LIMIT);
3703     }
3704     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3705   }
3706 
3707   // these are the only paths that produce 'true':
3708   phi-&gt;set_req(_prim_same_path,   intcon(1));
3709   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3710 
3711   // pull together the cases:
3712   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3713   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3714     Node* ctl = region-&gt;in(i);
3715     if (ctl == NULL || ctl == top()) {
3716       region-&gt;set_req(i, top());
3717       phi   -&gt;set_req(i, top());
3718     } else if (phi-&gt;in(i) == NULL) {
3719       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3720     }
3721   }
3722 
3723   set_control(_gvn.transform(region));
3724   set_result(_gvn.transform(phi));
3725   return true;
3726 }
3727 
3728 //---------------------generate_array_guard_common------------------------
3729 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3730                                                   bool obj_array, bool not_array) {
3731   // If obj_array/non_array==false/false:
3732   // Branch around if the given klass is in fact an array (either obj or prim).
3733   // If obj_array/non_array==false/true:
3734   // Branch around if the given klass is not an array klass of any kind.
3735   // If obj_array/non_array==true/true:
3736   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3737   // If obj_array/non_array==true/false:
3738   // Branch around if the kls is an oop array (Object[] or subtype)
3739   //
3740   // Like generate_guard, adds a new path onto the region.
3741   jint  layout_con = 0;
3742   Node* layout_val = get_layout_helper(kls, layout_con);
3743   if (layout_val == NULL) {
3744     bool query = (obj_array
3745                   ? Klass::layout_helper_is_objArray(layout_con)
3746                   : Klass::layout_helper_is_array(layout_con));
3747     if (query == not_array) {
3748       return NULL;                       // never a branch
3749     } else {                             // always a branch
3750       Node* always_branch = control();
3751       if (region != NULL)
3752         region-&gt;add_req(always_branch);
3753       set_control(top());
3754       return always_branch;
3755     }
3756   }
3757   // Now test the correct condition.
3758   jint  nval = (obj_array
3759                 ? ((jint)Klass::_lh_array_tag_type_value
3760                    &lt;&lt;    Klass::_lh_array_tag_shift)
3761                 : Klass::_lh_neutral_value);
3762   Node* cmp = _gvn.transform(new(C) CmpINode(layout_val, intcon(nval)));
3763   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3764   // invert the test if we are looking for a non-array
3765   if (not_array)  btest = BoolTest(btest).negate();
3766   Node* bol = _gvn.transform(new(C) BoolNode(cmp, btest));
3767   return generate_fair_guard(bol, region);
3768 }
3769 
3770 
3771 //-----------------------inline_native_newArray--------------------------
3772 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3773 bool LibraryCallKit::inline_native_newArray() {
3774   Node* mirror    = argument(0);
3775   Node* count_val = argument(1);
3776 
3777   mirror = null_check(mirror);
3778   // If mirror or obj is dead, only null-path is taken.
3779   if (stopped())  return true;
3780 
3781   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3782   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
3783   PhiNode*    result_val = new(C) PhiNode(result_reg,
3784                                           TypeInstPtr::NOTNULL);
3785   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
3786   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
3787                                           TypePtr::BOTTOM);
3788 
3789   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3790   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3791                                                   result_reg, _slow_path);
3792   Node* normal_ctl   = control();
3793   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3794 
3795   // Generate code for the slow case.  We make a call to newArray().
3796   set_control(no_array_ctl);
3797   if (!stopped()) {
3798     // Either the input type is void.class, or else the
3799     // array klass has not yet been cached.  Either the
3800     // ensuing call will throw an exception, or else it
3801     // will cache the array klass for next time.
3802     PreserveJVMState pjvms(this);
3803     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3804     Node* slow_result = set_results_for_java_call(slow_call);
3805     // this-&gt;control() comes from set_results_for_java_call
3806     result_reg-&gt;set_req(_slow_path, control());
3807     result_val-&gt;set_req(_slow_path, slow_result);
3808     result_io -&gt;set_req(_slow_path, i_o());
3809     result_mem-&gt;set_req(_slow_path, reset_memory());
3810   }
3811 
3812   set_control(normal_ctl);
3813   if (!stopped()) {
3814     // Normal case:  The array type has been cached in the java.lang.Class.
3815     // The following call works fine even if the array type is polymorphic.
3816     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3817     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3818     result_reg-&gt;init_req(_normal_path, control());
3819     result_val-&gt;init_req(_normal_path, obj);
3820     result_io -&gt;init_req(_normal_path, i_o());
3821     result_mem-&gt;init_req(_normal_path, reset_memory());
3822   }
3823 
3824   // Return the combined state.
3825   set_i_o(        _gvn.transform(result_io)  );
3826   set_all_memory( _gvn.transform(result_mem));
3827 
3828   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3829   set_result(result_reg, result_val);
3830   return true;
3831 }
3832 
3833 //----------------------inline_native_getLength--------------------------
3834 // public static native int java.lang.reflect.Array.getLength(Object array);
3835 bool LibraryCallKit::inline_native_getLength() {
3836   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3837 
3838   Node* array = null_check(argument(0));
3839   // If array is dead, only null-path is taken.
3840   if (stopped())  return true;
3841 
3842   // Deoptimize if it is a non-array.
3843   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3844 
3845   if (non_array != NULL) {
3846     PreserveJVMState pjvms(this);
3847     set_control(non_array);
3848     uncommon_trap(Deoptimization::Reason_intrinsic,
3849                   Deoptimization::Action_maybe_recompile);
3850   }
3851 
3852   // If control is dead, only non-array-path is taken.
3853   if (stopped())  return true;
3854 
3855   // The works fine even if the array type is polymorphic.
3856   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3857   Node* result = load_array_length(array);
3858 
3859   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3860   set_result(result);
3861   return true;
3862 }
3863 
3864 //------------------------inline_array_copyOf----------------------------
3865 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3866 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3867 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3868   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3869 
3870   // Get the arguments.
3871   Node* original          = argument(0);
3872   Node* start             = is_copyOfRange? argument(1): intcon(0);
3873   Node* end               = is_copyOfRange? argument(2): argument(1);
3874   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3875 
3876   Node* newcopy;
3877 
3878   // Set the original stack and the reexecute bit for the interpreter to reexecute
3879   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3880   { PreserveReexecuteState preexecs(this);
3881     jvms()-&gt;set_should_reexecute(true);
3882 
3883     array_type_mirror = null_check(array_type_mirror);
3884     original          = null_check(original);
3885 
3886     // Check if a null path was taken unconditionally.
3887     if (stopped())  return true;
3888 
3889     Node* orig_length = load_array_length(original);
3890 
3891     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3892     klass_node = null_check(klass_node);
3893 
3894     RegionNode* bailout = new (C) RegionNode(1);
3895     record_for_igvn(bailout);
3896 
3897     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3898     // Bail out if that is so.
3899     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3900     if (not_objArray != NULL) {
3901       // Improve the klass node's type from the new optimistic assumption:
3902       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3903       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3904       Node* cast = new (C) CastPPNode(klass_node, akls);
3905       cast-&gt;init_req(0, control());
3906       klass_node = _gvn.transform(cast);
3907     }
3908 
3909     // Bail out if either start or end is negative.
3910     generate_negative_guard(start, bailout, &amp;start);
3911     generate_negative_guard(end,   bailout, &amp;end);
3912 
3913     Node* length = end;
3914     if (_gvn.type(start) != TypeInt::ZERO) {
3915       length = _gvn.transform(new (C) SubINode(end, start));
3916     }
3917 
3918     // Bail out if length is negative.
3919     // Without this the new_array would throw
3920     // NegativeArraySizeException but IllegalArgumentException is what
3921     // should be thrown
3922     generate_negative_guard(length, bailout, &amp;length);
3923 
3924     if (bailout-&gt;req() &gt; 1) {
3925       PreserveJVMState pjvms(this);
3926       set_control(_gvn.transform(bailout));
3927       uncommon_trap(Deoptimization::Reason_intrinsic,
3928                     Deoptimization::Action_maybe_recompile);
3929     }
3930 
3931     if (!stopped()) {
3932       // How many elements will we copy from the original?
3933       // The answer is MinI(orig_length - start, length).
3934       Node* orig_tail = _gvn.transform(new (C) SubINode(orig_length, start));
3935       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3936 
3937       newcopy = new_array(klass_node, length, 0);  // no argments to push
3938 
3939       // Generate a direct call to the right arraycopy function(s).
3940       // We know the copy is disjoint but we might not know if the
3941       // oop stores need checking.
3942       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3943       // This will fail a store-check if x contains any non-nulls.
3944       bool disjoint_bases = true;
3945       // if start &gt; orig_length then the length of the copy may be
3946       // negative.
3947       bool length_never_negative = !is_copyOfRange;
3948       generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
3949                          original, start, newcopy, intcon(0), moved,
3950                          disjoint_bases, length_never_negative);
3951     }
3952   } // original reexecute is set back here
3953 
3954   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3955   if (!stopped()) {
3956     set_result(newcopy);
3957   }
3958   return true;
3959 }
3960 
3961 
3962 //----------------------generate_virtual_guard---------------------------
3963 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
3964 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
3965                                              RegionNode* slow_region) {
3966   ciMethod* method = callee();
3967   int vtable_index = method-&gt;vtable_index();
3968   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
3969          err_msg_res("bad index %d", vtable_index));
3970   // Get the Method* out of the appropriate vtable entry.
3971   int entry_offset  = (InstanceKlass::vtable_start_offset() +
3972                      vtable_index*vtableEntry::size()) * wordSize +
3973                      vtableEntry::method_offset_in_bytes();
3974   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
3975   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3976 
3977   // Compare the target method with the expected method (e.g., Object.hashCode).
3978   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
3979 
3980   Node* native_call = makecon(native_call_addr);
3981   Node* chk_native  = _gvn.transform(new(C) CmpPNode(target_call, native_call));
3982   Node* test_native = _gvn.transform(new(C) BoolNode(chk_native, BoolTest::ne));
3983 
3984   return generate_slow_guard(test_native, slow_region);
3985 }
3986 
3987 //-----------------------generate_method_call----------------------------
3988 // Use generate_method_call to make a slow-call to the real
3989 // method if the fast path fails.  An alternative would be to
3990 // use a stub like OptoRuntime::slow_arraycopy_Java.
3991 // This only works for expanding the current library call,
3992 // not another intrinsic.  (E.g., don't use this for making an
3993 // arraycopy call inside of the copyOf intrinsic.)
3994 CallJavaNode*
3995 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
3996   // When compiling the intrinsic method itself, do not use this technique.
3997   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
3998 
3999   ciMethod* method = callee();
4000   // ensure the JVMS we have will be correct for this call
4001   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4002 
4003   const TypeFunc* tf = TypeFunc::make(method);
4004   CallJavaNode* slow_call;
4005   if (is_static) {
4006     assert(!is_virtual, "");
4007     slow_call = new(C) CallStaticJavaNode(C, tf,
4008                            SharedRuntime::get_resolve_static_call_stub(),
4009                            method, bci());
4010   } else if (is_virtual) {
4011     null_check_receiver();
4012     int vtable_index = Method::invalid_vtable_index;
4013     if (UseInlineCaches) {
4014       // Suppress the vtable call
4015     } else {
4016       // hashCode and clone are not a miranda methods,
4017       // so the vtable index is fixed.
4018       // No need to use the linkResolver to get it.
4019        vtable_index = method-&gt;vtable_index();
4020        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4021               err_msg_res("bad index %d", vtable_index));
4022     }
4023     slow_call = new(C) CallDynamicJavaNode(tf,
4024                           SharedRuntime::get_resolve_virtual_call_stub(),
4025                           method, vtable_index, bci());
4026   } else {  // neither virtual nor static:  opt_virtual
4027     null_check_receiver();
4028     slow_call = new(C) CallStaticJavaNode(C, tf,
4029                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4030                                 method, bci());
4031     slow_call-&gt;set_optimized_virtual(true);
4032   }
4033   set_arguments_for_java_call(slow_call);
4034   set_edges_for_java_call(slow_call);
4035   return slow_call;
4036 }
4037 
4038 
4039 /**
4040  * Build special case code for calls to hashCode on an object. This call may
4041  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4042  * slightly different code.
4043  */
4044 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4045   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4046   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4047 
4048   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4049 
4050   RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4051   PhiNode*    result_val = new(C) PhiNode(result_reg, TypeInt::INT);
4052   PhiNode*    result_io  = new(C) PhiNode(result_reg, Type::ABIO);
4053   PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4054   Node* obj = NULL;
4055   if (!is_static) {
4056     // Check for hashing null object
4057     obj = null_check_receiver();
4058     if (stopped())  return true;        // unconditionally null
4059     result_reg-&gt;init_req(_null_path, top());
4060     result_val-&gt;init_req(_null_path, top());
4061   } else {
4062     // Do a null check, and return zero if null.
4063     // System.identityHashCode(null) == 0
4064     obj = argument(0);
4065     Node* null_ctl = top();
4066     obj = null_check_oop(obj, &amp;null_ctl);
4067     result_reg-&gt;init_req(_null_path, null_ctl);
4068     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4069   }
4070 
4071   // Unconditionally null?  Then return right away.
4072   if (stopped()) {
4073     set_control( result_reg-&gt;in(_null_path));
4074     if (!stopped())
4075       set_result(result_val-&gt;in(_null_path));
4076     return true;
4077   }
4078 
4079   // We only go to the fast case code if we pass a number of guards.  The
4080   // paths which do not pass are accumulated in the slow_region.
4081   RegionNode* slow_region = new (C) RegionNode(1);
4082   record_for_igvn(slow_region);
4083 
4084   // If this is a virtual call, we generate a funny guard.  We pull out
4085   // the vtable entry corresponding to hashCode() from the target object.
4086   // If the target method which we are calling happens to be the native
4087   // Object hashCode() method, we pass the guard.  We do not need this
4088   // guard for non-virtual calls -- the caller is known to be the native
4089   // Object hashCode().
4090   if (is_virtual) {
4091     // After null check, get the object's klass.
4092     Node* obj_klass = load_object_klass(obj);
4093     generate_virtual_guard(obj_klass, slow_region);
4094   }
4095 
4096   // Get the header out of the object, use LoadMarkNode when available
4097   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4098   // The control of the load must be NULL. Otherwise, the load can move before
4099   // the null check after castPP removal.
4100   Node* no_ctrl = NULL;
4101   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4102 
4103   // Test the header to see if it is unlocked.
4104   Node* lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4105   Node* lmasked_header = _gvn.transform(new (C) AndXNode(header, lock_mask));
4106   Node* unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4107   Node* chk_unlocked   = _gvn.transform(new (C) CmpXNode( lmasked_header, unlocked_val));
4108   Node* test_unlocked  = _gvn.transform(new (C) BoolNode( chk_unlocked, BoolTest::ne));
4109 
4110   generate_slow_guard(test_unlocked, slow_region);
4111 
4112   // Get the hash value and check to see that it has been properly assigned.
4113   // We depend on hash_mask being at most 32 bits and avoid the use of
4114   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4115   // vm: see markOop.hpp.
4116   Node* hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4117   Node* hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4118   Node* hshifted_header= _gvn.transform(new (C) URShiftXNode(header, hash_shift));
4119   // This hack lets the hash bits live anywhere in the mark object now, as long
4120   // as the shift drops the relevant bits into the low 32 bits.  Note that
4121   // Java spec says that HashCode is an int so there's no point in capturing
4122   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4123   hshifted_header      = ConvX2I(hshifted_header);
4124   Node* hash_val       = _gvn.transform(new (C) AndINode(hshifted_header, hash_mask));
4125 
4126   Node* no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4127   Node* chk_assigned   = _gvn.transform(new (C) CmpINode( hash_val, no_hash_val));
4128   Node* test_assigned  = _gvn.transform(new (C) BoolNode( chk_assigned, BoolTest::eq));
4129 
4130   generate_slow_guard(test_assigned, slow_region);
4131 
4132   Node* init_mem = reset_memory();
4133   // fill in the rest of the null path:
4134   result_io -&gt;init_req(_null_path, i_o());
4135   result_mem-&gt;init_req(_null_path, init_mem);
4136 
4137   result_val-&gt;init_req(_fast_path, hash_val);
4138   result_reg-&gt;init_req(_fast_path, control());
4139   result_io -&gt;init_req(_fast_path, i_o());
4140   result_mem-&gt;init_req(_fast_path, init_mem);
4141 
4142   // Generate code for the slow case.  We make a call to hashCode().
4143   set_control(_gvn.transform(slow_region));
4144   if (!stopped()) {
4145     // No need for PreserveJVMState, because we're using up the present state.
4146     set_all_memory(init_mem);
4147     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4148     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4149     Node* slow_result = set_results_for_java_call(slow_call);
4150     // this-&gt;control() comes from set_results_for_java_call
4151     result_reg-&gt;init_req(_slow_path, control());
4152     result_val-&gt;init_req(_slow_path, slow_result);
4153     result_io  -&gt;set_req(_slow_path, i_o());
4154     result_mem -&gt;set_req(_slow_path, reset_memory());
4155   }
4156 
4157   // Return the combined state.
4158   set_i_o(        _gvn.transform(result_io)  );
4159   set_all_memory( _gvn.transform(result_mem));
4160 
4161   set_result(result_reg, result_val);
4162   return true;
4163 }
4164 
4165 //---------------------------inline_native_getClass----------------------------
4166 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4167 //
4168 // Build special case code for calls to getClass on an object.
4169 bool LibraryCallKit::inline_native_getClass() {
4170   Node* obj = null_check_receiver();
4171   if (stopped())  return true;
4172   set_result(load_mirror_from_klass(load_object_klass(obj)));
4173   return true;
4174 }
4175 
4176 //-----------------inline_native_Reflection_getCallerClass---------------------
4177 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4178 //
4179 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4180 //
4181 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4182 // in that it must skip particular security frames and checks for
4183 // caller sensitive methods.
4184 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4185 #ifndef PRODUCT
4186   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4187     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4188   }
4189 #endif
4190 
4191   if (!jvms()-&gt;has_method()) {
4192 #ifndef PRODUCT
4193     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4194       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4195     }
4196 #endif
4197     return false;
4198   }
4199 
4200   // Walk back up the JVM state to find the caller at the required
4201   // depth.
4202   JVMState* caller_jvms = jvms();
4203 
4204   // Cf. JVM_GetCallerClass
4205   // NOTE: Start the loop at depth 1 because the current JVM state does
4206   // not include the Reflection.getCallerClass() frame.
4207   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4208     ciMethod* m = caller_jvms-&gt;method();
4209     switch (n) {
4210     case 0:
4211       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4212       break;
4213     case 1:
4214       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4215       if (!m-&gt;caller_sensitive()) {
4216 #ifndef PRODUCT
4217         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4218           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4219         }
4220 #endif
4221         return false;  // bail-out; let JVM_GetCallerClass do the work
4222       }
4223       break;
4224     default:
4225       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4226         // We have reached the desired frame; return the holder class.
4227         // Acquire method holder as java.lang.Class and push as constant.
4228         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4229         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4230         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4231 
4232 #ifndef PRODUCT
4233         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4234           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4235           tty-&gt;print_cr("  JVM state at this point:");
4236           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4237             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4238             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4239           }
4240         }
4241 #endif
4242         return true;
4243       }
4244       break;
4245     }
4246   }
4247 
4248 #ifndef PRODUCT
4249   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4250     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4251     tty-&gt;print_cr("  JVM state at this point:");
4252     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4253       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4254       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4255     }
4256   }
4257 #endif
4258 
4259   return false;  // bail-out; let JVM_GetCallerClass do the work
4260 }
4261 
4262 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4263   Node* arg = argument(0);
4264   Node* result;
4265 
4266   switch (id) {
4267   case vmIntrinsics::_floatToRawIntBits:    result = new (C) MoveF2INode(arg);  break;
4268   case vmIntrinsics::_intBitsToFloat:       result = new (C) MoveI2FNode(arg);  break;
4269   case vmIntrinsics::_doubleToRawLongBits:  result = new (C) MoveD2LNode(arg);  break;
4270   case vmIntrinsics::_longBitsToDouble:     result = new (C) MoveL2DNode(arg);  break;
4271 
4272   case vmIntrinsics::_doubleToLongBits: {
4273     // two paths (plus control) merge in a wood
4274     RegionNode *r = new (C) RegionNode(3);
4275     Node *phi = new (C) PhiNode(r, TypeLong::LONG);
4276 
4277     Node *cmpisnan = _gvn.transform(new (C) CmpDNode(arg, arg));
4278     // Build the boolean node
4279     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4280 
4281     // Branch either way.
4282     // NaN case is less traveled, which makes all the difference.
4283     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4284     Node *opt_isnan = _gvn.transform(ifisnan);
4285     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4286     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4287     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4288 
4289     set_control(iftrue);
4290 
4291     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4292     Node *slow_result = longcon(nan_bits); // return NaN
4293     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4294     r-&gt;init_req(1, iftrue);
4295 
4296     // Else fall through
4297     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4298     set_control(iffalse);
4299 
4300     phi-&gt;init_req(2, _gvn.transform(new (C) MoveD2LNode(arg)));
4301     r-&gt;init_req(2, iffalse);
4302 
4303     // Post merge
4304     set_control(_gvn.transform(r));
4305     record_for_igvn(r);
4306 
4307     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4308     result = phi;
4309     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4310     break;
4311   }
4312 
4313   case vmIntrinsics::_floatToIntBits: {
4314     // two paths (plus control) merge in a wood
4315     RegionNode *r = new (C) RegionNode(3);
4316     Node *phi = new (C) PhiNode(r, TypeInt::INT);
4317 
4318     Node *cmpisnan = _gvn.transform(new (C) CmpFNode(arg, arg));
4319     // Build the boolean node
4320     Node *bolisnan = _gvn.transform(new (C) BoolNode(cmpisnan, BoolTest::ne));
4321 
4322     // Branch either way.
4323     // NaN case is less traveled, which makes all the difference.
4324     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4325     Node *opt_isnan = _gvn.transform(ifisnan);
4326     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4327     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4328     Node *iftrue = _gvn.transform(new (C) IfTrueNode(opt_ifisnan));
4329 
4330     set_control(iftrue);
4331 
4332     static const jint nan_bits = 0x7fc00000;
4333     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4334     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4335     r-&gt;init_req(1, iftrue);
4336 
4337     // Else fall through
4338     Node *iffalse = _gvn.transform(new (C) IfFalseNode(opt_ifisnan));
4339     set_control(iffalse);
4340 
4341     phi-&gt;init_req(2, _gvn.transform(new (C) MoveF2INode(arg)));
4342     r-&gt;init_req(2, iffalse);
4343 
4344     // Post merge
4345     set_control(_gvn.transform(r));
4346     record_for_igvn(r);
4347 
4348     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4349     result = phi;
4350     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4351     break;
4352   }
4353 
4354   default:
4355     fatal_unexpected_iid(id);
4356     break;
4357   }
4358   set_result(_gvn.transform(result));
4359   return true;
4360 }
4361 
4362 #ifdef _LP64
4363 #define XTOP ,top() /*additional argument*/
4364 #else  //_LP64
4365 #define XTOP        /*no additional argument*/
4366 #endif //_LP64
4367 
4368 //----------------------inline_unsafe_copyMemory-------------------------
4369 // public native void sun.misc.Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4370 bool LibraryCallKit::inline_unsafe_copyMemory() {
4371   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4372   null_check_receiver();  // null-check receiver
4373   if (stopped())  return true;
4374 
4375   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4376 
4377   Node* src_ptr =         argument(1);   // type: oop
4378   Node* src_off = ConvL2X(argument(2));  // type: long
4379   Node* dst_ptr =         argument(4);   // type: oop
4380   Node* dst_off = ConvL2X(argument(5));  // type: long
4381   Node* size    = ConvL2X(argument(7));  // type: long
4382 
4383   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4384          "fieldOffset must be byte-scaled");
4385 
4386   Node* src = make_unsafe_address(src_ptr, src_off);
4387   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4388 
4389   // Conservatively insert a memory barrier on all memory slices.
4390   // Do not let writes of the copy source or destination float below the copy.
4391   insert_mem_bar(Op_MemBarCPUOrder);
4392 
4393   // Call it.  Note that the length argument is not scaled.
4394   make_runtime_call(RC_LEAF|RC_NO_FP,
4395                     OptoRuntime::fast_arraycopy_Type(),
4396                     StubRoutines::unsafe_arraycopy(),
4397                     "unsafe_arraycopy",
4398                     TypeRawPtr::BOTTOM,
4399                     src, dst, size XTOP);
4400 
4401   // Do not let reads of the copy destination float above the copy.
4402   insert_mem_bar(Op_MemBarCPUOrder);
4403 
4404   return true;
4405 }
4406 
4407 //------------------------clone_coping-----------------------------------
4408 // Helper function for inline_native_clone.
4409 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4410   assert(obj_size != NULL, "");
4411   Node* raw_obj = alloc_obj-&gt;in(1);
4412   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4413 
4414   AllocateNode* alloc = NULL;
4415   if (ReduceBulkZeroing) {
4416     // We will be completely responsible for initializing this object -
4417     // mark Initialize node as complete.
4418     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4419     // The object was just allocated - there should be no any stores!
4420     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4421     // Mark as complete_with_arraycopy so that on AllocateNode
4422     // expansion, we know this AllocateNode is initialized by an array
4423     // copy and a StoreStore barrier exists after the array copy.
4424     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4425   }
4426 
4427   // Copy the fastest available way.
4428   // TODO: generate fields copies for small objects instead.
4429   Node* src  = obj;
4430   Node* dest = alloc_obj;
4431   Node* size = _gvn.transform(obj_size);
4432 
4433   // Exclude the header but include array length to copy by 8 bytes words.
4434   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4435   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4436                             instanceOopDesc::base_offset_in_bytes();
4437   // base_off:
4438   // 8  - 32-bit VM
4439   // 12 - 64-bit VM, compressed klass
4440   // 16 - 64-bit VM, normal klass
4441   if (base_off % BytesPerLong != 0) {
4442     assert(UseCompressedClassPointers, "");
4443     if (is_array) {
4444       // Exclude length to copy by 8 bytes words.
4445       base_off += sizeof(int);
4446     } else {
4447       // Include klass to copy by 8 bytes words.
4448       base_off = instanceOopDesc::klass_offset_in_bytes();
4449     }
4450     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4451   }
4452   src  = basic_plus_adr(src,  base_off);
4453   dest = basic_plus_adr(dest, base_off);
4454 
4455   // Compute the length also, if needed:
4456   Node* countx = size;
4457   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(base_off)));
4458   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong) ));
4459 
4460   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4461   bool disjoint_bases = true;
4462   generate_unchecked_arraycopy(raw_adr_type, T_LONG, disjoint_bases,
4463                                src, NULL, dest, NULL, countx,
4464                                /*dest_uninitialized*/true);
4465 
4466   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4467   if (card_mark) {
4468     assert(!is_array, "");
4469     // Put in store barrier for any and all oops we are sticking
4470     // into this object.  (We could avoid this if we could prove
4471     // that the object type contains no oop fields at all.)
4472     Node* no_particular_value = NULL;
4473     Node* no_particular_field = NULL;
4474     int raw_adr_idx = Compile::AliasIdxRaw;
4475     post_barrier(control(),
4476                  memory(raw_adr_type),
4477                  alloc_obj,
4478                  no_particular_field,
4479                  raw_adr_idx,
4480                  no_particular_value,
4481                  T_OBJECT,
4482                  false);
4483   }
4484 
4485   // Do not let reads from the cloned object float above the arraycopy.
4486   if (alloc != NULL) {
4487     // Do not let stores that initialize this object be reordered with
4488     // a subsequent store that would make this object accessible by
4489     // other threads.
4490     // Record what AllocateNode this StoreStore protects so that
4491     // escape analysis can go from the MemBarStoreStoreNode to the
4492     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4493     // based on the escape status of the AllocateNode.
4494     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4495   } else {
4496     insert_mem_bar(Op_MemBarCPUOrder);
4497   }
4498 }
4499 
4500 //------------------------inline_native_clone----------------------------
4501 // protected native Object java.lang.Object.clone();
4502 //
4503 // Here are the simple edge cases:
4504 //  null receiver =&gt; normal trap
4505 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4506 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4507 //
4508 // The general case has two steps, allocation and copying.
4509 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4510 //
4511 // Copying also has two cases, oop arrays and everything else.
4512 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4513 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4514 //
4515 // These steps fold up nicely if and when the cloned object's klass
4516 // can be sharply typed as an object array, a type array, or an instance.
4517 //
4518 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4519   PhiNode* result_val;
4520 
4521   // Set the reexecute bit for the interpreter to reexecute
4522   // the bytecode that invokes Object.clone if deoptimization happens.
4523   { PreserveReexecuteState preexecs(this);
4524     jvms()-&gt;set_should_reexecute(true);
4525 
4526     Node* obj = null_check_receiver();
4527     if (stopped())  return true;
4528 
4529     Node* obj_klass = load_object_klass(obj);
4530     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4531     const TypeOopPtr*   toop   = ((tklass != NULL)
4532                                 ? tklass-&gt;as_instance_type()
4533                                 : TypeInstPtr::NOTNULL);
4534 
4535     // Conservatively insert a memory barrier on all memory slices.
4536     // Do not let writes into the original float below the clone.
4537     insert_mem_bar(Op_MemBarCPUOrder);
4538 
4539     // paths into result_reg:
4540     enum {
4541       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4542       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4543       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4544       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4545       PATH_LIMIT
4546     };
4547     RegionNode* result_reg = new(C) RegionNode(PATH_LIMIT);
4548     result_val             = new(C) PhiNode(result_reg,
4549                                             TypeInstPtr::NOTNULL);
4550     PhiNode*    result_i_o = new(C) PhiNode(result_reg, Type::ABIO);
4551     PhiNode*    result_mem = new(C) PhiNode(result_reg, Type::MEMORY,
4552                                             TypePtr::BOTTOM);
4553     record_for_igvn(result_reg);
4554 
4555     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4556     int raw_adr_idx = Compile::AliasIdxRaw;
4557 
4558     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4559     if (array_ctl != NULL) {
4560       // It's an array.
4561       PreserveJVMState pjvms(this);
4562       set_control(array_ctl);
4563       Node* obj_length = load_array_length(obj);
4564       Node* obj_size  = NULL;
4565       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4566 
4567       if (!use_ReduceInitialCardMarks()) {
4568         // If it is an oop array, it requires very special treatment,
4569         // because card marking is required on each card of the array.
4570         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4571         if (is_obja != NULL) {
4572           PreserveJVMState pjvms2(this);
4573           set_control(is_obja);
4574           // Generate a direct call to the right arraycopy function(s).
4575           bool disjoint_bases = true;
4576           bool length_never_negative = true;
4577           generate_arraycopy(TypeAryPtr::OOPS, T_OBJECT,
4578                              obj, intcon(0), alloc_obj, intcon(0),
4579                              obj_length,
4580                              disjoint_bases, length_never_negative);
4581           result_reg-&gt;init_req(_objArray_path, control());
4582           result_val-&gt;init_req(_objArray_path, alloc_obj);
4583           result_i_o -&gt;set_req(_objArray_path, i_o());
4584           result_mem -&gt;set_req(_objArray_path, reset_memory());
4585         }
4586       }
4587       // Otherwise, there are no card marks to worry about.
4588       // (We can dispense with card marks if we know the allocation
4589       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4590       //  causes the non-eden paths to take compensating steps to
4591       //  simulate a fresh allocation, so that no further
4592       //  card marks are required in compiled code to initialize
4593       //  the object.)
4594 
4595       if (!stopped()) {
4596         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4597 
4598         // Present the results of the copy.
4599         result_reg-&gt;init_req(_array_path, control());
4600         result_val-&gt;init_req(_array_path, alloc_obj);
4601         result_i_o -&gt;set_req(_array_path, i_o());
4602         result_mem -&gt;set_req(_array_path, reset_memory());
4603       }
4604     }
4605 
4606     // We only go to the instance fast case code if we pass a number of guards.
4607     // The paths which do not pass are accumulated in the slow_region.
4608     RegionNode* slow_region = new (C) RegionNode(1);
4609     record_for_igvn(slow_region);
4610     if (!stopped()) {
4611       // It's an instance (we did array above).  Make the slow-path tests.
4612       // If this is a virtual call, we generate a funny guard.  We grab
4613       // the vtable entry corresponding to clone() from the target object.
4614       // If the target method which we are calling happens to be the
4615       // Object clone() method, we pass the guard.  We do not need this
4616       // guard for non-virtual calls; the caller is known to be the native
4617       // Object clone().
4618       if (is_virtual) {
4619         generate_virtual_guard(obj_klass, slow_region);
4620       }
4621 
4622       // The object must be cloneable and must not have a finalizer.
4623       // Both of these conditions may be checked in a single test.
4624       // We could optimize the cloneable test further, but we don't care.
4625       generate_access_flags_guard(obj_klass,
4626                                   // Test both conditions:
4627                                   JVM_ACC_IS_CLONEABLE | JVM_ACC_HAS_FINALIZER,
4628                                   // Must be cloneable but not finalizer:
4629                                   JVM_ACC_IS_CLONEABLE,
4630                                   slow_region);
4631     }
4632 
4633     if (!stopped()) {
4634       // It's an instance, and it passed the slow-path tests.
4635       PreserveJVMState pjvms(this);
4636       Node* obj_size  = NULL;
4637       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4638       // is reexecuted if deoptimization occurs and there could be problems when merging
4639       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4640       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4641 
4642       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4643 
4644       // Present the results of the slow call.
4645       result_reg-&gt;init_req(_instance_path, control());
4646       result_val-&gt;init_req(_instance_path, alloc_obj);
4647       result_i_o -&gt;set_req(_instance_path, i_o());
4648       result_mem -&gt;set_req(_instance_path, reset_memory());
4649     }
4650 
4651     // Generate code for the slow case.  We make a call to clone().
4652     set_control(_gvn.transform(slow_region));
4653     if (!stopped()) {
4654       PreserveJVMState pjvms(this);
4655       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4656       Node* slow_result = set_results_for_java_call(slow_call);
4657       // this-&gt;control() comes from set_results_for_java_call
4658       result_reg-&gt;init_req(_slow_path, control());
4659       result_val-&gt;init_req(_slow_path, slow_result);
4660       result_i_o -&gt;set_req(_slow_path, i_o());
4661       result_mem -&gt;set_req(_slow_path, reset_memory());
4662     }
4663 
4664     // Return the combined state.
4665     set_control(    _gvn.transform(result_reg));
4666     set_i_o(        _gvn.transform(result_i_o));
4667     set_all_memory( _gvn.transform(result_mem));
4668   } // original reexecute is set back here
4669 
4670   set_result(_gvn.transform(result_val));
4671   return true;
4672 }
4673 
4674 //------------------------------basictype2arraycopy----------------------------
4675 address LibraryCallKit::basictype2arraycopy(BasicType t,
4676                                             Node* src_offset,
4677                                             Node* dest_offset,
4678                                             bool disjoint_bases,
4679                                             const char* &amp;name,
4680                                             bool dest_uninitialized) {
4681   const TypeInt* src_offset_inttype  = gvn().find_int_type(src_offset);;
4682   const TypeInt* dest_offset_inttype = gvn().find_int_type(dest_offset);;
4683 
4684   bool aligned = false;
4685   bool disjoint = disjoint_bases;
4686 
4687   // if the offsets are the same, we can treat the memory regions as
4688   // disjoint, because either the memory regions are in different arrays,
4689   // or they are identical (which we can treat as disjoint.)  We can also
4690   // treat a copy with a destination index  less that the source index
4691   // as disjoint since a low-&gt;high copy will work correctly in this case.
4692   if (src_offset_inttype != NULL &amp;&amp; src_offset_inttype-&gt;is_con() &amp;&amp;
4693       dest_offset_inttype != NULL &amp;&amp; dest_offset_inttype-&gt;is_con()) {
4694     // both indices are constants
4695     int s_offs = src_offset_inttype-&gt;get_con();
4696     int d_offs = dest_offset_inttype-&gt;get_con();
4697     int element_size = type2aelembytes(t);
4698     aligned = ((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &amp;&amp;
4699               ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0);
4700     if (s_offs &gt;= d_offs)  disjoint = true;
4701   } else if (src_offset == dest_offset &amp;&amp; src_offset != NULL) {
4702     // This can occur if the offsets are identical non-constants.
4703     disjoint = true;
4704   }
4705 
4706   return StubRoutines::select_arraycopy_function(t, aligned, disjoint, name, dest_uninitialized);
4707 }
4708 
4709 
4710 //------------------------------inline_arraycopy-----------------------
4711 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4712 //                                                      Object dest, int destPos,
4713 //                                                      int length);
4714 bool LibraryCallKit::inline_arraycopy() {
4715   // Get the arguments.
4716   Node* src         = argument(0);  // type: oop
4717   Node* src_offset  = argument(1);  // type: int
4718   Node* dest        = argument(2);  // type: oop
4719   Node* dest_offset = argument(3);  // type: int
4720   Node* length      = argument(4);  // type: int
4721 
4722   // Compile time checks.  If any of these checks cannot be verified at compile time,
4723   // we do not make a fast path for this call.  Instead, we let the call remain as it
4724   // is.  The checks we choose to mandate at compile time are:
4725   //
4726   // (1) src and dest are arrays.
4727   const Type* src_type  = src-&gt;Value(&amp;_gvn);
4728   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
4729   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4730   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4731 
4732   // Do we have the type of src?
4733   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4734   // Do we have the type of dest?
4735   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4736   // Is the type for src from speculation?
4737   bool src_spec = false;
4738   // Is the type for dest from speculation?
4739   bool dest_spec = false;
4740 
4741   if (!has_src || !has_dest) {
4742     // We don't have sufficient type information, let's see if
4743     // speculative types can help. We need to have types for both src
4744     // and dest so that it pays off.
4745 
4746     // Do we already have or could we have type information for src
4747     bool could_have_src = has_src;
4748     // Do we already have or could we have type information for dest
4749     bool could_have_dest = has_dest;
4750 
4751     ciKlass* src_k = NULL;
4752     if (!has_src) {
4753       src_k = src_type-&gt;speculative_type();
4754       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4755         could_have_src = true;
4756       }
4757     }
4758 
4759     ciKlass* dest_k = NULL;
4760     if (!has_dest) {
4761       dest_k = dest_type-&gt;speculative_type();
4762       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4763         could_have_dest = true;
4764       }
4765     }
4766 
4767     if (could_have_src &amp;&amp; could_have_dest) {
4768       // This is going to pay off so emit the required guards
4769       if (!has_src) {
4770         src = maybe_cast_profiled_obj(src, src_k);
4771         src_type  = _gvn.type(src);
4772         top_src  = src_type-&gt;isa_aryptr();
4773         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4774         src_spec = true;
4775       }
4776       if (!has_dest) {
4777         dest = maybe_cast_profiled_obj(dest, dest_k);
4778         dest_type  = _gvn.type(dest);
4779         top_dest  = dest_type-&gt;isa_aryptr();
4780         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4781         dest_spec = true;
4782       }
4783     }
4784   }
4785 
4786   if (!has_src || !has_dest) {
4787     // Conservatively insert a memory barrier on all memory slices.
4788     // Do not let writes into the source float below the arraycopy.
4789     insert_mem_bar(Op_MemBarCPUOrder);
4790 
4791     // Call StubRoutines::generic_arraycopy stub.
4792     generate_arraycopy(TypeRawPtr::BOTTOM, T_CONFLICT,
4793                        src, src_offset, dest, dest_offset, length);
4794 
4795     // Do not let reads from the destination float above the arraycopy.
4796     // Since we cannot type the arrays, we don't know which slices
4797     // might be affected.  We could restrict this barrier only to those
4798     // memory slices which pertain to array elements--but don't bother.
4799     if (!InsertMemBarAfterArraycopy)
4800       // (If InsertMemBarAfterArraycopy, there is already one in place.)
4801       insert_mem_bar(Op_MemBarCPUOrder);
4802     return true;
4803   }
4804 
4805   // (2) src and dest arrays must have elements of the same BasicType
4806   // Figure out the size and type of the elements we will be copying.
4807   BasicType src_elem  =  top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4808   BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4809   if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
4810   if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
4811 
4812   if (src_elem != dest_elem || dest_elem == T_VOID) {
4813     // The component types are not the same or are not recognized.  Punt.
4814     // (But, avoid the native method wrapper to JVM_ArrayCopy.)
4815     generate_slow_arraycopy(TypePtr::BOTTOM,
4816                             src, src_offset, dest, dest_offset, length,
4817                             /*dest_uninitialized*/false);
4818     return true;
4819   }
4820 
4821   if (src_elem == T_OBJECT) {
4822     // If both arrays are object arrays then having the exact types
4823     // for both will remove the need for a subtype check at runtime
4824     // before the call and may make it possible to pick a faster copy
4825     // routine (without a subtype check on every element)
4826     // Do we have the exact type of src?
4827     bool could_have_src = src_spec;
4828     // Do we have the exact type of dest?
4829     bool could_have_dest = dest_spec;
4830     ciKlass* src_k = top_src-&gt;klass();
4831     ciKlass* dest_k = top_dest-&gt;klass();
4832     if (!src_spec) {
4833       src_k = src_type-&gt;speculative_type();
4834       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4835           could_have_src = true;
4836       }
4837     }
4838     if (!dest_spec) {
4839       dest_k = dest_type-&gt;speculative_type();
4840       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4841         could_have_dest = true;
4842       }
4843     }
4844     if (could_have_src &amp;&amp; could_have_dest) {
4845       // If we can have both exact types, emit the missing guards
4846       if (could_have_src &amp;&amp; !src_spec) {
4847         src = maybe_cast_profiled_obj(src, src_k);
4848       }
4849       if (could_have_dest &amp;&amp; !dest_spec) {
4850         dest = maybe_cast_profiled_obj(dest, dest_k);
4851       }
4852     }
4853   }
4854 
4855   //---------------------------------------------------------------------------
4856   // We will make a fast path for this call to arraycopy.
4857 
4858   // We have the following tests left to perform:
4859   //
4860   // (3) src and dest must not be null.
4861   // (4) src_offset must not be negative.
4862   // (5) dest_offset must not be negative.
4863   // (6) length must not be negative.
4864   // (7) src_offset + length must not exceed length of src.
4865   // (8) dest_offset + length must not exceed length of dest.
4866   // (9) each element of an oop array must be assignable
4867 
4868   RegionNode* slow_region = new (C) RegionNode(1);
4869   record_for_igvn(slow_region);
4870 
4871   // (3) operands must not be null
4872   // We currently perform our null checks with the null_check routine.
4873   // This means that the null exceptions will be reported in the caller
4874   // rather than (correctly) reported inside of the native arraycopy call.
4875   // This should be corrected, given time.  We do our null check with the
4876   // stack pointer restored.
4877   src  = null_check(src,  T_ARRAY);
4878   dest = null_check(dest, T_ARRAY);
4879 
4880   // (4) src_offset must not be negative.
4881   generate_negative_guard(src_offset, slow_region);
4882 
4883   // (5) dest_offset must not be negative.
4884   generate_negative_guard(dest_offset, slow_region);
4885 
4886   // (6) length must not be negative (moved to generate_arraycopy()).
4887   // generate_negative_guard(length, slow_region);
4888 
4889   // (7) src_offset + length must not exceed length of src.
4890   generate_limit_guard(src_offset, length,
4891                        load_array_length(src),
4892                        slow_region);
4893 
4894   // (8) dest_offset + length must not exceed length of dest.
4895   generate_limit_guard(dest_offset, length,
4896                        load_array_length(dest),
4897                        slow_region);
4898 
4899   // (9) each element of an oop array must be assignable
4900   // The generate_arraycopy subroutine checks this.
4901 
4902   // This is where the memory effects are placed:
4903   const TypePtr* adr_type = TypeAryPtr::get_array_body_type(dest_elem);
4904   generate_arraycopy(adr_type, dest_elem,
4905                      src, src_offset, dest, dest_offset, length,
4906                      false, false, slow_region);
4907 
4908   return true;
4909 }
4910 
4911 //-----------------------------generate_arraycopy----------------------
4912 // Generate an optimized call to arraycopy.
4913 // Caller must guard against non-arrays.
4914 // Caller must determine a common array basic-type for both arrays.
4915 // Caller must validate offsets against array bounds.
4916 // The slow_region has already collected guard failure paths
4917 // (such as out of bounds length or non-conformable array types).
4918 // The generated code has this shape, in general:
4919 //
4920 //     if (length == 0)  return   // via zero_path
4921 //     slowval = -1
4922 //     if (types unknown) {
4923 //       slowval = call generic copy loop
4924 //       if (slowval == 0)  return  // via checked_path
4925 //     } else if (indexes in bounds) {
4926 //       if ((is object array) &amp;&amp; !(array type check)) {
4927 //         slowval = call checked copy loop
4928 //         if (slowval == 0)  return  // via checked_path
4929 //       } else {
4930 //         call bulk copy loop
4931 //         return  // via fast_path
4932 //       }
4933 //     }
4934 //     // adjust params for remaining work:
4935 //     if (slowval != -1) {
4936 //       n = -1^slowval; src_offset += n; dest_offset += n; length -= n
4937 //     }
4938 //   slow_region:
4939 //     call slow arraycopy(src, src_offset, dest, dest_offset, length)
4940 //     return  // via slow_call_path
4941 //
4942 // This routine is used from several intrinsics:  System.arraycopy,
4943 // Object.clone (the array subcase), and Arrays.copyOf[Range].
4944 //
4945 void
4946 LibraryCallKit::generate_arraycopy(const TypePtr* adr_type,
4947                                    BasicType basic_elem_type,
4948                                    Node* src,  Node* src_offset,
4949                                    Node* dest, Node* dest_offset,
4950                                    Node* copy_length,
4951                                    bool disjoint_bases,
4952                                    bool length_never_negative,
4953                                    RegionNode* slow_region) {
4954 
4955   if (slow_region == NULL) {
4956     slow_region = new(C) RegionNode(1);
4957     record_for_igvn(slow_region);
4958   }
4959 
4960   Node* original_dest      = dest;
4961   AllocateArrayNode* alloc = NULL;  // used for zeroing, if needed
4962   bool  dest_uninitialized = false;
4963 
4964   // See if this is the initialization of a newly-allocated array.
4965   // If so, we will take responsibility here for initializing it to zero.
4966   // (Note:  Because tightly_coupled_allocation performs checks on the
4967   // out-edges of the dest, we need to avoid making derived pointers
4968   // from it until we have checked its uses.)
4969   if (ReduceBulkZeroing
4970       &amp;&amp; !ZeroTLAB              // pointless if already zeroed
4971       &amp;&amp; basic_elem_type != T_CONFLICT // avoid corner case
4972       &amp;&amp; !src-&gt;eqv_uncast(dest)
4973       &amp;&amp; ((alloc = tightly_coupled_allocation(dest, slow_region))
4974           != NULL)
4975       &amp;&amp; _gvn.find_int_con(alloc-&gt;in(AllocateNode::ALength), 1) &gt; 0
4976       &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn)) {
4977     // "You break it, you buy it."
4978     InitializeNode* init = alloc-&gt;initialization();
4979     assert(init-&gt;is_complete(), "we just did this");
4980     init-&gt;set_complete_with_arraycopy();
4981     assert(dest-&gt;is_CheckCastPP(), "sanity");
4982     assert(dest-&gt;in(0)-&gt;in(0) == init, "dest pinned");
4983     adr_type = TypeRawPtr::BOTTOM;  // all initializations are into raw memory
4984     // From this point on, every exit path is responsible for
4985     // initializing any non-copied parts of the object to zero.
4986     // Also, if this flag is set we make sure that arraycopy interacts properly
4987     // with G1, eliding pre-barriers. See CR 6627983.
4988     dest_uninitialized = true;
4989   } else {
4990     // No zeroing elimination here.
4991     alloc             = NULL;
4992     //original_dest   = dest;
4993     //dest_uninitialized = false;
4994   }
4995 
4996   // Results are placed here:
4997   enum { fast_path        = 1,  // normal void-returning assembly stub
4998          checked_path     = 2,  // special assembly stub with cleanup
4999          slow_call_path   = 3,  // something went wrong; call the VM
5000          zero_path        = 4,  // bypass when length of copy is zero
5001          bcopy_path       = 5,  // copy primitive array by 64-bit blocks
5002          PATH_LIMIT       = 6
5003   };
5004   RegionNode* result_region = new(C) RegionNode(PATH_LIMIT);
5005   PhiNode*    result_i_o    = new(C) PhiNode(result_region, Type::ABIO);
5006   PhiNode*    result_memory = new(C) PhiNode(result_region, Type::MEMORY, adr_type);
5007   record_for_igvn(result_region);
5008   _gvn.set_type_bottom(result_i_o);
5009   _gvn.set_type_bottom(result_memory);
5010   assert(adr_type != TypePtr::BOTTOM, "must be RawMem or a T[] slice");
5011 
5012   // The slow_control path:
5013   Node* slow_control;
5014   Node* slow_i_o = i_o();
5015   Node* slow_mem = memory(adr_type);
5016   debug_only(slow_control = (Node*) badAddress);
5017 
5018   // Checked control path:
5019   Node* checked_control = top();
5020   Node* checked_mem     = NULL;
5021   Node* checked_i_o     = NULL;
5022   Node* checked_value   = NULL;
5023 
5024   if (basic_elem_type == T_CONFLICT) {
5025     assert(!dest_uninitialized, "");
5026     Node* cv = generate_generic_arraycopy(adr_type,
5027                                           src, src_offset, dest, dest_offset,
5028                                           copy_length, dest_uninitialized);
5029     if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5030     checked_control = control();
5031     checked_i_o     = i_o();
5032     checked_mem     = memory(adr_type);
5033     checked_value   = cv;
5034     set_control(top());         // no fast path
5035   }
5036 
5037   Node* not_pos = generate_nonpositive_guard(copy_length, length_never_negative);
5038   if (not_pos != NULL) {
5039     PreserveJVMState pjvms(this);
5040     set_control(not_pos);
5041 
5042     // (6) length must not be negative.
5043     if (!length_never_negative) {
5044       generate_negative_guard(copy_length, slow_region);
5045     }
5046 
5047     // copy_length is 0.
5048     if (!stopped() &amp;&amp; dest_uninitialized) {
5049       Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
5050       if (copy_length-&gt;eqv_uncast(dest_length)
5051           || _gvn.find_int_con(dest_length, 1) &lt;= 0) {
5052         // There is no zeroing to do. No need for a secondary raw memory barrier.
5053       } else {
5054         // Clear the whole thing since there are no source elements to copy.
5055         generate_clear_array(adr_type, dest, basic_elem_type,
5056                              intcon(0), NULL,
5057                              alloc-&gt;in(AllocateNode::AllocSize));
5058         // Use a secondary InitializeNode as raw memory barrier.
5059         // Currently it is needed only on this path since other
5060         // paths have stub or runtime calls as raw memory barriers.
5061         InitializeNode* init = insert_mem_bar_volatile(Op_Initialize,
5062                                                        Compile::AliasIdxRaw,
5063                                                        top())-&gt;as_Initialize();
5064         init-&gt;set_complete(&amp;_gvn);  // (there is no corresponding AllocateNode)
5065       }
5066     }
5067 
5068     // Present the results of the fast call.
5069     result_region-&gt;init_req(zero_path, control());
5070     result_i_o   -&gt;init_req(zero_path, i_o());
5071     result_memory-&gt;init_req(zero_path, memory(adr_type));
5072   }
5073 
5074   if (!stopped() &amp;&amp; dest_uninitialized) {
5075     // We have to initialize the *uncopied* part of the array to zero.
5076     // The copy destination is the slice dest[off..off+len].  The other slices
5077     // are dest_head = dest[0..off] and dest_tail = dest[off+len..dest.length].
5078     Node* dest_size   = alloc-&gt;in(AllocateNode::AllocSize);
5079     Node* dest_length = alloc-&gt;in(AllocateNode::ALength);
5080     Node* dest_tail   = _gvn.transform(new(C) AddINode(dest_offset,
5081                                                           copy_length));
5082 
5083     // If there is a head section that needs zeroing, do it now.
5084     if (find_int_con(dest_offset, -1) != 0) {
5085       generate_clear_array(adr_type, dest, basic_elem_type,
5086                            intcon(0), dest_offset,
5087                            NULL);
5088     }
5089 
5090     // Next, perform a dynamic check on the tail length.
5091     // It is often zero, and we can win big if we prove this.
5092     // There are two wins:  Avoid generating the ClearArray
5093     // with its attendant messy index arithmetic, and upgrade
5094     // the copy to a more hardware-friendly word size of 64 bits.
5095     Node* tail_ctl = NULL;
5096     if (!stopped() &amp;&amp; !dest_tail-&gt;eqv_uncast(dest_length)) {
5097       Node* cmp_lt   = _gvn.transform(new(C) CmpINode(dest_tail, dest_length));
5098       Node* bol_lt   = _gvn.transform(new(C) BoolNode(cmp_lt, BoolTest::lt));
5099       tail_ctl = generate_slow_guard(bol_lt, NULL);
5100       assert(tail_ctl != NULL || !stopped(), "must be an outcome");
5101     }
5102 
5103     // At this point, let's assume there is no tail.
5104     if (!stopped() &amp;&amp; alloc != NULL &amp;&amp; basic_elem_type != T_OBJECT) {
5105       // There is no tail.  Try an upgrade to a 64-bit copy.
5106       bool didit = false;
5107       { PreserveJVMState pjvms(this);
5108         didit = generate_block_arraycopy(adr_type, basic_elem_type, alloc,
5109                                          src, src_offset, dest, dest_offset,
5110                                          dest_size, dest_uninitialized);
5111         if (didit) {
5112           // Present the results of the block-copying fast call.
5113           result_region-&gt;init_req(bcopy_path, control());
5114           result_i_o   -&gt;init_req(bcopy_path, i_o());
5115           result_memory-&gt;init_req(bcopy_path, memory(adr_type));
5116         }
5117       }
5118       if (didit)
5119         set_control(top());     // no regular fast path
5120     }
5121 
5122     // Clear the tail, if any.
5123     if (tail_ctl != NULL) {
5124       Node* notail_ctl = stopped() ? NULL : control();
5125       set_control(tail_ctl);
5126       if (notail_ctl == NULL) {
5127         generate_clear_array(adr_type, dest, basic_elem_type,
5128                              dest_tail, NULL,
5129                              dest_size);
5130       } else {
5131         // Make a local merge.
5132         Node* done_ctl = new(C) RegionNode(3);
5133         Node* done_mem = new(C) PhiNode(done_ctl, Type::MEMORY, adr_type);
5134         done_ctl-&gt;init_req(1, notail_ctl);
5135         done_mem-&gt;init_req(1, memory(adr_type));
5136         generate_clear_array(adr_type, dest, basic_elem_type,
5137                              dest_tail, NULL,
5138                              dest_size);
5139         done_ctl-&gt;init_req(2, control());
5140         done_mem-&gt;init_req(2, memory(adr_type));
5141         set_control( _gvn.transform(done_ctl));
5142         set_memory(  _gvn.transform(done_mem), adr_type );
5143       }
5144     }
5145   }
5146 
5147   BasicType copy_type = basic_elem_type;
5148   assert(basic_elem_type != T_ARRAY, "caller must fix this");
5149   if (!stopped() &amp;&amp; copy_type == T_OBJECT) {
5150     // If src and dest have compatible element types, we can copy bits.
5151     // Types S[] and D[] are compatible if D is a supertype of S.
5152     //
5153     // If they are not, we will use checked_oop_disjoint_arraycopy,
5154     // which performs a fast optimistic per-oop check, and backs off
5155     // further to JVM_ArrayCopy on the first per-oop check that fails.
5156     // (Actually, we don't move raw bits only; the GC requires card marks.)
5157 
5158     // Get the Klass* for both src and dest
5159     Node* src_klass  = load_object_klass(src);
5160     Node* dest_klass = load_object_klass(dest);
5161 
5162     // Generate the subtype check.
5163     // This might fold up statically, or then again it might not.
5164     //
5165     // Non-static example:  Copying List&lt;String&gt;.elements to a new String[].
5166     // The backing store for a List&lt;String&gt; is always an Object[],
5167     // but its elements are always type String, if the generic types
5168     // are correct at the source level.
5169     //
5170     // Test S[] against D[], not S against D, because (probably)
5171     // the secondary supertype cache is less busy for S[] than S.
5172     // This usually only matters when D is an interface.
5173     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5174     // Plug failing path into checked_oop_disjoint_arraycopy
5175     if (not_subtype_ctrl != top()) {
5176       PreserveJVMState pjvms(this);
5177       set_control(not_subtype_ctrl);
5178       // (At this point we can assume disjoint_bases, since types differ.)
5179       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
5180       Node* p1 = basic_plus_adr(dest_klass, ek_offset);
5181       Node* n1 = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p1, TypeRawPtr::BOTTOM);
5182       Node* dest_elem_klass = _gvn.transform(n1);
5183       Node* cv = generate_checkcast_arraycopy(adr_type,
5184                                               dest_elem_klass,
5185                                               src, src_offset, dest, dest_offset,
5186                                               ConvI2X(copy_length), dest_uninitialized);
5187       if (cv == NULL)  cv = intcon(-1);  // failure (no stub available)
5188       checked_control = control();
5189       checked_i_o     = i_o();
5190       checked_mem     = memory(adr_type);
5191       checked_value   = cv;
5192     }
5193     // At this point we know we do not need type checks on oop stores.
5194 
5195     // Let's see if we need card marks:
5196     if (alloc != NULL &amp;&amp; use_ReduceInitialCardMarks()) {
5197       // If we do not need card marks, copy using the jint or jlong stub.
5198       copy_type = LP64_ONLY(UseCompressedOops ? T_INT : T_LONG) NOT_LP64(T_INT);
5199       assert(type2aelembytes(basic_elem_type) == type2aelembytes(copy_type),
5200              "sizes agree");
5201     }
5202   }
5203 
5204   if (!stopped()) {
5205     // Generate the fast path, if possible.
5206     PreserveJVMState pjvms(this);
5207     generate_unchecked_arraycopy(adr_type, copy_type, disjoint_bases,
5208                                  src, src_offset, dest, dest_offset,
5209                                  ConvI2X(copy_length), dest_uninitialized);
5210 
5211     // Present the results of the fast call.
5212     result_region-&gt;init_req(fast_path, control());
5213     result_i_o   -&gt;init_req(fast_path, i_o());
5214     result_memory-&gt;init_req(fast_path, memory(adr_type));
5215   }
5216 
5217   // Here are all the slow paths up to this point, in one bundle:
5218   slow_control = top();
5219   if (slow_region != NULL)
5220     slow_control = _gvn.transform(slow_region);
5221   DEBUG_ONLY(slow_region = (RegionNode*)badAddress);
5222 
5223   set_control(checked_control);
5224   if (!stopped()) {
5225     // Clean up after the checked call.
5226     // The returned value is either 0 or -1^K,
5227     // where K = number of partially transferred array elements.
5228     Node* cmp = _gvn.transform(new(C) CmpINode(checked_value, intcon(0)));
5229     Node* bol = _gvn.transform(new(C) BoolNode(cmp, BoolTest::eq));
5230     IfNode* iff = create_and_map_if(control(), bol, PROB_MAX, COUNT_UNKNOWN);
5231 
5232     // If it is 0, we are done, so transfer to the end.
5233     Node* checks_done = _gvn.transform(new(C) IfTrueNode(iff));
5234     result_region-&gt;init_req(checked_path, checks_done);
5235     result_i_o   -&gt;init_req(checked_path, checked_i_o);
5236     result_memory-&gt;init_req(checked_path, checked_mem);
5237 
5238     // If it is not zero, merge into the slow call.
5239     set_control( _gvn.transform(new(C) IfFalseNode(iff) ));
5240     RegionNode* slow_reg2 = new(C) RegionNode(3);
5241     PhiNode*    slow_i_o2 = new(C) PhiNode(slow_reg2, Type::ABIO);
5242     PhiNode*    slow_mem2 = new(C) PhiNode(slow_reg2, Type::MEMORY, adr_type);
5243     record_for_igvn(slow_reg2);
5244     slow_reg2  -&gt;init_req(1, slow_control);
5245     slow_i_o2  -&gt;init_req(1, slow_i_o);
5246     slow_mem2  -&gt;init_req(1, slow_mem);
5247     slow_reg2  -&gt;init_req(2, control());
5248     slow_i_o2  -&gt;init_req(2, checked_i_o);
5249     slow_mem2  -&gt;init_req(2, checked_mem);
5250 
5251     slow_control = _gvn.transform(slow_reg2);
5252     slow_i_o     = _gvn.transform(slow_i_o2);
5253     slow_mem     = _gvn.transform(slow_mem2);
5254 
5255     if (alloc != NULL) {
5256       // We'll restart from the very beginning, after zeroing the whole thing.
5257       // This can cause double writes, but that's OK since dest is brand new.
5258       // So we ignore the low 31 bits of the value returned from the stub.
5259     } else {
5260       // We must continue the copy exactly where it failed, or else
5261       // another thread might see the wrong number of writes to dest.
5262       Node* checked_offset = _gvn.transform(new(C) XorINode(checked_value, intcon(-1)));
5263       Node* slow_offset    = new(C) PhiNode(slow_reg2, TypeInt::INT);
5264       slow_offset-&gt;init_req(1, intcon(0));
5265       slow_offset-&gt;init_req(2, checked_offset);
5266       slow_offset  = _gvn.transform(slow_offset);
5267 
5268       // Adjust the arguments by the conditionally incoming offset.
5269       Node* src_off_plus  = _gvn.transform(new(C) AddINode(src_offset,  slow_offset));
5270       Node* dest_off_plus = _gvn.transform(new(C) AddINode(dest_offset, slow_offset));
5271       Node* length_minus  = _gvn.transform(new(C) SubINode(copy_length, slow_offset));
5272 
5273       // Tweak the node variables to adjust the code produced below:
5274       src_offset  = src_off_plus;
5275       dest_offset = dest_off_plus;
5276       copy_length = length_minus;
5277     }
5278   }
5279 
5280   set_control(slow_control);
5281   if (!stopped()) {
5282     // Generate the slow path, if needed.
5283     PreserveJVMState pjvms(this);   // replace_in_map may trash the map
5284 
5285     set_memory(slow_mem, adr_type);
5286     set_i_o(slow_i_o);
5287 
5288     if (dest_uninitialized) {
5289       generate_clear_array(adr_type, dest, basic_elem_type,
5290                            intcon(0), NULL,
5291                            alloc-&gt;in(AllocateNode::AllocSize));
5292     }
5293 
5294     generate_slow_arraycopy(adr_type,
5295                             src, src_offset, dest, dest_offset,
5296                             copy_length, /*dest_uninitialized*/false);
5297 
5298     result_region-&gt;init_req(slow_call_path, control());
5299     result_i_o   -&gt;init_req(slow_call_path, i_o());
5300     result_memory-&gt;init_req(slow_call_path, memory(adr_type));
5301   }
5302 
5303   // Remove unused edges.
5304   for (uint i = 1; i &lt; result_region-&gt;req(); i++) {
5305     if (result_region-&gt;in(i) == NULL)
5306       result_region-&gt;init_req(i, top());
5307   }
5308 
5309   // Finished; return the combined state.
5310   set_control( _gvn.transform(result_region));
5311   set_i_o(     _gvn.transform(result_i_o)    );
5312   set_memory(  _gvn.transform(result_memory), adr_type );
5313 
5314   // The memory edges above are precise in order to model effects around
5315   // array copies accurately to allow value numbering of field loads around
5316   // arraycopy.  Such field loads, both before and after, are common in Java
5317   // collections and similar classes involving header/array data structures.
5318   //
5319   // But with low number of register or when some registers are used or killed
5320   // by arraycopy calls it causes registers spilling on stack. See 6544710.
5321   // The next memory barrier is added to avoid it. If the arraycopy can be
5322   // optimized away (which it can, sometimes) then we can manually remove
5323   // the membar also.
5324   //
5325   // Do not let reads from the cloned object float above the arraycopy.
5326   if (alloc != NULL) {
5327     // Do not let stores that initialize this object be reordered with
5328     // a subsequent store that would make this object accessible by
5329     // other threads.
5330     // Record what AllocateNode this StoreStore protects so that
5331     // escape analysis can go from the MemBarStoreStoreNode to the
5332     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
5333     // based on the escape status of the AllocateNode.
5334     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
5335   } else if (InsertMemBarAfterArraycopy)
5336     insert_mem_bar(Op_MemBarCPUOrder);
5337 }
5338 
5339 
5340 // Helper function which determines if an arraycopy immediately follows
5341 // an allocation, with no intervening tests or other escapes for the object.
5342 AllocateArrayNode*
5343 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5344                                            RegionNode* slow_region) {
5345   if (stopped())             return NULL;  // no fast path
5346   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5347 
5348   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5349   if (alloc == NULL)  return NULL;
5350 
5351   Node* rawmem = memory(Compile::AliasIdxRaw);
5352   // Is the allocation's memory state untouched?
5353   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5354     // Bail out if there have been raw-memory effects since the allocation.
5355     // (Example:  There might have been a call or safepoint.)
5356     return NULL;
5357   }
5358   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5359   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5360     return NULL;
5361   }
5362 
5363   // There must be no unexpected observers of this allocation.
5364   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5365     Node* obs = ptr-&gt;fast_out(i);
5366     if (obs != this-&gt;map()) {
5367       return NULL;
5368     }
5369   }
5370 
5371   // This arraycopy must unconditionally follow the allocation of the ptr.
5372   Node* alloc_ctl = ptr-&gt;in(0);
5373   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5374 
5375   Node* ctl = control();
5376   while (ctl != alloc_ctl) {
5377     // There may be guards which feed into the slow_region.
5378     // Any other control flow means that we might not get a chance
5379     // to finish initializing the allocated object.
5380     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5381       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5382       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5383       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5384       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5385         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5386         continue;
5387       }
5388       // One more try:  Various low-level checks bottom out in
5389       // uncommon traps.  If the debug-info of the trap omits
5390       // any reference to the allocation, as we've already
5391       // observed, then there can be no objection to the trap.
5392       bool found_trap = false;
5393       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5394         Node* obs = not_ctl-&gt;fast_out(j);
5395         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5396             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5397           found_trap = true; break;
5398         }
5399       }
5400       if (found_trap) {
5401         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5402         continue;
5403       }
5404     }
5405     return NULL;
5406   }
5407 
5408   // If we get this far, we have an allocation which immediately
5409   // precedes the arraycopy, and we can take over zeroing the new object.
5410   // The arraycopy will finish the initialization, and provide
5411   // a new control state to which we will anchor the destination pointer.
5412 
5413   return alloc;
5414 }
5415 
5416 // Helper for initialization of arrays, creating a ClearArray.
5417 // It writes zero bits in [start..end), within the body of an array object.
5418 // The memory effects are all chained onto the 'adr_type' alias category.
5419 //
5420 // Since the object is otherwise uninitialized, we are free
5421 // to put a little "slop" around the edges of the cleared area,
5422 // as long as it does not go back into the array's header,
5423 // or beyond the array end within the heap.
5424 //
5425 // The lower edge can be rounded down to the nearest jint and the
5426 // upper edge can be rounded up to the nearest MinObjAlignmentInBytes.
5427 //
5428 // Arguments:
5429 //   adr_type           memory slice where writes are generated
5430 //   dest               oop of the destination array
5431 //   basic_elem_type    element type of the destination
5432 //   slice_idx          array index of first element to store
5433 //   slice_len          number of elements to store (or NULL)
5434 //   dest_size          total size in bytes of the array object
5435 //
5436 // Exactly one of slice_len or dest_size must be non-NULL.
5437 // If dest_size is non-NULL, zeroing extends to the end of the object.
5438 // If slice_len is non-NULL, the slice_idx value must be a constant.
5439 void
5440 LibraryCallKit::generate_clear_array(const TypePtr* adr_type,
5441                                      Node* dest,
5442                                      BasicType basic_elem_type,
5443                                      Node* slice_idx,
5444                                      Node* slice_len,
5445                                      Node* dest_size) {
5446   // one or the other but not both of slice_len and dest_size:
5447   assert((slice_len != NULL? 1: 0) + (dest_size != NULL? 1: 0) == 1, "");
5448   if (slice_len == NULL)  slice_len = top();
5449   if (dest_size == NULL)  dest_size = top();
5450 
5451   // operate on this memory slice:
5452   Node* mem = memory(adr_type); // memory slice to operate on
5453 
5454   // scaling and rounding of indexes:
5455   int scale = exact_log2(type2aelembytes(basic_elem_type));
5456   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5457   int clear_low = (-1 &lt;&lt; scale) &amp; (BytesPerInt  - 1);
5458   int bump_bit  = (-1 &lt;&lt; scale) &amp; BytesPerInt;
5459 
5460   // determine constant starts and ends
5461   const intptr_t BIG_NEG = -128;
5462   assert(BIG_NEG + 2*abase &lt; 0, "neg enough");
5463   intptr_t slice_idx_con = (intptr_t) find_int_con(slice_idx, BIG_NEG);
5464   intptr_t slice_len_con = (intptr_t) find_int_con(slice_len, BIG_NEG);
5465   if (slice_len_con == 0) {
5466     return;                     // nothing to do here
5467   }
5468   intptr_t start_con = (abase + (slice_idx_con &lt;&lt; scale)) &amp; ~clear_low;
5469   intptr_t end_con   = find_intptr_t_con(dest_size, -1);
5470   if (slice_idx_con &gt;= 0 &amp;&amp; slice_len_con &gt;= 0) {
5471     assert(end_con &lt; 0, "not two cons");
5472     end_con = round_to(abase + ((slice_idx_con + slice_len_con) &lt;&lt; scale),
5473                        BytesPerLong);
5474   }
5475 
5476   if (start_con &gt;= 0 &amp;&amp; end_con &gt;= 0) {
5477     // Constant start and end.  Simple.
5478     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5479                                        start_con, end_con, &amp;_gvn);
5480   } else if (start_con &gt;= 0 &amp;&amp; dest_size != top()) {
5481     // Constant start, pre-rounded end after the tail of the array.
5482     Node* end = dest_size;
5483     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5484                                        start_con, end, &amp;_gvn);
5485   } else if (start_con &gt;= 0 &amp;&amp; slice_len != top()) {
5486     // Constant start, non-constant end.  End needs rounding up.
5487     // End offset = round_up(abase + ((slice_idx_con + slice_len) &lt;&lt; scale), 8)
5488     intptr_t end_base  = abase + (slice_idx_con &lt;&lt; scale);
5489     int      end_round = (-1 &lt;&lt; scale) &amp; (BytesPerLong  - 1);
5490     Node*    end       = ConvI2X(slice_len);
5491     if (scale != 0)
5492       end = _gvn.transform(new(C) LShiftXNode(end, intcon(scale) ));
5493     end_base += end_round;
5494     end = _gvn.transform(new(C) AddXNode(end, MakeConX(end_base)));
5495     end = _gvn.transform(new(C) AndXNode(end, MakeConX(~end_round)));
5496     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5497                                        start_con, end, &amp;_gvn);
5498   } else if (start_con &lt; 0 &amp;&amp; dest_size != top()) {
5499     // Non-constant start, pre-rounded end after the tail of the array.
5500     // This is almost certainly a "round-to-end" operation.
5501     Node* start = slice_idx;
5502     start = ConvI2X(start);
5503     if (scale != 0)
5504       start = _gvn.transform(new(C) LShiftXNode( start, intcon(scale) ));
5505     start = _gvn.transform(new(C) AddXNode(start, MakeConX(abase)));
5506     if ((bump_bit | clear_low) != 0) {
5507       int to_clear = (bump_bit | clear_low);
5508       // Align up mod 8, then store a jint zero unconditionally
5509       // just before the mod-8 boundary.
5510       if (((abase + bump_bit) &amp; ~to_clear) - bump_bit
5511           &lt; arrayOopDesc::length_offset_in_bytes() + BytesPerInt) {
5512         bump_bit = 0;
5513         assert((abase &amp; to_clear) == 0, "array base must be long-aligned");
5514       } else {
5515         // Bump 'start' up to (or past) the next jint boundary:
5516         start = _gvn.transform(new(C) AddXNode(start, MakeConX(bump_bit)));
5517         assert((abase &amp; clear_low) == 0, "array base must be int-aligned");
5518       }
5519       // Round bumped 'start' down to jlong boundary in body of array.
5520       start = _gvn.transform(new(C) AndXNode(start, MakeConX(~to_clear)));
5521       if (bump_bit != 0) {
5522         // Store a zero to the immediately preceding jint:
5523         Node* x1 = _gvn.transform(new(C) AddXNode(start, MakeConX(-bump_bit)));
5524         Node* p1 = basic_plus_adr(dest, x1);
5525         mem = StoreNode::make(_gvn, control(), mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);
5526         mem = _gvn.transform(mem);
5527       }
5528     }
5529     Node* end = dest_size; // pre-rounded
5530     mem = ClearArrayNode::clear_memory(control(), mem, dest,
5531                                        start, end, &amp;_gvn);
5532   } else {
5533     // Non-constant start, unrounded non-constant end.
5534     // (Nobody zeroes a random midsection of an array using this routine.)
5535     ShouldNotReachHere();       // fix caller
5536   }
5537 
5538   // Done.
5539   set_memory(mem, adr_type);
5540 }
5541 
5542 
5543 bool
5544 LibraryCallKit::generate_block_arraycopy(const TypePtr* adr_type,
5545                                          BasicType basic_elem_type,
5546                                          AllocateNode* alloc,
5547                                          Node* src,  Node* src_offset,
5548                                          Node* dest, Node* dest_offset,
5549                                          Node* dest_size, bool dest_uninitialized) {
5550   // See if there is an advantage from block transfer.
5551   int scale = exact_log2(type2aelembytes(basic_elem_type));
5552   if (scale &gt;= LogBytesPerLong)
5553     return false;               // it is already a block transfer
5554 
5555   // Look at the alignment of the starting offsets.
5556   int abase = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
5557 
5558   intptr_t src_off_con  = (intptr_t) find_int_con(src_offset, -1);
5559   intptr_t dest_off_con = (intptr_t) find_int_con(dest_offset, -1);
5560   if (src_off_con &lt; 0 || dest_off_con &lt; 0)
5561     // At present, we can only understand constants.
5562     return false;
5563 
5564   intptr_t src_off  = abase + (src_off_con  &lt;&lt; scale);
5565   intptr_t dest_off = abase + (dest_off_con &lt;&lt; scale);
5566 
5567   if (((src_off | dest_off) &amp; (BytesPerLong-1)) != 0) {
5568     // Non-aligned; too bad.
5569     // One more chance:  Pick off an initial 32-bit word.
5570     // This is a common case, since abase can be odd mod 8.
5571     if (((src_off | dest_off) &amp; (BytesPerLong-1)) == BytesPerInt &amp;&amp;
5572         ((src_off ^ dest_off) &amp; (BytesPerLong-1)) == 0) {
5573       Node* sptr = basic_plus_adr(src,  src_off);
5574       Node* dptr = basic_plus_adr(dest, dest_off);
5575       Node* sval = make_load(control(), sptr, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
5576       store_to_memory(control(), dptr, sval, T_INT, adr_type, MemNode::unordered);
5577       src_off += BytesPerInt;
5578       dest_off += BytesPerInt;
5579     } else {
5580       return false;
5581     }
5582   }
5583   assert(src_off % BytesPerLong == 0, "");
5584   assert(dest_off % BytesPerLong == 0, "");
5585 
5586   // Do this copy by giant steps.
5587   Node* sptr  = basic_plus_adr(src,  src_off);
5588   Node* dptr  = basic_plus_adr(dest, dest_off);
5589   Node* countx = dest_size;
5590   countx = _gvn.transform(new (C) SubXNode(countx, MakeConX(dest_off)));
5591   countx = _gvn.transform(new (C) URShiftXNode(countx, intcon(LogBytesPerLong)));
5592 
5593   bool disjoint_bases = true;   // since alloc != NULL
5594   generate_unchecked_arraycopy(adr_type, T_LONG, disjoint_bases,
5595                                sptr, NULL, dptr, NULL, countx, dest_uninitialized);
5596 
5597   return true;
5598 }
5599 
5600 
5601 // Helper function; generates code for the slow case.
5602 // We make a call to a runtime method which emulates the native method,
5603 // but without the native wrapper overhead.
5604 void
5605 LibraryCallKit::generate_slow_arraycopy(const TypePtr* adr_type,
5606                                         Node* src,  Node* src_offset,
5607                                         Node* dest, Node* dest_offset,
5608                                         Node* copy_length, bool dest_uninitialized) {
5609   assert(!dest_uninitialized, "Invariant");
5610   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON,
5611                                  OptoRuntime::slow_arraycopy_Type(),
5612                                  OptoRuntime::slow_arraycopy_Java(),
5613                                  "slow_arraycopy", adr_type,
5614                                  src, src_offset, dest, dest_offset,
5615                                  copy_length);
5616 
5617   // Handle exceptions thrown by this fellow:
5618   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
5619 }
5620 
5621 // Helper function; generates code for cases requiring runtime checks.
5622 Node*
5623 LibraryCallKit::generate_checkcast_arraycopy(const TypePtr* adr_type,
5624                                              Node* dest_elem_klass,
5625                                              Node* src,  Node* src_offset,
5626                                              Node* dest, Node* dest_offset,
5627                                              Node* copy_length, bool dest_uninitialized) {
5628   if (stopped())  return NULL;
5629 
5630   address copyfunc_addr = StubRoutines::checkcast_arraycopy(dest_uninitialized);
5631   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5632     return NULL;
5633   }
5634 
5635   // Pick out the parameters required to perform a store-check
5636   // for the target array.  This is an optimistic check.  It will
5637   // look in each non-null element's class, at the desired klass's
5638   // super_check_offset, for the desired klass.
5639   int sco_offset = in_bytes(Klass::super_check_offset_offset());
5640   Node* p3 = basic_plus_adr(dest_elem_klass, sco_offset);
5641   Node* n3 = new(C) LoadINode(NULL, memory(p3), p3, _gvn.type(p3)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered);
5642   Node* check_offset = ConvI2X(_gvn.transform(n3));
5643   Node* check_value  = dest_elem_klass;
5644 
5645   Node* src_start  = array_element_address(src,  src_offset,  T_OBJECT);
5646   Node* dest_start = array_element_address(dest, dest_offset, T_OBJECT);
5647 
5648   // (We know the arrays are never conjoint, because their types differ.)
5649   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5650                                  OptoRuntime::checkcast_arraycopy_Type(),
5651                                  copyfunc_addr, "checkcast_arraycopy", adr_type,
5652                                  // five arguments, of which two are
5653                                  // intptr_t (jlong in LP64)
5654                                  src_start, dest_start,
5655                                  copy_length XTOP,
5656                                  check_offset XTOP,
5657                                  check_value);
5658 
5659   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5660 }
5661 
5662 
5663 // Helper function; generates code for cases requiring runtime checks.
5664 Node*
5665 LibraryCallKit::generate_generic_arraycopy(const TypePtr* adr_type,
5666                                            Node* src,  Node* src_offset,
5667                                            Node* dest, Node* dest_offset,
5668                                            Node* copy_length, bool dest_uninitialized) {
5669   assert(!dest_uninitialized, "Invariant");
5670   if (stopped())  return NULL;
5671   address copyfunc_addr = StubRoutines::generic_arraycopy();
5672   if (copyfunc_addr == NULL) { // Stub was not generated, go slow path.
5673     return NULL;
5674   }
5675 
5676   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5677                     OptoRuntime::generic_arraycopy_Type(),
5678                     copyfunc_addr, "generic_arraycopy", adr_type,
5679                     src, src_offset, dest, dest_offset, copy_length);
5680 
5681   return _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5682 }
5683 
5684 // Helper function; generates the fast out-of-line call to an arraycopy stub.
5685 void
5686 LibraryCallKit::generate_unchecked_arraycopy(const TypePtr* adr_type,
5687                                              BasicType basic_elem_type,
5688                                              bool disjoint_bases,
5689                                              Node* src,  Node* src_offset,
5690                                              Node* dest, Node* dest_offset,
5691                                              Node* copy_length, bool dest_uninitialized) {
5692   if (stopped())  return;               // nothing to do
5693 
5694   Node* src_start  = src;
5695   Node* dest_start = dest;
5696   if (src_offset != NULL || dest_offset != NULL) {
5697     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
5698     src_start  = array_element_address(src,  src_offset,  basic_elem_type);
5699     dest_start = array_element_address(dest, dest_offset, basic_elem_type);
5700   }
5701 
5702   // Figure out which arraycopy runtime method to call.
5703   const char* copyfunc_name = "arraycopy";
5704   address     copyfunc_addr =
5705       basictype2arraycopy(basic_elem_type, src_offset, dest_offset,
5706                           disjoint_bases, copyfunc_name, dest_uninitialized);
5707 
5708   // Call it.  Note that the count_ix value is not scaled to a byte-size.
5709   make_runtime_call(RC_LEAF|RC_NO_FP,
5710                     OptoRuntime::fast_arraycopy_Type(),
5711                     copyfunc_addr, copyfunc_name, adr_type,
5712                     src_start, dest_start, copy_length XTOP);
5713 }
5714 
5715 //-------------inline_encodeISOArray-----------------------------------
5716 // encode char[] to byte[] in ISO_8859_1
5717 bool LibraryCallKit::inline_encodeISOArray() {
5718   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5719   // no receiver since it is static method
5720   Node *src         = argument(0);
5721   Node *src_offset  = argument(1);
5722   Node *dst         = argument(2);
5723   Node *dst_offset  = argument(3);
5724   Node *length      = argument(4);
5725 
5726   const Type* src_type = src-&gt;Value(&amp;_gvn);
5727   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5728   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5729   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5730   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5731       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5732     // failed array check
5733     return false;
5734   }
5735 
5736   // Figure out the size and type of the elements we will be copying.
5737   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5738   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5739   if (src_elem != T_CHAR || dst_elem != T_BYTE) {
5740     return false;
5741   }
5742   Node* src_start = array_element_address(src, src_offset, src_elem);
5743   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5744   // 'src_start' points to src array + scaled offset
5745   // 'dst_start' points to dst array + scaled offset
5746 
5747   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5748   Node* enc = new (C) EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5749   enc = _gvn.transform(enc);
5750   Node* res_mem = _gvn.transform(new (C) SCMemProjNode(enc));
5751   set_memory(res_mem, mtype);
5752   set_result(enc);
5753   return true;
5754 }
5755 
5756 //-------------inline_multiplyToLen-----------------------------------
5757 bool LibraryCallKit::inline_multiplyToLen() {
5758   assert(UseMultiplyToLenIntrinsic, "not implementated on this platform");
5759 
5760   address stubAddr = StubRoutines::multiplyToLen();
5761   if (stubAddr == NULL) {
5762     return false; // Intrinsic's stub is not implemented on this platform
5763   }
5764   const char* stubName = "multiplyToLen";
5765 
5766   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5767 
5768   Node* x    = argument(1);
5769   Node* xlen = argument(2);
5770   Node* y    = argument(3);
5771   Node* ylen = argument(4);
5772   Node* z    = argument(5);
5773 
5774   const Type* x_type = x-&gt;Value(&amp;_gvn);
5775   const Type* y_type = y-&gt;Value(&amp;_gvn);
5776   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5777   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5778   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5779       top_y == NULL || top_y-&gt;klass() == NULL) {
5780     // failed array check
5781     return false;
5782   }
5783 
5784   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5785   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5786   if (x_elem != T_INT || y_elem != T_INT) {
5787     return false;
5788   }
5789 
5790   // Set the original stack and the reexecute bit for the interpreter to reexecute
5791   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5792   // on the return from z array allocation in runtime.
5793   { PreserveReexecuteState preexecs(this);
5794     jvms()-&gt;set_should_reexecute(true);
5795 
5796     Node* x_start = array_element_address(x, intcon(0), x_elem);
5797     Node* y_start = array_element_address(y, intcon(0), y_elem);
5798     // 'x_start' points to x array + scaled xlen
5799     // 'y_start' points to y array + scaled ylen
5800 
5801     // Allocate the result array
5802     Node* zlen = _gvn.transform(new(C) AddINode(xlen, ylen));
5803     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5804     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5805 
5806     IdealKit ideal(this);
5807 
5808 #define __ ideal.
5809      Node* one = __ ConI(1);
5810      Node* zero = __ ConI(0);
5811      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5812      __ set(need_alloc, zero);
5813      __ set(z_alloc, z);
5814      __ if_then(z, BoolTest::eq, null()); {
5815        __ increment (need_alloc, one);
5816      } __ else_(); {
5817        // Update graphKit memory and control from IdealKit.
5818        sync_kit(ideal);
5819        Node* zlen_arg = load_array_length(z);
5820        // Update IdealKit memory and control from graphKit.
5821        __ sync_kit(this);
5822        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5823          __ increment (need_alloc, one);
5824        } __ end_if();
5825      } __ end_if();
5826 
5827      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5828        // Update graphKit memory and control from IdealKit.
5829        sync_kit(ideal);
5830        Node * narr = new_array(klass_node, zlen, 1);
5831        // Update IdealKit memory and control from graphKit.
5832        __ sync_kit(this);
5833        __ set(z_alloc, narr);
5834      } __ end_if();
5835 
5836      sync_kit(ideal);
5837      z = __ value(z_alloc);
5838      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5839      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5840      // Final sync IdealKit and GraphKit.
5841      final_sync(ideal);
5842 #undef __
5843 
5844     Node* z_start = array_element_address(z, intcon(0), T_INT);
5845 
5846     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5847                                    OptoRuntime::multiplyToLen_Type(),
5848                                    stubAddr, stubName, TypePtr::BOTTOM,
5849                                    x_start, xlen, y_start, ylen, z_start, zlen);
5850   } // original reexecute is set back here
5851 
5852   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5853   set_result(z);
5854   return true;
5855 }
5856 
5857 
5858 /**
5859  * Calculate CRC32 for byte.
5860  * int java.util.zip.CRC32.update(int crc, int b)
5861  */
5862 bool LibraryCallKit::inline_updateCRC32() {
5863   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5864   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5865   // no receiver since it is static method
5866   Node* crc  = argument(0); // type: int
5867   Node* b    = argument(1); // type: int
5868 
5869   /*
5870    *    int c = ~ crc;
5871    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5872    *    b = b ^ (c &gt;&gt;&gt; 8);
5873    *    crc = ~b;
5874    */
5875 
5876   Node* M1 = intcon(-1);
5877   crc = _gvn.transform(new (C) XorINode(crc, M1));
5878   Node* result = _gvn.transform(new (C) XorINode(crc, b));
5879   result = _gvn.transform(new (C) AndINode(result, intcon(0xFF)));
5880 
5881   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5882   Node* offset = _gvn.transform(new (C) LShiftINode(result, intcon(0x2)));
5883   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5884   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5885 
5886   crc = _gvn.transform(new (C) URShiftINode(crc, intcon(8)));
5887   result = _gvn.transform(new (C) XorINode(crc, result));
5888   result = _gvn.transform(new (C) XorINode(result, M1));
5889   set_result(result);
5890   return true;
5891 }
5892 
5893 /**
5894  * Calculate CRC32 for byte[] array.
5895  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5896  */
5897 bool LibraryCallKit::inline_updateBytesCRC32() {
5898   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5899   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5900   // no receiver since it is static method
5901   Node* crc     = argument(0); // type: int
5902   Node* src     = argument(1); // type: oop
5903   Node* offset  = argument(2); // type: int
5904   Node* length  = argument(3); // type: int
5905 
5906   const Type* src_type = src-&gt;Value(&amp;_gvn);
5907   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5908   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5909     // failed array check
5910     return false;
5911   }
5912 
5913   // Figure out the size and type of the elements we will be copying.
5914   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5915   if (src_elem != T_BYTE) {
5916     return false;
5917   }
5918 
5919   // 'src_start' points to src array + scaled offset
5920   Node* src_start = array_element_address(src, offset, src_elem);
5921 
5922   // We assume that range check is done by caller.
5923   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5924 
5925   // Call the stub.
5926   address stubAddr = StubRoutines::updateBytesCRC32();
5927   const char *stubName = "updateBytesCRC32";
5928 
5929   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5930                                  stubAddr, stubName, TypePtr::BOTTOM,
5931                                  crc, src_start, length);
5932   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5933   set_result(result);
5934   return true;
5935 }
5936 
5937 /**
5938  * Calculate CRC32 for ByteBuffer.
5939  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5940  */
5941 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5942   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5943   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5944   // no receiver since it is static method
5945   Node* crc     = argument(0); // type: int
5946   Node* src     = argument(1); // type: long
5947   Node* offset  = argument(3); // type: int
5948   Node* length  = argument(4); // type: int
5949 
5950   src = ConvL2X(src);  // adjust Java long to machine word
5951   Node* base = _gvn.transform(new (C) CastX2PNode(src));
5952   offset = ConvI2X(offset);
5953 
5954   // 'src_start' points to src array + scaled offset
5955   Node* src_start = basic_plus_adr(top(), base, offset);
5956 
5957   // Call the stub.
5958   address stubAddr = StubRoutines::updateBytesCRC32();
5959   const char *stubName = "updateBytesCRC32";
5960 
5961   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5962                                  stubAddr, stubName, TypePtr::BOTTOM,
5963                                  crc, src_start, length);
5964   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
5965   set_result(result);
5966   return true;
5967 }
5968 
5969 //----------------------------inline_reference_get----------------------------
5970 // public T java.lang.ref.Reference.get();
5971 bool LibraryCallKit::inline_reference_get() {
5972   const int referent_offset = java_lang_ref_Reference::referent_offset;
5973   guarantee(referent_offset &gt; 0, "should have already been set");
5974 
5975   // Get the argument:
5976   Node* reference_obj = null_check_receiver();
5977   if (stopped()) return true;
5978 
5979   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5980 
5981   ciInstanceKlass* klass = env()-&gt;Object_klass();
5982   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5983 
5984   Node* no_ctrl = NULL;
5985   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5986 
5987   // Use the pre-barrier to record the value in the referent field
5988   pre_barrier(false /* do_load */,
5989               control(),
5990               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5991               result /* pre_val */,
5992               T_OBJECT);
5993 
5994   // Add memory barrier to prevent commoning reads from this field
5995   // across safepoint since GC can change its value.
5996   insert_mem_bar(Op_MemBarCPUOrder);
5997 
5998   set_result(result);
5999   return true;
6000 }
6001 
6002 
6003 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
6004                                               bool is_exact=true, bool is_static=false) {
6005 
6006   const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
6007   assert(tinst != NULL, "obj is null");
6008   assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
6009   assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
6010 
6011   ciField* field = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_name(ciSymbol::make(fieldName),
6012                                                                           ciSymbol::make(fieldTypeString),
6013                                                                           is_static);
6014   if (field == NULL) return (Node *) NULL;
6015   assert (field != NULL, "undefined field");
6016 
6017   // Next code  copied from Parse::do_get_xxx():
6018 
6019   // Compute address and memory type.
6020   int offset  = field-&gt;offset_in_bytes();
6021   bool is_vol = field-&gt;is_volatile();
6022   ciType* field_klass = field-&gt;type();
6023   assert(field_klass-&gt;is_loaded(), "should be loaded");
6024   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
6025   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
6026   BasicType bt = field-&gt;layout_type();
6027 
6028   // Build the resultant type of the load
6029   const Type *type;
6030   if (bt == T_OBJECT) {
6031     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
6032   } else {
6033     type = Type::get_const_basic_type(bt);
6034   }
6035 
6036   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
6037     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
6038   }
6039   // Build the load.
6040   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
6041   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, is_vol);
6042   // If reference is volatile, prevent following memory ops from
6043   // floating up past the volatile read.  Also prevents commoning
6044   // another volatile read.
6045   if (is_vol) {
6046     // Memory barrier includes bogus read of value to force load BEFORE membar
6047     insert_mem_bar(Op_MemBarAcquire, loadedField);
6048   }
6049   return loadedField;
6050 }
6051 
<a name="9" id="anc9"></a>
















































































































































6052 
6053 //------------------------------inline_aescrypt_Block-----------------------
6054 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
6055   address stubAddr;
6056   const char *stubName;
6057   assert(UseAES, "need AES instruction support");
6058 
6059   switch(id) {
6060   case vmIntrinsics::_aescrypt_encryptBlock:
6061     stubAddr = StubRoutines::aescrypt_encryptBlock();
6062     stubName = "aescrypt_encryptBlock";
6063     break;
6064   case vmIntrinsics::_aescrypt_decryptBlock:
6065     stubAddr = StubRoutines::aescrypt_decryptBlock();
6066     stubName = "aescrypt_decryptBlock";
6067     break;
6068   }
6069   if (stubAddr == NULL) return false;
6070 
6071   Node* aescrypt_object = argument(0);
6072   Node* src             = argument(1);
6073   Node* src_offset      = argument(2);
6074   Node* dest            = argument(3);
6075   Node* dest_offset     = argument(4);
6076 
6077   // (1) src and dest are arrays.
6078   const Type* src_type = src-&gt;Value(&amp;_gvn);
6079   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6080   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6081   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6082   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6083 
6084   // for the quick and dirty code we will skip all the checks.
6085   // we are just trying to get the call to be generated.
6086   Node* src_start  = src;
6087   Node* dest_start = dest;
6088   if (src_offset != NULL || dest_offset != NULL) {
6089     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6090     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6091     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6092   }
6093 
6094   // now need to get the start of its expanded key array
6095   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6096   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6097   if (k_start == NULL) return false;
6098 
6099   if (Matcher::pass_original_key_for_aes()) {
6100     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6101     // compatibility issues between Java key expansion and SPARC crypto instructions
6102     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6103     if (original_k_start == NULL) return false;
6104 
6105     // Call the stub.
6106     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6107                       stubAddr, stubName, TypePtr::BOTTOM,
6108                       src_start, dest_start, k_start, original_k_start);
6109   } else {
6110     // Call the stub.
6111     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6112                       stubAddr, stubName, TypePtr::BOTTOM,
6113                       src_start, dest_start, k_start);
6114   }
6115 
6116   return true;
6117 }
6118 
6119 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
6120 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
6121   address stubAddr;
6122   const char *stubName;
6123 
6124   assert(UseAES, "need AES instruction support");
6125 
6126   switch(id) {
6127   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
6128     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
6129     stubName = "cipherBlockChaining_encryptAESCrypt";
6130     break;
6131   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
6132     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
6133     stubName = "cipherBlockChaining_decryptAESCrypt";
6134     break;
6135   }
6136   if (stubAddr == NULL) return false;
6137 
6138   Node* cipherBlockChaining_object = argument(0);
6139   Node* src                        = argument(1);
6140   Node* src_offset                 = argument(2);
6141   Node* len                        = argument(3);
6142   Node* dest                       = argument(4);
6143   Node* dest_offset                = argument(5);
6144 
6145   // (1) src and dest are arrays.
6146   const Type* src_type = src-&gt;Value(&amp;_gvn);
6147   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6148   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6149   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6150   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
6151           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6152 
6153   // checks are the responsibility of the caller
6154   Node* src_start  = src;
6155   Node* dest_start = dest;
6156   if (src_offset != NULL || dest_offset != NULL) {
6157     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6158     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6159     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6160   }
6161 
6162   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
6163   // (because of the predicated logic executed earlier).
6164   // so we cast it here safely.
6165   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6166 
6167   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6168   if (embeddedCipherObj == NULL) return false;
6169 
6170   // cast it to what we know it will be at runtime
6171   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
6172   assert(tinst != NULL, "CBC obj is null");
6173   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
6174   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6175   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
6176 
6177   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6178   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
6179   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6180   Node* aescrypt_object = new(C) CheckCastPPNode(control(), embeddedCipherObj, xtype);
6181   aescrypt_object = _gvn.transform(aescrypt_object);
6182 
6183   // we need to get the start of the aescrypt_object's expanded key array
6184   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6185   if (k_start == NULL) return false;
6186 
6187   // similarly, get the start address of the r vector
6188   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
6189   if (objRvec == NULL) return false;
6190   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
6191 
6192   Node* cbcCrypt;
6193   if (Matcher::pass_original_key_for_aes()) {
6194     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6195     // compatibility issues between Java key expansion and SPARC crypto instructions
6196     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6197     if (original_k_start == NULL) return false;
6198 
6199     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
6200     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6201                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6202                                  stubAddr, stubName, TypePtr::BOTTOM,
6203                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6204   } else {
6205     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6206     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6207                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6208                                  stubAddr, stubName, TypePtr::BOTTOM,
6209                                  src_start, dest_start, k_start, r_start, len);
6210   }
6211 
6212   // return cipher length (int)
6213   Node* retvalue = _gvn.transform(new (C) ProjNode(cbcCrypt, TypeFunc::Parms));
6214   set_result(retvalue);
6215   return true;
6216 }
6217 
6218 //------------------------------get_key_start_from_aescrypt_object-----------------------
6219 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6220   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6221   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6222   if (objAESCryptKey == NULL) return (Node *) NULL;
6223 
6224   // now have the array, need to get the start address of the K array
6225   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6226   return k_start;
6227 }
6228 
6229 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6230 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6231   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6232   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6233   if (objAESCryptKey == NULL) return (Node *) NULL;
6234 
6235   // now have the array, need to get the start address of the lastKey array
6236   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6237   return original_k_start;
6238 }
6239 
6240 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6241 // Return node representing slow path of predicate check.
6242 // the pseudo code we want to emulate with this predicate is:
6243 // for encryption:
6244 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6245 // for decryption:
6246 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6247 //    note cipher==plain is more conservative than the original java code but that's OK
6248 //
6249 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6250   // The receiver was checked for NULL already.
6251   Node* objCBC = argument(0);
6252 
6253   // Load embeddedCipher field of CipherBlockChaining object.
6254   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6255 
6256   // get AESCrypt klass for instanceOf check
6257   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6258   // will have same classloader as CipherBlockChaining object
6259   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6260   assert(tinst != NULL, "CBCobj is null");
6261   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6262 
6263   // we want to do an instanceof comparison against the AESCrypt class
6264   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6265   if (!klass_AESCrypt-&gt;is_loaded()) {
6266     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6267     Node* ctrl = control();
6268     set_control(top()); // no regular fast path
6269     return ctrl;
6270   }
6271   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6272 
6273   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6274   Node* cmp_instof  = _gvn.transform(new (C) CmpINode(instof, intcon(1)));
6275   Node* bool_instof  = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6276 
6277   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6278 
6279   // for encryption, we are done
6280   if (!decrypting)
6281     return instof_false;  // even if it is NULL
6282 
6283   // for decryption, we need to add a further check to avoid
6284   // taking the intrinsic path when cipher and plain are the same
6285   // see the original java code for why.
6286   RegionNode* region = new(C) RegionNode(3);
6287   region-&gt;init_req(1, instof_false);
6288   Node* src = argument(1);
6289   Node* dest = argument(4);
6290   Node* cmp_src_dest = _gvn.transform(new (C) CmpPNode(src, dest));
6291   Node* bool_src_dest = _gvn.transform(new (C) BoolNode(cmp_src_dest, BoolTest::eq));
6292   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6293   region-&gt;init_req(2, src_dest_conjoint);
6294 
6295   record_for_igvn(region);
6296   return _gvn.transform(region);
6297 }
6298 
6299 //------------------------------inline_sha_implCompress-----------------------
6300 //
6301 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6302 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6303 //
6304 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6305 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6306 //
6307 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6308 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6309 //
6310 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6311   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6312 
6313   Node* sha_obj = argument(0);
6314   Node* src     = argument(1); // type oop
6315   Node* ofs     = argument(2); // type int
6316 
6317   const Type* src_type = src-&gt;Value(&amp;_gvn);
6318   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6319   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6320     // failed array check
6321     return false;
6322   }
6323   // Figure out the size and type of the elements we will be copying.
6324   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6325   if (src_elem != T_BYTE) {
6326     return false;
6327   }
6328   // 'src_start' points to src array + offset
6329   Node* src_start = array_element_address(src, ofs, src_elem);
6330   Node* state = NULL;
6331   address stubAddr;
6332   const char *stubName;
6333 
6334   switch(id) {
6335   case vmIntrinsics::_sha_implCompress:
6336     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6337     state = get_state_from_sha_object(sha_obj);
6338     stubAddr = StubRoutines::sha1_implCompress();
6339     stubName = "sha1_implCompress";
6340     break;
6341   case vmIntrinsics::_sha2_implCompress:
6342     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6343     state = get_state_from_sha_object(sha_obj);
6344     stubAddr = StubRoutines::sha256_implCompress();
6345     stubName = "sha256_implCompress";
6346     break;
6347   case vmIntrinsics::_sha5_implCompress:
6348     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6349     state = get_state_from_sha5_object(sha_obj);
6350     stubAddr = StubRoutines::sha512_implCompress();
6351     stubName = "sha512_implCompress";
6352     break;
6353   default:
6354     fatal_unexpected_iid(id);
6355     return false;
6356   }
6357   if (state == NULL) return false;
6358 
6359   // Call the stub.
6360   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6361                                  stubAddr, stubName, TypePtr::BOTTOM,
6362                                  src_start, state);
6363 
6364   return true;
6365 }
6366 
6367 //------------------------------inline_digestBase_implCompressMB-----------------------
6368 //
6369 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6370 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6371 //
6372 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6373   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6374          "need SHA1/SHA256/SHA512 instruction support");
6375   assert((uint)predicate &lt; 3, "sanity");
6376   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6377 
6378   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6379   Node* src            = argument(1); // byte[] array
6380   Node* ofs            = argument(2); // type int
6381   Node* limit          = argument(3); // type int
6382 
6383   const Type* src_type = src-&gt;Value(&amp;_gvn);
6384   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6385   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6386     // failed array check
6387     return false;
6388   }
6389   // Figure out the size and type of the elements we will be copying.
6390   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6391   if (src_elem != T_BYTE) {
6392     return false;
6393   }
6394   // 'src_start' points to src array + offset
6395   Node* src_start = array_element_address(src, ofs, src_elem);
6396 
6397   const char* klass_SHA_name = NULL;
6398   const char* stub_name = NULL;
6399   address     stub_addr = NULL;
6400   bool        long_state = false;
6401 
6402   switch (predicate) {
6403   case 0:
6404     if (UseSHA1Intrinsics) {
6405       klass_SHA_name = "sun/security/provider/SHA";
6406       stub_name = "sha1_implCompressMB";
6407       stub_addr = StubRoutines::sha1_implCompressMB();
6408     }
6409     break;
6410   case 1:
6411     if (UseSHA256Intrinsics) {
6412       klass_SHA_name = "sun/security/provider/SHA2";
6413       stub_name = "sha256_implCompressMB";
6414       stub_addr = StubRoutines::sha256_implCompressMB();
6415     }
6416     break;
6417   case 2:
6418     if (UseSHA512Intrinsics) {
6419       klass_SHA_name = "sun/security/provider/SHA5";
6420       stub_name = "sha512_implCompressMB";
6421       stub_addr = StubRoutines::sha512_implCompressMB();
6422       long_state = true;
6423     }
6424     break;
6425   default:
6426     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6427   }
6428   if (klass_SHA_name != NULL) {
6429     // get DigestBase klass to lookup for SHA klass
6430     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6431     assert(tinst != NULL, "digestBase_obj is not instance???");
6432     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6433 
6434     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6435     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6436     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6437     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6438   }
6439   return false;
6440 }
6441 //------------------------------inline_sha_implCompressMB-----------------------
6442 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6443                                                bool long_state, address stubAddr, const char *stubName,
6444                                                Node* src_start, Node* ofs, Node* limit) {
6445   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6446   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6447   Node* sha_obj = new (C) CheckCastPPNode(control(), digestBase_obj, xtype);
6448   sha_obj = _gvn.transform(sha_obj);
6449 
6450   Node* state;
6451   if (long_state) {
6452     state = get_state_from_sha5_object(sha_obj);
6453   } else {
6454     state = get_state_from_sha_object(sha_obj);
6455   }
6456   if (state == NULL) return false;
6457 
6458   // Call the stub.
6459   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6460                                  OptoRuntime::digestBase_implCompressMB_Type(),
6461                                  stubAddr, stubName, TypePtr::BOTTOM,
6462                                  src_start, state, ofs, limit);
6463   // return ofs (int)
6464   Node* result = _gvn.transform(new (C) ProjNode(call, TypeFunc::Parms));
6465   set_result(result);
6466 
6467   return true;
6468 }
6469 
6470 //------------------------------get_state_from_sha_object-----------------------
6471 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6472   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6473   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6474   if (sha_state == NULL) return (Node *) NULL;
6475 
6476   // now have the array, need to get the start address of the state array
6477   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6478   return state;
6479 }
6480 
6481 //------------------------------get_state_from_sha5_object-----------------------
6482 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6483   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6484   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6485   if (sha_state == NULL) return (Node *) NULL;
6486 
6487   // now have the array, need to get the start address of the state array
6488   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6489   return state;
6490 }
6491 
6492 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6493 // Return node representing slow path of predicate check.
6494 // the pseudo code we want to emulate with this predicate is:
6495 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6496 //
6497 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6498   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6499          "need SHA1/SHA256/SHA512 instruction support");
6500   assert((uint)predicate &lt; 3, "sanity");
6501 
6502   // The receiver was checked for NULL already.
6503   Node* digestBaseObj = argument(0);
6504 
6505   // get DigestBase klass for instanceOf check
6506   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6507   assert(tinst != NULL, "digestBaseObj is null");
6508   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6509 
6510   const char* klass_SHA_name = NULL;
6511   switch (predicate) {
6512   case 0:
6513     if (UseSHA1Intrinsics) {
6514       // we want to do an instanceof comparison against the SHA class
6515       klass_SHA_name = "sun/security/provider/SHA";
6516     }
6517     break;
6518   case 1:
6519     if (UseSHA256Intrinsics) {
6520       // we want to do an instanceof comparison against the SHA2 class
6521       klass_SHA_name = "sun/security/provider/SHA2";
6522     }
6523     break;
6524   case 2:
6525     if (UseSHA512Intrinsics) {
6526       // we want to do an instanceof comparison against the SHA5 class
6527       klass_SHA_name = "sun/security/provider/SHA5";
6528     }
6529     break;
6530   default:
6531     fatal(err_msg_res("unknown SHA intrinsic predicate: %d", predicate));
6532   }
6533 
6534   ciKlass* klass_SHA = NULL;
6535   if (klass_SHA_name != NULL) {
6536     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6537   }
6538   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6539     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6540     Node* ctrl = control();
6541     set_control(top()); // no intrinsic path
6542     return ctrl;
6543   }
6544   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6545 
6546   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6547   Node* cmp_instof = _gvn.transform(new (C) CmpINode(instofSHA, intcon(1)));
6548   Node* bool_instof = _gvn.transform(new (C) BoolNode(cmp_instof, BoolTest::ne));
6549   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6550 
6551   return instof_false;  // even if it is NULL
<a name="10" id="anc10"></a><span class="removed">6552 }</span>
<span class="removed">6553 </span>
<span class="removed">6554 bool LibraryCallKit::inline_profileBoolean() {</span>
<span class="removed">6555   Node* counts = argument(1);</span>
<span class="removed">6556   const TypeAryPtr* ary = NULL;</span>
<span class="removed">6557   ciArray* aobj = NULL;</span>
<span class="removed">6558   if (counts-&gt;is_Con()</span>
<span class="removed">6559       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL</span>
<span class="removed">6560       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL</span>
<span class="removed">6561       &amp;&amp; (aobj-&gt;length() == 2)) {</span>
<span class="removed">6562     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.</span>
<span class="removed">6563     jint false_cnt = aobj-&gt;element_value(0).as_int();</span>
<span class="removed">6564     jint  true_cnt = aobj-&gt;element_value(1).as_int();</span>
<span class="removed">6565 </span>
<span class="removed">6566     method()-&gt;set_injected_profile(true);</span>
<span class="removed">6567 </span>
<span class="removed">6568     if (C-&gt;log() != NULL) {</span>
<span class="removed">6569       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",</span>
<span class="removed">6570                      false_cnt, true_cnt);</span>
<span class="removed">6571     }</span>
<span class="removed">6572 </span>
<span class="removed">6573     if (false_cnt + true_cnt == 0) {</span>
<span class="removed">6574       // According to profile, never executed.</span>
<span class="removed">6575       uncommon_trap_exact(Deoptimization::Reason_intrinsic,</span>
<span class="removed">6576                           Deoptimization::Action_reinterpret);</span>
<span class="removed">6577       return true;</span>
<span class="removed">6578     }</span>
<span class="removed">6579 </span>
<span class="removed">6580     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)</span>
<span class="removed">6581     // is a number of each value occurrences.</span>
<span class="removed">6582     Node* result = argument(0);</span>
<span class="removed">6583     if (false_cnt == 0 || true_cnt == 0) {</span>
<span class="removed">6584       // According to profile, one value has been never seen.</span>
<span class="removed">6585       int expected_val = (false_cnt == 0) ? 1 : 0;</span>
<span class="removed">6586 </span>
<span class="removed">6587       Node* cmp  = _gvn.transform(new (C) CmpINode(result, intcon(expected_val)));</span>
<span class="removed">6588       Node* test = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));</span>
<span class="removed">6589 </span>
<span class="removed">6590       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);</span>
<span class="removed">6591       Node* fast_path = _gvn.transform(new (C) IfTrueNode(check));</span>
<span class="removed">6592       Node* slow_path = _gvn.transform(new (C) IfFalseNode(check));</span>
<span class="removed">6593 </span>
<span class="removed">6594       { // Slow path: uncommon trap for never seen value and then reexecute</span>
<span class="removed">6595         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows</span>
<span class="removed">6596         // the value has been seen at least once.</span>
<span class="removed">6597         PreserveJVMState pjvms(this);</span>
<span class="removed">6598         PreserveReexecuteState preexecs(this);</span>
<span class="removed">6599         jvms()-&gt;set_should_reexecute(true);</span>
<span class="removed">6600 </span>
<span class="removed">6601         set_control(slow_path);</span>
<span class="removed">6602         set_i_o(i_o());</span>
<span class="removed">6603 </span>
<span class="removed">6604         uncommon_trap_exact(Deoptimization::Reason_intrinsic,</span>
<span class="removed">6605                             Deoptimization::Action_reinterpret);</span>
<span class="removed">6606       }</span>
<span class="removed">6607       // The guard for never seen value enables sharpening of the result and</span>
<span class="removed">6608       // returning a constant. It allows to eliminate branches on the same value</span>
<span class="removed">6609       // later on.</span>
<span class="removed">6610       set_control(fast_path);</span>
<span class="removed">6611       result = intcon(expected_val);</span>
<span class="removed">6612     }</span>
<span class="removed">6613     // Stop profiling.</span>
<span class="removed">6614     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.</span>
<span class="removed">6615     // By replacing method body with profile data (represented as ProfileBooleanNode</span>
<span class="removed">6616     // on IR level) we effectively disable profiling.</span>
<span class="removed">6617     // It enables full speed execution once optimized code is generated.</span>
<span class="removed">6618     Node* profile = _gvn.transform(new (C) ProfileBooleanNode(result, false_cnt, true_cnt));</span>
<span class="removed">6619     C-&gt;record_for_igvn(profile);</span>
<span class="removed">6620     set_result(profile);</span>
<span class="removed">6621     return true;</span>
<span class="removed">6622   } else {</span>
<span class="removed">6623     // Continue profiling.</span>
<span class="removed">6624     // Profile data isn't available at the moment. So, execute method's bytecode version.</span>
<span class="removed">6625     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod</span>
<span class="removed">6626     // is compiled and counters aren't available since corresponding MethodHandle</span>
<span class="removed">6627     // isn't a compile-time constant.</span>
<span class="removed">6628     return false;</span>
<span class="removed">6629   }</span>
6630 }
<a name="11" id="anc11"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="11" type="hidden" /></form></body></html>
