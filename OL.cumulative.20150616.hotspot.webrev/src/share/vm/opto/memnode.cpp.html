<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/memnode.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "compiler/compileLog.hpp"
  28 #include "memory/allocation.inline.hpp"
  29 #include "oops/objArrayKlass.hpp"
  30 #include "opto/addnode.hpp"
  31 #include "opto/cfgnode.hpp"
  32 #include "opto/compile.hpp"
  33 #include "opto/connode.hpp"
  34 #include "opto/loopnode.hpp"
  35 #include "opto/machnode.hpp"
  36 #include "opto/matcher.hpp"
  37 #include "opto/memnode.hpp"
  38 #include "opto/mulnode.hpp"
  39 #include "opto/phaseX.hpp"
  40 #include "opto/regmask.hpp"
  41 
  42 // Portions of code courtesy of Clifford Click
  43 
  44 // Optimization - Graph Style
  45 
  46 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  47 
  48 //=============================================================================
  49 uint MemNode::size_of() const { return sizeof(*this); }
  50 
  51 const TypePtr *MemNode::adr_type() const {
  52   Node* adr = in(Address);
  53   const TypePtr* cross_check = NULL;
  54   DEBUG_ONLY(cross_check = _adr_type);
  55   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  56 }
  57 
  58 #ifndef PRODUCT
  59 void MemNode::dump_spec(outputStream *st) const {
  60   if (in(Address) == NULL)  return; // node is dead
  61 #ifndef ASSERT
  62   // fake the missing field
  63   const TypePtr* _adr_type = NULL;
  64   if (in(Address) != NULL)
  65     _adr_type = in(Address)-&gt;bottom_type()-&gt;isa_ptr();
  66 #endif
  67   dump_adr_type(this, _adr_type, st);
  68 
  69   Compile* C = Compile::current();
  70   if( C-&gt;alias_type(_adr_type)-&gt;is_volatile() )
  71     st-&gt;print(" Volatile!");
  72 }
  73 
  74 void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {
  75   st-&gt;print(" @");
  76   if (adr_type == NULL) {
  77     st-&gt;print("NULL");
  78   } else {
  79     adr_type-&gt;dump_on(st);
  80     Compile* C = Compile::current();
  81     Compile::AliasType* atp = NULL;
  82     if (C-&gt;have_alias_type(adr_type))  atp = C-&gt;alias_type(adr_type);
  83     if (atp == NULL)
  84       st-&gt;print(", idx=?\?;");
  85     else if (atp-&gt;index() == Compile::AliasIdxBot)
  86       st-&gt;print(", idx=Bot;");
  87     else if (atp-&gt;index() == Compile::AliasIdxTop)
  88       st-&gt;print(", idx=Top;");
  89     else if (atp-&gt;index() == Compile::AliasIdxRaw)
  90       st-&gt;print(", idx=Raw;");
  91     else {
  92       ciField* field = atp-&gt;field();
  93       if (field) {
  94         st-&gt;print(", name=");
  95         field-&gt;print_name_on(st);
  96       }
  97       st-&gt;print(", idx=%d;", atp-&gt;index());
  98     }
  99   }
 100 }
 101 
 102 extern void print_alias_types();
 103 
 104 #endif
 105 
 106 Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {
 107   assert((t_oop != NULL), "sanity");
 108   bool is_instance = t_oop-&gt;is_known_instance_field();
 109   bool is_boxed_value_load = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp;
 110                              (load != NULL) &amp;&amp; load-&gt;is_Load() &amp;&amp;
 111                              (phase-&gt;is_IterGVN() != NULL);
 112   if (!(is_instance || is_boxed_value_load))
 113     return mchain;  // don't try to optimize non-instance types
 114   uint instance_id = t_oop-&gt;instance_id();
 115   Node *start_mem = phase-&gt;C-&gt;start()-&gt;proj_out(TypeFunc::Memory);
 116   Node *prev = NULL;
 117   Node *result = mchain;
 118   while (prev != result) {
 119     prev = result;
 120     if (result == start_mem)
 121       break;  // hit one of our sentinels
 122     // skip over a call which does not affect this memory slice
 123     if (result-&gt;is_Proj() &amp;&amp; result-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
 124       Node *proj_in = result-&gt;in(0);
 125       if (proj_in-&gt;is_Allocate() &amp;&amp; proj_in-&gt;_idx == instance_id) {
 126         break;  // hit one of our sentinels
 127       } else if (proj_in-&gt;is_Call()) {
 128         CallNode *call = proj_in-&gt;as_Call();
 129         if (!call-&gt;may_modify(t_oop, phase)) { // returns false for instances
 130           result = call-&gt;in(TypeFunc::Memory);
 131         }
 132       } else if (proj_in-&gt;is_Initialize()) {
 133         AllocateNode* alloc = proj_in-&gt;as_Initialize()-&gt;allocation();
 134         // Stop if this is the initialization for the object instance which
 135         // which contains this memory slice, otherwise skip over it.
 136         if ((alloc == NULL) || (alloc-&gt;_idx == instance_id)) {
 137           break;
 138         }
 139         if (is_instance) {
 140           result = proj_in-&gt;in(TypeFunc::Memory);
 141         } else if (is_boxed_value_load) {
 142           Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
 143           const TypeKlassPtr* tklass = phase-&gt;type(klass)-&gt;is_klassptr();
 144           if (tklass-&gt;klass_is_exact() &amp;&amp; !tklass-&gt;klass()-&gt;equals(t_oop-&gt;klass())) {
 145             result = proj_in-&gt;in(TypeFunc::Memory); // not related allocation
 146           }
 147         }
 148       } else if (proj_in-&gt;is_MemBar()) {
 149         result = proj_in-&gt;in(TypeFunc::Memory);
 150       } else {
 151         assert(false, "unexpected projection");
 152       }
 153     } else if (result-&gt;is_ClearArray()) {
 154       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 155         // Can not bypass initialization of the instance
 156         // we are looking for.
 157         break;
 158       }
 159       // Otherwise skip it (the call updated 'result' value).
 160     } else if (result-&gt;is_MergeMem()) {
 161       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 162     }
 163   }
 164   return result;
 165 }
 166 
 167 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 168   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 169   if (t_oop == NULL)
 170     return mchain;  // don't try to optimize non-oop types
 171   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 172   bool is_instance = t_oop-&gt;is_known_instance_field();
 173   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 174   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {
 175     PhiNode *mphi = result-&gt;as_Phi();
 176     assert(mphi-&gt;bottom_type() == Type::MEMORY, "memory phi required");
 177     const TypePtr *t = mphi-&gt;adr_type();
 178     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 179         t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 180         t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 181          -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 182          -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop) {
 183       // clone the Phi with our address type
 184       result = mphi-&gt;split_out_instance(t_adr, igvn);
 185     } else {
 186       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), "correct memory chain");
 187     }
 188   }
 189   return result;
 190 }
 191 
 192 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 193   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 194   Node *mem = mmem;
 195 #ifdef ASSERT
 196   {
 197     // Check that current type is consistent with the alias index used during graph construction
 198     assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not be a bad alias_idx");
 199     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 200                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 201     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 202     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
 203                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;
 204         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 205         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 206           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 207           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 208       // don't assert if it is dead code.
 209       consistent = true;
 210     }
 211     if( !consistent ) {
 212       st-&gt;print("alias_idx==%d, adr_check==", alias_idx);
 213       if( adr_check == NULL ) {
 214         st-&gt;print("NULL");
 215       } else {
 216         adr_check-&gt;dump();
 217       }
 218       st-&gt;cr();
 219       print_alias_types();
 220       assert(consistent, "adr_check must match alias idx");
 221     }
 222   }
 223 #endif
 224   // TypeOopPtr::NOTNULL+any is an OOP with unknown offset - generally
 225   // means an array I have not precisely typed yet.  Do not do any
 226   // alias stuff with it any time soon.
 227   const TypeOopPtr *toop = tp-&gt;isa_oopptr();
 228   if( tp-&gt;base() != Type::AnyPtr &amp;&amp;
 229       !(toop &amp;&amp;
 230         toop-&gt;klass() != NULL &amp;&amp;
 231         toop-&gt;klass()-&gt;is_java_lang_Object() &amp;&amp;
 232         toop-&gt;offset() == Type::OffsetBot) ) {
 233     // compress paths and change unreachable cycles to TOP
 234     // If not, we can update the input infinitely along a MergeMem cycle
 235     // Equivalent code in PhiNode::Ideal
 236     Node* m  = phase-&gt;transform(mmem);
 237     // If transformed to a MergeMem, get the desired slice
 238     // Otherwise the returned node represents memory for every slice
 239     mem = (m-&gt;is_MergeMem())? m-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : m;
 240     // Update input if it is progress over what we have now
 241   }
 242   return mem;
 243 }
 244 
 245 //--------------------------Ideal_common---------------------------------------
 246 // Look for degenerate control and memory inputs.  Bypass MergeMem inputs.
 247 // Unhook non-raw memories from complete (macro-expanded) initializations.
 248 Node *MemNode::Ideal_common(PhaseGVN *phase, bool can_reshape) {
 249   // If our control input is a dead region, kill all below the region
 250   Node *ctl = in(MemNode::Control);
 251   if (ctl &amp;&amp; remove_dead_region(phase, can_reshape))
 252     return this;
 253   ctl = in(MemNode::Control);
 254   // Don't bother trying to transform a dead node
 255   if (ctl &amp;&amp; ctl-&gt;is_top())  return NodeSentinel;
 256 
 257   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 258   // Wait if control on the worklist.
 259   if (ctl &amp;&amp; can_reshape &amp;&amp; igvn != NULL) {
 260     Node* bol = NULL;
 261     Node* cmp = NULL;
 262     if (ctl-&gt;in(0)-&gt;is_If()) {
 263       assert(ctl-&gt;is_IfTrue() || ctl-&gt;is_IfFalse(), "sanity");
 264       bol = ctl-&gt;in(0)-&gt;in(1);
 265       if (bol-&gt;is_Bool())
 266         cmp = ctl-&gt;in(0)-&gt;in(1)-&gt;in(1);
 267     }
 268     if (igvn-&gt;_worklist.member(ctl) ||
 269         (bol != NULL &amp;&amp; igvn-&gt;_worklist.member(bol)) ||
 270         (cmp != NULL &amp;&amp; igvn-&gt;_worklist.member(cmp)) ) {
 271       // This control path may be dead.
 272       // Delay this memory node transformation until the control is processed.
 273       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 274       return NodeSentinel; // caller will return NULL
 275     }
 276   }
 277   // Ignore if memory is dead, or self-loop
 278   Node *mem = in(MemNode::Memory);
 279   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 280   assert(mem != this, "dead loop in MemNode::Ideal");
 281 
 282   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 283     // This memory slice may be dead.
 284     // Delay this mem node transformation until the memory is processed.
 285     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 286     return NodeSentinel; // caller will return NULL
 287   }
 288 
 289   Node *address = in(MemNode::Address);
 290   const Type *t_adr = phase-&gt;type(address);
 291   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 292 
 293   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 294       (igvn-&gt;_worklist.member(address) ||
 295        igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; (t_adr != adr_type())) ) {
 296     // The address's base and type may change when the address is processed.
 297     // Delay this mem node transformation until the address is processed.
 298     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 299     return NodeSentinel; // caller will return NULL
 300   }
 301 
 302   // Do NOT remove or optimize the next lines: ensure a new alias index
 303   // is allocated for an oop pointer type before Escape Analysis.
 304   // Note: C++ will not remove it since the call has side effect.
 305   if (t_adr-&gt;isa_oopptr()) {
 306     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 307   }
 308 
 309   Node* base = NULL;
 310   if (address-&gt;is_AddP()) {
 311     base = address-&gt;in(AddPNode::Base);
 312   }
 313   if (base != NULL &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NULL_PTR) &amp;&amp;
 314       !t_adr-&gt;isa_rawptr()) {
 315     // Note: raw address has TOP base and top-&gt;higher_equal(TypePtr::NULL_PTR) is true.
 316     // Skip this node optimization if its address has TOP base.
 317     return NodeSentinel; // caller will return NULL
 318   }
 319 
 320   // Avoid independent memory operations
 321   Node* old_mem = mem;
 322 
 323   // The code which unhooks non-raw memories from complete (macro-expanded)
 324   // initializations was removed. After macro-expansion all stores catched
 325   // by Initialize node became raw stores and there is no information
 326   // which memory slices they modify. So it is unsafe to move any memory
 327   // operation above these stores. Also in most cases hooked non-raw memories
 328   // were already unhooked by using information from detect_ptr_independence()
 329   // and find_previous_store().
 330 
 331   if (mem-&gt;is_MergeMem()) {
 332     MergeMemNode* mmem = mem-&gt;as_MergeMem();
 333     const TypePtr *tp = t_adr-&gt;is_ptr();
 334 
 335     mem = step_through_mergemem(phase, mmem, tp, adr_type(), tty);
 336   }
 337 
 338   if (mem != old_mem) {
 339     set_req(MemNode::Memory, mem);
 340     if (can_reshape &amp;&amp; old_mem-&gt;outcnt() == 0) {
 341         igvn-&gt;_worklist.push(old_mem);
 342     }
 343     if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel;
 344     return this;
 345   }
 346 
 347   // let the subclass continue analyzing...
 348   return NULL;
 349 }
 350 
 351 // Helper function for proving some simple control dominations.
 352 // Attempt to prove that all control inputs of 'dom' dominate 'sub'.
 353 // Already assumes that 'dom' is available at 'sub', and that 'sub'
 354 // is not a constant (dominated by the method's StartNode).
 355 // Used by MemNode::find_previous_store to prove that the
 356 // control input of a memory operation predates (dominates)
 357 // an allocation it wants to look past.
 358 bool MemNode::all_controls_dominate(Node* dom, Node* sub) {
 359   if (dom == NULL || dom-&gt;is_top() || sub == NULL || sub-&gt;is_top())
 360     return false; // Conservative answer for dead code
 361 
 362   // Check 'dom'. Skip Proj and CatchProj nodes.
 363   dom = dom-&gt;find_exact_control(dom);
 364   if (dom == NULL || dom-&gt;is_top())
 365     return false; // Conservative answer for dead code
 366 
 367   if (dom == sub) {
 368     // For the case when, for example, 'sub' is Initialize and the original
 369     // 'dom' is Proj node of the 'sub'.
 370     return false;
 371   }
 372 
 373   if (dom-&gt;is_Con() || dom-&gt;is_Start() || dom-&gt;is_Root() || dom == sub)
 374     return true;
 375 
 376   // 'dom' dominates 'sub' if its control edge and control edges
 377   // of all its inputs dominate or equal to sub's control edge.
 378 
 379   // Currently 'sub' is either Allocate, Initialize or Start nodes.
 380   // Or Region for the check in LoadNode::Ideal();
 381   // 'sub' should have sub-&gt;in(0) != NULL.
 382   assert(sub-&gt;is_Allocate() || sub-&gt;is_Initialize() || sub-&gt;is_Start() ||
 383          sub-&gt;is_Region() || sub-&gt;is_Call(), "expecting only these nodes");
 384 
 385   // Get control edge of 'sub'.
 386   Node* orig_sub = sub;
 387   sub = sub-&gt;find_exact_control(sub-&gt;in(0));
 388   if (sub == NULL || sub-&gt;is_top())
 389     return false; // Conservative answer for dead code
 390 
 391   assert(sub-&gt;is_CFG(), "expecting control");
 392 
 393   if (sub == dom)
 394     return true;
 395 
 396   if (sub-&gt;is_Start() || sub-&gt;is_Root())
 397     return false;
 398 
 399   {
 400     // Check all control edges of 'dom'.
 401 
 402     ResourceMark rm;
 403     Arena* arena = Thread::current()-&gt;resource_area();
 404     Node_List nlist(arena);
 405     Unique_Node_List dom_list(arena);
 406 
 407     dom_list.push(dom);
 408     bool only_dominating_controls = false;
 409 
 410     for (uint next = 0; next &lt; dom_list.size(); next++) {
 411       Node* n = dom_list.at(next);
 412       if (n == orig_sub)
 413         return false; // One of dom's inputs dominated by sub.
 414       if (!n-&gt;is_CFG() &amp;&amp; n-&gt;pinned()) {
 415         // Check only own control edge for pinned non-control nodes.
 416         n = n-&gt;find_exact_control(n-&gt;in(0));
 417         if (n == NULL || n-&gt;is_top())
 418           return false; // Conservative answer for dead code
 419         assert(n-&gt;is_CFG(), "expecting control");
 420         dom_list.push(n);
 421       } else if (n-&gt;is_Con() || n-&gt;is_Start() || n-&gt;is_Root()) {
 422         only_dominating_controls = true;
 423       } else if (n-&gt;is_CFG()) {
 424         if (n-&gt;dominates(sub, nlist))
 425           only_dominating_controls = true;
 426         else
 427           return false;
 428       } else {
 429         // First, own control edge.
 430         Node* m = n-&gt;find_exact_control(n-&gt;in(0));
 431         if (m != NULL) {
 432           if (m-&gt;is_top())
 433             return false; // Conservative answer for dead code
 434           dom_list.push(m);
 435         }
 436         // Now, the rest of edges.
 437         uint cnt = n-&gt;req();
 438         for (uint i = 1; i &lt; cnt; i++) {
 439           m = n-&gt;find_exact_control(n-&gt;in(i));
 440           if (m == NULL || m-&gt;is_top())
 441             continue;
 442           dom_list.push(m);
 443         }
 444       }
 445     }
 446     return only_dominating_controls;
 447   }
 448 }
 449 
 450 //---------------------detect_ptr_independence---------------------------------
 451 // Used by MemNode::find_previous_store to prove that two base
 452 // pointers are never equal.
 453 // The pointers are accompanied by their associated allocations,
 454 // if any, which have been previously discovered by the caller.
 455 bool MemNode::detect_ptr_independence(Node* p1, AllocateNode* a1,
 456                                       Node* p2, AllocateNode* a2,
 457                                       PhaseTransform* phase) {
 458   // Attempt to prove that these two pointers cannot be aliased.
 459   // They may both manifestly be allocations, and they should differ.
 460   // Or, if they are not both allocations, they can be distinct constants.
 461   // Otherwise, one is an allocation and the other a pre-existing value.
 462   if (a1 == NULL &amp;&amp; a2 == NULL) {           // neither an allocation
 463     return (p1 != p2) &amp;&amp; p1-&gt;is_Con() &amp;&amp; p2-&gt;is_Con();
 464   } else if (a1 != NULL &amp;&amp; a2 != NULL) {    // both allocations
 465     return (a1 != a2);
 466   } else if (a1 != NULL) {                  // one allocation a1
 467     // (Note:  p2-&gt;is_Con implies p2-&gt;in(0)-&gt;is_Root, which dominates.)
 468     return all_controls_dominate(p2, a1);
 469   } else { //(a2 != NULL)                   // one allocation a2
 470     return all_controls_dominate(p1, a2);
 471   }
 472   return false;
 473 }
 474 
 475 
 476 // The logic for reordering loads and stores uses four steps:
 477 // (a) Walk carefully past stores and initializations which we
 478 //     can prove are independent of this load.
 479 // (b) Observe that the next memory state makes an exact match
 480 //     with self (load or store), and locate the relevant store.
 481 // (c) Ensure that, if we were to wire self directly to the store,
 482 //     the optimizer would fold it up somehow.
 483 // (d) Do the rewiring, and return, depending on some other part of
 484 //     the optimizer to fold up the load.
 485 // This routine handles steps (a) and (b).  Steps (c) and (d) are
 486 // specific to loads and stores, so they are handled by the callers.
 487 // (Currently, only LoadNode::Ideal has steps (c), (d).  More later.)
 488 //
 489 Node* MemNode::find_previous_store(PhaseTransform* phase) {
 490   Node*         ctrl   = in(MemNode::Control);
 491   Node*         adr    = in(MemNode::Address);
 492   intptr_t      offset = 0;
 493   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
 494   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
 495 
 496   if (offset == Type::OffsetBot)
 497     return NULL;            // cannot unalias unless there are precise offsets
 498 
 499   const TypeOopPtr *addr_t = adr-&gt;bottom_type()-&gt;isa_oopptr();
 500 
 501   intptr_t size_in_bytes = memory_size();
 502 
 503   Node* mem = in(MemNode::Memory);   // start searching here...
 504 
 505   int cnt = 50;             // Cycle limiter
 506   for (;;) {                // While we can dance past unrelated stores...
 507     if (--cnt &lt; 0)  break;  // Caught in cycle or a complicated dance?
 508 
 509     if (mem-&gt;is_Store()) {
 510       Node* st_adr = mem-&gt;in(MemNode::Address);
 511       intptr_t st_offset = 0;
 512       Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_offset);
 513       if (st_base == NULL)
 514         break;              // inscrutable pointer
 515       if (st_offset != offset &amp;&amp; st_offset != Type::OffsetBot) {
 516         const int MAX_STORE = BytesPerLong;
 517         if (st_offset &gt;= offset + size_in_bytes ||
 518             st_offset &lt;= offset - MAX_STORE ||
 519             st_offset &lt;= offset - mem-&gt;as_Store()-&gt;memory_size()) {
 520           // Success:  The offsets are provably independent.
 521           // (You may ask, why not just test st_offset != offset and be done?
 522           // The answer is that stores of different sizes can co-exist
 523           // in the same sequence of RawMem effects.  We sometimes initialize
 524           // a whole 'tile' of array elements with a single jint or jlong.)
 525           mem = mem-&gt;in(MemNode::Memory);
 526           continue;           // (a) advance through independent store memory
 527         }
 528       }
 529       if (st_base != base &amp;&amp;
 530           detect_ptr_independence(base, alloc,
 531                                   st_base,
 532                                   AllocateNode::Ideal_allocation(st_base, phase),
 533                                   phase)) {
 534         // Success:  The bases are provably independent.
 535         mem = mem-&gt;in(MemNode::Memory);
 536         continue;           // (a) advance through independent store memory
 537       }
 538 
 539       // (b) At this point, if the bases or offsets do not agree, we lose,
 540       // since we have not managed to prove 'this' and 'mem' independent.
 541       if (st_base == base &amp;&amp; st_offset == offset) {
 542         return mem;         // let caller handle steps (c), (d)
 543       }
 544 
 545     } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
 546       InitializeNode* st_init = mem-&gt;in(0)-&gt;as_Initialize();
 547       AllocateNode*  st_alloc = st_init-&gt;allocation();
 548       if (st_alloc == NULL)
 549         break;              // something degenerated
 550       bool known_identical = false;
 551       bool known_independent = false;
 552       if (alloc == st_alloc)
 553         known_identical = true;
 554       else if (alloc != NULL)
 555         known_independent = true;
 556       else if (all_controls_dominate(this, st_alloc))
 557         known_independent = true;
 558 
 559       if (known_independent) {
 560         // The bases are provably independent: Either they are
 561         // manifestly distinct allocations, or else the control
 562         // of this load dominates the store's allocation.
 563         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 564         if (alias_idx == Compile::AliasIdxRaw) {
 565           mem = st_alloc-&gt;in(TypeFunc::Memory);
 566         } else {
 567           mem = st_init-&gt;memory(alias_idx);
 568         }
 569         continue;           // (a) advance through independent store memory
 570       }
 571 
 572       // (b) at this point, if we are not looking at a store initializing
 573       // the same allocation we are loading from, we lose.
 574       if (known_identical) {
 575         // From caller, can_see_stored_value will consult find_captured_store.
 576         return mem;         // let caller handle steps (c), (d)
 577       }
 578 
 579     } else if (addr_t != NULL &amp;&amp; addr_t-&gt;is_known_instance_field()) {
 580       // Can't use optimize_simple_memory_chain() since it needs PhaseGVN.
 581       if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Call()) {
 582         CallNode *call = mem-&gt;in(0)-&gt;as_Call();
 583         if (!call-&gt;may_modify(addr_t, phase)) {
 584           mem = call-&gt;in(TypeFunc::Memory);
 585           continue;         // (a) advance through independent call memory
 586         }
 587       } else if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_MemBar()) {
 588         mem = mem-&gt;in(0)-&gt;in(TypeFunc::Memory);
 589         continue;           // (a) advance through independent MemBar memory
 590       } else if (mem-&gt;is_ClearArray()) {
 591         if (ClearArrayNode::step_through(&amp;mem, (uint)addr_t-&gt;instance_id(), phase)) {
 592           // (the call updated 'mem' value)
 593           continue;         // (a) advance through independent allocation memory
 594         } else {
 595           // Can not bypass initialization of the instance
 596           // we are looking for.
 597           return mem;
 598         }
 599       } else if (mem-&gt;is_MergeMem()) {
 600         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 601         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 602         continue;           // (a) advance through independent MergeMem memory
 603       }
 604     }
 605 
 606     // Unless there is an explicit 'continue', we must bail out here,
 607     // because 'mem' is an inscrutable memory state (e.g., a call).
 608     break;
 609   }
 610 
 611   return NULL;              // bail out
 612 }
 613 
 614 //----------------------calculate_adr_type-------------------------------------
 615 // Helper function.  Notices when the given type of address hits top or bottom.
 616 // Also, asserts a cross-check of the type against the expected address type.
 617 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 618   if (t == Type::TOP)  return NULL; // does not touch memory any more?
 619   #ifdef PRODUCT
 620   cross_check = NULL;
 621   #else
 622   if (!VerifyAliases || is_error_reported() || Node::in_dump())  cross_check = NULL;
 623   #endif
 624   const TypePtr* tp = t-&gt;isa_ptr();
 625   if (tp == NULL) {
 626     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, "expected memory type must be wide");
 627     return TypePtr::BOTTOM;           // touches lots of memory
 628   } else {
 629     #ifdef ASSERT
 630     // %%%% [phh] We don't check the alias index if cross_check is
 631     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 632     if (cross_check != NULL &amp;&amp;
 633         cross_check != TypePtr::BOTTOM &amp;&amp;
 634         cross_check != TypeRawPtr::BOTTOM) {
 635       // Recheck the alias index, to see if it has changed (due to a bug).
 636       Compile* C = Compile::current();
 637       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 638              "must stay in the original alias category");
 639       // The type of the address must be contained in the adr_type,
 640       // disregarding "null"-ness.
 641       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 642       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 643       assert(cross_check-&gt;meet(tp_notnull) == cross_check-&gt;remove_speculative(),
 644              "real address must not escape from expected memory type");
 645     }
 646     #endif
 647     return tp;
 648   }
 649 }
 650 
 651 //------------------------adr_phi_is_loop_invariant----------------------------
 652 // A helper function for Ideal_DU_postCCP to check if a Phi in a counted
 653 // loop is loop invariant. Make a quick traversal of Phi and associated
 654 // CastPP nodes, looking to see if they are a closed group within the loop.
 655 bool MemNode::adr_phi_is_loop_invariant(Node* adr_phi, Node* cast) {
 656   // The idea is that the phi-nest must boil down to only CastPP nodes
 657   // with the same data. This implies that any path into the loop already
 658   // includes such a CastPP, and so the original cast, whatever its input,
 659   // must be covered by an equivalent cast, with an earlier control input.
 660   ResourceMark rm;
 661 
 662   // The loop entry input of the phi should be the unique dominating
 663   // node for every Phi/CastPP in the loop.
 664   Unique_Node_List closure;
 665   closure.push(adr_phi-&gt;in(LoopNode::EntryControl));
 666 
 667   // Add the phi node and the cast to the worklist.
 668   Unique_Node_List worklist;
 669   worklist.push(adr_phi);
 670   if( cast != NULL ){
 671     if( !cast-&gt;is_ConstraintCast() ) return false;
 672     worklist.push(cast);
 673   }
 674 
 675   // Begin recursive walk of phi nodes.
 676   while( worklist.size() ){
 677     // Take a node off the worklist
 678     Node *n = worklist.pop();
 679     if( !closure.member(n) ){
 680       // Add it to the closure.
 681       closure.push(n);
 682       // Make a sanity check to ensure we don't waste too much time here.
 683       if( closure.size() &gt; 20) return false;
 684       // This node is OK if:
 685       //  - it is a cast of an identical value
 686       //  - or it is a phi node (then we add its inputs to the worklist)
 687       // Otherwise, the node is not OK, and we presume the cast is not invariant
 688       if( n-&gt;is_ConstraintCast() ){
 689         worklist.push(n-&gt;in(1));
 690       } else if( n-&gt;is_Phi() ) {
 691         for( uint i = 1; i &lt; n-&gt;req(); i++ ) {
 692           worklist.push(n-&gt;in(i));
 693         }
 694       } else {
 695         return false;
 696       }
 697     }
 698   }
 699 
 700   // Quit when the worklist is empty, and we've found no offending nodes.
 701   return true;
 702 }
 703 
 704 //------------------------------Ideal_DU_postCCP-------------------------------
 705 // Find any cast-away of null-ness and keep its control.  Null cast-aways are
 706 // going away in this pass and we need to make this memory op depend on the
 707 // gating null check.
 708 Node *MemNode::Ideal_DU_postCCP( PhaseCCP *ccp ) {
 709   return Ideal_common_DU_postCCP(ccp, this, in(MemNode::Address));
 710 }
 711 
 712 // I tried to leave the CastPP's in.  This makes the graph more accurate in
 713 // some sense; we get to keep around the knowledge that an oop is not-null
 714 // after some test.  Alas, the CastPP's interfere with GVN (some values are
 715 // the regular oop, some are the CastPP of the oop, all merge at Phi's which
 716 // cannot collapse, etc).  This cost us 10% on SpecJVM, even when I removed
 717 // some of the more trivial cases in the optimizer.  Removing more useless
 718 // Phi's started allowing Loads to illegally float above null checks.  I gave
 719 // up on this approach.  CNC 10/20/2000
 720 // This static method may be called not from MemNode (EncodePNode calls it).
 721 // Only the control edge of the node 'n' might be updated.
 722 Node *MemNode::Ideal_common_DU_postCCP( PhaseCCP *ccp, Node* n, Node* adr ) {
 723   Node *skipped_cast = NULL;
 724   // Need a null check?  Regular static accesses do not because they are
 725   // from constant addresses.  Array ops are gated by the range check (which
 726   // always includes a NULL check).  Just check field ops.
 727   if( n-&gt;in(MemNode::Control) == NULL ) {
 728     // Scan upwards for the highest location we can place this memory op.
 729     while( true ) {
 730       switch( adr-&gt;Opcode() ) {
 731 
 732       case Op_AddP:             // No change to NULL-ness, so peek thru AddP's
 733         adr = adr-&gt;in(AddPNode::Base);
 734         continue;
 735 
 736       case Op_DecodeN:         // No change to NULL-ness, so peek thru
 737       case Op_DecodeNKlass:
 738         adr = adr-&gt;in(1);
 739         continue;
 740 
 741       case Op_EncodeP:
 742       case Op_EncodePKlass:
 743         // EncodeP node's control edge could be set by this method
 744         // when EncodeP node depends on CastPP node.
 745         //
 746         // Use its control edge for memory op because EncodeP may go away
 747         // later when it is folded with following or preceding DecodeN node.
 748         if (adr-&gt;in(0) == NULL) {
 749           // Keep looking for cast nodes.
 750           adr = adr-&gt;in(1);
 751           continue;
 752         }
 753         ccp-&gt;hash_delete(n);
 754         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 755         ccp-&gt;hash_insert(n);
 756         return n;
 757 
 758       case Op_CastPP:
 759       case Op_CastDerived:
 760         // If the CastPP is useless, just peek on through it.
 761         if( ccp-&gt;type(adr) == ccp-&gt;type(adr-&gt;in(1)) ) {
 762           // Remember the cast that we've peeked though. If we peek
 763           // through more than one, then we end up remembering the highest
 764           // one, that is, if in a loop, the one closest to the top.
 765           skipped_cast = adr;
 766           adr = adr-&gt;in(1);
 767           continue;
 768         }
 769         // CastPP is going away in this pass!  We need this memory op to be
 770         // control-dependent on the test that is guarding the CastPP.
 771         ccp-&gt;hash_delete(n);
 772         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 773         ccp-&gt;hash_insert(n);
 774         return n;
 775 
 776       case Op_Phi:
 777         // Attempt to float above a Phi to some dominating point.
 778         if (adr-&gt;in(0) != NULL &amp;&amp; adr-&gt;in(0)-&gt;is_CountedLoop()) {
 779           // If we've already peeked through a Cast (which could have set the
 780           // control), we can't float above a Phi, because the skipped Cast
 781           // may not be loop invariant.
 782           if (adr_phi_is_loop_invariant(adr, skipped_cast)) {
 783             adr = adr-&gt;in(1);
 784             continue;
 785           }
 786         }
 787 
 788         // Intentional fallthrough!
 789 
 790         // No obvious dominating point.  The mem op is pinned below the Phi
 791         // by the Phi itself.  If the Phi goes away (no true value is merged)
 792         // then the mem op can float, but not indefinitely.  It must be pinned
 793         // behind the controls leading to the Phi.
 794       case Op_CheckCastPP:
 795         // These usually stick around to change address type, however a
 796         // useless one can be elided and we still need to pick up a control edge
 797         if (adr-&gt;in(0) == NULL) {
 798           // This CheckCastPP node has NO control and is likely useless. But we
 799           // need check further up the ancestor chain for a control input to keep
 800           // the node in place. 4959717.
 801           skipped_cast = adr;
 802           adr = adr-&gt;in(1);
 803           continue;
 804         }
 805         ccp-&gt;hash_delete(n);
 806         n-&gt;set_req(MemNode::Control, adr-&gt;in(0));
 807         ccp-&gt;hash_insert(n);
 808         return n;
 809 
 810         // List of "safe" opcodes; those that implicitly block the memory
 811         // op below any null check.
 812       case Op_CastX2P:          // no null checks on native pointers
 813       case Op_Parm:             // 'this' pointer is not null
 814       case Op_LoadP:            // Loading from within a klass
 815       case Op_LoadN:            // Loading from within a klass
 816       case Op_LoadKlass:        // Loading from within a klass
 817       case Op_LoadNKlass:       // Loading from within a klass
 818       case Op_ConP:             // Loading from a klass
 819       case Op_ConN:             // Loading from a klass
 820       case Op_ConNKlass:        // Loading from a klass
 821       case Op_CreateEx:         // Sucking up the guts of an exception oop
 822       case Op_Con:              // Reading from TLS
 823       case Op_CMoveP:           // CMoveP is pinned
 824       case Op_CMoveN:           // CMoveN is pinned
 825         break;                  // No progress
 826 
 827       case Op_Proj:             // Direct call to an allocation routine
 828       case Op_SCMemProj:        // Memory state from store conditional ops
 829 #ifdef ASSERT
 830         {
 831           assert(adr-&gt;as_Proj()-&gt;_con == TypeFunc::Parms, "must be return value");
 832           const Node* call = adr-&gt;in(0);
 833           if (call-&gt;is_CallJava()) {
 834             const CallJavaNode* call_java = call-&gt;as_CallJava();
 835             const TypeTuple *r = call_java-&gt;tf()-&gt;range();
 836             assert(r-&gt;cnt() &gt; TypeFunc::Parms, "must return value");
 837             const Type* ret_type = r-&gt;field_at(TypeFunc::Parms);
 838             assert(ret_type &amp;&amp; ret_type-&gt;isa_ptr(), "must return pointer");
 839             // We further presume that this is one of
 840             // new_instance_Java, new_array_Java, or
 841             // the like, but do not assert for this.
 842           } else if (call-&gt;is_Allocate()) {
 843             // similar case to new_instance_Java, etc.
 844           } else if (!call-&gt;is_CallLeaf()) {
 845             // Projections from fetch_oop (OSR) are allowed as well.
 846             ShouldNotReachHere();
 847           }
 848         }
 849 #endif
 850         break;
 851       default:
 852         ShouldNotReachHere();
 853       }
 854       break;
 855     }
 856   }
 857 
 858   return  NULL;               // No progress
 859 }
 860 
 861 
 862 //=============================================================================
 863 // Should LoadNode::Ideal() attempt to remove control edges?
 864 bool LoadNode::can_remove_control() const {
 865   return true;
 866 }
 867 uint LoadNode::size_of() const { return sizeof(*this); }
 868 uint LoadNode::cmp( const Node &amp;n ) const
 869 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 870 const Type *LoadNode::bottom_type() const { return _type; }
 871 uint LoadNode::ideal_reg() const {
 872   return _type-&gt;ideal_reg();
 873 }
 874 
 875 #ifndef PRODUCT
 876 void LoadNode::dump_spec(outputStream *st) const {
 877   MemNode::dump_spec(st);
 878   if( !Verbose &amp;&amp; !WizardMode ) {
 879     // standard dump does this in Verbose and WizardMode
 880     st-&gt;print(" #"); _type-&gt;dump_on(st);
 881   }
 882 }
 883 #endif
 884 
 885 #ifdef ASSERT
 886 //----------------------------is_immutable_value-------------------------------
 887 // Helper function to allow a raw load without control edge for some cases
 888 bool LoadNode::is_immutable_value(Node* adr) {
 889   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 890           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 891           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 892            in_bytes(JavaThread::osthread_offset())));
 893 }
 894 #endif
 895 
 896 //----------------------------LoadNode::make-----------------------------------
 897 // Polymorphic factory method:
 898 Node *LoadNode::make(PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo) {
 899   Compile* C = gvn.C;
 900 
 901   // sanity check the alias category against the created node type
 902   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 903            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 904          "use LoadKlassNode instead");
 905   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 906            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 907          "use LoadRangeNode instead");
 908   // Check control edge of raw loads
 909   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 910           // oop will be recorded in oop map if load crosses safepoint
 911           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 912           "raw memory operations should have control edge");
 913   switch (bt) {
 914   case T_BOOLEAN: return new (C) LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 915   case T_BYTE:    return new (C) LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 916   case T_INT:     return new (C) LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 917   case T_CHAR:    return new (C) LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 918   case T_SHORT:   return new (C) LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo);
 919   case T_LONG:    return new (C) LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo);
 920   case T_FLOAT:   return new (C) LoadFNode (ctl, mem, adr, adr_type, rt,            mo);
 921   case T_DOUBLE:  return new (C) LoadDNode (ctl, mem, adr, adr_type, rt,            mo);
 922   case T_ADDRESS: return new (C) LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo);
 923   case T_OBJECT:
 924 #ifdef _LP64
 925     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 926       Node* load  = gvn.transform(new (C) LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo));
 927       return new (C) DecodeNNode(load, load-&gt;bottom_type()-&gt;make_ptr());
 928     } else
 929 #endif
 930     {
 931       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), "should have got back a narrow oop");
 932       return new (C) LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_oopptr(), mo);
 933     }
 934   }
 935   ShouldNotReachHere();
 936   return (LoadNode*)NULL;
 937 }
 938 
 939 LoadLNode* LoadLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo) {
 940   bool require_atomic = true;
 941   return new (C) LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, require_atomic);
 942 }
 943 
 944 
 945 
 946 
 947 //------------------------------hash-------------------------------------------
 948 uint LoadNode::hash() const {
 949   // unroll addition of interesting fields
 950   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 951 }
 952 
 953 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 954   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 955     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 956     bool is_stable_ary = FoldStableValues &amp;&amp;
 957                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 958                          tp-&gt;isa_aryptr()-&gt;is_stable();
 959 
 960     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 961   }
 962 
 963   return false;
 964 }
 965 
 966 //---------------------------can_see_stored_value------------------------------
 967 // This routine exists to make sure this set of tests is done the same
 968 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 969 // will change the graph shape in a way which makes memory alive twice at the
 970 // same time (uses the Oracle model of aliasing), then some
 971 // LoadXNode::Identity will fold things back to the equivalence-class model
 972 // of aliasing.
 973 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 974   Node* ld_adr = in(MemNode::Address);
 975   intptr_t ld_off = 0;
 976   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 977   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 978   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
 979   // This is more general than load from boxing objects.
 980   if (skip_through_membars(atp, tp, phase-&gt;C-&gt;eliminate_boxing())) {
 981     uint alias_idx = atp-&gt;index();
 982     bool final = !atp-&gt;is_rewritable();
 983     Node* result = NULL;
 984     Node* current = st;
 985     // Skip through chains of MemBarNodes checking the MergeMems for
 986     // new states for the slice of this load.  Stop once any other
 987     // kind of node is encountered.  Loads from final memory can skip
 988     // through any kind of MemBar but normal loads shouldn't skip
 989     // through MemBarAcquire since the could allow them to move out of
 990     // a synchronized region.
 991     while (current-&gt;is_Proj()) {
 992       int opc = current-&gt;in(0)-&gt;Opcode();
 993       if ((final &amp;&amp; (opc == Op_MemBarAcquire ||
 994                      opc == Op_MemBarAcquireLock ||
 995                      opc == Op_LoadFence)) ||
 996           opc == Op_MemBarRelease ||
 997           opc == Op_StoreFence ||
 998           opc == Op_MemBarReleaseLock ||
 999           opc == Op_MemBarCPUOrder) {
1000         Node* mem = current-&gt;in(0)-&gt;in(TypeFunc::Memory);
1001         if (mem-&gt;is_MergeMem()) {
1002           MergeMemNode* merge = mem-&gt;as_MergeMem();
1003           Node* new_st = merge-&gt;memory_at(alias_idx);
1004           if (new_st == merge-&gt;base_memory()) {
1005             // Keep searching
1006             current = new_st;
1007             continue;
1008           }
1009           // Save the new memory state for the slice and fall through
1010           // to exit.
1011           result = new_st;
1012         }
1013       }
1014       break;
1015     }
1016     if (result != NULL) {
1017       st = result;
1018     }
1019   }
1020 
1021   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1022   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1023   for (int trip = 0; trip &lt;= 1; trip++) {
1024 
1025     if (st-&gt;is_Store()) {
1026       Node* st_adr = st-&gt;in(MemNode::Address);
1027       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1028         // Try harder before giving up...  Match raw and non-raw pointers.
1029         intptr_t st_off = 0;
1030         AllocateNode* alloc = AllocateNode::Ideal_allocation(st_adr, phase, st_off);
1031         if (alloc == NULL)       return NULL;
1032         if (alloc != ld_alloc)   return NULL;
1033         if (ld_off != st_off)    return NULL;
1034         // At this point we have proven something like this setup:
1035         //  A = Allocate(...)
1036         //  L = LoadQ(,  AddP(CastPP(, A.Parm),, #Off))
1037         //  S = StoreQ(, AddP(,        A.Parm  , #Off), V)
1038         // (Actually, we haven't yet proven the Q's are the same.)
1039         // In other words, we are loading from a casted version of
1040         // the same pointer-and-offset that we stored to.
1041         // Thus, we are able to replace L by V.
1042       }
1043       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1044       if (store_Opcode() != st-&gt;Opcode())
1045         return NULL;
1046       return st-&gt;in(MemNode::ValueIn);
1047     }
1048 
1049     // A load from a freshly-created object always returns zero.
1050     // (This can happen after LoadNode::Ideal resets the load's memory input
1051     // to find_captured_store, which returned InitializeNode::zero_memory.)
1052     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1053         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1054         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1055       // return a zero value for the load's basic type
1056       // (This is one of the few places where a generic PhaseTransform
1057       // can create new nodes.  Think of it as lazily manifesting
1058       // virtually pre-existing constants.)
1059       return phase-&gt;zerocon(memory_type());
1060     }
1061 
1062     // A load from an initialization barrier can match a captured store.
1063     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1064       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1065       AllocateNode* alloc = init-&gt;allocation();
1066       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1067         // examine a captured store value
1068         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1069         if (st != NULL)
1070           continue;             // take one more trip around
1071       }
1072     }
1073 
1074     // Load boxed value from result of valueOf() call is input parameter.
1075     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1076         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1077       intptr_t ignore = 0;
1078       Node* base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ignore);
1079       if (base != NULL &amp;&amp; base-&gt;is_Proj() &amp;&amp;
1080           base-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp;
1081           base-&gt;in(0)-&gt;is_CallStaticJava() &amp;&amp;
1082           base-&gt;in(0)-&gt;as_CallStaticJava()-&gt;is_boxing_method()) {
1083         return base-&gt;in(0)-&gt;in(TypeFunc::Parms);
1084       }
1085     }
1086 
1087     break;
1088   }
1089 
1090   return NULL;
1091 }
1092 
1093 //----------------------is_instance_field_load_with_local_phi------------------
1094 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1095   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1096       in(Address)-&gt;is_AddP() ) {
1097     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1098     // Only instances and boxed values.
1099     if( t_oop != NULL &amp;&amp;
1100         (t_oop-&gt;is_ptr_to_boxed_value() ||
1101          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1102         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1103         t_oop-&gt;offset() != Type::OffsetTop) {
1104       return true;
1105     }
1106   }
1107   return false;
1108 }
1109 
1110 //------------------------------Identity---------------------------------------
1111 // Loads are identity if previous store is to same address
1112 Node *LoadNode::Identity( PhaseTransform *phase ) {
1113   // If the previous store-maker is the right kind of Store, and the store is
1114   // to the same address, then we are equal to the value stored.
1115   Node* mem = in(Memory);
1116   Node* value = can_see_stored_value(mem, phase);
1117   if( value ) {
1118     // byte, short &amp; char stores truncate naturally.
1119     // A load has to load the truncated value which requires
1120     // some sort of masking operation and that requires an
1121     // Ideal call instead of an Identity call.
1122     if (memory_size() &lt; BytesPerInt) {
1123       // If the input to the store does not fit with the load's result type,
1124       // it must be truncated via an Ideal call.
1125       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1126         return this;
1127     }
1128     // (This works even when value is a Con, but LoadNode::Value
1129     // usually runs first, producing the singleton type of the Con.)
1130     return value;
1131   }
1132 
1133   // Search for an existing data phi which was generated before for the same
1134   // instance's field to avoid infinite generation of phis in a loop.
1135   Node *region = mem-&gt;in(0);
1136   if (is_instance_field_load_with_local_phi(region)) {
1137     const TypeOopPtr *addr_t = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1138     int this_index  = phase-&gt;C-&gt;get_alias_index(addr_t);
1139     int this_offset = addr_t-&gt;offset();
1140     int this_iid    = addr_t-&gt;instance_id();
1141     if (!addr_t-&gt;is_known_instance() &amp;&amp;
1142          addr_t-&gt;is_ptr_to_boxed_value()) {
1143       // Use _idx of address base (could be Phi node) for boxed values.
1144       intptr_t   ignore = 0;
1145       Node*      base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1146       this_iid = base-&gt;_idx;
1147     }
1148     const Type* this_type = bottom_type();
1149     for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1150       Node* phi = region-&gt;fast_out(i);
1151       if (phi-&gt;is_Phi() &amp;&amp; phi != mem &amp;&amp;
1152           phi-&gt;as_Phi()-&gt;is_same_inst_field(this_type, this_iid, this_index, this_offset)) {
1153         return phi;
1154       }
1155     }
1156   }
1157 
1158   return this;
1159 }
1160 
1161 // We're loading from an object which has autobox behaviour.
1162 // If this object is result of a valueOf call we'll have a phi
1163 // merging a newly allocated object and a load from the cache.
1164 // We want to replace this load with the original incoming
1165 // argument to the valueOf call.
1166 Node* LoadNode::eliminate_autobox(PhaseGVN* phase) {
1167   assert(phase-&gt;C-&gt;eliminate_boxing(), "sanity");
1168   intptr_t ignore = 0;
1169   Node* base = AddPNode::Ideal_base_and_offset(in(Address), phase, ignore);
1170   if ((base == NULL) || base-&gt;is_Phi()) {
1171     // Push the loads from the phi that comes from valueOf up
1172     // through it to allow elimination of the loads and the recovery
1173     // of the original value. It is done in split_through_phi().
1174     return NULL;
1175   } else if (base-&gt;is_Load() ||
1176              base-&gt;is_DecodeN() &amp;&amp; base-&gt;in(1)-&gt;is_Load()) {
1177     // Eliminate the load of boxed value for integer types from the cache
1178     // array by deriving the value from the index into the array.
1179     // Capture the offset of the load and then reverse the computation.
1180 
1181     // Get LoadN node which loads a boxing object from 'cache' array.
1182     if (base-&gt;is_DecodeN()) {
1183       base = base-&gt;in(1);
1184     }
1185     if (!base-&gt;in(Address)-&gt;is_AddP()) {
1186       return NULL; // Complex address
1187     }
1188     AddPNode* address = base-&gt;in(Address)-&gt;as_AddP();
1189     Node* cache_base = address-&gt;in(AddPNode::Base);
1190     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_DecodeN()) {
1191       // Get ConP node which is static 'cache' field.
1192       cache_base = cache_base-&gt;in(1);
1193     }
1194     if ((cache_base != NULL) &amp;&amp; cache_base-&gt;is_Con()) {
1195       const TypeAryPtr* base_type = cache_base-&gt;bottom_type()-&gt;isa_aryptr();
1196       if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1197         Node* elements[4];
1198         int shift = exact_log2(type2aelembytes(T_OBJECT));
1199         int count = address-&gt;unpack_offsets(elements, ARRAY_SIZE(elements));
1200         if ((count &gt;  0) &amp;&amp; elements[0]-&gt;is_Con() &amp;&amp;
1201             ((count == 1) ||
1202              (count == 2) &amp;&amp; elements[1]-&gt;Opcode() == Op_LShiftX &amp;&amp;
1203                              elements[1]-&gt;in(2) == phase-&gt;intcon(shift))) {
1204           ciObjArray* array = base_type-&gt;const_oop()-&gt;as_obj_array();
1205           // Fetch the box object cache[0] at the base of the array and get its value
1206           ciInstance* box = array-&gt;obj_at(0)-&gt;as_instance();
1207           ciInstanceKlass* ik = box-&gt;klass()-&gt;as_instance_klass();
1208           assert(ik-&gt;is_box_klass(), "sanity");
1209           assert(ik-&gt;nof_nonstatic_fields() == 1, "change following code");
1210           if (ik-&gt;nof_nonstatic_fields() == 1) {
1211             // This should be true nonstatic_field_at requires calling
1212             // nof_nonstatic_fields so check it anyway
1213             ciConstant c = box-&gt;field_value(ik-&gt;nonstatic_field_at(0));
1214             BasicType bt = c.basic_type();
1215             // Only integer types have boxing cache.
1216             assert(bt == T_BOOLEAN || bt == T_CHAR  ||
1217                    bt == T_BYTE    || bt == T_SHORT ||
1218                    bt == T_INT     || bt == T_LONG, err_msg_res("wrong type = %s", type2name(bt)));
1219             jlong cache_low = (bt == T_LONG) ? c.as_long() : c.as_int();
1220             if (cache_low != (int)cache_low) {
1221               return NULL; // should not happen since cache is array indexed by value
1222             }
1223             jlong offset = arrayOopDesc::base_offset_in_bytes(T_OBJECT) - (cache_low &lt;&lt; shift);
1224             if (offset != (int)offset) {
1225               return NULL; // should not happen since cache is array indexed by value
1226             }
1227            // Add up all the offsets making of the address of the load
1228             Node* result = elements[0];
1229             for (int i = 1; i &lt; count; i++) {
1230               result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, elements[i]));
1231             }
1232             // Remove the constant offset from the address and then
1233             result = phase-&gt;transform(new (phase-&gt;C) AddXNode(result, phase-&gt;MakeConX(-(int)offset)));
1234             // remove the scaling of the offset to recover the original index.
1235             if (result-&gt;Opcode() == Op_LShiftX &amp;&amp; result-&gt;in(2) == phase-&gt;intcon(shift)) {
1236               // Peel the shift off directly but wrap it in a dummy node
1237               // since Ideal can't return existing nodes
1238               result = new (phase-&gt;C) RShiftXNode(result-&gt;in(1), phase-&gt;intcon(0));
1239             } else if (result-&gt;is_Add() &amp;&amp; result-&gt;in(2)-&gt;is_Con() &amp;&amp;
1240                        result-&gt;in(1)-&gt;Opcode() == Op_LShiftX &amp;&amp;
1241                        result-&gt;in(1)-&gt;in(2) == phase-&gt;intcon(shift)) {
1242               // We can't do general optimization: ((X&lt;&lt;Z) + Y) &gt;&gt; Z ==&gt; X + (Y&gt;&gt;Z)
1243               // but for boxing cache access we know that X&lt;&lt;Z will not overflow
1244               // (there is range check) so we do this optimizatrion by hand here.
1245               Node* add_con = new (phase-&gt;C) RShiftXNode(result-&gt;in(2), phase-&gt;intcon(shift));
1246               result = new (phase-&gt;C) AddXNode(result-&gt;in(1)-&gt;in(1), phase-&gt;transform(add_con));
1247             } else {
1248               result = new (phase-&gt;C) RShiftXNode(result, phase-&gt;intcon(shift));
1249             }
1250 #ifdef _LP64
1251             if (bt != T_LONG) {
1252               result = new (phase-&gt;C) ConvL2INode(phase-&gt;transform(result));
1253             }
1254 #else
1255             if (bt == T_LONG) {
1256               result = new (phase-&gt;C) ConvI2LNode(phase-&gt;transform(result));
1257             }
1258 #endif
1259             // Boxing/unboxing can be done from signed &amp; unsigned loads (e.g. LoadUB -&gt; ... -&gt; LoadB pair).
1260             // Need to preserve unboxing load type if it is unsigned.
1261             switch(this-&gt;Opcode()) {
1262               case Op_LoadUB:
1263                 result = new (phase-&gt;C) AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFF));
1264                 break;
1265               case Op_LoadUS:
1266                 result = new (phase-&gt;C) AndINode(phase-&gt;transform(result), phase-&gt;intcon(0xFFFF));
1267                 break;
1268             }
1269             return result;
1270           }
1271         }
1272       }
1273     }
1274   }
1275   return NULL;
1276 }
1277 
1278 static bool stable_phi(PhiNode* phi, PhaseGVN *phase) {
1279   Node* region = phi-&gt;in(0);
1280   if (region == NULL) {
1281     return false; // Wait stable graph
1282   }
1283   uint cnt = phi-&gt;req();
1284   for (uint i = 1; i &lt; cnt; i++) {
1285     Node* rc = region-&gt;in(i);
1286     if (rc == NULL || phase-&gt;type(rc) == Type::TOP)
1287       return false; // Wait stable graph
1288     Node* in = phi-&gt;in(i);
1289     if (in == NULL || phase-&gt;type(in) == Type::TOP)
1290       return false; // Wait stable graph
1291   }
1292   return true;
1293 }
1294 //------------------------------split_through_phi------------------------------
1295 // Split instance or boxed field load through Phi.
1296 Node *LoadNode::split_through_phi(PhaseGVN *phase) {
1297   Node* mem     = in(Memory);
1298   Node* address = in(Address);
1299   const TypeOopPtr *t_oop = phase-&gt;type(address)-&gt;isa_oopptr();
1300 
1301   assert((t_oop != NULL) &amp;&amp;
1302          (t_oop-&gt;is_known_instance_field() ||
1303           t_oop-&gt;is_ptr_to_boxed_value()), "invalide conditions");
1304 
1305   Compile* C = phase-&gt;C;
1306   intptr_t ignore = 0;
1307   Node*    base = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1308   bool base_is_phi = (base != NULL) &amp;&amp; base-&gt;is_Phi();
1309   bool load_boxed_values = t_oop-&gt;is_ptr_to_boxed_value() &amp;&amp; C-&gt;aggressive_unboxing() &amp;&amp;
1310                            (base != NULL) &amp;&amp; (base == address-&gt;in(AddPNode::Base)) &amp;&amp;
1311                            phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL);
1312 
1313   if (!((mem-&gt;is_Phi() || base_is_phi) &amp;&amp;
1314         (load_boxed_values || t_oop-&gt;is_known_instance_field()))) {
1315     return NULL; // memory is not Phi
1316   }
1317 
1318   if (mem-&gt;is_Phi()) {
1319     if (!stable_phi(mem-&gt;as_Phi(), phase)) {
1320       return NULL; // Wait stable graph
1321     }
1322     uint cnt = mem-&gt;req();
1323     // Check for loop invariant memory.
1324     if (cnt == 3) {
1325       for (uint i = 1; i &lt; cnt; i++) {
1326         Node* in = mem-&gt;in(i);
1327         Node*  m = optimize_memory_chain(in, t_oop, this, phase);
1328         if (m == mem) {
1329           set_req(Memory, mem-&gt;in(cnt - i));
1330           return this; // made change
1331         }
1332       }
1333     }
1334   }
1335   if (base_is_phi) {
1336     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1337       return NULL; // Wait stable graph
1338     }
1339     uint cnt = base-&gt;req();
1340     // Check for loop invariant memory.
1341     if (cnt == 3) {
1342       for (uint i = 1; i &lt; cnt; i++) {
1343         if (base-&gt;in(i) == base) {
1344           return NULL; // Wait stable graph
1345         }
1346       }
1347     }
1348   }
1349 
1350   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));
1351 
1352   // Split through Phi (see original code in loopopts.cpp).
1353   assert(C-&gt;have_alias_type(t_oop), "instance should have alias type");
1354 
1355   // Do nothing here if Identity will find a value
1356   // (to avoid infinite chain of value phis generation).
1357   if (!phase-&gt;eqv(this, this-&gt;Identity(phase)))
1358     return NULL;
1359 
1360   // Select Region to split through.
1361   Node* region;
1362   if (!base_is_phi) {
1363     assert(mem-&gt;is_Phi(), "sanity");
1364     region = mem-&gt;in(0);
1365     // Skip if the region dominates some control edge of the address.
1366     if (!MemNode::all_controls_dominate(address, region))
1367       return NULL;
1368   } else if (!mem-&gt;is_Phi()) {
1369     assert(base_is_phi, "sanity");
1370     region = base-&gt;in(0);
1371     // Skip if the region dominates some control edge of the memory.
1372     if (!MemNode::all_controls_dominate(mem, region))
1373       return NULL;
1374   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1375     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), "sanity");
1376     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1377       region = base-&gt;in(0);
1378     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
1379       region = mem-&gt;in(0);
1380     } else {
1381       return NULL; // complex graph
1382     }
1383   } else {
1384     assert(base-&gt;in(0) == mem-&gt;in(0), "sanity");
1385     region = mem-&gt;in(0);
1386   }
1387 
1388   const Type* this_type = this-&gt;bottom_type();
1389   int this_index  = C-&gt;get_alias_index(t_oop);
1390   int this_offset = t_oop-&gt;offset();
1391   int this_iid    = t_oop-&gt;instance_id();
1392   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1393     // Use _idx of address base for boxed values.
1394     this_iid = base-&gt;_idx;
1395   }
1396   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1397   Node* phi = new (C) PhiNode(region, this_type, NULL, this_iid, this_index, this_offset);
1398   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1399     Node* x;
1400     Node* the_clone = NULL;
1401     if (region-&gt;in(i) == C-&gt;top()) {
1402       x = C-&gt;top();      // Dead path?  Use a dead data op
1403     } else {
1404       x = this-&gt;clone();        // Else clone up the data op
1405       the_clone = x;            // Remember for possible deletion.
1406       // Alter data node to use pre-phi inputs
1407       if (this-&gt;in(0) == region) {
1408         x-&gt;set_req(0, region-&gt;in(i));
1409       } else {
1410         x-&gt;set_req(0, NULL);
1411       }
1412       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1413         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1414       }
1415       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1416         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1417       }
1418       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1419         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1420         Node* adr_x = phase-&gt;transform(new (C) AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1421         x-&gt;set_req(Address, adr_x);
1422       }
1423     }
1424     // Check for a 'win' on some paths
1425     const Type *t = x-&gt;Value(igvn);
1426 
1427     bool singleton = t-&gt;singleton();
1428 
1429     // See comments in PhaseIdealLoop::split_thru_phi().
1430     if (singleton &amp;&amp; t == Type::TOP) {
1431       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1432     }
1433 
1434     if (singleton) {
1435       x = igvn-&gt;makecon(t);
1436     } else {
1437       // We now call Identity to try to simplify the cloned node.
1438       // Note that some Identity methods call phase-&gt;type(this).
1439       // Make sure that the type array is big enough for
1440       // our new node, even though we may throw the node away.
1441       // (This tweaking with igvn only works because x is a new node.)
1442       igvn-&gt;set_type(x, t);
1443       // If x is a TypeNode, capture any more-precise type permanently into Node
1444       // otherwise it will be not updated during igvn-&gt;transform since
1445       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1446       x-&gt;raise_bottom_type(t);
1447       Node *y = x-&gt;Identity(igvn);
1448       if (y != x) {
1449         x = y;
1450       } else {
1451         y = igvn-&gt;hash_find_insert(x);
1452         if (y) {
1453           x = y;
1454         } else {
1455           // Else x is a new node we are keeping
1456           // We do not need register_new_node_with_optimizer
1457           // because set_type has already been called.
1458           igvn-&gt;_worklist.push(x);
1459         }
1460       }
1461     }
1462     if (x != the_clone &amp;&amp; the_clone != NULL) {
1463       igvn-&gt;remove_dead_node(the_clone);
1464     }
1465     phi-&gt;set_req(i, x);
1466   }
1467   // Record Phi
1468   igvn-&gt;register_new_node_with_optimizer(phi);
1469   return phi;
1470 }
1471 
1472 //------------------------------Ideal------------------------------------------
1473 // If the load is from Field memory and the pointer is non-null, it might be possible to
1474 // zero out the control input.
1475 // If the offset is constant and the base is an object allocation,
1476 // try to hook me up to the exact initializing store.
1477 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1478   Node* p = MemNode::Ideal_common(phase, can_reshape);
1479   if (p)  return (p == NodeSentinel) ? NULL : p;
1480 
1481   Node* ctrl    = in(MemNode::Control);
1482   Node* address = in(MemNode::Address);
1483 
1484   // Skip up past a SafePoint control.  Cannot do this for Stores because
1485   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1486   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1487       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw ) {
1488     ctrl = ctrl-&gt;in(0);
1489     set_req(MemNode::Control,ctrl);
1490   }
1491 
1492   intptr_t ignore = 0;
1493   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1494   if (base != NULL
1495       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1496     // Check for useless control edge in some common special cases
1497     if (in(MemNode::Control) != NULL
1498         &amp;&amp; can_remove_control()
1499         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1500         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1501       // A method-invariant, non-null address (constant or 'this' argument).
1502       set_req(MemNode::Control, NULL);
1503     }
1504   }
1505 
1506   Node* mem = in(MemNode::Memory);
1507   const TypePtr *addr_t = phase-&gt;type(address)-&gt;isa_ptr();
1508 
1509   if (can_reshape &amp;&amp; (addr_t != NULL)) {
1510     // try to optimize our memory input
1511     Node* opt_mem = MemNode::optimize_memory_chain(mem, addr_t, this, phase);
1512     if (opt_mem != mem) {
1513       set_req(MemNode::Memory, opt_mem);
1514       if (phase-&gt;type( opt_mem ) == Type::TOP) return NULL;
1515       return this;
1516     }
1517     const TypeOopPtr *t_oop = addr_t-&gt;isa_oopptr();
1518     if ((t_oop != NULL) &amp;&amp;
1519         (t_oop-&gt;is_known_instance_field() ||
1520          t_oop-&gt;is_ptr_to_boxed_value())) {
1521       PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
1522       if (igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(opt_mem)) {
1523         // Delay this transformation until memory Phi is processed.
1524         phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
1525         return NULL;
1526       }
1527       // Split instance field load through Phi.
1528       Node* result = split_through_phi(phase);
1529       if (result != NULL) return result;
1530 
1531       if (t_oop-&gt;is_ptr_to_boxed_value()) {
1532         Node* result = eliminate_autobox(phase);
1533         if (result != NULL) return result;
1534       }
1535     }
1536   }
1537 
1538   // Check for prior store with a different base or offset; make Load
1539   // independent.  Skip through any number of them.  Bail out if the stores
1540   // are in an endless dead cycle and report no progress.  This is a key
1541   // transform for Reflection.  However, if after skipping through the Stores
1542   // we can't then fold up against a prior store do NOT do the transform as
1543   // this amounts to using the 'Oracle' model of aliasing.  It leaves the same
1544   // array memory alive twice: once for the hoisted Load and again after the
1545   // bypassed Store.  This situation only works if EVERYBODY who does
1546   // anti-dependence work knows how to bypass.  I.e. we need all
1547   // anti-dependence checks to ask the same Oracle.  Right now, that Oracle is
1548   // the alias index stuff.  So instead, peek through Stores and IFF we can
1549   // fold up, do so.
1550   Node* prev_mem = find_previous_store(phase);
1551   // Steps (a), (b):  Walk past independent stores to find an exact match.
1552   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1553     // (c) See if we can fold up on the spot, but don't fold up here.
1554     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1555     // just return a prior value, which is done by Identity calls.
1556     if (can_see_stored_value(prev_mem, phase)) {
1557       // Make ready for step (d):
1558       set_req(MemNode::Memory, prev_mem);
1559       return this;
1560     }
1561   }
1562 
1563   return NULL;                  // No further progress
1564 }
1565 
1566 // Helper to recognize certain Klass fields which are invariant across
1567 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1568 const Type*
1569 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1570                                  ciKlass* klass) const {
1571   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1572     // The field is Klass::_modifier_flags.  Return its (constant) value.
1573     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1574     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _modifier_flags");
1575     return TypeInt::make(klass-&gt;modifier_flags());
1576   }
1577   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1578     // The field is Klass::_access_flags.  Return its (constant) value.
1579     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1580     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _access_flags");
1581     return TypeInt::make(klass-&gt;access_flags());
1582   }
1583   if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())) {
1584     // The field is Klass::_layout_helper.  Return its constant value if known.
1585     assert(this-&gt;Opcode() == Op_LoadI, "must load an int from _layout_helper");
1586     return TypeInt::make(klass-&gt;layout_helper());
1587   }
1588 
1589   // No match.
1590   return NULL;
1591 }
1592 
1593 // Try to constant-fold a stable array element.
1594 static const Type* fold_stable_ary_elem(const TypeAryPtr* ary, int off, BasicType loadbt) {
1595   assert(ary-&gt;const_oop(), "array should be constant");
1596   assert(ary-&gt;is_stable(), "array should be stable");
1597 
1598   // Decode the results of GraphKit::array_element_address.
1599   ciArray* aobj = ary-&gt;const_oop()-&gt;as_array();
1600   ciConstant con = aobj-&gt;element_value_by_offset(off);
1601 
1602   if (con.basic_type() != T_ILLEGAL &amp;&amp; !con.is_null_or_zero()) {
1603     const Type* con_type = Type::make_from_constant(con);
1604     if (con_type != NULL) {
1605       if (con_type-&gt;isa_aryptr()) {
1606         // Join with the array element type, in case it is also stable.
1607         int dim = ary-&gt;stable_dimension();
1608         con_type = con_type-&gt;is_aryptr()-&gt;cast_to_stable(true, dim-1);
1609       }
1610       if (loadbt == T_NARROWOOP &amp;&amp; con_type-&gt;isa_oopptr()) {
1611         con_type = con_type-&gt;make_narrowoop();
1612       }
1613 #ifndef PRODUCT
1614       if (TraceIterativeGVN) {
1615         tty-&gt;print("FoldStableValues: array element [off=%d]: con_type=", off);
1616         con_type-&gt;dump(); tty-&gt;cr();
1617       }
1618 #endif //PRODUCT
1619       return con_type;
1620     }
1621   }
1622   return NULL;
1623 }
1624 
1625 //------------------------------Value-----------------------------------------
1626 const Type *LoadNode::Value( PhaseTransform *phase ) const {
1627   // Either input is TOP ==&gt; the result is TOP
1628   Node* mem = in(MemNode::Memory);
1629   const Type *t1 = phase-&gt;type(mem);
1630   if (t1 == Type::TOP)  return Type::TOP;
1631   Node* adr = in(MemNode::Address);
1632   const TypePtr* tp = phase-&gt;type(adr)-&gt;isa_ptr();
1633   if (tp == NULL || tp-&gt;empty())  return Type::TOP;
1634   int off = tp-&gt;offset();
1635   assert(off != Type::OffsetTop, "case covered by TypePtr::empty");
1636   Compile* C = phase-&gt;C;
1637 
1638   // Try to guess loaded type from pointer type
1639   if (tp-&gt;isa_aryptr()) {
1640     const TypeAryPtr* ary = tp-&gt;is_aryptr();
1641     const Type* t = ary-&gt;elem();
1642 
1643     // Determine whether the reference is beyond the header or not, by comparing
1644     // the offset against the offset of the start of the array's data.
1645     // Different array types begin at slightly different offsets (12 vs. 16).
1646     // We choose T_BYTE as an example base type that is least restrictive
1647     // as to alignment, which will therefore produce the smallest
1648     // possible base offset.
1649     const int min_base_off = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1650     const bool off_beyond_header = ((uint)off &gt;= (uint)min_base_off);
1651 
1652     // Try to constant-fold a stable array element.
1653     if (FoldStableValues &amp;&amp; ary-&gt;is_stable() &amp;&amp; ary-&gt;const_oop() != NULL) {
1654       // Make sure the reference is not into the header and the offset is constant
1655       if (off_beyond_header &amp;&amp; adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1656         const Type* con_type = fold_stable_ary_elem(ary, off, memory_type());
1657         if (con_type != NULL) {
1658           return con_type;
1659         }
1660       }
1661     }
1662 
1663     // Don't do this for integer types. There is only potential profit if
1664     // the element type t is lower than _type; that is, for int types, if _type is
1665     // more restrictive than t.  This only happens here if one is short and the other
1666     // char (both 16 bits), and in those cases we've made an intentional decision
1667     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1668     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1669     //
1670     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1671     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1672     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1673     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1674     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1675     // In fact, that could have been the original type of p1, and p1 could have
1676     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1677     // expression (LShiftL quux 3) independently optimized to the constant 8.
1678     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1679         &amp;&amp; (_type-&gt;isa_vect() == NULL)
1680         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1681       // t might actually be lower than _type, if _type is a unique
1682       // concrete subclass of abstract class t.
1683       if (off_beyond_header) {  // is the offset beyond the header?
1684         const Type* jt = t-&gt;join_speculative(_type);
1685         // In any case, do not allow the join, per se, to empty out the type.
1686         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1687           // This can happen if a interface-typed array narrows to a class type.
1688           jt = _type;
1689         }
1690 #ifdef ASSERT
1691         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1692           // The pointers in the autobox arrays are always non-null
1693           Node* base = adr-&gt;in(AddPNode::Base);
1694           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1695             // Get LoadN node which loads IntegerCache.cache field
1696             base = base-&gt;in(1);
1697           }
1698           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1699             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1700             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1701               // It could be narrow oop
1702               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,"sanity");
1703             }
1704           }
1705         }
1706 #endif
1707         return jt;
1708       }
1709     }
1710   } else if (tp-&gt;base() == Type::InstPtr) {
1711     ciEnv* env = C-&gt;env();
1712     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1713     ciKlass* klass = tinst-&gt;klass();
1714     assert( off != Type::OffsetBot ||
1715             // arrays can be cast to Objects
1716             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1717             // unsafe field access may not have a constant offset
1718             C-&gt;has_unsafe_access(),
1719             "Field accesses must be precise" );
1720     // For oop loads, we expect the _type to be precise
1721     if (klass == env-&gt;String_klass() &amp;&amp;
1722         adr-&gt;is_AddP() &amp;&amp; off != Type::OffsetBot) {
1723       // For constant Strings treat the final fields as compile time constants.
1724       Node* base = adr-&gt;in(AddPNode::Base);
1725       const TypeOopPtr* t = phase-&gt;type(base)-&gt;isa_oopptr();
1726       if (t != NULL &amp;&amp; t-&gt;singleton()) {
1727         ciField* field = env-&gt;String_klass()-&gt;get_field_by_offset(off, false);
1728         if (field != NULL &amp;&amp; field-&gt;is_final()) {
1729           ciObject* string = t-&gt;const_oop();
1730           ciConstant constant = string-&gt;as_instance()-&gt;field_value(field);
1731           if (constant.basic_type() == T_INT) {
1732             return TypeInt::make(constant.as_int());
1733           } else if (constant.basic_type() == T_ARRAY) {
1734             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1735               return TypeNarrowOop::make_from_constant(constant.as_object(), true);
1736             } else {
1737               return TypeOopPtr::make_from_constant(constant.as_object(), true);
1738             }
1739           }
1740         }
1741       }
1742     }
1743     // Optimizations for constant objects
1744     ciObject* const_oop = tinst-&gt;const_oop();
1745     if (const_oop != NULL) {
1746       // For constant Boxed value treat the target field as a compile time constant.
1747       if (tinst-&gt;is_ptr_to_boxed_value()) {
1748         return tinst-&gt;get_const_boxed_value();
1749       } else
1750       // For constant CallSites treat the target field as a compile time constant.
1751       if (const_oop-&gt;is_call_site()) {
1752         ciCallSite* call_site = const_oop-&gt;as_call_site();
1753         ciField* field = call_site-&gt;klass()-&gt;as_instance_klass()-&gt;get_field_by_offset(off, /*is_static=*/ false);
1754         if (field != NULL &amp;&amp; field-&gt;is_call_site_target()) {
1755           ciMethodHandle* target = call_site-&gt;get_target();
1756           if (target != NULL) {  // just in case
1757             ciConstant constant(T_OBJECT, target);
1758             const Type* t;
1759             if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
1760               t = TypeNarrowOop::make_from_constant(constant.as_object(), true);
1761             } else {
1762               t = TypeOopPtr::make_from_constant(constant.as_object(), true);
1763             }
1764             // Add a dependence for invalidation of the optimization.
1765             if (!call_site-&gt;is_constant_call_site()) {
1766               C-&gt;dependencies()-&gt;assert_call_site_target_value(call_site, target);
1767             }
1768             return t;
1769           }
1770         }
1771       }
1772     }
1773   } else if (tp-&gt;base() == Type::KlassPtr) {
1774     assert( off != Type::OffsetBot ||
1775             // arrays can be cast to Objects
1776             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1777             // also allow array-loading from the primary supertype
1778             // array during subtype checks
1779             Opcode() == Op_LoadKlass,
1780             "Field accesses must be precise" );
1781     // For klass/static loads, we expect the _type to be precise
1782   }
1783 
1784   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1785   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1786     ciKlass* klass = tkls-&gt;klass();
1787     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1788       // We are loading a field from a Klass metaobject whose identity
1789       // is known at compile time (the type is "exact" or "precise").
1790       // Check for fields we know are maintained as constants by the VM.
1791       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1792         // The field is Klass::_super_check_offset.  Return its (constant) value.
1793         // (Folds up type checking code.)
1794         assert(Opcode() == Op_LoadI, "must load an int from _super_check_offset");
1795         return TypeInt::make(klass-&gt;super_check_offset());
1796       }
1797       // Compute index into primary_supers array
1798       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1799       // Check for overflowing; use unsigned compare to handle the negative case.
1800       if( depth &lt; ciKlass::primary_super_limit() ) {
1801         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1802         // (Folds up type checking code.)
1803         assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1804         ciKlass *ss = klass-&gt;super_of_depth(depth);
1805         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1806       }
1807       const Type* aift = load_array_final_field(tkls, klass);
1808       if (aift != NULL)  return aift;
1809       if (tkls-&gt;offset() == in_bytes(ArrayKlass::component_mirror_offset())
1810           &amp;&amp; klass-&gt;is_array_klass()) {
1811         // The field is ArrayKlass::_component_mirror.  Return its (constant) value.
1812         // (Folds up aClassConstant.getComponentType, common in Arrays.copyOf.)
1813         assert(Opcode() == Op_LoadP, "must load an oop from _component_mirror");
1814         return TypeInstPtr::make(klass-&gt;as_array_klass()-&gt;component_mirror());
1815       }
1816       if (tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1817         // The field is Klass::_java_mirror.  Return its (constant) value.
1818         // (Folds up the 2nd indirection in anObjConstant.getClass().)
1819         assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
1820         return TypeInstPtr::make(klass-&gt;java_mirror());
1821       }
1822     }
1823 
1824     // We can still check if we are loading from the primary_supers array at a
1825     // shallow enough depth.  Even though the klass is not exact, entries less
1826     // than or equal to its super depth are correct.
1827     if (klass-&gt;is_loaded() ) {
1828       ciType *inner = klass;
1829       while( inner-&gt;is_obj_array_klass() )
1830         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1831       if( inner-&gt;is_instance_klass() &amp;&amp;
1832           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1833         // Compute index into primary_supers array
1834         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1835         // Check for overflowing; use unsigned compare to handle the negative case.
1836         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1837             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1838           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1839           // (Folds up type checking code.)
1840           assert(Opcode() == Op_LoadKlass, "must load a klass from _primary_supers");
1841           ciKlass *ss = klass-&gt;super_of_depth(depth);
1842           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1843         }
1844       }
1845     }
1846 
1847     // If the type is enough to determine that the thing is not an array,
1848     // we can give the layout_helper a positive interval type.
1849     // This will help short-circuit some reflective code.
1850     if (tkls-&gt;offset() == in_bytes(Klass::layout_helper_offset())
1851         &amp;&amp; !klass-&gt;is_array_klass() // not directly typed as an array
1852         &amp;&amp; !klass-&gt;is_interface()  // specifically not Serializable &amp; Cloneable
1853         &amp;&amp; !klass-&gt;is_java_lang_Object()   // not the supertype of all T[]
1854         ) {
1855       // Note:  When interfaces are reliable, we can narrow the interface
1856       // test to (klass != Serializable &amp;&amp; klass != Cloneable).
1857       assert(Opcode() == Op_LoadI, "must load an int from _layout_helper");
1858       jint min_size = Klass::instance_layout_helper(oopDesc::header_size(), false);
1859       // The key property of this type is that it folds up tests
1860       // for array-ness, since it proves that the layout_helper is positive.
1861       // Thus, a generic value like the basic object layout helper works fine.
1862       return TypeInt::make(min_size, max_jint, Type::WidenMin);
1863     }
1864   }
1865 
1866   // If we are loading from a freshly-allocated object, produce a zero,
1867   // if the load is provably beyond the header of the object.
1868   // (Also allow a variable load from a fresh array to produce zero.)
1869   const TypeOopPtr *tinst = tp-&gt;isa_oopptr();
1870   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1871   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1872   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1873     Node* value = can_see_stored_value(mem,phase);
1874     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1875       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),"sanity");
1876       return value-&gt;bottom_type();
1877     }
1878   }
1879 
1880   if (is_instance) {
1881     // If we have an instance type and our memory input is the
1882     // programs's initial memory state, there is no matching store,
1883     // so just return a zero of the appropriate type
1884     Node *mem = in(MemNode::Memory);
1885     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1886       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, "must be memory Parm");
1887       return Type::get_zero_type(_type-&gt;basic_type());
1888     }
1889   }
1890   return _type;
1891 }
1892 
1893 //------------------------------match_edge-------------------------------------
1894 // Do we Match on this edge index or not?  Match only the address.
1895 uint LoadNode::match_edge(uint idx) const {
1896   return idx == MemNode::Address;
1897 }
1898 
1899 //--------------------------LoadBNode::Ideal--------------------------------------
1900 //
1901 //  If the previous store is to the same address as this load,
1902 //  and the value stored was larger than a byte, replace this load
1903 //  with the value stored truncated to a byte.  If no truncation is
1904 //  needed, the replacement is done in LoadNode::Identity().
1905 //
1906 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1907   Node* mem = in(MemNode::Memory);
1908   Node* value = can_see_stored_value(mem,phase);
1909   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
1910     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(24)) );
1911     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(24));
1912   }
1913   // Identity call will handle the case where truncation is not needed.
1914   return LoadNode::Ideal(phase, can_reshape);
1915 }
1916 
1917 const Type* LoadBNode::Value(PhaseTransform *phase) const {
1918   Node* mem = in(MemNode::Memory);
1919   Node* value = can_see_stored_value(mem,phase);
1920   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1921       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1922     // If the input to the store does not fit with the load's result type,
1923     // it must be truncated. We can't delay until Ideal call since
1924     // a singleton Value is needed for split_thru_phi optimization.
1925     int con = value-&gt;get_int();
1926     return TypeInt::make((con &lt;&lt; 24) &gt;&gt; 24);
1927   }
1928   return LoadNode::Value(phase);
1929 }
1930 
1931 //--------------------------LoadUBNode::Ideal-------------------------------------
1932 //
1933 //  If the previous store is to the same address as this load,
1934 //  and the value stored was larger than a byte, replace this load
1935 //  with the value stored truncated to a byte.  If no truncation is
1936 //  needed, the replacement is done in LoadNode::Identity().
1937 //
1938 Node* LoadUBNode::Ideal(PhaseGVN* phase, bool can_reshape) {
1939   Node* mem = in(MemNode::Memory);
1940   Node* value = can_see_stored_value(mem, phase);
1941   if (value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal(_type))
1942     return new (phase-&gt;C) AndINode(value, phase-&gt;intcon(0xFF));
1943   // Identity call will handle the case where truncation is not needed.
1944   return LoadNode::Ideal(phase, can_reshape);
1945 }
1946 
1947 const Type* LoadUBNode::Value(PhaseTransform *phase) const {
1948   Node* mem = in(MemNode::Memory);
1949   Node* value = can_see_stored_value(mem,phase);
1950   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1951       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1952     // If the input to the store does not fit with the load's result type,
1953     // it must be truncated. We can't delay until Ideal call since
1954     // a singleton Value is needed for split_thru_phi optimization.
1955     int con = value-&gt;get_int();
1956     return TypeInt::make(con &amp; 0xFF);
1957   }
1958   return LoadNode::Value(phase);
1959 }
1960 
1961 //--------------------------LoadUSNode::Ideal-------------------------------------
1962 //
1963 //  If the previous store is to the same address as this load,
1964 //  and the value stored was larger than a char, replace this load
1965 //  with the value stored truncated to a char.  If no truncation is
1966 //  needed, the replacement is done in LoadNode::Identity().
1967 //
1968 Node *LoadUSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1969   Node* mem = in(MemNode::Memory);
1970   Node* value = can_see_stored_value(mem,phase);
1971   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) )
1972     return new (phase-&gt;C) AndINode(value,phase-&gt;intcon(0xFFFF));
1973   // Identity call will handle the case where truncation is not needed.
1974   return LoadNode::Ideal(phase, can_reshape);
1975 }
1976 
1977 const Type* LoadUSNode::Value(PhaseTransform *phase) const {
1978   Node* mem = in(MemNode::Memory);
1979   Node* value = can_see_stored_value(mem,phase);
1980   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
1981       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
1982     // If the input to the store does not fit with the load's result type,
1983     // it must be truncated. We can't delay until Ideal call since
1984     // a singleton Value is needed for split_thru_phi optimization.
1985     int con = value-&gt;get_int();
1986     return TypeInt::make(con &amp; 0xFFFF);
1987   }
1988   return LoadNode::Value(phase);
1989 }
1990 
1991 //--------------------------LoadSNode::Ideal--------------------------------------
1992 //
1993 //  If the previous store is to the same address as this load,
1994 //  and the value stored was larger than a short, replace this load
1995 //  with the value stored truncated to a short.  If no truncation is
1996 //  needed, the replacement is done in LoadNode::Identity().
1997 //
1998 Node *LoadSNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1999   Node* mem = in(MemNode::Memory);
2000   Node* value = can_see_stored_value(mem,phase);
2001   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
2002     Node *result = phase-&gt;transform( new (phase-&gt;C) LShiftINode(value, phase-&gt;intcon(16)) );
2003     return new (phase-&gt;C) RShiftINode(result, phase-&gt;intcon(16));
2004   }
2005   // Identity call will handle the case where truncation is not needed.
2006   return LoadNode::Ideal(phase, can_reshape);
2007 }
2008 
2009 const Type* LoadSNode::Value(PhaseTransform *phase) const {
2010   Node* mem = in(MemNode::Memory);
2011   Node* value = can_see_stored_value(mem,phase);
2012   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2013       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2014     // If the input to the store does not fit with the load's result type,
2015     // it must be truncated. We can't delay until Ideal call since
2016     // a singleton Value is needed for split_thru_phi optimization.
2017     int con = value-&gt;get_int();
2018     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2019   }
2020   return LoadNode::Value(phase);
2021 }
2022 
2023 //=============================================================================
2024 //----------------------------LoadKlassNode::make------------------------------
2025 // Polymorphic factory method:
2026 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node *mem, Node *adr, const TypePtr* at, const TypeKlassPtr *tk) {
2027   Compile* C = gvn.C;
2028   // sanity check the alias category against the created node type
2029   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2030   assert(adr_type != NULL, "expecting TypeKlassPtr");
2031 #ifdef _LP64
2032   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2033     assert(UseCompressedClassPointers, "no compressed klasses");
2034     Node* load_klass = gvn.transform(new (C) LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2035     return new (C) DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2036   }
2037 #endif
2038   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), "should have got back a narrow oop");
2039   return new (C) LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2040 }
2041 
2042 //------------------------------Value------------------------------------------
2043 const Type *LoadKlassNode::Value( PhaseTransform *phase ) const {
2044   return klass_value_common(phase);
2045 }
2046 
2047 // In most cases, LoadKlassNode does not have the control input set. If the control
2048 // input is set, it must not be removed (by LoadNode::Ideal()).
2049 bool LoadKlassNode::can_remove_control() const {
2050   return false;
2051 }
2052 
2053 const Type *LoadNode::klass_value_common( PhaseTransform *phase ) const {
2054   // Either input is TOP ==&gt; the result is TOP
2055   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2056   if (t1 == Type::TOP)  return Type::TOP;
2057   Node *adr = in(MemNode::Address);
2058   const Type *t2 = phase-&gt;type( adr );
2059   if (t2 == Type::TOP)  return Type::TOP;
2060   const TypePtr *tp = t2-&gt;is_ptr();
2061   if (TypePtr::above_centerline(tp-&gt;ptr()) ||
2062       tp-&gt;ptr() == TypePtr::Null)  return Type::TOP;
2063 
2064   // Return a more precise klass, if possible
2065   const TypeInstPtr *tinst = tp-&gt;isa_instptr();
2066   if (tinst != NULL) {
2067     ciInstanceKlass* ik = tinst-&gt;klass()-&gt;as_instance_klass();
2068     int offset = tinst-&gt;offset();
2069     if (ik == phase-&gt;C-&gt;env()-&gt;Class_klass()
2070         &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2071             offset == java_lang_Class::array_klass_offset_in_bytes())) {
2072       // We are loading a special hidden field from a Class mirror object,
2073       // the field which points to the VM's Klass metaobject.
2074       ciType* t = tinst-&gt;java_mirror_type();
2075       // java_mirror_type returns non-null for compile-time Class constants.
2076       if (t != NULL) {
2077         // constant oop =&gt; constant klass
2078         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2079           if (t-&gt;is_void()) {
2080             // We cannot create a void array.  Since void is a primitive type return null
2081             // klass.  Users of this result need to do a null check on the returned klass.
2082             return TypePtr::NULL_PTR;
2083           }
2084           return TypeKlassPtr::make(ciArrayKlass::make(t));
2085         }
2086         if (!t-&gt;is_klass()) {
2087           // a primitive Class (e.g., int.class) has NULL for a klass field
2088           return TypePtr::NULL_PTR;
2089         }
2090         // (Folds up the 1st indirection in aClassConstant.getModifiers().)
2091         return TypeKlassPtr::make(t-&gt;as_klass());
2092       }
2093       // non-constant mirror, so we can't tell what's going on
2094     }
2095     if( !ik-&gt;is_loaded() )
2096       return _type;             // Bail out if not loaded
2097     if (offset == oopDesc::klass_offset_in_bytes()) {
2098       if (tinst-&gt;klass_is_exact()) {
2099         return TypeKlassPtr::make(ik);
2100       }
2101       // See if we can become precise: no subklasses and no interface
2102       // (Note:  We need to support verified interfaces.)
2103       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2104         //assert(!UseExactTypes, "this code should be useless with exact types");
2105         // Add a dependence; if any subclass added we need to recompile
2106         if (!ik-&gt;is_final()) {
2107           // %%% should use stronger assert_unique_concrete_subtype instead
2108           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2109         }
2110         // Return precise klass
2111         return TypeKlassPtr::make(ik);
2112       }
2113 
2114       // Return root of possible klass
2115       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);
2116     }
2117   }
2118 
2119   // Check for loading klass from an array
2120   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
2121   if( tary != NULL ) {
2122     ciKlass *tary_klass = tary-&gt;klass();
2123     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2124         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2125       if (tary-&gt;klass_is_exact()) {
2126         return TypeKlassPtr::make(tary_klass);
2127       }
2128       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();
2129       // If the klass is an object array, we defer the question to the
2130       // array component klass.
2131       if( ak-&gt;is_obj_array_klass() ) {
2132         assert( ak-&gt;is_loaded(), "" );
2133         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
2134         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {
2135           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();
2136           // See if we can become precise: no subklasses and no interface
2137           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2138             //assert(!UseExactTypes, "this code should be useless with exact types");
2139             // Add a dependence; if any subclass added we need to recompile
2140             if (!ik-&gt;is_final()) {
2141               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2142             }
2143             // Return precise array klass
2144             return TypeKlassPtr::make(ak);
2145           }
2146         }
2147         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
2148       } else {                  // Found a type-array?
2149         //assert(!UseExactTypes, "this code should be useless with exact types");
2150         assert( ak-&gt;is_type_array_klass(), "" );
2151         return TypeKlassPtr::make(ak); // These are always precise
2152       }
2153     }
2154   }
2155 
2156   // Check for loading klass from an array klass
2157   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2158   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2159     ciKlass* klass = tkls-&gt;klass();
2160     if( !klass-&gt;is_loaded() )
2161       return _type;             // Bail out if not loaded
2162     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2163         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2164       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2165       // // Always returning precise element type is incorrect,
2166       // // e.g., element type could be object and array may contain strings
2167       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2168 
2169       // The array's TypeKlassPtr was declared 'precise' or 'not precise'
2170       // according to the element type's subclassing.
2171       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);
2172     }
2173     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2174         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2175       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2176       // The field is Klass::_super.  Return its (constant) value.
2177       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2178       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2179     }
2180   }
2181 
2182   // Bailout case
2183   return LoadNode::Value(phase);
2184 }
2185 
2186 //------------------------------Identity---------------------------------------
2187 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2188 // Also feed through the klass in Allocate(...klass...)._klass.
2189 Node* LoadKlassNode::Identity( PhaseTransform *phase ) {
2190   return klass_identity_common(phase);
2191 }
2192 
2193 Node* LoadNode::klass_identity_common(PhaseTransform *phase ) {
2194   Node* x = LoadNode::Identity(phase);
2195   if (x != this)  return x;
2196 
2197   // Take apart the address into an oop and and offset.
2198   // Return 'this' if we cannot.
2199   Node*    adr    = in(MemNode::Address);
2200   intptr_t offset = 0;
2201   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2202   if (base == NULL)     return this;
2203   const TypeOopPtr* toop = phase-&gt;type(adr)-&gt;isa_oopptr();
2204   if (toop == NULL)     return this;
2205 
2206   // We can fetch the klass directly through an AllocateNode.
2207   // This works even if the klass is not constant (clone or newArray).
2208   if (offset == oopDesc::klass_offset_in_bytes()) {
2209     Node* allocated_klass = AllocateNode::Ideal_klass(base, phase);
2210     if (allocated_klass != NULL) {
2211       return allocated_klass;
2212     }
2213   }
2214 
2215   // Simplify k.java_mirror.as_klass to plain k, where k is a Klass*.
2216   // Simplify ak.component_mirror.array_klass to plain ak, ak an ArrayKlass.
2217   // See inline_native_Class_query for occurrences of these patterns.
2218   // Java Example:  x.getClass().isAssignableFrom(y)
2219   // Java Example:  Array.newInstance(x.getClass().getComponentType(), n)
2220   //
2221   // This improves reflective code, often making the Class
2222   // mirror go completely dead.  (Current exception:  Class
2223   // mirrors may appear in debug info, but we could clean them out by
2224   // introducing a new debug info operator for Klass*.java_mirror).
2225   if (toop-&gt;isa_instptr() &amp;&amp; toop-&gt;klass() == phase-&gt;C-&gt;env()-&gt;Class_klass()
2226       &amp;&amp; (offset == java_lang_Class::klass_offset_in_bytes() ||
2227           offset == java_lang_Class::array_klass_offset_in_bytes())) {
2228     // We are loading a special hidden field from a Class mirror,
2229     // the field which points to its Klass or ArrayKlass metaobject.
2230     if (base-&gt;is_Load()) {
2231       Node* adr2 = base-&gt;in(MemNode::Address);
2232       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
2233       if (tkls != NULL &amp;&amp; !tkls-&gt;empty()
2234           &amp;&amp; (tkls-&gt;klass()-&gt;is_instance_klass() ||
2235               tkls-&gt;klass()-&gt;is_array_klass())
2236           &amp;&amp; adr2-&gt;is_AddP()
2237           ) {
2238         int mirror_field = in_bytes(Klass::java_mirror_offset());
2239         if (offset == java_lang_Class::array_klass_offset_in_bytes()) {
2240           mirror_field = in_bytes(ArrayKlass::component_mirror_offset());
2241         }
2242         if (tkls-&gt;offset() == mirror_field) {
2243           return adr2-&gt;in(AddPNode::Base);
2244         }
2245       }
2246     }
2247   }
2248 
2249   return this;
2250 }
2251 
2252 
2253 //------------------------------Value------------------------------------------
2254 const Type *LoadNKlassNode::Value( PhaseTransform *phase ) const {
2255   const Type *t = klass_value_common(phase);
2256   if (t == Type::TOP)
2257     return t;
2258 
2259   return t-&gt;make_narrowklass();
2260 }
2261 
2262 //------------------------------Identity---------------------------------------
2263 // To clean up reflective code, simplify k.java_mirror.as_klass to narrow k.
2264 // Also feed through the klass in Allocate(...klass...)._klass.
2265 Node* LoadNKlassNode::Identity( PhaseTransform *phase ) {
2266   Node *x = klass_identity_common(phase);
2267 
2268   const Type *t = phase-&gt;type( x );
2269   if( t == Type::TOP ) return x;
2270   if( t-&gt;isa_narrowklass()) return x;
2271   assert (!t-&gt;isa_narrowoop(), "no narrow oop here");
2272 
2273   return phase-&gt;transform(new (phase-&gt;C) EncodePKlassNode(x, t-&gt;make_narrowklass()));
2274 }
2275 
2276 //------------------------------Value-----------------------------------------
2277 const Type *LoadRangeNode::Value( PhaseTransform *phase ) const {
2278   // Either input is TOP ==&gt; the result is TOP
2279   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2280   if( t1 == Type::TOP ) return Type::TOP;
2281   Node *adr = in(MemNode::Address);
2282   const Type *t2 = phase-&gt;type( adr );
2283   if( t2 == Type::TOP ) return Type::TOP;
2284   const TypePtr *tp = t2-&gt;is_ptr();
2285   if (TypePtr::above_centerline(tp-&gt;ptr()))  return Type::TOP;
2286   const TypeAryPtr *tap = tp-&gt;isa_aryptr();
2287   if( !tap ) return _type;
2288   return tap-&gt;size();
2289 }
2290 
2291 //-------------------------------Ideal---------------------------------------
2292 // Feed through the length in AllocateArray(...length...)._length.
2293 Node *LoadRangeNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2294   Node* p = MemNode::Ideal_common(phase, can_reshape);
2295   if (p)  return (p == NodeSentinel) ? NULL : p;
2296 
2297   // Take apart the address into an oop and and offset.
2298   // Return 'this' if we cannot.
2299   Node*    adr    = in(MemNode::Address);
2300   intptr_t offset = 0;
2301   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase,  offset);
2302   if (base == NULL)     return NULL;
2303   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2304   if (tary == NULL)     return NULL;
2305 
2306   // We can fetch the length directly through an AllocateArrayNode.
2307   // This works even if the length is not constant (clone or newArray).
2308   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2309     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2310     if (alloc != NULL) {
2311       Node* allocated_length = alloc-&gt;Ideal_length();
2312       Node* len = alloc-&gt;make_ideal_length(tary, phase);
2313       if (allocated_length != len) {
2314         // New CastII improves on this.
2315         return len;
2316       }
2317     }
2318   }
2319 
2320   return NULL;
2321 }
2322 
2323 //------------------------------Identity---------------------------------------
2324 // Feed through the length in AllocateArray(...length...)._length.
2325 Node* LoadRangeNode::Identity( PhaseTransform *phase ) {
2326   Node* x = LoadINode::Identity(phase);
2327   if (x != this)  return x;
2328 
2329   // Take apart the address into an oop and and offset.
2330   // Return 'this' if we cannot.
2331   Node*    adr    = in(MemNode::Address);
2332   intptr_t offset = 0;
2333   Node*    base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
2334   if (base == NULL)     return this;
2335   const TypeAryPtr* tary = phase-&gt;type(adr)-&gt;isa_aryptr();
2336   if (tary == NULL)     return this;
2337 
2338   // We can fetch the length directly through an AllocateArrayNode.
2339   // This works even if the length is not constant (clone or newArray).
2340   if (offset == arrayOopDesc::length_offset_in_bytes()) {
2341     AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(base, phase);
2342     if (alloc != NULL) {
2343       Node* allocated_length = alloc-&gt;Ideal_length();
2344       // Do not allow make_ideal_length to allocate a CastII node.
2345       Node* len = alloc-&gt;make_ideal_length(tary, phase, false);
2346       if (allocated_length == len) {
2347         // Return allocated_length only if it would not be improved by a CastII.
2348         return allocated_length;
2349       }
2350     }
2351   }
2352 
2353   return this;
2354 
2355 }
2356 
2357 //=============================================================================
2358 //---------------------------StoreNode::make-----------------------------------
2359 // Polymorphic factory method:
2360 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2361   assert((mo == unordered || mo == release), "unexpected");
2362   Compile* C = gvn.C;
2363   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2364          ctl != NULL, "raw memory operations should have control edge");
2365 
2366   switch (bt) {
2367   case T_BOOLEAN:
2368   case T_BYTE:    return new (C) StoreBNode(ctl, mem, adr, adr_type, val, mo);
2369   case T_INT:     return new (C) StoreINode(ctl, mem, adr, adr_type, val, mo);
2370   case T_CHAR:
2371   case T_SHORT:   return new (C) StoreCNode(ctl, mem, adr, adr_type, val, mo);
2372   case T_LONG:    return new (C) StoreLNode(ctl, mem, adr, adr_type, val, mo);
2373   case T_FLOAT:   return new (C) StoreFNode(ctl, mem, adr, adr_type, val, mo);
2374   case T_DOUBLE:  return new (C) StoreDNode(ctl, mem, adr, adr_type, val, mo);
2375   case T_METADATA:
2376   case T_ADDRESS:
2377   case T_OBJECT:
2378 #ifdef _LP64
2379     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2380       val = gvn.transform(new (C) EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2381       return new (C) StoreNNode(ctl, mem, adr, adr_type, val, mo);
2382     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2383                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2384                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2385       val = gvn.transform(new (C) EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2386       return new (C) StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2387     }
2388 #endif
2389     {
2390       return new (C) StorePNode(ctl, mem, adr, adr_type, val, mo);
2391     }
2392   }
2393   ShouldNotReachHere();
2394   return (StoreNode*)NULL;
2395 }
2396 
2397 StoreLNode* StoreLNode::make_atomic(Compile *C, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {
2398   bool require_atomic = true;
2399   return new (C) StoreLNode(ctl, mem, adr, adr_type, val, mo, require_atomic);
2400 }
2401 
2402 
2403 //--------------------------bottom_type----------------------------------------
2404 const Type *StoreNode::bottom_type() const {
2405   return Type::MEMORY;
2406 }
2407 
2408 //------------------------------hash-------------------------------------------
2409 uint StoreNode::hash() const {
2410   // unroll addition of interesting fields
2411   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2412 
2413   // Since they are not commoned, do not hash them:
2414   return NO_HASH;
2415 }
2416 
2417 //------------------------------Ideal------------------------------------------
2418 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2419 // When a store immediately follows a relevant allocation/initialization,
2420 // try to capture it into the initialization, or hoist it above.
2421 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2422   Node* p = MemNode::Ideal_common(phase, can_reshape);
2423   if (p)  return (p == NodeSentinel) ? NULL : p;
2424 
2425   Node* mem     = in(MemNode::Memory);
2426   Node* address = in(MemNode::Address);
2427 
2428   // Back-to-back stores to same address?  Fold em up.  Generally
2429   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2430   // since they must follow each StoreP operation.  Redundant StoreCMs
2431   // are eliminated just before matching in final_graph_reshape.
2432   if (mem-&gt;is_Store() &amp;&amp; mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2433       mem-&gt;Opcode() != Op_StoreCM) {
2434     // Looking at a dead closed cycle of memory?
2435     assert(mem != mem-&gt;in(MemNode::Memory), "dead loop in StoreNode::Ideal");
2436 
2437     assert(Opcode() == mem-&gt;Opcode() ||
2438            phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw,
2439            "no mismatched stores, except on raw memory");
2440 
2441     if (mem-&gt;outcnt() == 1 &amp;&amp;           // check for intervening uses
2442         mem-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2443       // If anybody other than 'this' uses 'mem', we cannot fold 'mem' away.
2444       // For example, 'mem' might be the final state at a conditional return.
2445       // Or, 'mem' might be used by some node which is live at the same time
2446       // 'this' is live, which might be unschedulable.  So, require exactly
2447       // ONE user, the 'this' store, until such time as we clone 'mem' for
2448       // each of 'mem's uses (thus making the exactly-1-user-rule hold true).
2449       if (can_reshape) {  // (%%% is this an anachronism?)
2450         set_req_X(MemNode::Memory, mem-&gt;in(MemNode::Memory),
2451                   phase-&gt;is_IterGVN());
2452       } else {
2453         // It's OK to do this in the parser, since DU info is always accurate,
2454         // and the parser always refers to nodes via SafePointNode maps.
2455         set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2456       }
2457       return this;
2458     }
2459   }
2460 
2461   // Capture an unaliased, unconditional, simple store into an initializer.
2462   // Or, if it is independent of the allocation, hoist it above the allocation.
2463   if (ReduceFieldZeroing &amp;&amp; /*can_reshape &amp;&amp;*/
2464       mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Initialize()) {
2465     InitializeNode* init = mem-&gt;in(0)-&gt;as_Initialize();
2466     intptr_t offset = init-&gt;can_capture_store(this, phase, can_reshape);
2467     if (offset &gt; 0) {
2468       Node* moved = init-&gt;capture_store(this, offset, phase, can_reshape);
2469       // If the InitializeNode captured me, it made a raw copy of me,
2470       // and I need to disappear.
2471       if (moved != NULL) {
2472         // %%% hack to ensure that Ideal returns a new node:
2473         mem = MergeMemNode::make(phase-&gt;C, mem);
2474         return mem;             // fold me away
2475       }
2476     }
2477   }
2478 
2479   return NULL;                  // No further progress
2480 }
2481 
2482 //------------------------------Value-----------------------------------------
2483 const Type *StoreNode::Value( PhaseTransform *phase ) const {
2484   // Either input is TOP ==&gt; the result is TOP
2485   const Type *t1 = phase-&gt;type( in(MemNode::Memory) );
2486   if( t1 == Type::TOP ) return Type::TOP;
2487   const Type *t2 = phase-&gt;type( in(MemNode::Address) );
2488   if( t2 == Type::TOP ) return Type::TOP;
2489   const Type *t3 = phase-&gt;type( in(MemNode::ValueIn) );
2490   if( t3 == Type::TOP ) return Type::TOP;
2491   return Type::MEMORY;
2492 }
2493 
2494 //------------------------------Identity---------------------------------------
2495 // Remove redundant stores:
2496 //   Store(m, p, Load(m, p)) changes to m.
2497 //   Store(, p, x) -&gt; Store(m, p, x) changes to Store(m, p, x).
2498 Node *StoreNode::Identity( PhaseTransform *phase ) {
2499   Node* mem = in(MemNode::Memory);
2500   Node* adr = in(MemNode::Address);
2501   Node* val = in(MemNode::ValueIn);
2502 
2503   // Load then Store?  Then the Store is useless
2504   if (val-&gt;is_Load() &amp;&amp;
2505       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2506       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2507       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2508     return mem;
2509   }
2510 
2511   // Two stores in a row of the same value?
2512   if (mem-&gt;is_Store() &amp;&amp;
2513       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2514       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2515       mem-&gt;Opcode() == Opcode()) {
2516     return mem;
2517   }
2518 
2519   // Store of zero anywhere into a freshly-allocated object?
2520   // Then the store is useless.
2521   // (It must already have been captured by the InitializeNode.)
2522   if (ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {
2523     // a newly allocated object is already all-zeroes everywhere
2524     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {
2525       return mem;
2526     }
2527 
2528     // the store may also apply to zero-bits in an earlier object
2529     Node* prev_mem = find_previous_store(phase);
2530     // Steps (a), (b):  Walk past independent stores to find an exact match.
2531     if (prev_mem != NULL) {
2532       Node* prev_val = can_see_stored_value(prev_mem, phase);
2533       if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2534         // prev_val and val might differ by a cast; it would be good
2535         // to keep the more informative of the two.
2536         return mem;
2537       }
2538     }
2539   }
2540 
2541   return this;
2542 }
2543 
2544 //------------------------------match_edge-------------------------------------
2545 // Do we Match on this edge index or not?  Match only memory &amp; value
2546 uint StoreNode::match_edge(uint idx) const {
2547   return idx == MemNode::Address || idx == MemNode::ValueIn;
2548 }
2549 
2550 //------------------------------cmp--------------------------------------------
2551 // Do not common stores up together.  They generally have to be split
2552 // back up anyways, so do not bother.
2553 uint StoreNode::cmp( const Node &amp;n ) const {
2554   return (&amp;n == this);          // Always fail except on self
2555 }
2556 
2557 //------------------------------Ideal_masked_input-----------------------------
2558 // Check for a useless mask before a partial-word store
2559 // (StoreB ... (AndI valIn conIa) )
2560 // If (conIa &amp; mask == mask) this simplifies to
2561 // (StoreB ... (valIn) )
2562 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2563   Node *val = in(MemNode::ValueIn);
2564   if( val-&gt;Opcode() == Op_AndI ) {
2565     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2566     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2567       set_req(MemNode::ValueIn, val-&gt;in(1));
2568       return this;
2569     }
2570   }
2571   return NULL;
2572 }
2573 
2574 
2575 //------------------------------Ideal_sign_extended_input----------------------
2576 // Check for useless sign-extension before a partial-word store
2577 // (StoreB ... (RShiftI _ (LShiftI _ valIn conIL ) conIR) )
2578 // If (conIL == conIR &amp;&amp; conIR &lt;= num_bits)  this simplifies to
2579 // (StoreB ... (valIn) )
2580 Node *StoreNode::Ideal_sign_extended_input(PhaseGVN *phase, int num_bits) {
2581   Node *val = in(MemNode::ValueIn);
2582   if( val-&gt;Opcode() == Op_RShiftI ) {
2583     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2584     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &lt;= num_bits) ) {
2585       Node *shl = val-&gt;in(1);
2586       if( shl-&gt;Opcode() == Op_LShiftI ) {
2587         const TypeInt *t2 = phase-&gt;type( shl-&gt;in(2) )-&gt;isa_int();
2588         if( t2 &amp;&amp; t2-&gt;is_con() &amp;&amp; (t2-&gt;get_con() == t-&gt;get_con()) ) {
2589           set_req(MemNode::ValueIn, shl-&gt;in(1));
2590           return this;
2591         }
2592       }
2593     }
2594   }
2595   return NULL;
2596 }
2597 
2598 //------------------------------value_never_loaded-----------------------------------
2599 // Determine whether there are any possible loads of the value stored.
2600 // For simplicity, we actually check if there are any loads from the
2601 // address stored to, not just for loads of the value stored by this node.
2602 //
2603 bool StoreNode::value_never_loaded( PhaseTransform *phase) const {
2604   Node *adr = in(Address);
2605   const TypeOopPtr *adr_oop = phase-&gt;type(adr)-&gt;isa_oopptr();
2606   if (adr_oop == NULL)
2607     return false;
2608   if (!adr_oop-&gt;is_known_instance_field())
2609     return false; // if not a distinct instance, there may be aliases of the address
2610   for (DUIterator_Fast imax, i = adr-&gt;fast_outs(imax); i &lt; imax; i++) {
2611     Node *use = adr-&gt;fast_out(i);
2612     int opc = use-&gt;Opcode();
2613     if (use-&gt;is_Load() || use-&gt;is_LoadStore()) {
2614       return false;
2615     }
2616   }
2617   return true;
2618 }
2619 
2620 //=============================================================================
2621 //------------------------------Ideal------------------------------------------
2622 // If the store is from an AND mask that leaves the low bits untouched, then
2623 // we can skip the AND operation.  If the store is from a sign-extension
2624 // (a left shift, then right shift) we can skip both.
2625 Node *StoreBNode::Ideal(PhaseGVN *phase, bool can_reshape){
2626   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFF);
2627   if( progress != NULL ) return progress;
2628 
2629   progress = StoreNode::Ideal_sign_extended_input(phase, 24);
2630   if( progress != NULL ) return progress;
2631 
2632   // Finally check the default case
2633   return StoreNode::Ideal(phase, can_reshape);
2634 }
2635 
2636 //=============================================================================
2637 //------------------------------Ideal------------------------------------------
2638 // If the store is from an AND mask that leaves the low bits untouched, then
2639 // we can skip the AND operation
2640 Node *StoreCNode::Ideal(PhaseGVN *phase, bool can_reshape){
2641   Node *progress = StoreNode::Ideal_masked_input(phase, 0xFFFF);
2642   if( progress != NULL ) return progress;
2643 
2644   progress = StoreNode::Ideal_sign_extended_input(phase, 16);
2645   if( progress != NULL ) return progress;
2646 
2647   // Finally check the default case
2648   return StoreNode::Ideal(phase, can_reshape);
2649 }
2650 
2651 //=============================================================================
2652 //------------------------------Identity---------------------------------------
2653 Node *StoreCMNode::Identity( PhaseTransform *phase ) {
2654   // No need to card mark when storing a null ptr
2655   Node* my_store = in(MemNode::OopStore);
2656   if (my_store-&gt;is_Store()) {
2657     const Type *t1 = phase-&gt;type( my_store-&gt;in(MemNode::ValueIn) );
2658     if( t1 == TypePtr::NULL_PTR ) {
2659       return in(MemNode::Memory);
2660     }
2661   }
2662   return this;
2663 }
2664 
2665 //=============================================================================
2666 //------------------------------Ideal---------------------------------------
2667 Node *StoreCMNode::Ideal(PhaseGVN *phase, bool can_reshape){
2668   Node* progress = StoreNode::Ideal(phase, can_reshape);
2669   if (progress != NULL) return progress;
2670 
2671   Node* my_store = in(MemNode::OopStore);
2672   if (my_store-&gt;is_MergeMem()) {
2673     Node* mem = my_store-&gt;as_MergeMem()-&gt;memory_at(oop_alias_idx());
2674     set_req(MemNode::OopStore, mem);
2675     return this;
2676   }
2677 
2678   return NULL;
2679 }
2680 
2681 //------------------------------Value-----------------------------------------
2682 const Type *StoreCMNode::Value( PhaseTransform *phase ) const {
2683   // Either input is TOP ==&gt; the result is TOP
2684   const Type *t = phase-&gt;type( in(MemNode::Memory) );
2685   if( t == Type::TOP ) return Type::TOP;
2686   t = phase-&gt;type( in(MemNode::Address) );
2687   if( t == Type::TOP ) return Type::TOP;
2688   t = phase-&gt;type( in(MemNode::ValueIn) );
2689   if( t == Type::TOP ) return Type::TOP;
2690   // If extra input is TOP ==&gt; the result is TOP
2691   t = phase-&gt;type( in(MemNode::OopStore) );
2692   if( t == Type::TOP ) return Type::TOP;
2693 
2694   return StoreNode::Value( phase );
2695 }
2696 
2697 
2698 //=============================================================================
2699 //----------------------------------SCMemProjNode------------------------------
2700 const Type * SCMemProjNode::Value( PhaseTransform *phase ) const
2701 {
2702   return bottom_type();
2703 }
2704 
2705 //=============================================================================
2706 //----------------------------------LoadStoreNode------------------------------
2707 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2708   : Node(required),
2709     _type(rt),
2710     _adr_type(at)
2711 {
2712   init_req(MemNode::Control, c  );
2713   init_req(MemNode::Memory , mem);
2714   init_req(MemNode::Address, adr);
2715   init_req(MemNode::ValueIn, val);
2716   init_class_id(Class_LoadStore);
2717 }
2718 
2719 uint LoadStoreNode::ideal_reg() const {
2720   return _type-&gt;ideal_reg();
2721 }
2722 
2723 bool LoadStoreNode::result_not_used() const {
2724   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2725     Node *x = fast_out(i);
2726     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2727     return false;
2728   }
2729   return true;
2730 }
2731 
2732 uint LoadStoreNode::size_of() const { return sizeof(*this); }
2733 
2734 //=============================================================================
2735 //----------------------------------LoadStoreConditionalNode--------------------
2736 LoadStoreConditionalNode::LoadStoreConditionalNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex ) : LoadStoreNode(c, mem, adr, val, NULL, TypeInt::BOOL, 5) {
2737   init_req(ExpectedIn, ex );
2738 }
2739 
2740 //=============================================================================
2741 //-------------------------------adr_type--------------------------------------
2742 // Do we Match on this edge index or not?  Do not match memory
2743 const TypePtr* ClearArrayNode::adr_type() const {
2744   Node *adr = in(3);
2745   return MemNode::calculate_adr_type(adr-&gt;bottom_type());
2746 }
2747 
2748 //------------------------------match_edge-------------------------------------
2749 // Do we Match on this edge index or not?  Do not match memory
2750 uint ClearArrayNode::match_edge(uint idx) const {
2751   return idx &gt; 1;
2752 }
2753 
2754 //------------------------------Identity---------------------------------------
2755 // Clearing a zero length array does nothing
2756 Node *ClearArrayNode::Identity( PhaseTransform *phase ) {
2757   return phase-&gt;type(in(2))-&gt;higher_equal(TypeX::ZERO)  ? in(1) : this;
2758 }
2759 
2760 //------------------------------Idealize---------------------------------------
2761 // Clearing a short array is faster with stores
2762 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape){
2763   const int unit = BytesPerLong;
2764   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2765   if (!t)  return NULL;
2766   if (!t-&gt;is_con())  return NULL;
2767   intptr_t raw_count = t-&gt;get_con();
2768   intptr_t size = raw_count;
2769   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2770   // Clearing nothing uses the Identity call.
2771   // Negative clears are possible on dead ClearArrays
2772   // (see jck test stmt114.stmt11402.val).
2773   if (size &lt;= 0 || size % unit != 0)  return NULL;
2774   intptr_t count = size / unit;
2775   // Length too long; use fast hardware clear
2776   if (size &gt; Matcher::init_array_short_size)  return NULL;
2777   Node *mem = in(1);
2778   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2779   Node *adr = in(3);
2780   const Type* at = phase-&gt;type(adr);
2781   if( at==Type::TOP ) return NULL;
2782   const TypePtr* atp = at-&gt;isa_ptr();
2783   // adjust atp to be the correct array element address type
2784   if (atp == NULL)  atp = TypePtr::BOTTOM;
2785   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2786   // Get base for derived pointer purposes
2787   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2788   Node *base = adr-&gt;in(1);
2789 
2790   Node *zero = phase-&gt;makecon(TypeLong::ZERO);
2791   Node *off  = phase-&gt;MakeConX(BytesPerLong);
2792   mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2793   count--;
2794   while( count-- ) {
2795     mem = phase-&gt;transform(mem);
2796     adr = phase-&gt;transform(new (phase-&gt;C) AddPNode(base,adr,off));
2797     mem = new (phase-&gt;C) StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);
2798   }
2799   return mem;
2800 }
2801 
2802 //----------------------------step_through----------------------------------
2803 // Return allocation input memory edge if it is different instance
2804 // or itself if it is the one we are looking for.
2805 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2806   Node* n = *np;
2807   assert(n-&gt;is_ClearArray(), "sanity");
2808   intptr_t offset;
2809   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2810   // This method is called only before Allocate nodes are expanded during
2811   // macro nodes expansion. Before that ClearArray nodes are only generated
2812   // in LibraryCallKit::generate_arraycopy() which follows allocations.
2813   assert(alloc != NULL, "should have allocation");
2814   if (alloc-&gt;_idx == instance_id) {
2815     // Can not bypass initialization of the instance we are looking for.
2816     return false;
2817   }
2818   // Otherwise skip it.
2819   InitializeNode* init = alloc-&gt;initialization();
2820   if (init != NULL)
2821     *np = init-&gt;in(TypeFunc::Memory);
2822   else
2823     *np = alloc-&gt;in(TypeFunc::Memory);
2824   return true;
2825 }
2826 
2827 //----------------------------clear_memory-------------------------------------
2828 // Generate code to initialize object storage to zero.
2829 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2830                                    intptr_t start_offset,
2831                                    Node* end_offset,
2832                                    PhaseGVN* phase) {
2833   Compile* C = phase-&gt;C;
2834   intptr_t offset = start_offset;
2835 
2836   int unit = BytesPerLong;
2837   if ((offset % unit) != 0) {
2838     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(offset));
2839     adr = phase-&gt;transform(adr);
2840     const TypePtr* atp = TypeRawPtr::BOTTOM;
2841     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2842     mem = phase-&gt;transform(mem);
2843     offset += BytesPerInt;
2844   }
2845   assert((offset % unit) == 0, "");
2846 
2847   // Initialize the remaining stuff, if any, with a ClearArray.
2848   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);
2849 }
2850 
2851 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2852                                    Node* start_offset,
2853                                    Node* end_offset,
2854                                    PhaseGVN* phase) {
2855   if (start_offset == end_offset) {
2856     // nothing to do
2857     return mem;
2858   }
2859 
2860   Compile* C = phase-&gt;C;
2861   int unit = BytesPerLong;
2862   Node* zbase = start_offset;
2863   Node* zend  = end_offset;
2864 
2865   // Scale to the unit required by the CPU:
2866   if (!Matcher::init_array_count_is_in_bytes) {
2867     Node* shift = phase-&gt;intcon(exact_log2(unit));
2868     zbase = phase-&gt;transform( new(C) URShiftXNode(zbase, shift) );
2869     zend  = phase-&gt;transform( new(C) URShiftXNode(zend,  shift) );
2870   }
2871 
2872   // Bulk clear double-words
2873   Node* zsize = phase-&gt;transform( new(C) SubXNode(zend, zbase) );
2874   Node* adr = phase-&gt;transform( new(C) AddPNode(dest, dest, start_offset) );
2875   mem = new (C) ClearArrayNode(ctl, mem, zsize, adr);
2876   return phase-&gt;transform(mem);
2877 }
2878 
2879 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
2880                                    intptr_t start_offset,
2881                                    intptr_t end_offset,
2882                                    PhaseGVN* phase) {
2883   if (start_offset == end_offset) {
2884     // nothing to do
2885     return mem;
2886   }
2887 
2888   Compile* C = phase-&gt;C;
2889   assert((end_offset % BytesPerInt) == 0, "odd end offset");
2890   intptr_t done_offset = end_offset;
2891   if ((done_offset % BytesPerLong) != 0) {
2892     done_offset -= BytesPerInt;
2893   }
2894   if (done_offset &gt; start_offset) {
2895     mem = clear_memory(ctl, mem, dest,
2896                        start_offset, phase-&gt;MakeConX(done_offset), phase);
2897   }
2898   if (done_offset &lt; end_offset) { // emit the final 32-bit store
2899     Node* adr = new (C) AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
2900     adr = phase-&gt;transform(adr);
2901     const TypePtr* atp = TypeRawPtr::BOTTOM;
2902     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);
2903     mem = phase-&gt;transform(mem);
2904     done_offset += BytesPerInt;
2905   }
2906   assert(done_offset == end_offset, "");
2907   return mem;
2908 }
2909 
2910 //=============================================================================
2911 // Do not match memory edge.
2912 uint StrIntrinsicNode::match_edge(uint idx) const {
2913   return idx == 2 || idx == 3;
2914 }
2915 
2916 //------------------------------Ideal------------------------------------------
2917 // Return a node which is more "ideal" than the current node.  Strip out
2918 // control copies
2919 Node *StrIntrinsicNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2920   if (remove_dead_region(phase, can_reshape)) return this;
2921   // Don't bother trying to transform a dead node
2922   if (in(0) &amp;&amp; in(0)-&gt;is_top())  return NULL;
2923 
2924   if (can_reshape) {
2925     Node* mem = phase-&gt;transform(in(MemNode::Memory));
2926     // If transformed to a MergeMem, get the desired slice
2927     uint alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
2928     mem = mem-&gt;is_MergeMem() ? mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx) : mem;
2929     if (mem != in(MemNode::Memory)) {
2930       set_req(MemNode::Memory, mem);
2931       return this;
2932     }
2933   }
2934   return NULL;
2935 }
2936 
2937 //------------------------------Value------------------------------------------
2938 const Type *StrIntrinsicNode::Value( PhaseTransform *phase ) const {
2939   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2940   return bottom_type();
2941 }
2942 
2943 //=============================================================================
2944 //------------------------------match_edge-------------------------------------
2945 // Do not match memory edge
2946 uint EncodeISOArrayNode::match_edge(uint idx) const {
2947   return idx == 2 || idx == 3; // EncodeISOArray src (Binary dst len)
2948 }
2949 
2950 //------------------------------Ideal------------------------------------------
2951 // Return a node which is more "ideal" than the current node.  Strip out
2952 // control copies
2953 Node *EncodeISOArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2954   return remove_dead_region(phase, can_reshape) ? this : NULL;
2955 }
2956 
2957 //------------------------------Value------------------------------------------
2958 const Type *EncodeISOArrayNode::Value(PhaseTransform *phase) const {
2959   if (in(0) &amp;&amp; phase-&gt;type(in(0)) == Type::TOP) return Type::TOP;
2960   return bottom_type();
2961 }
2962 
2963 //=============================================================================
2964 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
2965   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
2966     _adr_type(C-&gt;get_adr_type(alias_idx))
2967 {
2968   init_class_id(Class_MemBar);
2969   Node* top = C-&gt;top();
2970   init_req(TypeFunc::I_O,top);
2971   init_req(TypeFunc::FramePtr,top);
2972   init_req(TypeFunc::ReturnAdr,top);
2973   if (precedent != NULL)
2974     init_req(TypeFunc::Parms, precedent);
2975 }
2976 
2977 //------------------------------cmp--------------------------------------------
2978 uint MemBarNode::hash() const { return NO_HASH; }
2979 uint MemBarNode::cmp( const Node &amp;n ) const {
2980   return (&amp;n == this);          // Always fail except on self
2981 }
2982 
2983 //------------------------------make-------------------------------------------
2984 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
2985   switch (opcode) {
2986   case Op_MemBarAcquire:     return new(C) MemBarAcquireNode(C, atp, pn);
2987   case Op_LoadFence:         return new(C) LoadFenceNode(C, atp, pn);
2988   case Op_MemBarRelease:     return new(C) MemBarReleaseNode(C, atp, pn);
2989   case Op_StoreFence:        return new(C) StoreFenceNode(C, atp, pn);
2990   case Op_MemBarAcquireLock: return new(C) MemBarAcquireLockNode(C, atp, pn);
2991   case Op_MemBarReleaseLock: return new(C) MemBarReleaseLockNode(C, atp, pn);
2992   case Op_MemBarVolatile:    return new(C) MemBarVolatileNode(C, atp, pn);
2993   case Op_MemBarCPUOrder:    return new(C) MemBarCPUOrderNode(C, atp, pn);
2994   case Op_Initialize:        return new(C) InitializeNode(C, atp, pn);
2995   case Op_MemBarStoreStore:  return new(C) MemBarStoreStoreNode(C, atp, pn);
2996   default: ShouldNotReachHere(); return NULL;
2997   }
2998 }
2999 
3000 //------------------------------Ideal------------------------------------------
3001 // Return a node which is more "ideal" than the current node.  Strip out
3002 // control copies
3003 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
3004   if (remove_dead_region(phase, can_reshape)) return this;
3005   // Don't bother trying to transform a dead node
3006   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
3007     return NULL;
3008   }
3009 
3010   // Eliminate volatile MemBars for scalar replaced objects.
3011   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
3012     bool eliminate = false;
3013     int opc = Opcode();
3014     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
3015       // Volatile field loads and stores.
3016       Node* my_mem = in(MemBarNode::Precedent);
3017       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
3018       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
3019         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
3020         // replace this Precedent (decodeN) with the Load instead.
3021         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
3022           Node* load_node = my_mem-&gt;in(1);
3023           set_req(MemBarNode::Precedent, load_node);
3024           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
3025           my_mem = load_node;
3026         } else {
3027           assert(my_mem-&gt;unique_out() == this, "sanity");
3028           del_req(Precedent);
3029           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem); // remove dead node later
3030           my_mem = NULL;
3031         }
3032       }
3033       if (my_mem != NULL &amp;&amp; my_mem-&gt;is_Mem()) {
3034         const TypeOopPtr* t_oop = my_mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_oopptr();
3035         // Check for scalar replaced object reference.
3036         if( t_oop != NULL &amp;&amp; t_oop-&gt;is_known_instance_field() &amp;&amp;
3037             t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
3038             t_oop-&gt;offset() != Type::OffsetTop) {
3039           eliminate = true;
3040         }
3041       }
3042     } else if (opc == Op_MemBarRelease) {
3043       // Final field stores.
3044       Node* alloc = AllocateNode::Ideal_allocation(in(MemBarNode::Precedent), phase);
3045       if ((alloc != NULL) &amp;&amp; alloc-&gt;is_Allocate() &amp;&amp;
3046           alloc-&gt;as_Allocate()-&gt;_is_non_escaping) {
3047         // The allocated object does not escape.
3048         eliminate = true;
3049       }
3050     }
3051     if (eliminate) {
3052       // Replace MemBar projections by its inputs.
3053       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3054       igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3055       igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3056       // Must return either the original node (now dead) or a new node
3057       // (Do not return a top here, since that would break the uniqueness of top.)
3058       return new (phase-&gt;C) ConINode(TypeInt::ZERO);
3059     }
3060   }
3061   return NULL;
3062 }
3063 
3064 //------------------------------Value------------------------------------------
3065 const Type *MemBarNode::Value( PhaseTransform *phase ) const {
3066   if( !in(0) ) return Type::TOP;
3067   if( phase-&gt;type(in(0)) == Type::TOP )
3068     return Type::TOP;
3069   return TypeTuple::MEMBAR;
3070 }
3071 
3072 //------------------------------match------------------------------------------
3073 // Construct projections for memory.
3074 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {
3075   switch (proj-&gt;_con) {
3076   case TypeFunc::Control:
3077   case TypeFunc::Memory:
3078     return new (m-&gt;C) MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3079   }
3080   ShouldNotReachHere();
3081   return NULL;
3082 }
3083 
3084 //===========================InitializeNode====================================
3085 // SUMMARY:
3086 // This node acts as a memory barrier on raw memory, after some raw stores.
3087 // The 'cooked' oop value feeds from the Initialize, not the Allocation.
3088 // The Initialize can 'capture' suitably constrained stores as raw inits.
3089 // It can coalesce related raw stores into larger units (called 'tiles').
3090 // It can avoid zeroing new storage for memory units which have raw inits.
3091 // At macro-expansion, it is marked 'complete', and does not optimize further.
3092 //
3093 // EXAMPLE:
3094 // The object 'new short[2]' occupies 16 bytes in a 32-bit machine.
3095 //   ctl = incoming control; mem* = incoming memory
3096 // (Note:  A star * on a memory edge denotes I/O and other standard edges.)
3097 // First allocate uninitialized memory and fill in the header:
3098 //   alloc = (Allocate ctl mem* 16 #short[].klass ...)
3099 //   ctl := alloc.Control; mem* := alloc.Memory*
3100 //   rawmem = alloc.Memory; rawoop = alloc.RawAddress
3101 // Then initialize to zero the non-header parts of the raw memory block:
3102 //   init = (Initialize alloc.Control alloc.Memory* alloc.RawAddress)
3103 //   ctl := init.Control; mem.SLICE(#short[*]) := init.Memory
3104 // After the initialize node executes, the object is ready for service:
3105 //   oop := (CheckCastPP init.Control alloc.RawAddress #short[])
3106 // Suppose its body is immediately initialized as {1,2}:
3107 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3108 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3109 //   mem.SLICE(#short[*]) := store2
3110 //
3111 // DETAILS:
3112 // An InitializeNode collects and isolates object initialization after
3113 // an AllocateNode and before the next possible safepoint.  As a
3114 // memory barrier (MemBarNode), it keeps critical stores from drifting
3115 // down past any safepoint or any publication of the allocation.
3116 // Before this barrier, a newly-allocated object may have uninitialized bits.
3117 // After this barrier, it may be treated as a real oop, and GC is allowed.
3118 //
3119 // The semantics of the InitializeNode include an implicit zeroing of
3120 // the new object from object header to the end of the object.
3121 // (The object header and end are determined by the AllocateNode.)
3122 //
3123 // Certain stores may be added as direct inputs to the InitializeNode.
3124 // These stores must update raw memory, and they must be to addresses
3125 // derived from the raw address produced by AllocateNode, and with
3126 // a constant offset.  They must be ordered by increasing offset.
3127 // The first one is at in(RawStores), the last at in(req()-1).
3128 // Unlike most memory operations, they are not linked in a chain,
3129 // but are displayed in parallel as users of the rawmem output of
3130 // the allocation.
3131 //
3132 // (See comments in InitializeNode::capture_store, which continue
3133 // the example given above.)
3134 //
3135 // When the associated Allocate is macro-expanded, the InitializeNode
3136 // may be rewritten to optimize collected stores.  A ClearArrayNode
3137 // may also be created at that point to represent any required zeroing.
3138 // The InitializeNode is then marked 'complete', prohibiting further
3139 // capturing of nearby memory operations.
3140 //
3141 // During macro-expansion, all captured initializations which store
3142 // constant values of 32 bits or smaller are coalesced (if advantageous)
3143 // into larger 'tiles' 32 or 64 bits.  This allows an object to be
3144 // initialized in fewer memory operations.  Memory words which are
3145 // covered by neither tiles nor non-constant stores are pre-zeroed
3146 // by explicit stores of zero.  (The code shape happens to do all
3147 // zeroing first, then all other stores, with both sequences occurring
3148 // in order of ascending offsets.)
3149 //
3150 // Alternatively, code may be inserted between an AllocateNode and its
3151 // InitializeNode, to perform arbitrary initialization of the new object.
3152 // E.g., the object copying intrinsics insert complex data transfers here.
3153 // The initialization must then be marked as 'complete' disable the
3154 // built-in zeroing semantics and the collection of initializing stores.
3155 //
3156 // While an InitializeNode is incomplete, reads from the memory state
3157 // produced by it are optimizable if they match the control edge and
3158 // new oop address associated with the allocation/initialization.
3159 // They return a stored value (if the offset matches) or else zero.
3160 // A write to the memory state, if it matches control and address,
3161 // and if it is to a constant offset, may be 'captured' by the
3162 // InitializeNode.  It is cloned as a raw memory operation and rewired
3163 // inside the initialization, to the raw oop produced by the allocation.
3164 // Operations on addresses which are provably distinct (e.g., to
3165 // other AllocateNodes) are allowed to bypass the initialization.
3166 //
3167 // The effect of all this is to consolidate object initialization
3168 // (both arrays and non-arrays, both piecewise and bulk) into a
3169 // single location, where it can be optimized as a unit.
3170 //
3171 // Only stores with an offset less than TrackedInitializationLimit words
3172 // will be considered for capture by an InitializeNode.  This puts a
3173 // reasonable limit on the complexity of optimized initializations.
3174 
3175 //---------------------------InitializeNode------------------------------------
3176 InitializeNode::InitializeNode(Compile* C, int adr_type, Node* rawoop)
3177   : _is_complete(Incomplete), _does_not_escape(false),
3178     MemBarNode(C, adr_type, rawoop)
3179 {
3180   init_class_id(Class_Initialize);
3181 
3182   assert(adr_type == Compile::AliasIdxRaw, "only valid atp");
3183   assert(in(RawAddress) == rawoop, "proper init");
3184   // Note:  allocation() can be NULL, for secondary initialization barriers
3185 }
3186 
3187 // Since this node is not matched, it will be processed by the
3188 // register allocator.  Declare that there are no constraints
3189 // on the allocation of the RawAddress edge.
3190 const RegMask &amp;InitializeNode::in_RegMask(uint idx) const {
3191   // This edge should be set to top, by the set_complete.  But be conservative.
3192   if (idx == InitializeNode::RawAddress)
3193     return *(Compile::current()-&gt;matcher()-&gt;idealreg2spillmask[in(idx)-&gt;ideal_reg()]);
3194   return RegMask::Empty;
3195 }
3196 
3197 Node* InitializeNode::memory(uint alias_idx) {
3198   Node* mem = in(Memory);
3199   if (mem-&gt;is_MergeMem()) {
3200     return mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
3201   } else {
3202     // incoming raw memory is not split
3203     return mem;
3204   }
3205 }
3206 
3207 bool InitializeNode::is_non_zero() {
3208   if (is_complete())  return false;
3209   remove_extra_zeroes();
3210   return (req() &gt; RawStores);
3211 }
3212 
3213 void InitializeNode::set_complete(PhaseGVN* phase) {
3214   assert(!is_complete(), "caller responsibility");
3215   _is_complete = Complete;
3216 
3217   // After this node is complete, it contains a bunch of
3218   // raw-memory initializations.  There is no need for
3219   // it to have anything to do with non-raw memory effects.
3220   // Therefore, tell all non-raw users to re-optimize themselves,
3221   // after skipping the memory effects of this initialization.
3222   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3223   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3224 }
3225 
3226 // convenience function
3227 // return false if the init contains any stores already
3228 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3229   InitializeNode* init = initialization();
3230   if (init == NULL || init-&gt;is_complete())  return false;
3231   init-&gt;remove_extra_zeroes();
3232   // for now, if this allocation has already collected any inits, bail:
3233   if (init-&gt;is_non_zero())  return false;
3234   init-&gt;set_complete(phase);
3235   return true;
3236 }
3237 
3238 void InitializeNode::remove_extra_zeroes() {
3239   if (req() == RawStores)  return;
3240   Node* zmem = zero_memory();
3241   uint fill = RawStores;
3242   for (uint i = fill; i &lt; req(); i++) {
3243     Node* n = in(i);
3244     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3245     if (fill &lt; i)  set_req(fill, n);          // compact
3246     ++fill;
3247   }
3248   // delete any empty spaces created:
3249   while (fill &lt; req()) {
3250     del_req(fill);
3251   }
3252 }
3253 
3254 // Helper for remembering which stores go with which offsets.
3255 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3256   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3257   intptr_t offset = -1;
3258   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3259                                                phase, offset);
3260   if (base == NULL)     return -1;  // something is dead,
3261   if (offset &lt; 0)       return -1;  //        dead, dead
3262   return offset;
3263 }
3264 
3265 // Helper for proving that an initialization expression is
3266 // "simple enough" to be folded into an object initialization.
3267 // Attempts to prove that a store's initial value 'n' can be captured
3268 // within the initialization without creating a vicious cycle, such as:
3269 //     { Foo p = new Foo(); p.next = p; }
3270 // True for constants and parameters and small combinations thereof.
3271 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {
3272   if (n == NULL)      return true;   // (can this really happen?)
3273   if (n-&gt;is_Proj())   n = n-&gt;in(0);
3274   if (n == this)      return false;  // found a cycle
3275   if (n-&gt;is_Con())    return true;
3276   if (n-&gt;is_Start())  return true;   // params, etc., are OK
3277   if (n-&gt;is_Root())   return true;   // even better
3278 
3279   Node* ctl = n-&gt;in(0);
3280   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {
3281     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);
3282     if (ctl == this)  return false;
3283 
3284     // If we already know that the enclosing memory op is pinned right after
3285     // the init, then any control flow that the store has picked up
3286     // must have preceded the init, or else be equal to the init.
3287     // Even after loop optimizations (which might change control edges)
3288     // a store is never pinned *before* the availability of its inputs.
3289     if (!MemNode::all_controls_dominate(n, this))
3290       return false;                  // failed to prove a good control
3291   }
3292 
3293   // Check data edges for possible dependencies on 'this'.
3294   if ((count += 1) &gt; 20)  return false;  // complexity limit
3295   for (uint i = 1; i &lt; n-&gt;req(); i++) {
3296     Node* m = n-&gt;in(i);
3297     if (m == NULL || m == n || m-&gt;is_top())  continue;
3298     uint first_i = n-&gt;find_edge(m);
3299     if (i != first_i)  continue;  // process duplicate edge just once
3300     if (!detect_init_independence(m, count)) {
3301       return false;
3302     }
3303   }
3304 
3305   return true;
3306 }
3307 
3308 // Here are all the checks a Store must pass before it can be moved into
3309 // an initialization.  Returns zero if a check fails.
3310 // On success, returns the (constant) offset to which the store applies,
3311 // within the initialized memory.
3312 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {
3313   const int FAIL = 0;
3314   if (st-&gt;req() != MemNode::ValueIn + 1)
3315     return FAIL;                // an inscrutable StoreNode (card mark?)
3316   Node* ctl = st-&gt;in(MemNode::Control);
3317   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3318     return FAIL;                // must be unconditional after the initialization
3319   Node* mem = st-&gt;in(MemNode::Memory);
3320   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3321     return FAIL;                // must not be preceded by other stores
3322   Node* adr = st-&gt;in(MemNode::Address);
3323   intptr_t offset;
3324   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3325   if (alloc == NULL)
3326     return FAIL;                // inscrutable address
3327   if (alloc != allocation())
3328     return FAIL;                // wrong allocation!  (store needs to float up)
3329   Node* val = st-&gt;in(MemNode::ValueIn);
3330   int complexity_count = 0;
3331   if (!detect_init_independence(val, complexity_count))
3332     return FAIL;                // stored value must be 'simple enough'
3333 
3334   // The Store can be captured only if nothing after the allocation
3335   // and before the Store is using the memory location that the store
3336   // overwrites.
3337   bool failed = false;
3338   // If is_complete_with_arraycopy() is true the shape of the graph is
3339   // well defined and is safe so no need for extra checks.
3340   if (!is_complete_with_arraycopy()) {
3341     // We are going to look at each use of the memory state following
3342     // the allocation to make sure nothing reads the memory that the
3343     // Store writes.
3344     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3345     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3346     ResourceMark rm;
3347     Unique_Node_List mems;
3348     mems.push(mem);
3349     Node* unique_merge = NULL;
3350     for (uint next = 0; next &lt; mems.size(); ++next) {
3351       Node *m  = mems.at(next);
3352       for (DUIterator_Fast jmax, j = m-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3353         Node *n = m-&gt;fast_out(j);
3354         if (n-&gt;outcnt() == 0) {
3355           continue;
3356         }
3357         if (n == st) {
3358           continue;
3359         } else if (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0) != ctl) {
3360           // If the control of this use is different from the control
3361           // of the Store which is right after the InitializeNode then
3362           // this node cannot be between the InitializeNode and the
3363           // Store.
3364           continue;
3365         } else if (n-&gt;is_MergeMem()) {
3366           if (n-&gt;as_MergeMem()-&gt;memory_at(alias_idx) == m) {
3367             // We can hit a MergeMemNode (that will likely go away
3368             // later) that is a direct use of the memory state
3369             // following the InitializeNode on the same slice as the
3370             // store node that we'd like to capture. We need to check
3371             // the uses of the MergeMemNode.
3372             mems.push(n);
3373           }
3374         } else if (n-&gt;is_Mem()) {
3375           Node* other_adr = n-&gt;in(MemNode::Address);
3376           if (other_adr == adr) {
3377             failed = true;
3378             break;
3379           } else {
3380             const TypePtr* other_t_adr = phase-&gt;type(other_adr)-&gt;isa_ptr();
3381             if (other_t_adr != NULL) {
3382               int other_alias_idx = phase-&gt;C-&gt;get_alias_index(other_t_adr);
3383               if (other_alias_idx == alias_idx) {
3384                 // A load from the same memory slice as the store right
3385                 // after the InitializeNode. We check the control of the
3386                 // object/array that is loaded from. If it's the same as
3387                 // the store control then we cannot capture the store.
3388                 assert(!n-&gt;is_Store(), "2 stores to same slice on same control?");
3389                 Node* base = other_adr;
3390                 assert(base-&gt;is_AddP(), err_msg_res("should be addp but is %s", base-&gt;Name()));
3391                 base = base-&gt;in(AddPNode::Base);
3392                 if (base != NULL) {
3393                   base = base-&gt;uncast();
3394                   if (base-&gt;is_Proj() &amp;&amp; base-&gt;in(0) == alloc) {
3395                     failed = true;
3396                     break;
3397                   }
3398                 }
3399               }
3400             }
3401           }
3402         } else {
3403           failed = true;
3404           break;
3405         }
3406       }
3407     }
3408   }
3409   if (failed) {
3410     if (!can_reshape) {
3411       // We decided we couldn't capture the store during parsing. We
3412       // should try again during the next IGVN once the graph is
3413       // cleaner.
3414       phase-&gt;C-&gt;record_for_igvn(st);
3415     }
3416     return FAIL;
3417   }
3418 
3419   return offset;                // success
3420 }
3421 
3422 // Find the captured store in(i) which corresponds to the range
3423 // [start..start+size) in the initialized object.
3424 // If there is one, return its index i.  If there isn't, return the
3425 // negative of the index where it should be inserted.
3426 // Return 0 if the queried range overlaps an initialization boundary
3427 // or if dead code is encountered.
3428 // If size_in_bytes is zero, do not bother with overlap checks.
3429 int InitializeNode::captured_store_insertion_point(intptr_t start,
3430                                                    int size_in_bytes,
3431                                                    PhaseTransform* phase) {
3432   const int FAIL = 0, MAX_STORE = BytesPerLong;
3433 
3434   if (is_complete())
3435     return FAIL;                // arraycopy got here first; punt
3436 
3437   assert(allocation() != NULL, "must be present");
3438 
3439   // no negatives, no header fields:
3440   if (start &lt; (intptr_t) allocation()-&gt;minimum_header_size())  return FAIL;
3441 
3442   // after a certain size, we bail out on tracking all the stores:
3443   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3444   if (start &gt;= ti_limit)  return FAIL;
3445 
3446   for (uint i = InitializeNode::RawStores, limit = req(); ; ) {
3447     if (i &gt;= limit)  return -(int)i; // not found; here is where to put it
3448 
3449     Node*    st     = in(i);
3450     intptr_t st_off = get_store_offset(st, phase);
3451     if (st_off &lt; 0) {
3452       if (st != zero_memory()) {
3453         return FAIL;            // bail out if there is dead garbage
3454       }
3455     } else if (st_off &gt; start) {
3456       // ...we are done, since stores are ordered
3457       if (st_off &lt; start + size_in_bytes) {
3458         return FAIL;            // the next store overlaps
3459       }
3460       return -(int)i;           // not found; here is where to put it
3461     } else if (st_off &lt; start) {
3462       if (size_in_bytes != 0 &amp;&amp;
3463           start &lt; st_off + MAX_STORE &amp;&amp;
3464           start &lt; st_off + st-&gt;as_Store()-&gt;memory_size()) {
3465         return FAIL;            // the previous store overlaps
3466       }
3467     } else {
3468       if (size_in_bytes != 0 &amp;&amp;
3469           st-&gt;as_Store()-&gt;memory_size() != size_in_bytes) {
3470         return FAIL;            // mismatched store size
3471       }
3472       return i;
3473     }
3474 
3475     ++i;
3476   }
3477 }
3478 
3479 // Look for a captured store which initializes at the offset 'start'
3480 // with the given size.  If there is no such store, and no other
3481 // initialization interferes, then return zero_memory (the memory
3482 // projection of the AllocateNode).
3483 Node* InitializeNode::find_captured_store(intptr_t start, int size_in_bytes,
3484                                           PhaseTransform* phase) {
3485   assert(stores_are_sane(phase), "");
3486   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3487   if (i == 0) {
3488     return NULL;                // something is dead
3489   } else if (i &lt; 0) {
3490     return zero_memory();       // just primordial zero bits here
3491   } else {
3492     Node* st = in(i);           // here is the store at this position
3493     assert(get_store_offset(st-&gt;as_Store(), phase) == start, "sanity");
3494     return st;
3495   }
3496 }
3497 
3498 // Create, as a raw pointer, an address within my new object at 'offset'.
3499 Node* InitializeNode::make_raw_address(intptr_t offset,
3500                                        PhaseTransform* phase) {
3501   Node* addr = in(RawAddress);
3502   if (offset != 0) {
3503     Compile* C = phase-&gt;C;
3504     addr = phase-&gt;transform( new (C) AddPNode(C-&gt;top(), addr,
3505                                                  phase-&gt;MakeConX(offset)) );
3506   }
3507   return addr;
3508 }
3509 
3510 // Clone the given store, converting it into a raw store
3511 // initializing a field or element of my new object.
3512 // Caller is responsible for retiring the original store,
3513 // with subsume_node or the like.
3514 //
3515 // From the example above InitializeNode::InitializeNode,
3516 // here are the old stores to be captured:
3517 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3518 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3519 //
3520 // Here is the changed code; note the extra edges on init:
3521 //   alloc = (Allocate ...)
3522 //   rawoop = alloc.RawAddress
3523 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3524 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3525 //   init = (Initialize alloc.Control alloc.Memory rawoop
3526 //                      rawstore1 rawstore2)
3527 //
3528 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
3529                                     PhaseTransform* phase, bool can_reshape) {
3530   assert(stores_are_sane(phase), "");
3531 
3532   if (start &lt; 0)  return NULL;
3533   assert(can_capture_store(st, phase, can_reshape) == start, "sanity");
3534 
3535   Compile* C = phase-&gt;C;
3536   int size_in_bytes = st-&gt;memory_size();
3537   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3538   if (i == 0)  return NULL;     // bail out
3539   Node* prev_mem = NULL;        // raw memory for the captured store
3540   if (i &gt; 0) {
3541     prev_mem = in(i);           // there is a pre-existing store under this one
3542     set_req(i, C-&gt;top());       // temporarily disconnect it
3543     // See StoreNode::Ideal 'st-&gt;outcnt() == 1' for the reason to disconnect.
3544   } else {
3545     i = -i;                     // no pre-existing store
3546     prev_mem = zero_memory();   // a slice of the newly allocated object
3547     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3548       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3549     else
3550       ins_req(i, C-&gt;top());     // build a new edge
3551   }
3552   Node* new_st = st-&gt;clone();
3553   new_st-&gt;set_req(MemNode::Control, in(Control));
3554   new_st-&gt;set_req(MemNode::Memory,  prev_mem);
3555   new_st-&gt;set_req(MemNode::Address, make_raw_address(start, phase));
3556   new_st = phase-&gt;transform(new_st);
3557 
3558   // At this point, new_st might have swallowed a pre-existing store
3559   // at the same offset, or perhaps new_st might have disappeared,
3560   // if it redundantly stored the same value (or zero to fresh memory).
3561 
3562   // In any case, wire it in:
3563   set_req(i, new_st);
3564 
3565   // The caller may now kill the old guy.
3566   DEBUG_ONLY(Node* check_st = find_captured_store(start, size_in_bytes, phase));
3567   assert(check_st == new_st || check_st == NULL, "must be findable");
3568   assert(!is_complete(), "");
3569   return new_st;
3570 }
3571 
3572 static bool store_constant(jlong* tiles, int num_tiles,
3573                            intptr_t st_off, int st_size,
3574                            jlong con) {
3575   if ((st_off &amp; (st_size-1)) != 0)
3576     return false;               // strange store offset (assume size==2**N)
3577   address addr = (address)tiles + st_off;
3578   assert(st_off &gt;= 0 &amp;&amp; addr+st_size &lt;= (address)&amp;tiles[num_tiles], "oob");
3579   switch (st_size) {
3580   case sizeof(jbyte):  *(jbyte*) addr = (jbyte) con; break;
3581   case sizeof(jchar):  *(jchar*) addr = (jchar) con; break;
3582   case sizeof(jint):   *(jint*)  addr = (jint)  con; break;
3583   case sizeof(jlong):  *(jlong*) addr = (jlong) con; break;
3584   default: return false;        // strange store size (detect size!=2**N here)
3585   }
3586   return true;                  // return success to caller
3587 }
3588 
3589 // Coalesce subword constants into int constants and possibly
3590 // into long constants.  The goal, if the CPU permits,
3591 // is to initialize the object with a small number of 64-bit tiles.
3592 // Also, convert floating-point constants to bit patterns.
3593 // Non-constants are not relevant to this pass.
3594 //
3595 // In terms of the running example on InitializeNode::InitializeNode
3596 // and InitializeNode::capture_store, here is the transformation
3597 // of rawstore1 and rawstore2 into rawstore12:
3598 //   alloc = (Allocate ...)
3599 //   rawoop = alloc.RawAddress
3600 //   tile12 = 0x00010002
3601 //   rawstore12 = (StoreI alloc.Control alloc.Memory (+ rawoop 12) tile12)
3602 //   init = (Initialize alloc.Control alloc.Memory rawoop rawstore12)
3603 //
3604 void
3605 InitializeNode::coalesce_subword_stores(intptr_t header_size,
3606                                         Node* size_in_bytes,
3607                                         PhaseGVN* phase) {
3608   Compile* C = phase-&gt;C;
3609 
3610   assert(stores_are_sane(phase), "");
3611   // Note:  After this pass, they are not completely sane,
3612   // since there may be some overlaps.
3613 
3614   int old_subword = 0, old_long = 0, new_int = 0, new_long = 0;
3615 
3616   intptr_t ti_limit = (TrackedInitializationLimit * HeapWordSize);
3617   intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, ti_limit);
3618   size_limit = MIN2(size_limit, ti_limit);
3619   size_limit = align_size_up(size_limit, BytesPerLong);
3620   int num_tiles = size_limit / BytesPerLong;
3621 
3622   // allocate space for the tile map:
3623   const int small_len = DEBUG_ONLY(true ? 3 :) 30; // keep stack frames small
3624   jlong  tiles_buf[small_len];
3625   Node*  nodes_buf[small_len];
3626   jlong  inits_buf[small_len];
3627   jlong* tiles = ((num_tiles &lt;= small_len) ? &amp;tiles_buf[0]
3628                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3629   Node** nodes = ((num_tiles &lt;= small_len) ? &amp;nodes_buf[0]
3630                   : NEW_RESOURCE_ARRAY(Node*, num_tiles));
3631   jlong* inits = ((num_tiles &lt;= small_len) ? &amp;inits_buf[0]
3632                   : NEW_RESOURCE_ARRAY(jlong, num_tiles));
3633   // tiles: exact bitwise model of all primitive constants
3634   // nodes: last constant-storing node subsumed into the tiles model
3635   // inits: which bytes (in each tile) are touched by any initializations
3636 
3637   //// Pass A: Fill in the tile model with any relevant stores.
3638 
3639   Copy::zero_to_bytes(tiles, sizeof(tiles[0]) * num_tiles);
3640   Copy::zero_to_bytes(nodes, sizeof(nodes[0]) * num_tiles);
3641   Copy::zero_to_bytes(inits, sizeof(inits[0]) * num_tiles);
3642   Node* zmem = zero_memory(); // initially zero memory state
3643   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3644     Node* st = in(i);
3645     intptr_t st_off = get_store_offset(st, phase);
3646 
3647     // Figure out the store's offset and constant value:
3648     if (st_off &lt; header_size)             continue; //skip (ignore header)
3649     if (st-&gt;in(MemNode::Memory) != zmem)  continue; //skip (odd store chain)
3650     int st_size = st-&gt;as_Store()-&gt;memory_size();
3651     if (st_off + st_size &gt; size_limit)    break;
3652 
3653     // Record which bytes are touched, whether by constant or not.
3654     if (!store_constant(inits, num_tiles, st_off, st_size, (jlong) -1))
3655       continue;                 // skip (strange store size)
3656 
3657     const Type* val = phase-&gt;type(st-&gt;in(MemNode::ValueIn));
3658     if (!val-&gt;singleton())                continue; //skip (non-con store)
3659     BasicType type = val-&gt;basic_type();
3660 
3661     jlong con = 0;
3662     switch (type) {
3663     case T_INT:    con = val-&gt;is_int()-&gt;get_con();  break;
3664     case T_LONG:   con = val-&gt;is_long()-&gt;get_con(); break;
3665     case T_FLOAT:  con = jint_cast(val-&gt;getf());    break;
3666     case T_DOUBLE: con = jlong_cast(val-&gt;getd());   break;
3667     default:                              continue; //skip (odd store type)
3668     }
3669 
3670     if (type == T_LONG &amp;&amp; Matcher::isSimpleConstant64(con) &amp;&amp;
3671         st-&gt;Opcode() == Op_StoreL) {
3672       continue;                 // This StoreL is already optimal.
3673     }
3674 
3675     // Store down the constant.
3676     store_constant(tiles, num_tiles, st_off, st_size, con);
3677 
3678     intptr_t j = st_off &gt;&gt; LogBytesPerLong;
3679 
3680     if (type == T_INT &amp;&amp; st_size == BytesPerInt
3681         &amp;&amp; (st_off &amp; BytesPerInt) == BytesPerInt) {
3682       jlong lcon = tiles[j];
3683       if (!Matcher::isSimpleConstant64(lcon) &amp;&amp;
3684           st-&gt;Opcode() == Op_StoreI) {
3685         // This StoreI is already optimal by itself.
3686         jint* intcon = (jint*) &amp;tiles[j];
3687         intcon[1] = 0;  // undo the store_constant()
3688 
3689         // If the previous store is also optimal by itself, back up and
3690         // undo the action of the previous loop iteration... if we can.
3691         // But if we can't, just let the previous half take care of itself.
3692         st = nodes[j];
3693         st_off -= BytesPerInt;
3694         con = intcon[0];
3695         if (con != 0 &amp;&amp; st != NULL &amp;&amp; st-&gt;Opcode() == Op_StoreI) {
3696           assert(st_off &gt;= header_size, "still ignoring header");
3697           assert(get_store_offset(st, phase) == st_off, "must be");
3698           assert(in(i-1) == zmem, "must be");
3699           DEBUG_ONLY(const Type* tcon = phase-&gt;type(st-&gt;in(MemNode::ValueIn)));
3700           assert(con == tcon-&gt;is_int()-&gt;get_con(), "must be");
3701           // Undo the effects of the previous loop trip, which swallowed st:
3702           intcon[0] = 0;        // undo store_constant()
3703           set_req(i-1, st);     // undo set_req(i, zmem)
3704           nodes[j] = NULL;      // undo nodes[j] = st
3705           --old_subword;        // undo ++old_subword
3706         }
3707         continue;               // This StoreI is already optimal.
3708       }
3709     }
3710 
3711     // This store is not needed.
3712     set_req(i, zmem);
3713     nodes[j] = st;              // record for the moment
3714     if (st_size &lt; BytesPerLong) // something has changed
3715           ++old_subword;        // includes int/float, but who's counting...
3716     else  ++old_long;
3717   }
3718 
3719   if ((old_subword + old_long) == 0)
3720     return;                     // nothing more to do
3721 
3722   //// Pass B: Convert any non-zero tiles into optimal constant stores.
3723   // Be sure to insert them before overlapping non-constant stores.
3724   // (E.g., byte[] x = { 1,2,y,4 }  =&gt;  x[int 0] = 0x01020004, x[2]=y.)
3725   for (int j = 0; j &lt; num_tiles; j++) {
3726     jlong con  = tiles[j];
3727     jlong init = inits[j];
3728     if (con == 0)  continue;
3729     jint con0,  con1;           // split the constant, address-wise
3730     jint init0, init1;          // split the init map, address-wise
3731     { union { jlong con; jint intcon[2]; } u;
3732       u.con = con;
3733       con0  = u.intcon[0];
3734       con1  = u.intcon[1];
3735       u.con = init;
3736       init0 = u.intcon[0];
3737       init1 = u.intcon[1];
3738     }
3739 
3740     Node* old = nodes[j];
3741     assert(old != NULL, "need the prior store");
3742     intptr_t offset = (j * BytesPerLong);
3743 
3744     bool split = !Matcher::isSimpleConstant64(con);
3745 
3746     if (offset &lt; header_size) {
3747       assert(offset + BytesPerInt &gt;= header_size, "second int counts");
3748       assert(*(jint*)&amp;tiles[j] == 0, "junk in header");
3749       split = true;             // only the second word counts
3750       // Example:  int a[] = { 42 ... }
3751     } else if (con0 == 0 &amp;&amp; init0 == -1) {
3752       split = true;             // first word is covered by full inits
3753       // Example:  int a[] = { ... foo(), 42 ... }
3754     } else if (con1 == 0 &amp;&amp; init1 == -1) {
3755       split = true;             // second word is covered by full inits
3756       // Example:  int a[] = { ... 42, foo() ... }
3757     }
3758 
3759     // Here's a case where init0 is neither 0 nor -1:
3760     //   byte a[] = { ... 0,0,foo(),0,  0,0,0,42 ... }
3761     // Assuming big-endian memory, init0, init1 are 0x0000FF00, 0x000000FF.
3762     // In this case the tile is not split; it is (jlong)42.
3763     // The big tile is stored down, and then the foo() value is inserted.
3764     // (If there were foo(),foo() instead of foo(),0, init0 would be -1.)
3765 
3766     Node* ctl = old-&gt;in(MemNode::Control);
3767     Node* adr = make_raw_address(offset, phase);
3768     const TypePtr* atp = TypeRawPtr::BOTTOM;
3769 
3770     // One or two coalesced stores to plop down.
3771     Node*    st[2];
3772     intptr_t off[2];
3773     int  nst = 0;
3774     if (!split) {
3775       ++new_long;
3776       off[nst] = offset;
3777       st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3778                                   phase-&gt;longcon(con), T_LONG, MemNode::unordered);
3779     } else {
3780       // Omit either if it is a zero.
3781       if (con0 != 0) {
3782         ++new_int;
3783         off[nst]  = offset;
3784         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3785                                     phase-&gt;intcon(con0), T_INT, MemNode::unordered);
3786       }
3787       if (con1 != 0) {
3788         ++new_int;
3789         offset += BytesPerInt;
3790         adr = make_raw_address(offset, phase);
3791         off[nst]  = offset;
3792         st[nst++] = StoreNode::make(*phase, ctl, zmem, adr, atp,
3793                                     phase-&gt;intcon(con1), T_INT, MemNode::unordered);
3794       }
3795     }
3796 
3797     // Insert second store first, then the first before the second.
3798     // Insert each one just before any overlapping non-constant stores.
3799     while (nst &gt; 0) {
3800       Node* st1 = st[--nst];
3801       C-&gt;copy_node_notes_to(st1, old);
3802       st1 = phase-&gt;transform(st1);
3803       offset = off[nst];
3804       assert(offset &gt;= header_size, "do not smash header");
3805       int ins_idx = captured_store_insertion_point(offset, /*size:*/0, phase);
3806       guarantee(ins_idx != 0, "must re-insert constant store");
3807       if (ins_idx &lt; 0)  ins_idx = -ins_idx;  // never overlap
3808       if (ins_idx &gt; InitializeNode::RawStores &amp;&amp; in(ins_idx-1) == zmem)
3809         set_req(--ins_idx, st1);
3810       else
3811         ins_req(ins_idx, st1);
3812     }
3813   }
3814 
3815   if (PrintCompilation &amp;&amp; WizardMode)
3816     tty-&gt;print_cr("Changed %d/%d subword/long constants into %d/%d int/long",
3817                   old_subword, old_long, new_int, new_long);
3818   if (C-&gt;log() != NULL)
3819     C-&gt;log()-&gt;elem("comment that='%d/%d subword/long to %d/%d int/long'",
3820                    old_subword, old_long, new_int, new_long);
3821 
3822   // Clean up any remaining occurrences of zmem:
3823   remove_extra_zeroes();
3824 }
3825 
3826 // Explore forward from in(start) to find the first fully initialized
3827 // word, and return its offset.  Skip groups of subword stores which
3828 // together initialize full words.  If in(start) is itself part of a
3829 // fully initialized word, return the offset of in(start).  If there
3830 // are no following full-word stores, or if something is fishy, return
3831 // a negative value.
3832 intptr_t InitializeNode::find_next_fullword_store(uint start, PhaseGVN* phase) {
3833   int       int_map = 0;
3834   intptr_t  int_map_off = 0;
3835   const int FULL_MAP = right_n_bits(BytesPerInt);  // the int_map we hope for
3836 
3837   for (uint i = start, limit = req(); i &lt; limit; i++) {
3838     Node* st = in(i);
3839 
3840     intptr_t st_off = get_store_offset(st, phase);
3841     if (st_off &lt; 0)  break;  // return conservative answer
3842 
3843     int st_size = st-&gt;as_Store()-&gt;memory_size();
3844     if (st_size &gt;= BytesPerInt &amp;&amp; (st_off % BytesPerInt) == 0) {
3845       return st_off;            // we found a complete word init
3846     }
3847 
3848     // update the map:
3849 
3850     intptr_t this_int_off = align_size_down(st_off, BytesPerInt);
3851     if (this_int_off != int_map_off) {
3852       // reset the map:
3853       int_map = 0;
3854       int_map_off = this_int_off;
3855     }
3856 
3857     int subword_off = st_off - this_int_off;
3858     int_map |= right_n_bits(st_size) &lt;&lt; subword_off;
3859     if ((int_map &amp; FULL_MAP) == FULL_MAP) {
3860       return this_int_off;      // we found a complete word init
3861     }
3862 
3863     // Did this store hit or cross the word boundary?
3864     intptr_t next_int_off = align_size_down(st_off + st_size, BytesPerInt);
3865     if (next_int_off == this_int_off + BytesPerInt) {
3866       // We passed the current int, without fully initializing it.
3867       int_map_off = next_int_off;
3868       int_map &gt;&gt;= BytesPerInt;
3869     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
3870       // We passed the current and next int.
3871       return this_int_off + BytesPerInt;
3872     }
3873   }
3874 
3875   return -1;
3876 }
3877 
3878 
3879 // Called when the associated AllocateNode is expanded into CFG.
3880 // At this point, we may perform additional optimizations.
3881 // Linearize the stores by ascending offset, to make memory
3882 // activity as coherent as possible.
3883 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
3884                                       intptr_t header_size,
3885                                       Node* size_in_bytes,
3886                                       PhaseGVN* phase) {
3887   assert(!is_complete(), "not already complete");
3888   assert(stores_are_sane(phase), "");
3889   assert(allocation() != NULL, "must be present");
3890 
3891   remove_extra_zeroes();
3892 
3893   if (ReduceFieldZeroing || ReduceBulkZeroing)
3894     // reduce instruction count for common initialization patterns
3895     coalesce_subword_stores(header_size, size_in_bytes, phase);
3896 
3897   Node* zmem = zero_memory();   // initially zero memory state
3898   Node* inits = zmem;           // accumulating a linearized chain of inits
3899   #ifdef ASSERT
3900   intptr_t first_offset = allocation()-&gt;minimum_header_size();
3901   intptr_t last_init_off = first_offset;  // previous init offset
3902   intptr_t last_init_end = first_offset;  // previous init offset+size
3903   intptr_t last_tile_end = first_offset;  // previous tile offset+size
3904   #endif
3905   intptr_t zeroes_done = header_size;
3906 
3907   bool do_zeroing = true;       // we might give up if inits are very sparse
3908   int  big_init_gaps = 0;       // how many large gaps have we seen?
3909 
3910   if (ZeroTLAB)  do_zeroing = false;
3911   if (!ReduceFieldZeroing &amp;&amp; !ReduceBulkZeroing)  do_zeroing = false;
3912 
3913   for (uint i = InitializeNode::RawStores, limit = req(); i &lt; limit; i++) {
3914     Node* st = in(i);
3915     intptr_t st_off = get_store_offset(st, phase);
3916     if (st_off &lt; 0)
3917       break;                    // unknown junk in the inits
3918     if (st-&gt;in(MemNode::Memory) != zmem)
3919       break;                    // complicated store chains somehow in list
3920 
3921     int st_size = st-&gt;as_Store()-&gt;memory_size();
3922     intptr_t next_init_off = st_off + st_size;
3923 
3924     if (do_zeroing &amp;&amp; zeroes_done &lt; next_init_off) {
3925       // See if this store needs a zero before it or under it.
3926       intptr_t zeroes_needed = st_off;
3927 
3928       if (st_size &lt; BytesPerInt) {
3929         // Look for subword stores which only partially initialize words.
3930         // If we find some, we must lay down some word-level zeroes first,
3931         // underneath the subword stores.
3932         //
3933         // Examples:
3934         //   byte[] a = { p,q,r,s }  =&gt;  a[0]=p,a[1]=q,a[2]=r,a[3]=s
3935         //   byte[] a = { x,y,0,0 }  =&gt;  a[0..3] = 0, a[0]=x,a[1]=y
3936         //   byte[] a = { 0,0,z,0 }  =&gt;  a[0..3] = 0, a[2]=z
3937         //
3938         // Note:  coalesce_subword_stores may have already done this,
3939         // if it was prompted by constant non-zero subword initializers.
3940         // But this case can still arise with non-constant stores.
3941 
3942         intptr_t next_full_store = find_next_fullword_store(i, phase);
3943 
3944         // In the examples above:
3945         //   in(i)          p   q   r   s     x   y     z
3946         //   st_off        12  13  14  15    12  13    14
3947         //   st_size        1   1   1   1     1   1     1
3948         //   next_full_s.  12  16  16  16    16  16    16
3949         //   z's_done      12  16  16  16    12  16    12
3950         //   z's_needed    12  16  16  16    16  16    16
3951         //   zsize          0   0   0   0     4   0     4
3952         if (next_full_store &lt; 0) {
3953           // Conservative tack:  Zero to end of current word.
3954           zeroes_needed = align_size_up(zeroes_needed, BytesPerInt);
3955         } else {
3956           // Zero to beginning of next fully initialized word.
3957           // Or, don't zero at all, if we are already in that word.
3958           assert(next_full_store &gt;= zeroes_needed, "must go forward");
3959           assert((next_full_store &amp; (BytesPerInt-1)) == 0, "even boundary");
3960           zeroes_needed = next_full_store;
3961         }
3962       }
3963 
3964       if (zeroes_needed &gt; zeroes_done) {
3965         intptr_t zsize = zeroes_needed - zeroes_done;
3966         // Do some incremental zeroing on rawmem, in parallel with inits.
3967         zeroes_done = align_size_down(zeroes_done, BytesPerInt);
3968         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
3969                                               zeroes_done, zeroes_needed,
3970                                               phase);
3971         zeroes_done = zeroes_needed;
3972         if (zsize &gt; Matcher::init_array_short_size &amp;&amp; ++big_init_gaps &gt; 2)
3973           do_zeroing = false;   // leave the hole, next time
3974       }
3975     }
3976 
3977     // Collect the store and move on:
3978     st-&gt;set_req(MemNode::Memory, inits);
3979     inits = st;                 // put it on the linearized chain
3980     set_req(i, zmem);           // unhook from previous position
3981 
3982     if (zeroes_done == st_off)
3983       zeroes_done = next_init_off;
3984 
3985     assert(!do_zeroing || zeroes_done &gt;= next_init_off, "don't miss any");
3986 
3987     #ifdef ASSERT
3988     // Various order invariants.  Weaker than stores_are_sane because
3989     // a large constant tile can be filled in by smaller non-constant stores.
3990     assert(st_off &gt;= last_init_off, "inits do not reverse");
3991     last_init_off = st_off;
3992     const Type* val = NULL;
3993     if (st_size &gt;= BytesPerInt &amp;&amp;
3994         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
3995         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
3996       assert(st_off &gt;= last_tile_end, "tiles do not overlap");
3997       assert(st_off &gt;= last_init_end, "tiles do not overwrite inits");
3998       last_tile_end = MAX2(last_tile_end, next_init_off);
3999     } else {
4000       intptr_t st_tile_end = align_size_up(next_init_off, BytesPerLong);
4001       assert(st_tile_end &gt;= last_tile_end, "inits stay with tiles");
4002       assert(st_off      &gt;= last_init_end, "inits do not overlap");
4003       last_init_end = next_init_off;  // it's a non-tile
4004     }
4005     #endif //ASSERT
4006   }
4007 
4008   remove_extra_zeroes();        // clear out all the zmems left over
4009   add_req(inits);
4010 
4011   if (!ZeroTLAB) {
4012     // If anything remains to be zeroed, zero it all now.
4013     zeroes_done = align_size_down(zeroes_done, BytesPerInt);
4014     // if it is the last unused 4 bytes of an instance, forget about it
4015     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4016     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4017       assert(allocation() != NULL, "");
4018       if (allocation()-&gt;Opcode() == Op_Allocate) {
4019         Node* klass_node = allocation()-&gt;in(AllocateNode::KlassNode);
4020         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4021         if (zeroes_done == k-&gt;layout_helper())
4022           zeroes_done = size_limit;
4023       }
4024     }
4025     if (zeroes_done &lt; size_limit) {
4026       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
4027                                             zeroes_done, size_in_bytes, phase);
4028     }
4029   }
4030 
4031   set_complete(phase);
4032   return rawmem;
4033 }
4034 
4035 
4036 #ifdef ASSERT
4037 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4038   if (is_complete())
4039     return true;                // stores could be anything at this point
4040   assert(allocation() != NULL, "must be present");
4041   intptr_t last_off = allocation()-&gt;minimum_header_size();
4042   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4043     Node* st = in(i);
4044     intptr_t st_off = get_store_offset(st, phase);
4045     if (st_off &lt; 0)  continue;  // ignore dead garbage
4046     if (last_off &gt; st_off) {
4047       tty-&gt;print_cr("*** bad store offset at %d: " INTX_FORMAT " &gt; " INTX_FORMAT, i, last_off, st_off);
4048       this-&gt;dump(2);
4049       assert(false, "ascending store offsets");
4050       return false;
4051     }
4052     last_off = st_off + st-&gt;as_Store()-&gt;memory_size();
4053   }
4054   return true;
4055 }
4056 #endif //ASSERT
4057 
4058 
4059 
4060 
4061 //============================MergeMemNode=====================================
4062 //
4063 // SEMANTICS OF MEMORY MERGES:  A MergeMem is a memory state assembled from several
4064 // contributing store or call operations.  Each contributor provides the memory
4065 // state for a particular "alias type" (see Compile::alias_type).  For example,
4066 // if a MergeMem has an input X for alias category #6, then any memory reference
4067 // to alias category #6 may use X as its memory state input, as an exact equivalent
4068 // to using the MergeMem as a whole.
4069 //   Load&lt;6&gt;( MergeMem(&lt;6&gt;: X, ...), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4070 //
4071 // (Here, the &lt;N&gt; notation gives the index of the relevant adr_type.)
4072 //
4073 // In one special case (and more cases in the future), alias categories overlap.
4074 // The special alias category "Bot" (Compile::AliasIdxBot) includes all memory
4075 // states.  Therefore, if a MergeMem has only one contributing input W for Bot,
4076 // it is exactly equivalent to that state W:
4077 //   MergeMem(&lt;Bot&gt;: W) &lt;==&gt; W
4078 //
4079 // Usually, the merge has more than one input.  In that case, where inputs
4080 // overlap (i.e., one is Bot), the narrower alias type determines the memory
4081 // state for that type, and the wider alias type (Bot) fills in everywhere else:
4082 //   Load&lt;5&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;5&gt;(W,p)
4083 //   Load&lt;6&gt;( MergeMem(&lt;Bot&gt;: W, &lt;6&gt;: X), p ) &lt;==&gt; Load&lt;6&gt;(X,p)
4084 //
4085 // A merge can take a "wide" memory state as one of its narrow inputs.
4086 // This simply means that the merge observes out only the relevant parts of
4087 // the wide input.  That is, wide memory states arriving at narrow merge inputs
4088 // are implicitly "filtered" or "sliced" as necessary.  (This is rare.)
4089 //
4090 // These rules imply that MergeMem nodes may cascade (via their &lt;Bot&gt; links),
4091 // and that memory slices "leak through":
4092 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)) &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y)
4093 //
4094 // But, in such a cascade, repeated memory slices can "block the leak":
4095 //   MergeMem(&lt;Bot&gt;: MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y), &lt;7&gt;: Y') &lt;==&gt; MergeMem(&lt;Bot&gt;: W, &lt;7&gt;: Y')
4096 //
4097 // In the last example, Y is not part of the combined memory state of the
4098 // outermost MergeMem.  The system must, of course, prevent unschedulable
4099 // memory states from arising, so you can be sure that the state Y is somehow
4100 // a precursor to state Y'.
4101 //
4102 //
4103 // REPRESENTATION OF MEMORY MERGES: The indexes used to address the Node::in array
4104 // of each MergeMemNode array are exactly the numerical alias indexes, including
4105 // but not limited to AliasIdxTop, AliasIdxBot, and AliasIdxRaw.  The functions
4106 // Compile::alias_type (and kin) produce and manage these indexes.
4107 //
4108 // By convention, the value of in(AliasIdxTop) (i.e., in(1)) is always the top node.
4109 // (Note that this provides quick access to the top node inside MergeMem methods,
4110 // without the need to reach out via TLS to Compile::current.)
4111 //
4112 // As a consequence of what was just described, a MergeMem that represents a full
4113 // memory state has an edge in(AliasIdxBot) which is a "wide" memory state,
4114 // containing all alias categories.
4115 //
4116 // MergeMem nodes never (?) have control inputs, so in(0) is NULL.
4117 //
4118 // All other edges in(N) (including in(AliasIdxRaw), which is in(3)) are either
4119 // a memory state for the alias type &lt;N&gt;, or else the top node, meaning that
4120 // there is no particular input for that alias type.  Note that the length of
4121 // a MergeMem is variable, and may be extended at any time to accommodate new
4122 // memory states at larger alias indexes.  When merges grow, they are of course
4123 // filled with "top" in the unused in() positions.
4124 //
4125 // This use of top is named "empty_memory()", or "empty_mem" (no-memory) as a variable.
4126 // (Top was chosen because it works smoothly with passes like GCM.)
4127 //
4128 // For convenience, we hardwire the alias index for TypeRawPtr::BOTTOM.  (It is
4129 // the type of random VM bits like TLS references.)  Since it is always the
4130 // first non-Bot memory slice, some low-level loops use it to initialize an
4131 // index variable:  for (i = AliasIdxRaw; i &lt; req(); i++).
4132 //
4133 //
4134 // ACCESSORS:  There is a special accessor MergeMemNode::base_memory which returns
4135 // the distinguished "wide" state.  The accessor MergeMemNode::memory_at(N) returns
4136 // the memory state for alias type &lt;N&gt;, or (if there is no particular slice at &lt;N&gt;,
4137 // it returns the base memory.  To prevent bugs, memory_at does not accept &lt;Top&gt;
4138 // or &lt;Bot&gt; indexes.  The iterator MergeMemStream provides robust iteration over
4139 // MergeMem nodes or pairs of such nodes, ensuring that the non-top edges are visited.
4140 //
4141 // %%%% We may get rid of base_memory as a separate accessor at some point; it isn't
4142 // really that different from the other memory inputs.  An abbreviation called
4143 // "bot_memory()" for "memory_at(AliasIdxBot)" would keep code tidy.
4144 //
4145 //
4146 // PARTIAL MEMORY STATES:  During optimization, MergeMem nodes may arise that represent
4147 // partial memory states.  When a Phi splits through a MergeMem, the copy of the Phi
4148 // that "emerges though" the base memory will be marked as excluding the alias types
4149 // of the other (narrow-memory) copies which "emerged through" the narrow edges:
4150 //
4151 //   Phi&lt;Bot&gt;(U, MergeMem(&lt;Bot&gt;: W, &lt;8&gt;: Y))
4152 //     ==Ideal=&gt;  MergeMem(&lt;Bot&gt;: Phi&lt;Bot-8&gt;(U, W), Phi&lt;8&gt;(U, Y))
4153 //
4154 // This strange "subtraction" effect is necessary to ensure IGVN convergence.
4155 // (It is currently unimplemented.)  As you can see, the resulting merge is
4156 // actually a disjoint union of memory states, rather than an overlay.
4157 //
4158 
4159 //------------------------------MergeMemNode-----------------------------------
4160 Node* MergeMemNode::make_empty_memory() {
4161   Node* empty_memory = (Node*) Compile::current()-&gt;top();
4162   assert(empty_memory-&gt;is_top(), "correct sentinel identity");
4163   return empty_memory;
4164 }
4165 
4166 MergeMemNode::MergeMemNode(Node *new_base) : Node(1+Compile::AliasIdxRaw) {
4167   init_class_id(Class_MergeMem);
4168   // all inputs are nullified in Node::Node(int)
4169   // set_input(0, NULL);  // no control input
4170 
4171   // Initialize the edges uniformly to top, for starters.
4172   Node* empty_mem = make_empty_memory();
4173   for (uint i = Compile::AliasIdxTop; i &lt; req(); i++) {
4174     init_req(i,empty_mem);
4175   }
4176   assert(empty_memory() == empty_mem, "");
4177 
4178   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4179     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4180     assert(mdef-&gt;empty_memory() == empty_mem, "consistent sentinels");
4181     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4182       mms.set_memory(mms.memory2());
4183     }
4184     assert(base_memory() == mdef-&gt;base_memory(), "");
4185   } else {
4186     set_base_memory(new_base);
4187   }
4188 }
4189 
4190 // Make a new, untransformed MergeMem with the same base as 'mem'.
4191 // If mem is itself a MergeMem, populate the result with the same edges.
4192 MergeMemNode* MergeMemNode::make(Compile* C, Node* mem) {
4193   return new(C) MergeMemNode(mem);
4194 }
4195 
4196 //------------------------------cmp--------------------------------------------
4197 uint MergeMemNode::hash() const { return NO_HASH; }
4198 uint MergeMemNode::cmp( const Node &amp;n ) const {
4199   return (&amp;n == this);          // Always fail except on self
4200 }
4201 
4202 //------------------------------Identity---------------------------------------
4203 Node* MergeMemNode::Identity(PhaseTransform *phase) {
4204   // Identity if this merge point does not record any interesting memory
4205   // disambiguations.
4206   Node* base_mem = base_memory();
4207   Node* empty_mem = empty_memory();
4208   if (base_mem != empty_mem) {  // Memory path is not dead?
4209     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4210       Node* mem = in(i);
4211       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4212         return this;            // Many memory splits; no change
4213       }
4214     }
4215   }
4216   return base_mem;              // No memory splits; ID on the one true input
4217 }
4218 
4219 //------------------------------Ideal------------------------------------------
4220 // This method is invoked recursively on chains of MergeMem nodes
4221 Node *MergeMemNode::Ideal(PhaseGVN *phase, bool can_reshape) {
4222   // Remove chain'd MergeMems
4223   //
4224   // This is delicate, because the each "in(i)" (i &gt;= Raw) is interpreted
4225   // relative to the "in(Bot)".  Since we are patching both at the same time,
4226   // we have to be careful to read each "in(i)" relative to the old "in(Bot)",
4227   // but rewrite each "in(i)" relative to the new "in(Bot)".
4228   Node *progress = NULL;
4229 
4230 
4231   Node* old_base = base_memory();
4232   Node* empty_mem = empty_memory();
4233   if (old_base == empty_mem)
4234     return NULL; // Dead memory path.
4235 
4236   MergeMemNode* old_mbase;
4237   if (old_base != NULL &amp;&amp; old_base-&gt;is_MergeMem())
4238     old_mbase = old_base-&gt;as_MergeMem();
4239   else
4240     old_mbase = NULL;
4241   Node* new_base = old_base;
4242 
4243   // simplify stacked MergeMems in base memory
4244   if (old_mbase)  new_base = old_mbase-&gt;base_memory();
4245 
4246   // the base memory might contribute new slices beyond my req()
4247   if (old_mbase)  grow_to_match(old_mbase);
4248 
4249   // Look carefully at the base node if it is a phi.
4250   PhiNode* phi_base;
4251   if (new_base != NULL &amp;&amp; new_base-&gt;is_Phi())
4252     phi_base = new_base-&gt;as_Phi();
4253   else
4254     phi_base = NULL;
4255 
4256   Node*    phi_reg = NULL;
4257   uint     phi_len = (uint)-1;
4258   if (phi_base != NULL &amp;&amp; !phi_base-&gt;is_copy()) {
4259     // do not examine phi if degraded to a copy
4260     phi_reg = phi_base-&gt;region();
4261     phi_len = phi_base-&gt;req();
4262     // see if the phi is unfinished
4263     for (uint i = 1; i &lt; phi_len; i++) {
4264       if (phi_base-&gt;in(i) == NULL) {
4265         // incomplete phi; do not look at it yet!
4266         phi_reg = NULL;
4267         phi_len = (uint)-1;
4268         break;
4269       }
4270     }
4271   }
4272 
4273   // Note:  We do not call verify_sparse on entry, because inputs
4274   // can normalize to the base_memory via subsume_node or similar
4275   // mechanisms.  This method repairs that damage.
4276 
4277   assert(!old_mbase || old_mbase-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4278 
4279   // Look at each slice.
4280   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4281     Node* old_in = in(i);
4282     // calculate the old memory value
4283     Node* old_mem = old_in;
4284     if (old_mem == empty_mem)  old_mem = old_base;
4285     assert(old_mem == memory_at(i), "");
4286 
4287     // maybe update (reslice) the old memory value
4288 
4289     // simplify stacked MergeMems
4290     Node* new_mem = old_mem;
4291     MergeMemNode* old_mmem;
4292     if (old_mem != NULL &amp;&amp; old_mem-&gt;is_MergeMem())
4293       old_mmem = old_mem-&gt;as_MergeMem();
4294     else
4295       old_mmem = NULL;
4296     if (old_mmem == this) {
4297       // This can happen if loops break up and safepoints disappear.
4298       // A merge of BotPtr (default) with a RawPtr memory derived from a
4299       // safepoint can be rewritten to a merge of the same BotPtr with
4300       // the BotPtr phi coming into the loop.  If that phi disappears
4301       // also, we can end up with a self-loop of the mergemem.
4302       // In general, if loops degenerate and memory effects disappear,
4303       // a mergemem can be left looking at itself.  This simply means
4304       // that the mergemem's default should be used, since there is
4305       // no longer any apparent effect on this slice.
4306       // Note: If a memory slice is a MergeMem cycle, it is unreachable
4307       //       from start.  Update the input to TOP.
4308       new_mem = (new_base == this || new_base == empty_mem)? empty_mem : new_base;
4309     }
4310     else if (old_mmem != NULL) {
4311       new_mem = old_mmem-&gt;memory_at(i);
4312     }
4313     // else preceding memory was not a MergeMem
4314 
4315     // replace equivalent phis (unfortunately, they do not GVN together)
4316     if (new_mem != NULL &amp;&amp; new_mem != new_base &amp;&amp;
4317         new_mem-&gt;req() == phi_len &amp;&amp; new_mem-&gt;in(0) == phi_reg) {
4318       if (new_mem-&gt;is_Phi()) {
4319         PhiNode* phi_mem = new_mem-&gt;as_Phi();
4320         for (uint i = 1; i &lt; phi_len; i++) {
4321           if (phi_base-&gt;in(i) != phi_mem-&gt;in(i)) {
4322             phi_mem = NULL;
4323             break;
4324           }
4325         }
4326         if (phi_mem != NULL) {
4327           // equivalent phi nodes; revert to the def
4328           new_mem = new_base;
4329         }
4330       }
4331     }
4332 
4333     // maybe store down a new value
4334     Node* new_in = new_mem;
4335     if (new_in == new_base)  new_in = empty_mem;
4336 
4337     if (new_in != old_in) {
4338       // Warning:  Do not combine this "if" with the previous "if"
4339       // A memory slice might have be be rewritten even if it is semantically
4340       // unchanged, if the base_memory value has changed.
4341       set_req(i, new_in);
4342       progress = this;          // Report progress
4343     }
4344   }
4345 
4346   if (new_base != old_base) {
4347     set_req(Compile::AliasIdxBot, new_base);
4348     // Don't use set_base_memory(new_base), because we need to update du.
4349     assert(base_memory() == new_base, "");
4350     progress = this;
4351   }
4352 
4353   if( base_memory() == this ) {
4354     // a self cycle indicates this memory path is dead
4355     set_req(Compile::AliasIdxBot, empty_mem);
4356   }
4357 
4358   // Resolve external cycles by calling Ideal on a MergeMem base_memory
4359   // Recursion must occur after the self cycle check above
4360   if( base_memory()-&gt;is_MergeMem() ) {
4361     MergeMemNode *new_mbase = base_memory()-&gt;as_MergeMem();
4362     Node *m = phase-&gt;transform(new_mbase);  // Rollup any cycles
4363     if( m != NULL &amp;&amp; (m-&gt;is_top() ||
4364         m-&gt;is_MergeMem() &amp;&amp; m-&gt;as_MergeMem()-&gt;base_memory() == empty_mem) ) {
4365       // propagate rollup of dead cycle to self
4366       set_req(Compile::AliasIdxBot, empty_mem);
4367     }
4368   }
4369 
4370   if( base_memory() == empty_mem ) {
4371     progress = this;
4372     // Cut inputs during Parse phase only.
4373     // During Optimize phase a dead MergeMem node will be subsumed by Top.
4374     if( !can_reshape ) {
4375       for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4376         if( in(i) != empty_mem ) { set_req(i, empty_mem); }
4377       }
4378     }
4379   }
4380 
4381   if( !progress &amp;&amp; base_memory()-&gt;is_Phi() &amp;&amp; can_reshape ) {
4382     // Check if PhiNode::Ideal's "Split phis through memory merges"
4383     // transform should be attempted. Look for this-&gt;phi-&gt;this cycle.
4384     uint merge_width = req();
4385     if (merge_width &gt; Compile::AliasIdxRaw) {
4386       PhiNode* phi = base_memory()-&gt;as_Phi();
4387       for( uint i = 1; i &lt; phi-&gt;req(); ++i ) {// For all paths in
4388         if (phi-&gt;in(i) == this) {
4389           phase-&gt;is_IterGVN()-&gt;_worklist.push(phi);
4390           break;
4391         }
4392       }
4393     }
4394   }
4395 
4396   assert(progress || verify_sparse(), "please, no dups of base");
4397   return progress;
4398 }
4399 
4400 //-------------------------set_base_memory-------------------------------------
4401 void MergeMemNode::set_base_memory(Node *new_base) {
4402   Node* empty_mem = empty_memory();
4403   set_req(Compile::AliasIdxBot, new_base);
4404   assert(memory_at(req()) == new_base, "must set default memory");
4405   // Clear out other occurrences of new_base:
4406   if (new_base != empty_mem) {
4407     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4408       if (in(i) == new_base)  set_req(i, empty_mem);
4409     }
4410   }
4411 }
4412 
4413 //------------------------------out_RegMask------------------------------------
4414 const RegMask &amp;MergeMemNode::out_RegMask() const {
4415   return RegMask::Empty;
4416 }
4417 
4418 //------------------------------dump_spec--------------------------------------
4419 #ifndef PRODUCT
4420 void MergeMemNode::dump_spec(outputStream *st) const {
4421   st-&gt;print(" {");
4422   Node* base_mem = base_memory();
4423   for( uint i = Compile::AliasIdxRaw; i &lt; req(); i++ ) {
4424     Node* mem = memory_at(i);
4425     if (mem == base_mem) { st-&gt;print(" -"); continue; }
4426     st-&gt;print( " N%d:", mem-&gt;_idx );
4427     Compile::current()-&gt;get_adr_type(i)-&gt;dump_on(st);
4428   }
4429   st-&gt;print(" }");
4430 }
4431 #endif // !PRODUCT
4432 
4433 
4434 #ifdef ASSERT
4435 static bool might_be_same(Node* a, Node* b) {
4436   if (a == b)  return true;
4437   if (!(a-&gt;is_Phi() || b-&gt;is_Phi()))  return false;
4438   // phis shift around during optimization
4439   return true;  // pretty stupid...
4440 }
4441 
4442 // verify a narrow slice (either incoming or outgoing)
4443 static void verify_memory_slice(const MergeMemNode* m, int alias_idx, Node* n) {
4444   if (!VerifyAliases)       return;  // don't bother to verify unless requested
4445   if (is_error_reported())  return;  // muzzle asserts when debugging an error
4446   if (Node::in_dump())      return;  // muzzle asserts when printing
4447   assert(alias_idx &gt;= Compile::AliasIdxRaw, "must not disturb base_memory or sentinel");
4448   assert(n != NULL, "");
4449   // Elide intervening MergeMem's
4450   while (n-&gt;is_MergeMem()) {
4451     n = n-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
4452   }
4453   Compile* C = Compile::current();
4454   const TypePtr* n_adr_type = n-&gt;adr_type();
4455   if (n == m-&gt;empty_memory()) {
4456     // Implicit copy of base_memory()
4457   } else if (n_adr_type != TypePtr::BOTTOM) {
4458     assert(n_adr_type != NULL, "new memory must have a well-defined adr_type");
4459     assert(C-&gt;must_alias(n_adr_type, alias_idx), "new memory must match selected slice");
4460   } else {
4461     // A few places like make_runtime_call "know" that VM calls are narrow,
4462     // and can be used to update only the VM bits stored as TypeRawPtr::BOTTOM.
4463     bool expected_wide_mem = false;
4464     if (n == m-&gt;base_memory()) {
4465       expected_wide_mem = true;
4466     } else if (alias_idx == Compile::AliasIdxRaw ||
4467                n == m-&gt;memory_at(Compile::AliasIdxRaw)) {
4468       expected_wide_mem = true;
4469     } else if (!C-&gt;alias_type(alias_idx)-&gt;is_rewritable()) {
4470       // memory can "leak through" calls on channels that
4471       // are write-once.  Allow this also.
4472       expected_wide_mem = true;
4473     }
4474     assert(expected_wide_mem, "expected narrow slice replacement");
4475   }
4476 }
4477 #else // !ASSERT
4478 #define verify_memory_slice(m,i,n) (void)(0)  // PRODUCT version is no-op
4479 #endif
4480 
4481 
4482 //-----------------------------memory_at---------------------------------------
4483 Node* MergeMemNode::memory_at(uint alias_idx) const {
4484   assert(alias_idx &gt;= Compile::AliasIdxRaw ||
4485          alias_idx == Compile::AliasIdxBot &amp;&amp; Compile::current()-&gt;AliasLevel() == 0,
4486          "must avoid base_memory and AliasIdxTop");
4487 
4488   // Otherwise, it is a narrow slice.
4489   Node* n = alias_idx &lt; req() ? in(alias_idx) : empty_memory();
4490   Compile *C = Compile::current();
4491   if (is_empty_memory(n)) {
4492     // the array is sparse; empty slots are the "top" node
4493     n = base_memory();
4494     assert(Node::in_dump()
4495            || n == NULL || n-&gt;bottom_type() == Type::TOP
4496            || n-&gt;adr_type() == NULL // address is TOP
4497            || n-&gt;adr_type() == TypePtr::BOTTOM
4498            || n-&gt;adr_type() == TypeRawPtr::BOTTOM
4499            || Compile::current()-&gt;AliasLevel() == 0,
4500            "must be a wide memory");
4501     // AliasLevel == 0 if we are organizing the memory states manually.
4502     // See verify_memory_slice for comments on TypeRawPtr::BOTTOM.
4503   } else {
4504     // make sure the stored slice is sane
4505     #ifdef ASSERT
4506     if (is_error_reported() || Node::in_dump()) {
4507     } else if (might_be_same(n, base_memory())) {
4508       // Give it a pass:  It is a mostly harmless repetition of the base.
4509       // This can arise normally from node subsumption during optimization.
4510     } else {
4511       verify_memory_slice(this, alias_idx, n);
4512     }
4513     #endif
4514   }
4515   return n;
4516 }
4517 
4518 //---------------------------set_memory_at-------------------------------------
4519 void MergeMemNode::set_memory_at(uint alias_idx, Node *n) {
4520   verify_memory_slice(this, alias_idx, n);
4521   Node* empty_mem = empty_memory();
4522   if (n == base_memory())  n = empty_mem;  // collapse default
4523   uint need_req = alias_idx+1;
4524   if (req() &lt; need_req) {
4525     if (n == empty_mem)  return;  // already the default, so do not grow me
4526     // grow the sparse array
4527     do {
4528       add_req(empty_mem);
4529     } while (req() &lt; need_req);
4530   }
4531   set_req( alias_idx, n );
4532 }
4533 
4534 
4535 
4536 //--------------------------iteration_setup------------------------------------
4537 void MergeMemNode::iteration_setup(const MergeMemNode* other) {
4538   if (other != NULL) {
4539     grow_to_match(other);
4540     // invariant:  the finite support of mm2 is within mm-&gt;req()
4541     #ifdef ASSERT
4542     for (uint i = req(); i &lt; other-&gt;req(); i++) {
4543       assert(other-&gt;is_empty_memory(other-&gt;in(i)), "slice left uncovered");
4544     }
4545     #endif
4546   }
4547   // Replace spurious copies of base_memory by top.
4548   Node* base_mem = base_memory();
4549   if (base_mem != NULL &amp;&amp; !base_mem-&gt;is_top()) {
4550     for (uint i = Compile::AliasIdxBot+1, imax = req(); i &lt; imax; i++) {
4551       if (in(i) == base_mem)
4552         set_req(i, empty_memory());
4553     }
4554   }
4555 }
4556 
4557 //---------------------------grow_to_match-------------------------------------
4558 void MergeMemNode::grow_to_match(const MergeMemNode* other) {
4559   Node* empty_mem = empty_memory();
4560   assert(other-&gt;is_empty_memory(empty_mem), "consistent sentinels");
4561   // look for the finite support of the other memory
4562   for (uint i = other-&gt;req(); --i &gt;= req(); ) {
4563     if (other-&gt;in(i) != empty_mem) {
4564       uint new_len = i+1;
4565       while (req() &lt; new_len)  add_req(empty_mem);
4566       break;
4567     }
4568   }
4569 }
4570 
4571 //---------------------------verify_sparse-------------------------------------
4572 #ifndef PRODUCT
4573 bool MergeMemNode::verify_sparse() const {
4574   assert(is_empty_memory(make_empty_memory()), "sane sentinel");
4575   Node* base_mem = base_memory();
4576   // The following can happen in degenerate cases, since empty==top.
4577   if (is_empty_memory(base_mem))  return true;
4578   for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4579     assert(in(i) != NULL, "sane slice");
4580     if (in(i) == base_mem)  return false;  // should have been the sentinel value!
4581   }
4582   return true;
4583 }
4584 
4585 bool MergeMemStream::match_memory(Node* mem, const MergeMemNode* mm, int idx) {
4586   Node* n;
4587   n = mm-&gt;in(idx);
4588   if (mem == n)  return true;  // might be empty_memory()
4589   n = (idx == Compile::AliasIdxBot)? mm-&gt;base_memory(): mm-&gt;memory_at(idx);
4590   if (mem == n)  return true;
4591   while (n-&gt;is_Phi() &amp;&amp; (n = n-&gt;as_Phi()-&gt;is_copy()) != NULL) {
4592     if (mem == n)  return true;
4593     if (n == NULL)  break;
4594   }
4595   return false;
4596 }
4597 #endif // !PRODUCT
</pre></body></html>
