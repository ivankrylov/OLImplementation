<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/oops/instanceKlass.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/javaClasses.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/verifier.hpp"
  29 #include "classfile/vmSymbols.hpp"
  30 #include "compiler/compileBroker.hpp"
  31 #include "gc_implementation/shared/markSweep.inline.hpp"
  32 #include "gc_interface/collectedHeap.inline.hpp"
  33 #include "interpreter/oopMapCache.hpp"
  34 #include "interpreter/rewriter.hpp"
  35 #include "jvmtifiles/jvmti.h"
  36 #include "memory/genOopClosures.inline.hpp"
  37 #include "memory/heapInspection.hpp"
  38 #include "memory/iterator.inline.hpp"
  39 #include "memory/metadataFactory.hpp"
  40 #include "memory/oopFactory.hpp"
  41 #include "oops/fieldStreams.hpp"
  42 #include "oops/instanceClassLoaderKlass.hpp"
  43 #include "oops/instanceKlass.hpp"
  44 #include "oops/instanceMirrorKlass.hpp"
  45 #include "oops/instanceOop.hpp"
  46 #include "oops/klass.inline.hpp"
  47 #include "oops/method.hpp"
  48 #include "oops/oop.inline.hpp"
  49 #include "oops/symbol.hpp"
  50 #include "prims/jvmtiExport.hpp"
  51 #include "prims/jvmtiRedefineClassesTrace.hpp"
  52 #include "prims/jvmtiRedefineClasses.hpp"
  53 #include "prims/jvmtiThreadState.hpp"
  54 #include "prims/methodComparator.hpp"
  55 #include "runtime/fieldDescriptor.hpp"
  56 #include "runtime/handles.inline.hpp"
  57 #include "runtime/javaCalls.hpp"
  58 #include "runtime/mutexLocker.hpp"
  59 #include "runtime/orderAccess.inline.hpp"
  60 #include "runtime/thread.inline.hpp"
  61 #include "services/classLoadingService.hpp"
  62 #include "services/threadService.hpp"
  63 #include "utilities/dtrace.hpp"
  64 #include "utilities/macros.hpp"
  65 #if INCLUDE_ALL_GCS
  66 #include "gc_implementation/concurrentMarkSweep/cmsOopClosures.inline.hpp"
  67 #include "gc_implementation/g1/g1CollectedHeap.inline.hpp"
  68 #include "gc_implementation/g1/g1OopClosures.inline.hpp"
  69 #include "gc_implementation/g1/g1RemSet.inline.hpp"
  70 #include "gc_implementation/g1/heapRegionManager.inline.hpp"
  71 #include "gc_implementation/parNew/parOopClosures.inline.hpp"
  72 #include "gc_implementation/parallelScavenge/parallelScavengeHeap.inline.hpp"
  73 #include "gc_implementation/parallelScavenge/psPromotionManager.inline.hpp"
  74 #include "gc_implementation/parallelScavenge/psScavenge.inline.hpp"
  75 #include "oops/oop.pcgc.inline.hpp"
  76 #endif // INCLUDE_ALL_GCS
  77 #ifdef COMPILER1
  78 #include "c1/c1_Compiler.hpp"
  79 #endif
  80 
  81 PRAGMA_FORMAT_MUTE_WARNINGS_FOR_GCC
  82 
  83 #ifdef DTRACE_ENABLED
  84 
  85 #ifndef USDT2
  86 
  87 HS_DTRACE_PROBE_DECL4(hotspot, class__initialization__required,
  88   char*, intptr_t, oop, intptr_t);
  89 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__recursive,
  90   char*, intptr_t, oop, intptr_t, int);
  91 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__concurrent,
  92   char*, intptr_t, oop, intptr_t, int);
  93 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__erroneous,
  94   char*, intptr_t, oop, intptr_t, int);
  95 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__super__failed,
  96   char*, intptr_t, oop, intptr_t, int);
  97 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__clinit,
  98   char*, intptr_t, oop, intptr_t, int);
  99 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__error,
 100   char*, intptr_t, oop, intptr_t, int);
 101 HS_DTRACE_PROBE_DECL5(hotspot, class__initialization__end,
 102   char*, intptr_t, oop, intptr_t, int);
 103 
 104 #define DTRACE_CLASSINIT_PROBE(type, clss, thread_type)          \
 105   {                                                              \
 106     char* data = NULL;                                           \
 107     int len = 0;                                                 \
 108     Symbol* name = (clss)-&gt;name();                               \
 109     if (name != NULL) {                                          \
 110       data = (char*)name-&gt;bytes();                               \
 111       len = name-&gt;utf8_length();                                 \
 112     }                                                            \
 113     HS_DTRACE_PROBE4(hotspot, class__initialization__##type,     \
 114       data, len, (void *)(clss)-&gt;class_loader(), thread_type);           \
 115   }
 116 
 117 #define DTRACE_CLASSINIT_PROBE_WAIT(type, clss, thread_type, wait) \
 118   {                                                              \
 119     char* data = NULL;                                           \
 120     int len = 0;                                                 \
 121     Symbol* name = (clss)-&gt;name();                               \
 122     if (name != NULL) {                                          \
 123       data = (char*)name-&gt;bytes();                               \
 124       len = name-&gt;utf8_length();                                 \
 125     }                                                            \
 126     HS_DTRACE_PROBE5(hotspot, class__initialization__##type,     \
 127       data, len, (void *)(clss)-&gt;class_loader(), thread_type, wait);     \
 128   }
 129 #else /* USDT2 */
 130 
 131 #define HOTSPOT_CLASS_INITIALIZATION_required HOTSPOT_CLASS_INITIALIZATION_REQUIRED
 132 #define HOTSPOT_CLASS_INITIALIZATION_recursive HOTSPOT_CLASS_INITIALIZATION_RECURSIVE
 133 #define HOTSPOT_CLASS_INITIALIZATION_concurrent HOTSPOT_CLASS_INITIALIZATION_CONCURRENT
 134 #define HOTSPOT_CLASS_INITIALIZATION_erroneous HOTSPOT_CLASS_INITIALIZATION_ERRONEOUS
 135 #define HOTSPOT_CLASS_INITIALIZATION_super__failed HOTSPOT_CLASS_INITIALIZATION_SUPER_FAILED
 136 #define HOTSPOT_CLASS_INITIALIZATION_clinit HOTSPOT_CLASS_INITIALIZATION_CLINIT
 137 #define HOTSPOT_CLASS_INITIALIZATION_error HOTSPOT_CLASS_INITIALIZATION_ERROR
 138 #define HOTSPOT_CLASS_INITIALIZATION_end HOTSPOT_CLASS_INITIALIZATION_END
 139 #define DTRACE_CLASSINIT_PROBE(type, clss, thread_type)          \
 140   {                                                              \
 141     char* data = NULL;                                           \
 142     int len = 0;                                                 \
 143     Symbol* name = (clss)-&gt;name();                               \
 144     if (name != NULL) {                                          \
 145       data = (char*)name-&gt;bytes();                               \
 146       len = name-&gt;utf8_length();                                 \
 147     }                                                            \
 148     HOTSPOT_CLASS_INITIALIZATION_##type(                         \
 149       data, len, (clss)-&gt;class_loader(), thread_type);           \
 150   }
 151 
 152 #define DTRACE_CLASSINIT_PROBE_WAIT(type, clss, thread_type, wait) \
 153   {                                                              \
 154     char* data = NULL;                                           \
 155     int len = 0;                                                 \
 156     Symbol* name = (clss)-&gt;name();                               \
 157     if (name != NULL) {                                          \
 158       data = (char*)name-&gt;bytes();                               \
 159       len = name-&gt;utf8_length();                                 \
 160     }                                                            \
 161     HOTSPOT_CLASS_INITIALIZATION_##type(                         \
 162       data, len, (clss)-&gt;class_loader(), thread_type, wait);     \
 163   }
 164 #endif /* USDT2 */
 165 
 166 #else //  ndef DTRACE_ENABLED
 167 
 168 #define DTRACE_CLASSINIT_PROBE(type, clss, thread_type)
 169 #define DTRACE_CLASSINIT_PROBE_WAIT(type, clss, thread_type, wait)
 170 
 171 #endif //  ndef DTRACE_ENABLED
 172 
 173 volatile int InstanceKlass::_total_instanceKlass_count = 0;
 174 
 175 InstanceKlass* InstanceKlass::allocate_instance_klass(
 176                                               ClassLoaderData* loader_data,
 177                                               int vtable_len,
 178                                               int itable_len,
 179                                               int static_field_size,
 180                                               int nonstatic_oop_map_size,
 181                                               ReferenceType rt,
 182                                               AccessFlags access_flags,
 183                                               Symbol* name,
 184                                               Klass* super_klass,
 185                                               bool is_anonymous,
 186                                               TRAPS) {
 187 
 188   int size = InstanceKlass::size(vtable_len, itable_len, nonstatic_oop_map_size,
 189                                  access_flags.is_interface(), is_anonymous);
 190 
 191   // Allocation
 192   InstanceKlass* ik;
 193   if (rt == REF_NONE) {
 194     if (name == vmSymbols::java_lang_Class()) {
 195       ik = new (loader_data, size, THREAD) InstanceMirrorKlass(
 196         vtable_len, itable_len, static_field_size, nonstatic_oop_map_size, rt,
 197         access_flags, is_anonymous);
 198     } else if (name == vmSymbols::java_lang_ClassLoader() ||
 199           (SystemDictionary::ClassLoader_klass_loaded() &amp;&amp;
 200           super_klass != NULL &amp;&amp;
 201           super_klass-&gt;is_subtype_of(SystemDictionary::ClassLoader_klass()))) {
 202       ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(
 203         vtable_len, itable_len, static_field_size, nonstatic_oop_map_size, rt,
 204         access_flags, is_anonymous);
 205     } else {
 206       // normal class
 207       ik = new (loader_data, size, THREAD) InstanceKlass(
 208         vtable_len, itable_len, static_field_size, nonstatic_oop_map_size, rt,
 209         access_flags, is_anonymous);
 210     }
 211   } else {
 212     // reference klass
 213     ik = new (loader_data, size, THREAD) InstanceRefKlass(
 214         vtable_len, itable_len, static_field_size, nonstatic_oop_map_size, rt,
 215         access_flags, is_anonymous);
 216   }
 217 
 218   // Check for pending exception before adding to the loader data and incrementing
 219   // class count.  Can get OOM here.
 220   if (HAS_PENDING_EXCEPTION) {
 221     return NULL;
 222   }
 223 
 224   // Add all classes to our internal class loader list here,
 225   // including classes in the bootstrap (NULL) class loader.
 226   loader_data-&gt;add_class(ik);
 227 
 228   Atomic::inc(&amp;_total_instanceKlass_count);
 229   return ik;
 230 }
 231 
 232 
 233 // copy method ordering from resource area to Metaspace
 234 void InstanceKlass::copy_method_ordering(intArray* m, TRAPS) {
 235   if (m != NULL) {
 236     // allocate a new array and copy contents (memcpy?)
 237     _method_ordering = MetadataFactory::new_array&lt;int&gt;(class_loader_data(), m-&gt;length(), CHECK);
 238     for (int i = 0; i &lt; m-&gt;length(); i++) {
 239       _method_ordering-&gt;at_put(i, m-&gt;at(i));
 240     }
 241   } else {
 242     _method_ordering = Universe::the_empty_int_array();
 243   }
 244 }
 245 
 246 // create a new array of vtable_indices for default methods
 247 Array&lt;int&gt;* InstanceKlass::create_new_default_vtable_indices(int len, TRAPS) {
 248   Array&lt;int&gt;* vtable_indices = MetadataFactory::new_array&lt;int&gt;(class_loader_data(), len, CHECK_NULL);
 249   assert(default_vtable_indices() == NULL, "only create once");
 250   set_default_vtable_indices(vtable_indices);
 251   return vtable_indices;
 252 }
 253 
 254 InstanceKlass::InstanceKlass(int vtable_len,
 255                              int itable_len,
 256                              int static_field_size,
 257                              int nonstatic_oop_map_size,
 258                              ReferenceType rt,
 259                              AccessFlags access_flags,
 260                              bool is_anonymous) {
 261   No_Safepoint_Verifier no_safepoint; // until k becomes parsable
 262 
 263   int iksize = InstanceKlass::size(vtable_len, itable_len, nonstatic_oop_map_size,
 264                                    access_flags.is_interface(), is_anonymous);
 265 
 266   set_vtable_length(vtable_len);
 267   set_itable_length(itable_len);
 268   set_static_field_size(static_field_size);
 269   set_nonstatic_oop_map_size(nonstatic_oop_map_size);
 270   set_access_flags(access_flags);
 271   _misc_flags = 0;  // initialize to zero
 272   set_is_anonymous(is_anonymous);
 273   assert(size() == iksize, "wrong size for object");
 274 
 275   set_array_klasses(NULL);
 276   set_methods(NULL);
 277   set_method_ordering(NULL);
 278   set_default_methods(NULL);
 279   set_default_vtable_indices(NULL);
 280   set_local_interfaces(NULL);
 281   set_transitive_interfaces(NULL);
 282   init_implementor();
 283   set_fields(NULL, 0);
 284   set_constants(NULL);
 285   set_class_loader_data(NULL);
 286   set_source_file_name_index(0);
 287   set_source_debug_extension(NULL, 0);
 288   set_array_name(NULL);
 289   set_inner_classes(NULL);
 290   set_static_oop_field_count(0);
 291   set_nonstatic_field_size(0);
 292   set_is_marked_dependent(false);
 293   set_has_unloaded_dependent(false);
 294   set_init_state(InstanceKlass::allocated);
 295   set_init_thread(NULL);
 296   set_reference_type(rt);
 297   set_oop_map_cache(NULL);
 298   set_jni_ids(NULL);
 299   set_osr_nmethods_head(NULL);
 300   set_breakpoints(NULL);
 301   init_previous_versions();
 302   set_generic_signature_index(0);
 303   release_set_methods_jmethod_ids(NULL);
 304   set_annotations(NULL);
 305   set_jvmti_cached_class_field_map(NULL);
 306   set_initial_method_idnum(0);
 307   _dependencies = NULL;
 308   set_jvmti_cached_class_field_map(NULL);
 309   set_cached_class_file(NULL);
 310   set_initial_method_idnum(0);
 311   set_minor_version(0);
 312   set_major_version(0);
 313   NOT_PRODUCT(_verify_count = 0;)
 314 
 315   // initialize the non-header words to zero
 316   intptr_t* p = (intptr_t*)this;
 317   for (int index = InstanceKlass::header_size(); index &lt; iksize; index++) {
 318     p[index] = NULL_WORD;
 319   }
 320 
 321   // Set temporary value until parseClassFile updates it with the real instance
 322   // size.
 323   set_layout_helper(Klass::instance_layout_helper(0, true));
 324 }
 325 
 326 
 327 void InstanceKlass::deallocate_methods(ClassLoaderData* loader_data,
 328                                        Array&lt;Method*&gt;* methods) {
 329   if (methods != NULL &amp;&amp; methods != Universe::the_empty_method_array() &amp;&amp;
 330       !methods-&gt;is_shared()) {
 331     for (int i = 0; i &lt; methods-&gt;length(); i++) {
 332       Method* method = methods-&gt;at(i);
 333       if (method == NULL) continue;  // maybe null if error processing
 334       // Only want to delete methods that are not executing for RedefineClasses.
 335       // The previous version will point to them so they're not totally dangling
 336       assert (!method-&gt;on_stack(), "shouldn't be called with methods on stack");
 337       MetadataFactory::free_metadata(loader_data, method);
 338     }
 339     MetadataFactory::free_array&lt;Method*&gt;(loader_data, methods);
 340   }
 341 }
 342 
 343 void InstanceKlass::deallocate_interfaces(ClassLoaderData* loader_data,
 344                                           Klass* super_klass,
 345                                           Array&lt;Klass*&gt;* local_interfaces,
 346                                           Array&lt;Klass*&gt;* transitive_interfaces) {
 347   // Only deallocate transitive interfaces if not empty, same as super class
 348   // or same as local interfaces.  See code in parseClassFile.
 349   Array&lt;Klass*&gt;* ti = transitive_interfaces;
 350   if (ti != Universe::the_empty_klass_array() &amp;&amp; ti != local_interfaces) {
 351     // check that the interfaces don't come from super class
 352     Array&lt;Klass*&gt;* sti = (super_klass == NULL) ? NULL :
 353                     InstanceKlass::cast(super_klass)-&gt;transitive_interfaces();
 354     if (ti != sti &amp;&amp; ti != NULL &amp;&amp; !ti-&gt;is_shared()) {
 355       MetadataFactory::free_array&lt;Klass*&gt;(loader_data, ti);
 356     }
 357   }
 358 
 359   // local interfaces can be empty
 360   if (local_interfaces != Universe::the_empty_klass_array() &amp;&amp;
 361       local_interfaces != NULL &amp;&amp; !local_interfaces-&gt;is_shared()) {
 362     MetadataFactory::free_array&lt;Klass*&gt;(loader_data, local_interfaces);
 363   }
 364 }
 365 
 366 // This function deallocates the metadata and C heap pointers that the
 367 // InstanceKlass points to.
 368 void InstanceKlass::deallocate_contents(ClassLoaderData* loader_data) {
 369 
 370   // Orphan the mirror first, CMS thinks it's still live.
 371   if (java_mirror() != NULL) {
 372     java_lang_Class::set_klass(java_mirror(), NULL);
 373   }
 374 
 375   // Need to take this class off the class loader data list.
 376   loader_data-&gt;remove_class(this);
 377 
 378   // The array_klass for this class is created later, after error handling.
 379   // For class redefinition, we keep the original class so this scratch class
 380   // doesn't have an array class.  Either way, assert that there is nothing
 381   // to deallocate.
 382   assert(array_klasses() == NULL, "array classes shouldn't be created for this class yet");
 383 
 384   // Release C heap allocated data that this might point to, which includes
 385   // reference counting symbol names.
 386   release_C_heap_structures();
 387 
 388   deallocate_methods(loader_data, methods());
 389   set_methods(NULL);
 390 
 391   if (method_ordering() != NULL &amp;&amp;
 392       method_ordering() != Universe::the_empty_int_array() &amp;&amp;
 393       !method_ordering()-&gt;is_shared()) {
 394     MetadataFactory::free_array&lt;int&gt;(loader_data, method_ordering());
 395   }
 396   set_method_ordering(NULL);
 397 
 398   // default methods can be empty
 399   if (default_methods() != NULL &amp;&amp;
 400       default_methods() != Universe::the_empty_method_array() &amp;&amp;
 401       !default_methods()-&gt;is_shared()) {
 402     MetadataFactory::free_array&lt;Method*&gt;(loader_data, default_methods());
 403   }
 404   // Do NOT deallocate the default methods, they are owned by superinterfaces.
 405   set_default_methods(NULL);
 406 
 407   // default methods vtable indices can be empty
 408   if (default_vtable_indices() != NULL &amp;&amp;
 409       !default_vtable_indices()-&gt;is_shared()) {
 410     MetadataFactory::free_array&lt;int&gt;(loader_data, default_vtable_indices());
 411   }
 412   set_default_vtable_indices(NULL);
 413 
 414 
 415   // This array is in Klass, but remove it with the InstanceKlass since
 416   // this place would be the only caller and it can share memory with transitive
 417   // interfaces.
 418   if (secondary_supers() != NULL &amp;&amp;
 419       secondary_supers() != Universe::the_empty_klass_array() &amp;&amp;
 420       secondary_supers() != transitive_interfaces() &amp;&amp;
 421       !secondary_supers()-&gt;is_shared()) {
 422     MetadataFactory::free_array&lt;Klass*&gt;(loader_data, secondary_supers());
 423   }
 424   set_secondary_supers(NULL);
 425 
 426   deallocate_interfaces(loader_data, super(), local_interfaces(), transitive_interfaces());
 427   set_transitive_interfaces(NULL);
 428   set_local_interfaces(NULL);
 429 
 430   if (fields() != NULL &amp;&amp; !fields()-&gt;is_shared()) {
 431     MetadataFactory::free_array&lt;jushort&gt;(loader_data, fields());
 432   }
 433   set_fields(NULL, 0);
 434 
 435   // If a method from a redefined class is using this constant pool, don't
 436   // delete it, yet.  The new class's previous version will point to this.
 437   if (constants() != NULL) {
 438     assert (!constants()-&gt;on_stack(), "shouldn't be called if anything is onstack");
 439     if (!constants()-&gt;is_shared()) {
 440       MetadataFactory::free_metadata(loader_data, constants());
 441     }
 442     set_constants(NULL);
 443   }
 444 
 445   if (inner_classes() != NULL &amp;&amp;
 446       inner_classes() != Universe::the_empty_short_array() &amp;&amp;
 447       !inner_classes()-&gt;is_shared()) {
 448     MetadataFactory::free_array&lt;jushort&gt;(loader_data, inner_classes());
 449   }
 450   set_inner_classes(NULL);
 451 
 452   // We should deallocate the Annotations instance if it's not in shared spaces.
 453   if (annotations() != NULL &amp;&amp; !annotations()-&gt;is_shared()) {
 454     MetadataFactory::free_metadata(loader_data, annotations());
 455   }
 456   set_annotations(NULL);
 457 }
 458 
 459 bool InstanceKlass::should_be_initialized() const {
 460   return !is_initialized();
 461 }
 462 
 463 klassVtable* InstanceKlass::vtable() const {
 464   return new klassVtable(this, start_of_vtable(), vtable_length() / vtableEntry::size());
 465 }
 466 
 467 klassItable* InstanceKlass::itable() const {
 468   return new klassItable(instanceKlassHandle(this));
 469 }
 470 
 471 void InstanceKlass::eager_initialize(Thread *thread) {
 472   if (!EagerInitialization) return;
 473 
 474   if (this-&gt;is_not_initialized()) {
 475     // abort if the the class has a class initializer
 476     if (this-&gt;class_initializer() != NULL) return;
 477 
 478     // abort if it is java.lang.Object (initialization is handled in genesis)
 479     Klass* super = this-&gt;super();
 480     if (super == NULL) return;
 481 
 482     // abort if the super class should be initialized
 483     if (!InstanceKlass::cast(super)-&gt;is_initialized()) return;
 484 
 485     // call body to expose the this pointer
 486     instanceKlassHandle this_oop(thread, this);
 487     eager_initialize_impl(this_oop);
 488   }
 489 }
 490 
 491 // JVMTI spec thinks there are signers and protection domain in the
 492 // instanceKlass.  These accessors pretend these fields are there.
 493 // The hprof specification also thinks these fields are in InstanceKlass.
 494 oop InstanceKlass::protection_domain() const {
 495   // return the protection_domain from the mirror
 496   return java_lang_Class::protection_domain(java_mirror());
 497 }
 498 
 499 // To remove these from requires an incompatible change and CCC request.
 500 objArrayOop InstanceKlass::signers() const {
 501   // return the signers from the mirror
 502   return java_lang_Class::signers(java_mirror());
 503 }
 504 
 505 oop InstanceKlass::init_lock() const {
 506   // return the init lock from the mirror
 507   oop lock = java_lang_Class::init_lock(java_mirror());
 508   // Prevent reordering with any access of initialization state
 509   OrderAccess::loadload();
 510   assert((oop)lock != NULL || !is_not_initialized(), // initialized or in_error state
 511          "only fully initialized state can have a null lock");
 512   return lock;
 513 }
 514 
 515 // Set the initialization lock to null so the object can be GC'ed.  Any racing
 516 // threads to get this lock will see a null lock and will not lock.
 517 // That's okay because they all check for initialized state after getting
 518 // the lock and return.
 519 void InstanceKlass::fence_and_clear_init_lock() {
 520   // make sure previous stores are all done, notably the init_state.
 521   OrderAccess::storestore();
 522   java_lang_Class::set_init_lock(java_mirror(), NULL);
 523   assert(!is_not_initialized(), "class must be initialized now");
 524 }
 525 
 526 void InstanceKlass::eager_initialize_impl(instanceKlassHandle this_oop) {
 527   EXCEPTION_MARK;
 528   oop init_lock = this_oop-&gt;init_lock();
 529   ObjectLocker ol(init_lock, THREAD, init_lock != NULL);
 530 
 531   // abort if someone beat us to the initialization
 532   if (!this_oop-&gt;is_not_initialized()) return;  // note: not equivalent to is_initialized()
 533 
 534   ClassState old_state = this_oop-&gt;init_state();
 535   link_class_impl(this_oop, true, THREAD);
 536   if (HAS_PENDING_EXCEPTION) {
 537     CLEAR_PENDING_EXCEPTION;
 538     // Abort if linking the class throws an exception.
 539 
 540     // Use a test to avoid redundantly resetting the state if there's
 541     // no change.  Set_init_state() asserts that state changes make
 542     // progress, whereas here we might just be spinning in place.
 543     if( old_state != this_oop-&gt;_init_state )
 544       this_oop-&gt;set_init_state (old_state);
 545   } else {
 546     // linking successfull, mark class as initialized
 547     this_oop-&gt;set_init_state (fully_initialized);
 548     this_oop-&gt;fence_and_clear_init_lock();
 549     // trace
 550     if (TraceClassInitialization) {
 551       ResourceMark rm(THREAD);
 552       tty-&gt;print_cr("[Initialized %s without side effects]", this_oop-&gt;external_name());
 553     }
 554   }
 555 }
 556 
 557 
 558 // See "The Virtual Machine Specification" section 2.16.5 for a detailed explanation of the class initialization
 559 // process. The step comments refers to the procedure described in that section.
 560 // Note: implementation moved to static method to expose the this pointer.
 561 void InstanceKlass::initialize(TRAPS) {
 562   if (this-&gt;should_be_initialized()) {
 563     HandleMark hm(THREAD);
 564     instanceKlassHandle this_oop(THREAD, this);
 565     initialize_impl(this_oop, CHECK);
 566     // Note: at this point the class may be initialized
 567     //       OR it may be in the state of being initialized
 568     //       in case of recursive initialization!
 569   } else {
 570     assert(is_initialized(), "sanity check");
 571   }
 572 }
 573 
 574 
 575 bool InstanceKlass::verify_code(
 576     instanceKlassHandle this_oop, bool throw_verifyerror, TRAPS) {
 577   // 1) Verify the bytecodes
 578   Verifier::Mode mode =
 579     throw_verifyerror ? Verifier::ThrowException : Verifier::NoException;
 580   return Verifier::verify(this_oop, mode, this_oop-&gt;should_verify_class(), CHECK_false);
 581 }
 582 
 583 
 584 // Used exclusively by the shared spaces dump mechanism to prevent
 585 // classes mapped into the shared regions in new VMs from appearing linked.
 586 
 587 void InstanceKlass::unlink_class() {
 588   assert(is_linked(), "must be linked");
 589   _init_state = loaded;
 590 }
 591 
 592 void InstanceKlass::link_class(TRAPS) {
 593   assert(is_loaded(), "must be loaded");
 594   if (!is_linked()) {
 595     HandleMark hm(THREAD);
 596     instanceKlassHandle this_oop(THREAD, this);
 597     link_class_impl(this_oop, true, CHECK);
 598   }
 599 }
 600 
 601 // Called to verify that a class can link during initialization, without
 602 // throwing a VerifyError.
 603 bool InstanceKlass::link_class_or_fail(TRAPS) {
 604   assert(is_loaded(), "must be loaded");
 605   if (!is_linked()) {
 606     HandleMark hm(THREAD);
 607     instanceKlassHandle this_oop(THREAD, this);
 608     link_class_impl(this_oop, false, CHECK_false);
 609   }
 610   return is_linked();
 611 }
 612 
 613 bool InstanceKlass::link_class_impl(
 614     instanceKlassHandle this_oop, bool throw_verifyerror, TRAPS) {
 615   // check for error state
 616   if (this_oop-&gt;is_in_error_state()) {
 617     ResourceMark rm(THREAD);
 618     THROW_MSG_(vmSymbols::java_lang_NoClassDefFoundError(),
 619                this_oop-&gt;external_name(), false);
 620   }
 621   // return if already verified
 622   if (this_oop-&gt;is_linked()) {
 623     return true;
 624   }
 625 
 626   // Timing
 627   // timer handles recursion
 628   assert(THREAD-&gt;is_Java_thread(), "non-JavaThread in link_class_impl");
 629   JavaThread* jt = (JavaThread*)THREAD;
 630 
 631   // link super class before linking this class
 632   instanceKlassHandle super(THREAD, this_oop-&gt;super());
 633   if (super.not_null()) {
 634     if (super-&gt;is_interface()) {  // check if super class is an interface
 635       ResourceMark rm(THREAD);
 636       Exceptions::fthrow(
 637         THREAD_AND_LOCATION,
 638         vmSymbols::java_lang_IncompatibleClassChangeError(),
 639         "class %s has interface %s as super class",
 640         this_oop-&gt;external_name(),
 641         super-&gt;external_name()
 642       );
 643       return false;
 644     }
 645 
 646     link_class_impl(super, throw_verifyerror, CHECK_false);
 647   }
 648 
 649   // link all interfaces implemented by this class before linking this class
 650   Array&lt;Klass*&gt;* interfaces = this_oop-&gt;local_interfaces();
 651   int num_interfaces = interfaces-&gt;length();
 652   for (int index = 0; index &lt; num_interfaces; index++) {
 653     HandleMark hm(THREAD);
 654     instanceKlassHandle ih(THREAD, interfaces-&gt;at(index));
 655     link_class_impl(ih, throw_verifyerror, CHECK_false);
 656   }
 657 
 658   // in case the class is linked in the process of linking its superclasses
 659   if (this_oop-&gt;is_linked()) {
 660     return true;
 661   }
 662 
 663   // trace only the link time for this klass that includes
 664   // the verification time
 665   PerfClassTraceTime vmtimer(ClassLoader::perf_class_link_time(),
 666                              ClassLoader::perf_class_link_selftime(),
 667                              ClassLoader::perf_classes_linked(),
 668                              jt-&gt;get_thread_stat()-&gt;perf_recursion_counts_addr(),
 669                              jt-&gt;get_thread_stat()-&gt;perf_timers_addr(),
 670                              PerfClassTraceTime::CLASS_LINK);
 671 
 672   // verification &amp; rewriting
 673   {
 674     oop init_lock = this_oop-&gt;init_lock();
 675     ObjectLocker ol(init_lock, THREAD, init_lock != NULL);
 676     // rewritten will have been set if loader constraint error found
 677     // on an earlier link attempt
 678     // don't verify or rewrite if already rewritten
 679 
 680     if (!this_oop-&gt;is_linked()) {
 681       if (!this_oop-&gt;is_rewritten()) {
 682         {
 683           // Timer includes any side effects of class verification (resolution,
 684           // etc), but not recursive entry into verify_code().
 685           PerfClassTraceTime timer(ClassLoader::perf_class_verify_time(),
 686                                    ClassLoader::perf_class_verify_selftime(),
 687                                    ClassLoader::perf_classes_verified(),
 688                                    jt-&gt;get_thread_stat()-&gt;perf_recursion_counts_addr(),
 689                                    jt-&gt;get_thread_stat()-&gt;perf_timers_addr(),
 690                                    PerfClassTraceTime::CLASS_VERIFY);
 691           bool verify_ok = verify_code(this_oop, throw_verifyerror, THREAD);
 692           if (!verify_ok) {
 693             return false;
 694           }
 695         }
 696 
 697         // Just in case a side-effect of verify linked this class already
 698         // (which can sometimes happen since the verifier loads classes
 699         // using custom class loaders, which are free to initialize things)
 700         if (this_oop-&gt;is_linked()) {
 701           return true;
 702         }
 703 
 704         // also sets rewritten
 705         this_oop-&gt;rewrite_class(CHECK_false);
 706       }
 707 
 708       // relocate jsrs and link methods after they are all rewritten
 709       this_oop-&gt;link_methods(CHECK_false);
 710 
 711       // Initialize the vtable and interface table after
 712       // methods have been rewritten since rewrite may
 713       // fabricate new Method*s.
 714       // also does loader constraint checking
 715       if (!this_oop()-&gt;is_shared()) {
 716         ResourceMark rm(THREAD);
 717         this_oop-&gt;vtable()-&gt;initialize_vtable(true, CHECK_false);
 718         this_oop-&gt;itable()-&gt;initialize_itable(true, CHECK_false);
 719       }
 720 #ifdef ASSERT
 721       else {
 722         ResourceMark rm(THREAD);
 723         this_oop-&gt;vtable()-&gt;verify(tty, true);
 724         // In case itable verification is ever added.
 725         // this_oop-&gt;itable()-&gt;verify(tty, true);
 726       }
 727 #endif
 728       this_oop-&gt;set_init_state(linked);
 729       if (JvmtiExport::should_post_class_prepare()) {
 730         Thread *thread = THREAD;
 731         assert(thread-&gt;is_Java_thread(), "thread-&gt;is_Java_thread()");
 732         JvmtiExport::post_class_prepare((JavaThread *) thread, this_oop());
 733       }
 734     }
 735   }
 736   return true;
 737 }
 738 
 739 
 740 // Rewrite the byte codes of all of the methods of a class.
 741 // The rewriter must be called exactly once. Rewriting must happen after
 742 // verification but before the first method of the class is executed.
 743 void InstanceKlass::rewrite_class(TRAPS) {
 744   assert(is_loaded(), "must be loaded");
 745   instanceKlassHandle this_oop(THREAD, this);
 746   if (this_oop-&gt;is_rewritten()) {
 747     assert(this_oop()-&gt;is_shared(), "rewriting an unshared class?");
 748     return;
 749   }
 750   Rewriter::rewrite(this_oop, CHECK);
 751   this_oop-&gt;set_rewritten();
 752 }
 753 
 754 // Now relocate and link method entry points after class is rewritten.
 755 // This is outside is_rewritten flag. In case of an exception, it can be
 756 // executed more than once.
 757 void InstanceKlass::link_methods(TRAPS) {
 758   int len = methods()-&gt;length();
 759   for (int i = len-1; i &gt;= 0; i--) {
 760     methodHandle m(THREAD, methods()-&gt;at(i));
 761 
 762     // Set up method entry points for compiler and interpreter    .
 763     m-&gt;link_method(m, CHECK);
 764 
 765     // This is for JVMTI and unrelated to relocator but the last thing we do
 766 #ifdef ASSERT
 767     if (StressMethodComparator) {
 768       ResourceMark rm(THREAD);
 769       static int nmc = 0;
 770       for (int j = i; j &gt;= 0 &amp;&amp; j &gt;= i-4; j--) {
 771         if ((++nmc % 1000) == 0)  tty-&gt;print_cr("Have run MethodComparator %d times...", nmc);
 772         bool z = MethodComparator::methods_EMCP(m(),
 773                    methods()-&gt;at(j));
 774         if (j == i &amp;&amp; !z) {
 775           tty-&gt;print("MethodComparator FAIL: "); m-&gt;print(); m-&gt;print_codes();
 776           assert(z, "method must compare equal to itself");
 777         }
 778       }
 779     }
 780 #endif //ASSERT
 781   }
 782 }
 783 
 784 // Eagerly initialize superinterfaces that declare default methods (concrete instance: any access)
 785 void InstanceKlass::initialize_super_interfaces(instanceKlassHandle this_oop, TRAPS) {
 786   if (this_oop-&gt;has_default_methods()) {
 787     for (int i = 0; i &lt; this_oop-&gt;local_interfaces()-&gt;length(); ++i) {
 788       Klass* iface = this_oop-&gt;local_interfaces()-&gt;at(i);
 789       InstanceKlass* ik = InstanceKlass::cast(iface);
 790       if (ik-&gt;should_be_initialized()) {
 791         if (ik-&gt;has_default_methods()) {
 792           ik-&gt;initialize_super_interfaces(ik, THREAD);
 793         }
 794         // Only initialize() interfaces that "declare" concrete methods.
 795         // has_default_methods drives searching superinterfaces since it
 796         // means has_default_methods in its superinterface hierarchy
 797         if (!HAS_PENDING_EXCEPTION &amp;&amp; ik-&gt;declares_default_methods()) {
 798           ik-&gt;initialize(THREAD);
 799         }
 800         if (HAS_PENDING_EXCEPTION) {
 801           Handle e(THREAD, PENDING_EXCEPTION);
 802           CLEAR_PENDING_EXCEPTION;
 803           {
 804             EXCEPTION_MARK;
 805             // Locks object, set state, and notify all waiting threads
 806             this_oop-&gt;set_initialization_state_and_notify(
 807                 initialization_error, THREAD);
 808 
 809             // ignore any exception thrown, superclass initialization error is
 810             // thrown below
 811             CLEAR_PENDING_EXCEPTION;
 812           }
 813           THROW_OOP(e());
 814         }
 815       }
 816     }
 817   }
 818 }
 819 
 820 void InstanceKlass::initialize_impl(instanceKlassHandle this_oop, TRAPS) {
 821   // Make sure klass is linked (verified) before initialization
 822   // A class could already be verified, since it has been reflected upon.
 823   this_oop-&gt;link_class(CHECK);
 824 
 825   DTRACE_CLASSINIT_PROBE(required, InstanceKlass::cast(this_oop()), -1);
 826 
 827   bool wait = false;
 828 
 829   // refer to the JVM book page 47 for description of steps
 830   // Step 1
 831   {
 832     oop init_lock = this_oop-&gt;init_lock();
 833     ObjectLocker ol(init_lock, THREAD, init_lock != NULL);
 834 
 835     Thread *self = THREAD; // it's passed the current thread
 836 
 837     // Step 2
 838     // If we were to use wait() instead of waitInterruptibly() then
 839     // we might end up throwing IE from link/symbol resolution sites
 840     // that aren't expected to throw.  This would wreak havoc.  See 6320309.
 841     while(this_oop-&gt;is_being_initialized() &amp;&amp; !this_oop-&gt;is_reentrant_initialization(self)) {
 842         wait = true;
 843       ol.waitUninterruptibly(CHECK);
 844     }
 845 
 846     // Step 3
 847     if (this_oop-&gt;is_being_initialized() &amp;&amp; this_oop-&gt;is_reentrant_initialization(self)) {
 848       DTRACE_CLASSINIT_PROBE_WAIT(recursive, InstanceKlass::cast(this_oop()), -1,wait);
 849       return;
 850     }
 851 
 852     // Step 4
 853     if (this_oop-&gt;is_initialized()) {
 854       DTRACE_CLASSINIT_PROBE_WAIT(concurrent, InstanceKlass::cast(this_oop()), -1,wait);
 855       return;
 856     }
 857 
 858     // Step 5
 859     if (this_oop-&gt;is_in_error_state()) {
 860       DTRACE_CLASSINIT_PROBE_WAIT(erroneous, InstanceKlass::cast(this_oop()), -1,wait);
 861       ResourceMark rm(THREAD);
 862       const char* desc = "Could not initialize class ";
 863       const char* className = this_oop-&gt;external_name();
 864       size_t msglen = strlen(desc) + strlen(className) + 1;
 865       char* message = NEW_RESOURCE_ARRAY(char, msglen);
 866       if (NULL == message) {
 867         // Out of memory: can't create detailed error message
 868         THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);
 869       } else {
 870         jio_snprintf(message, msglen, "%s%s", desc, className);
 871         THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);
 872       }
 873     }
 874 
 875     // Step 6
 876     this_oop-&gt;set_init_state(being_initialized);
 877     this_oop-&gt;set_init_thread(self);
 878   }
 879 
 880   // Step 7
 881   Klass* super_klass = this_oop-&gt;super();
 882   if (super_klass != NULL &amp;&amp; !this_oop-&gt;is_interface() &amp;&amp; super_klass-&gt;should_be_initialized()) {
 883     super_klass-&gt;initialize(THREAD);
 884 
 885     if (HAS_PENDING_EXCEPTION) {
 886       Handle e(THREAD, PENDING_EXCEPTION);
 887       CLEAR_PENDING_EXCEPTION;
 888       {
 889         EXCEPTION_MARK;
 890         this_oop-&gt;set_initialization_state_and_notify(initialization_error, THREAD); // Locks object, set state, and notify all waiting threads
 891         CLEAR_PENDING_EXCEPTION;   // ignore any exception thrown, superclass initialization error is thrown below
 892       }
 893       DTRACE_CLASSINIT_PROBE_WAIT(super__failed, InstanceKlass::cast(this_oop()), -1,wait);
 894       THROW_OOP(e());
 895     }
 896   }
 897 
 898   // Recursively initialize any superinterfaces that declare default methods
 899   // Only need to recurse if has_default_methods which includes declaring and
 900   // inheriting default methods
 901   if (this_oop-&gt;has_default_methods()) {
 902     this_oop-&gt;initialize_super_interfaces(this_oop, CHECK);
 903   }
 904 
 905   // Step 8
 906   {
 907     assert(THREAD-&gt;is_Java_thread(), "non-JavaThread in initialize_impl");
 908     JavaThread* jt = (JavaThread*)THREAD;
 909     DTRACE_CLASSINIT_PROBE_WAIT(clinit, InstanceKlass::cast(this_oop()), -1,wait);
 910     // Timer includes any side effects of class initialization (resolution,
 911     // etc), but not recursive entry into call_class_initializer().
 912     PerfClassTraceTime timer(ClassLoader::perf_class_init_time(),
 913                              ClassLoader::perf_class_init_selftime(),
 914                              ClassLoader::perf_classes_inited(),
 915                              jt-&gt;get_thread_stat()-&gt;perf_recursion_counts_addr(),
 916                              jt-&gt;get_thread_stat()-&gt;perf_timers_addr(),
 917                              PerfClassTraceTime::CLASS_CLINIT);
 918     this_oop-&gt;call_class_initializer(THREAD);
 919   }
 920 
 921   // Step 9
 922   if (!HAS_PENDING_EXCEPTION) {
 923     this_oop-&gt;set_initialization_state_and_notify(fully_initialized, CHECK);
 924     { ResourceMark rm(THREAD);
 925       debug_only(this_oop-&gt;vtable()-&gt;verify(tty, true);)
 926     }
 927   }
 928   else {
 929     // Step 10 and 11
 930     Handle e(THREAD, PENDING_EXCEPTION);
 931     CLEAR_PENDING_EXCEPTION;
 932     // JVMTI has already reported the pending exception
 933     // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
 934     JvmtiExport::clear_detected_exception((JavaThread*)THREAD);
 935     {
 936       EXCEPTION_MARK;
 937       this_oop-&gt;set_initialization_state_and_notify(initialization_error, THREAD);
 938       CLEAR_PENDING_EXCEPTION;   // ignore any exception thrown, class initialization error is thrown below
 939       // JVMTI has already reported the pending exception
 940       // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
 941       JvmtiExport::clear_detected_exception((JavaThread*)THREAD);
 942     }
 943     DTRACE_CLASSINIT_PROBE_WAIT(error, InstanceKlass::cast(this_oop()), -1,wait);
 944     if (e-&gt;is_a(SystemDictionary::Error_klass())) {
 945       THROW_OOP(e());
 946     } else {
 947       JavaCallArguments args(e);
 948       THROW_ARG(vmSymbols::java_lang_ExceptionInInitializerError(),
 949                 vmSymbols::throwable_void_signature(),
 950                 &amp;args);
 951     }
 952   }
 953   DTRACE_CLASSINIT_PROBE_WAIT(end, InstanceKlass::cast(this_oop()), -1,wait);
 954 }
 955 
 956 
 957 // Note: implementation moved to static method to expose the this pointer.
 958 void InstanceKlass::set_initialization_state_and_notify(ClassState state, TRAPS) {
 959   instanceKlassHandle kh(THREAD, this);
 960   set_initialization_state_and_notify_impl(kh, state, CHECK);
 961 }
 962 
 963 void InstanceKlass::set_initialization_state_and_notify_impl(instanceKlassHandle this_oop, ClassState state, TRAPS) {
 964   oop init_lock = this_oop-&gt;init_lock();
 965   ObjectLocker ol(init_lock, THREAD, init_lock != NULL);
 966   this_oop-&gt;set_init_state(state);
 967   this_oop-&gt;fence_and_clear_init_lock();
 968   ol.notify_all(CHECK);
 969 }
 970 
 971 // The embedded _implementor field can only record one implementor.
 972 // When there are more than one implementors, the _implementor field
 973 // is set to the interface Klass* itself. Following are the possible
 974 // values for the _implementor field:
 975 //   NULL                  - no implementor
 976 //   implementor Klass*    - one implementor
 977 //   self                  - more than one implementor
 978 //
 979 // The _implementor field only exists for interfaces.
 980 void InstanceKlass::add_implementor(Klass* k) {
 981   assert(Compile_lock-&gt;owned_by_self(), "");
 982   assert(is_interface(), "not interface");
 983   // Filter out my subinterfaces.
 984   // (Note: Interfaces are never on the subklass list.)
 985   if (InstanceKlass::cast(k)-&gt;is_interface()) return;
 986 
 987   // Filter out subclasses whose supers already implement me.
 988   // (Note: CHA must walk subclasses of direct implementors
 989   // in order to locate indirect implementors.)
 990   Klass* sk = InstanceKlass::cast(k)-&gt;super();
 991   if (sk != NULL &amp;&amp; InstanceKlass::cast(sk)-&gt;implements_interface(this))
 992     // We only need to check one immediate superclass, since the
 993     // implements_interface query looks at transitive_interfaces.
 994     // Any supers of the super have the same (or fewer) transitive_interfaces.
 995     return;
 996 
 997   Klass* ik = implementor();
 998   if (ik == NULL) {
 999     set_implementor(k);
1000   } else if (ik != this) {
1001     // There is already an implementor. Use itself as an indicator of
1002     // more than one implementors.
1003     set_implementor(this);
1004   }
1005 
1006   // The implementor also implements the transitive_interfaces
1007   for (int index = 0; index &lt; local_interfaces()-&gt;length(); index++) {
1008     InstanceKlass::cast(local_interfaces()-&gt;at(index))-&gt;add_implementor(k);
1009   }
1010 }
1011 
1012 void InstanceKlass::init_implementor() {
1013   if (is_interface()) {
1014     set_implementor(NULL);
1015   }
1016 }
1017 
1018 
1019 void InstanceKlass::process_interfaces(Thread *thread) {
1020   // link this class into the implementors list of every interface it implements
1021   Klass* this_as_klass_oop = this;
1022   for (int i = local_interfaces()-&gt;length() - 1; i &gt;= 0; i--) {
1023     assert(local_interfaces()-&gt;at(i)-&gt;is_klass(), "must be a klass");
1024     InstanceKlass* interf = InstanceKlass::cast(local_interfaces()-&gt;at(i));
1025     assert(interf-&gt;is_interface(), "expected interface");
1026     interf-&gt;add_implementor(this_as_klass_oop);
1027   }
1028 }
1029 
1030 bool InstanceKlass::can_be_primary_super_slow() const {
1031   if (is_interface())
1032     return false;
1033   else
1034     return Klass::can_be_primary_super_slow();
1035 }
1036 
1037 GrowableArray&lt;Klass*&gt;* InstanceKlass::compute_secondary_supers(int num_extra_slots) {
1038   // The secondaries are the implemented interfaces.
1039   InstanceKlass* ik = InstanceKlass::cast(this);
1040   Array&lt;Klass*&gt;* interfaces = ik-&gt;transitive_interfaces();
1041   int num_secondaries = num_extra_slots + interfaces-&gt;length();
1042   if (num_secondaries == 0) {
1043     // Must share this for correct bootstrapping!
1044     set_secondary_supers(Universe::the_empty_klass_array());
1045     return NULL;
1046   } else if (num_extra_slots == 0) {
1047     // The secondary super list is exactly the same as the transitive interfaces.
1048     // Redefine classes has to be careful not to delete this!
1049     set_secondary_supers(interfaces);
1050     return NULL;
1051   } else {
1052     // Copy transitive interfaces to a temporary growable array to be constructed
1053     // into the secondary super list with extra slots.
1054     GrowableArray&lt;Klass*&gt;* secondaries = new GrowableArray&lt;Klass*&gt;(interfaces-&gt;length());
1055     for (int i = 0; i &lt; interfaces-&gt;length(); i++) {
1056       secondaries-&gt;push(interfaces-&gt;at(i));
1057     }
1058     return secondaries;
1059   }
1060 }
1061 
1062 bool InstanceKlass::compute_is_subtype_of(Klass* k) {
1063   if (k-&gt;is_interface()) {
1064     return implements_interface(k);
1065   } else {
1066     return Klass::compute_is_subtype_of(k);
1067   }
1068 }
1069 
1070 bool InstanceKlass::implements_interface(Klass* k) const {
1071   if (this == k) return true;
1072   assert(k-&gt;is_interface(), "should be an interface class");
1073   for (int i = 0; i &lt; transitive_interfaces()-&gt;length(); i++) {
1074     if (transitive_interfaces()-&gt;at(i) == k) {
1075       return true;
1076     }
1077   }
1078   return false;
1079 }
1080 
1081 bool InstanceKlass::is_same_or_direct_interface(Klass *k) const {
1082   // Verify direct super interface
1083   if (this == k) return true;
1084   assert(k-&gt;is_interface(), "should be an interface class");
1085   for (int i = 0; i &lt; local_interfaces()-&gt;length(); i++) {
1086     if (local_interfaces()-&gt;at(i) == k) {
1087       return true;
1088     }
1089   }
1090   return false;
1091 }
1092 
1093 objArrayOop InstanceKlass::allocate_objArray(int n, int length, TRAPS) {
1094   if (length &lt; 0) THROW_0(vmSymbols::java_lang_NegativeArraySizeException());
1095   if (length &gt; arrayOopDesc::max_array_length(T_OBJECT)) {
1096     report_java_out_of_memory("Requested array size exceeds VM limit");
1097     JvmtiExport::post_array_size_exhausted();
1098     THROW_OOP_0(Universe::out_of_memory_error_array_size());
1099   }
1100   int size = objArrayOopDesc::object_size(length);
1101   Klass* ak = array_klass(n, CHECK_NULL);
1102   KlassHandle h_ak (THREAD, ak);
1103   objArrayOop o =
1104     (objArrayOop)CollectedHeap::array_allocate(h_ak, size, length, CHECK_NULL);
1105   return o;
1106 }
1107 
1108 instanceOop InstanceKlass::register_finalizer(instanceOop i, TRAPS) {
1109   if (TraceFinalizerRegistration) {
1110     tty-&gt;print("Registered ");
1111     i-&gt;print_value_on(tty);
1112     tty-&gt;print_cr(" (" INTPTR_FORMAT ") as finalizable", (address)i);
1113   }
1114   instanceHandle h_i(THREAD, i);
1115   // Pass the handle as argument, JavaCalls::call expects oop as jobjects
1116   JavaValue result(T_VOID);
1117   JavaCallArguments args(h_i);
1118   methodHandle mh (THREAD, Universe::finalizer_register_method());
1119   JavaCalls::call(&amp;result, mh, &amp;args, CHECK_NULL);
1120   return h_i();
1121 }
1122 
1123 instanceOop InstanceKlass::allocate_instance(TRAPS) {
1124   bool has_finalizer_flag = has_finalizer(); // Query before possible GC
1125   int size = size_helper();  // Query before forming handle.
1126 
1127   KlassHandle h_k(THREAD, this);
1128 
1129   instanceOop i;
1130 
1131   i = (instanceOop)CollectedHeap::obj_allocate(h_k, size, CHECK_NULL);
1132   if (has_finalizer_flag &amp;&amp; !RegisterFinalizersAtInit) {
1133     i = register_finalizer(i, CHECK_NULL);
1134   }
1135   return i;
1136 }
1137 
1138 void InstanceKlass::check_valid_for_instantiation(bool throwError, TRAPS) {
1139   if (is_interface() || is_abstract()) {
1140     ResourceMark rm(THREAD);
1141     THROW_MSG(throwError ? vmSymbols::java_lang_InstantiationError()
1142               : vmSymbols::java_lang_InstantiationException(), external_name());
1143   }
1144   if (this == SystemDictionary::Class_klass()) {
1145     ResourceMark rm(THREAD);
1146     THROW_MSG(throwError ? vmSymbols::java_lang_IllegalAccessError()
1147               : vmSymbols::java_lang_IllegalAccessException(), external_name());
1148   }
1149 }
1150 
1151 Klass* InstanceKlass::array_klass_impl(bool or_null, int n, TRAPS) {
1152   instanceKlassHandle this_oop(THREAD, this);
1153   return array_klass_impl(this_oop, or_null, n, THREAD);
1154 }
1155 
1156 Klass* InstanceKlass::array_klass_impl(instanceKlassHandle this_oop, bool or_null, int n, TRAPS) {
1157   if (this_oop-&gt;array_klasses() == NULL) {
1158     if (or_null) return NULL;
1159 
1160     ResourceMark rm;
1161     JavaThread *jt = (JavaThread *)THREAD;
1162     {
1163       // Atomic creation of array_klasses
1164       MutexLocker mc(Compile_lock, THREAD);   // for vtables
1165       MutexLocker ma(MultiArray_lock, THREAD);
1166 
1167       // Check if update has already taken place
1168       if (this_oop-&gt;array_klasses() == NULL) {
1169         Klass*    k = ObjArrayKlass::allocate_objArray_klass(this_oop-&gt;class_loader_data(), 1, this_oop, CHECK_NULL);
1170         this_oop-&gt;set_array_klasses(k);
1171       }
1172     }
1173   }
1174   // _this will always be set at this point
1175   ObjArrayKlass* oak = (ObjArrayKlass*)this_oop-&gt;array_klasses();
1176   if (or_null) {
1177     return oak-&gt;array_klass_or_null(n);
1178   }
1179   return oak-&gt;array_klass(n, CHECK_NULL);
1180 }
1181 
1182 Klass* InstanceKlass::array_klass_impl(bool or_null, TRAPS) {
1183   return array_klass_impl(or_null, 1, THREAD);
1184 }
1185 
1186 void InstanceKlass::call_class_initializer(TRAPS) {
1187   instanceKlassHandle ik (THREAD, this);
1188   call_class_initializer_impl(ik, THREAD);
1189 }
1190 
1191 static int call_class_initializer_impl_counter = 0;   // for debugging
1192 
1193 Method* InstanceKlass::class_initializer() {
1194   Method* clinit = find_method(
1195       vmSymbols::class_initializer_name(), vmSymbols::void_method_signature());
1196   if (clinit != NULL &amp;&amp; clinit-&gt;has_valid_initializer_flags()) {
1197     return clinit;
1198   }
1199   return NULL;
1200 }
1201 
1202 void InstanceKlass::call_class_initializer_impl(instanceKlassHandle this_oop, TRAPS) {
1203   if (ReplayCompiles &amp;&amp;
1204       (ReplaySuppressInitializers == 1 ||
1205        ReplaySuppressInitializers &gt;= 2 &amp;&amp; this_oop-&gt;class_loader() != NULL)) {
1206     // Hide the existence of the initializer for the purpose of replaying the compile
1207     return;
1208   }
1209 
1210   methodHandle h_method(THREAD, this_oop-&gt;class_initializer());
1211   assert(!this_oop-&gt;is_initialized(), "we cannot initialize twice");
1212   if (TraceClassInitialization) {
1213     tty-&gt;print("%d Initializing ", call_class_initializer_impl_counter++);
1214     this_oop-&gt;name()-&gt;print_value();
1215     tty-&gt;print_cr("%s (" INTPTR_FORMAT ")", h_method() == NULL ? "(no method)" : "", (address)this_oop());
1216   }
1217   if (h_method() != NULL) {
1218     JavaCallArguments args; // No arguments
1219     JavaValue result(T_VOID);
1220     JavaCalls::call(&amp;result, h_method, &amp;args, CHECK); // Static call (no args)
1221   }
1222 }
1223 
1224 
1225 void InstanceKlass::mask_for(methodHandle method, int bci,
1226   InterpreterOopMap* entry_for) {
1227   // Dirty read, then double-check under a lock.
1228   if (_oop_map_cache == NULL) {
1229     // Otherwise, allocate a new one.
1230     MutexLocker x(OopMapCacheAlloc_lock);
1231     // First time use. Allocate a cache in C heap
1232     if (_oop_map_cache == NULL) {
1233       // Release stores from OopMapCache constructor before assignment
1234       // to _oop_map_cache. C++ compilers on ppc do not emit the
1235       // required memory barrier only because of the volatile
1236       // qualifier of _oop_map_cache.
1237       OrderAccess::release_store_ptr(&amp;_oop_map_cache, new OopMapCache());
1238     }
1239   }
1240   // _oop_map_cache is constant after init; lookup below does is own locking.
1241   _oop_map_cache-&gt;lookup(method, bci, entry_for);
1242 }
1243 
1244 
1245 bool InstanceKlass::find_local_field(Symbol* name, Symbol* sig, fieldDescriptor* fd) const {
1246   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
1247     Symbol* f_name = fs.name();
1248     Symbol* f_sig  = fs.signature();
1249     if (f_name == name &amp;&amp; f_sig == sig) {
1250       fd-&gt;reinitialize(const_cast&lt;InstanceKlass*&gt;(this), fs.index());
1251       return true;
1252     }
1253   }
1254   return false;
1255 }
1256 
1257 
1258 Klass* InstanceKlass::find_interface_field(Symbol* name, Symbol* sig, fieldDescriptor* fd) const {
1259   const int n = local_interfaces()-&gt;length();
1260   for (int i = 0; i &lt; n; i++) {
1261     Klass* intf1 = local_interfaces()-&gt;at(i);
1262     assert(intf1-&gt;is_interface(), "just checking type");
1263     // search for field in current interface
1264     if (InstanceKlass::cast(intf1)-&gt;find_local_field(name, sig, fd)) {
1265       assert(fd-&gt;is_static(), "interface field must be static");
1266       return intf1;
1267     }
1268     // search for field in direct superinterfaces
1269     Klass* intf2 = InstanceKlass::cast(intf1)-&gt;find_interface_field(name, sig, fd);
1270     if (intf2 != NULL) return intf2;
1271   }
1272   // otherwise field lookup fails
1273   return NULL;
1274 }
1275 
1276 
1277 Klass* InstanceKlass::find_field(Symbol* name, Symbol* sig, fieldDescriptor* fd) const {
1278   // search order according to newest JVM spec (5.4.3.2, p.167).
1279   // 1) search for field in current klass
1280   if (find_local_field(name, sig, fd)) {
1281     return const_cast&lt;InstanceKlass*&gt;(this);
1282   }
1283   // 2) search for field recursively in direct superinterfaces
1284   { Klass* intf = find_interface_field(name, sig, fd);
1285     if (intf != NULL) return intf;
1286   }
1287   // 3) apply field lookup recursively if superclass exists
1288   { Klass* supr = super();
1289     if (supr != NULL) return InstanceKlass::cast(supr)-&gt;find_field(name, sig, fd);
1290   }
1291   // 4) otherwise field lookup fails
1292   return NULL;
1293 }
1294 
1295 
1296 Klass* InstanceKlass::find_field(Symbol* name, Symbol* sig, bool is_static, fieldDescriptor* fd) const {
1297   // search order according to newest JVM spec (5.4.3.2, p.167).
1298   // 1) search for field in current klass
1299   if (find_local_field(name, sig, fd)) {
1300     if (fd-&gt;is_static() == is_static) return const_cast&lt;InstanceKlass*&gt;(this);
1301   }
1302   // 2) search for field recursively in direct superinterfaces
1303   if (is_static) {
1304     Klass* intf = find_interface_field(name, sig, fd);
1305     if (intf != NULL) return intf;
1306   }
1307   // 3) apply field lookup recursively if superclass exists
1308   { Klass* supr = super();
1309     if (supr != NULL) return InstanceKlass::cast(supr)-&gt;find_field(name, sig, is_static, fd);
1310   }
1311   // 4) otherwise field lookup fails
1312   return NULL;
1313 }
1314 
1315 
1316 bool InstanceKlass::find_local_field_from_offset(int offset, bool is_static, fieldDescriptor* fd) const {
1317   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
1318     if (fs.offset() == offset) {
1319       fd-&gt;reinitialize(const_cast&lt;InstanceKlass*&gt;(this), fs.index());
1320       if (fd-&gt;is_static() == is_static) return true;
1321     }
1322   }
1323   return false;
1324 }
1325 
1326 
1327 bool InstanceKlass::find_field_from_offset(int offset, bool is_static, fieldDescriptor* fd) const {
1328   Klass* klass = const_cast&lt;InstanceKlass*&gt;(this);
1329   while (klass != NULL) {
1330     if (InstanceKlass::cast(klass)-&gt;find_local_field_from_offset(offset, is_static, fd)) {
1331       return true;
1332     }
1333     klass = klass-&gt;super();
1334   }
1335   return false;
1336 }
1337 
1338 
1339 void InstanceKlass::methods_do(void f(Method* method)) {
1340   int len = methods()-&gt;length();
1341   for (int index = 0; index &lt; len; index++) {
1342     Method* m = methods()-&gt;at(index);
1343     assert(m-&gt;is_method(), "must be method");
1344     f(m);
1345   }
1346 }
1347 
1348 
1349 void InstanceKlass::do_local_static_fields(FieldClosure* cl) {
1350   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
1351     if (fs.access_flags().is_static()) {
1352       fieldDescriptor&amp; fd = fs.field_descriptor();
1353       cl-&gt;do_field(&amp;fd);
1354     }
1355   }
1356 }
1357 
1358 
1359 void InstanceKlass::do_local_static_fields(void f(fieldDescriptor*, Handle, TRAPS), Handle mirror, TRAPS) {
1360   instanceKlassHandle h_this(THREAD, this);
1361   do_local_static_fields_impl(h_this, f, mirror, CHECK);
1362 }
1363 
1364 
1365 void InstanceKlass::do_local_static_fields_impl(instanceKlassHandle this_k,
1366                              void f(fieldDescriptor* fd, Handle mirror, TRAPS), Handle mirror, TRAPS) {
1367   for (JavaFieldStream fs(this_k()); !fs.done(); fs.next()) {
1368     if (fs.access_flags().is_static()) {
1369       fieldDescriptor&amp; fd = fs.field_descriptor();
1370       f(&amp;fd, mirror, CHECK);
1371     }
1372   }
1373 }
1374 
1375 
1376 static int compare_fields_by_offset(int* a, int* b) {
1377   return a[0] - b[0];
1378 }
1379 
1380 void InstanceKlass::do_nonstatic_fields(FieldClosure* cl) {
1381   InstanceKlass* super = superklass();
1382   if (super != NULL) {
1383     super-&gt;do_nonstatic_fields(cl);
1384   }
1385   fieldDescriptor fd;
1386   int length = java_fields_count();
1387   // In DebugInfo nonstatic fields are sorted by offset.
1388   int* fields_sorted = NEW_C_HEAP_ARRAY(int, 2*(length+1), mtClass);
1389   int j = 0;
1390   for (int i = 0; i &lt; length; i += 1) {
1391     fd.reinitialize(this, i);
1392     if (!fd.is_static()) {
1393       fields_sorted[j + 0] = fd.offset();
1394       fields_sorted[j + 1] = i;
1395       j += 2;
1396     }
1397   }
1398   if (j &gt; 0) {
1399     length = j;
1400     // _sort_Fn is defined in growableArray.hpp.
1401     qsort(fields_sorted, length/2, 2*sizeof(int), (_sort_Fn)compare_fields_by_offset);
1402     for (int i = 0; i &lt; length; i += 2) {
1403       fd.reinitialize(this, fields_sorted[i + 1]);
1404       assert(!fd.is_static() &amp;&amp; fd.offset() == fields_sorted[i], "only nonstatic fields");
1405       cl-&gt;do_field(&amp;fd);
1406     }
1407   }
1408   FREE_C_HEAP_ARRAY(int, fields_sorted, mtClass);
1409 }
1410 
1411 
1412 void InstanceKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
1413   if (array_klasses() != NULL)
1414     ArrayKlass::cast(array_klasses())-&gt;array_klasses_do(f, THREAD);
1415 }
1416 
1417 void InstanceKlass::array_klasses_do(void f(Klass* k)) {
1418   if (array_klasses() != NULL)
1419     ArrayKlass::cast(array_klasses())-&gt;array_klasses_do(f);
1420 }
1421 
1422 #ifdef ASSERT
1423 static int linear_search(Array&lt;Method*&gt;* methods, Symbol* name, Symbol* signature) {
1424   int len = methods-&gt;length();
1425   for (int index = 0; index &lt; len; index++) {
1426     Method* m = methods-&gt;at(index);
1427     assert(m-&gt;is_method(), "must be method");
1428     if (m-&gt;signature() == signature &amp;&amp; m-&gt;name() == name) {
1429        return index;
1430     }
1431   }
1432   return -1;
1433 }
1434 #endif
1435 
1436 static int binary_search(Array&lt;Method*&gt;* methods, Symbol* name) {
1437   int len = methods-&gt;length();
1438   // methods are sorted, so do binary search
1439   int l = 0;
1440   int h = len - 1;
1441   while (l &lt;= h) {
1442     int mid = (l + h) &gt;&gt; 1;
1443     Method* m = methods-&gt;at(mid);
1444     assert(m-&gt;is_method(), "must be method");
1445     int res = m-&gt;name()-&gt;fast_compare(name);
1446     if (res == 0) {
1447       return mid;
1448     } else if (res &lt; 0) {
1449       l = mid + 1;
1450     } else {
1451       h = mid - 1;
1452     }
1453   }
1454   return -1;
1455 }
1456 
1457 // find_method looks up the name/signature in the local methods array
1458 Method* InstanceKlass::find_method(Symbol* name, Symbol* signature) const {
1459   return find_method_impl(name, signature, false);
1460 }
1461 
1462 Method* InstanceKlass::find_method_impl(Symbol* name, Symbol* signature, bool skipping_overpass) const {
1463   return InstanceKlass::find_method_impl(methods(), name, signature, skipping_overpass, false);
1464 }
1465 
1466 // find_instance_method looks up the name/signature in the local methods array
1467 // and skips over static methods
1468 Method* InstanceKlass::find_instance_method(
1469     Array&lt;Method*&gt;* methods, Symbol* name, Symbol* signature) {
1470   Method* meth = InstanceKlass::find_method_impl(methods, name, signature, false, true);
1471   return meth;
1472 }
1473 
1474 // find_instance_method looks up the name/signature in the local methods array
1475 // and skips over static methods
1476 Method* InstanceKlass::find_instance_method(Symbol* name, Symbol* signature) {
1477     return InstanceKlass::find_instance_method(methods(), name, signature);
1478 }
1479 
1480 // find_method looks up the name/signature in the local methods array
1481 Method* InstanceKlass::find_method(
1482     Array&lt;Method*&gt;* methods, Symbol* name, Symbol* signature) {
1483   return InstanceKlass::find_method_impl(methods, name, signature, false, false);
1484 }
1485 
1486 Method* InstanceKlass::find_method_impl(
1487     Array&lt;Method*&gt;* methods, Symbol* name, Symbol* signature, bool skipping_overpass, bool skipping_static) {
1488   int hit = find_method_index(methods, name, signature, skipping_overpass, skipping_static);
1489   return hit &gt;= 0 ? methods-&gt;at(hit): NULL;
1490 }
1491 
1492 bool InstanceKlass::method_matches(Method* m, Symbol* signature, bool skipping_overpass, bool skipping_static) {
1493     return (m-&gt;signature() == signature) &amp;&amp;
1494             (!skipping_overpass || !m-&gt;is_overpass()) &amp;&amp;
1495             (!skipping_static || !m-&gt;is_static());
1496 }
1497 
1498 // Used directly for default_methods to find the index into the
1499 // default_vtable_indices, and indirectly by find_method
1500 // find_method_index looks in the local methods array to return the index
1501 // of the matching name/signature. If, overpass methods are being ignored,
1502 // the search continues to find a potential non-overpass match.  This capability
1503 // is important during method resolution to prefer a static method, for example,
1504 // over an overpass method.
1505 int InstanceKlass::find_method_index(
1506     Array&lt;Method*&gt;* methods, Symbol* name, Symbol* signature, bool skipping_overpass, bool skipping_static) {
1507   int hit = binary_search(methods, name);
1508   if (hit != -1) {
1509     Method* m = methods-&gt;at(hit);
1510 
1511     // Do linear search to find matching signature.  First, quick check
1512     // for common case, ignoring overpasses if requested.
1513     if (method_matches(m, signature, skipping_overpass, skipping_static)) return hit;
1514 
1515     // search downwards through overloaded methods
1516     int i;
1517     for (i = hit - 1; i &gt;= 0; --i) {
1518         Method* m = methods-&gt;at(i);
1519         assert(m-&gt;is_method(), "must be method");
1520         if (m-&gt;name() != name) break;
1521         if (method_matches(m, signature, skipping_overpass, skipping_static)) return i;
1522     }
1523     // search upwards
1524     for (i = hit + 1; i &lt; methods-&gt;length(); ++i) {
1525         Method* m = methods-&gt;at(i);
1526         assert(m-&gt;is_method(), "must be method");
1527         if (m-&gt;name() != name) break;
1528         if (method_matches(m, signature, skipping_overpass, skipping_static)) return i;
1529     }
1530     // not found
1531 #ifdef ASSERT
1532     int index = skipping_overpass || skipping_static ? -1 : linear_search(methods, name, signature);
1533     assert(index == -1, err_msg("binary search should have found entry %d", index));
1534 #endif
1535   }
1536   return -1;
1537 }
1538 int InstanceKlass::find_method_by_name(Symbol* name, int* end) {
1539   return find_method_by_name(methods(), name, end);
1540 }
1541 
1542 int InstanceKlass::find_method_by_name(
1543     Array&lt;Method*&gt;* methods, Symbol* name, int* end_ptr) {
1544   assert(end_ptr != NULL, "just checking");
1545   int start = binary_search(methods, name);
1546   int end = start + 1;
1547   if (start != -1) {
1548     while (start - 1 &gt;= 0 &amp;&amp; (methods-&gt;at(start - 1))-&gt;name() == name) --start;
1549     while (end &lt; methods-&gt;length() &amp;&amp; (methods-&gt;at(end))-&gt;name() == name) ++end;
1550     *end_ptr = end;
1551     return start;
1552   }
1553   return -1;
1554 }
1555 
1556 // uncached_lookup_method searches both the local class methods array and all
1557 // superclasses methods arrays, skipping any overpass methods in superclasses.
1558 Method* InstanceKlass::uncached_lookup_method(Symbol* name, Symbol* signature, MethodLookupMode mode) const {
1559   MethodLookupMode lookup_mode = mode;
1560   Klass* klass = const_cast&lt;InstanceKlass*&gt;(this);
1561   while (klass != NULL) {
1562     Method* method = InstanceKlass::cast(klass)-&gt;find_method_impl(name, signature, (lookup_mode == skip_overpass));
1563     if (method != NULL) {
1564       return method;
1565     }
1566     klass = InstanceKlass::cast(klass)-&gt;super();
1567     lookup_mode = skip_overpass;   // Always ignore overpass methods in superclasses
1568   }
1569   return NULL;
1570 }
1571 
1572 // lookup a method in the default methods list then in all transitive interfaces
1573 // Do NOT return private or static methods
1574 Method* InstanceKlass::lookup_method_in_ordered_interfaces(Symbol* name,
1575                                                          Symbol* signature) const {
1576   Method* m = NULL;
1577   if (default_methods() != NULL) {
1578     m = find_method(default_methods(), name, signature);
1579   }
1580   // Look up interfaces
1581   if (m == NULL) {
1582     m = lookup_method_in_all_interfaces(name, signature, normal);
1583   }
1584   return m;
1585 }
1586 
1587 // lookup a method in all the interfaces that this class implements
1588 // Do NOT return private or static methods, new in JDK8 which are not externally visible
1589 // They should only be found in the initial InterfaceMethodRef
1590 Method* InstanceKlass::lookup_method_in_all_interfaces(Symbol* name,
1591                                                        Symbol* signature,
1592                                                        MethodLookupMode mode) const {
1593   Array&lt;Klass*&gt;* all_ifs = transitive_interfaces();
1594   int num_ifs = all_ifs-&gt;length();
1595   InstanceKlass *ik = NULL;
1596   for (int i = 0; i &lt; num_ifs; i++) {
1597     ik = InstanceKlass::cast(all_ifs-&gt;at(i));
1598     Method* m = ik-&gt;lookup_method(name, signature);
1599     if (m != NULL &amp;&amp; m-&gt;is_public() &amp;&amp; !m-&gt;is_static() &amp;&amp;
1600         ((mode != skip_defaults) || !m-&gt;is_default_method())) {
1601       return m;
1602     }
1603   }
1604   return NULL;
1605 }
1606 
1607 /* jni_id_for_impl for jfieldIds only */
1608 JNIid* InstanceKlass::jni_id_for_impl(instanceKlassHandle this_oop, int offset) {
1609   MutexLocker ml(JfieldIdCreation_lock);
1610   // Retry lookup after we got the lock
1611   JNIid* probe = this_oop-&gt;jni_ids() == NULL ? NULL : this_oop-&gt;jni_ids()-&gt;find(offset);
1612   if (probe == NULL) {
1613     // Slow case, allocate new static field identifier
1614     probe = new JNIid(this_oop(), offset, this_oop-&gt;jni_ids());
1615     this_oop-&gt;set_jni_ids(probe);
1616   }
1617   return probe;
1618 }
1619 
1620 
1621 /* jni_id_for for jfieldIds only */
1622 JNIid* InstanceKlass::jni_id_for(int offset) {
1623   JNIid* probe = jni_ids() == NULL ? NULL : jni_ids()-&gt;find(offset);
1624   if (probe == NULL) {
1625     probe = jni_id_for_impl(this, offset);
1626   }
1627   return probe;
1628 }
1629 
1630 u2 InstanceKlass::enclosing_method_data(int offset) {
1631   Array&lt;jushort&gt;* inner_class_list = inner_classes();
1632   if (inner_class_list == NULL) {
1633     return 0;
1634   }
1635   int length = inner_class_list-&gt;length();
1636   if (length % inner_class_next_offset == 0) {
1637     return 0;
1638   } else {
1639     int index = length - enclosing_method_attribute_size;
1640     assert(offset &lt; enclosing_method_attribute_size, "invalid offset");
1641     return inner_class_list-&gt;at(index + offset);
1642   }
1643 }
1644 
1645 void InstanceKlass::set_enclosing_method_indices(u2 class_index,
1646                                                  u2 method_index) {
1647   Array&lt;jushort&gt;* inner_class_list = inner_classes();
1648   assert (inner_class_list != NULL, "_inner_classes list is not set up");
1649   int length = inner_class_list-&gt;length();
1650   if (length % inner_class_next_offset == enclosing_method_attribute_size) {
1651     int index = length - enclosing_method_attribute_size;
1652     inner_class_list-&gt;at_put(
1653       index + enclosing_method_class_index_offset, class_index);
1654     inner_class_list-&gt;at_put(
1655       index + enclosing_method_method_index_offset, method_index);
1656   }
1657 }
1658 
1659 // Lookup or create a jmethodID.
1660 // This code is called by the VMThread and JavaThreads so the
1661 // locking has to be done very carefully to avoid deadlocks
1662 // and/or other cache consistency problems.
1663 //
1664 jmethodID InstanceKlass::get_jmethod_id(instanceKlassHandle ik_h, methodHandle method_h) {
1665   size_t idnum = (size_t)method_h-&gt;method_idnum();
1666   jmethodID* jmeths = ik_h-&gt;methods_jmethod_ids_acquire();
1667   size_t length = 0;
1668   jmethodID id = NULL;
1669 
1670   // We use a double-check locking idiom here because this cache is
1671   // performance sensitive. In the normal system, this cache only
1672   // transitions from NULL to non-NULL which is safe because we use
1673   // release_set_methods_jmethod_ids() to advertise the new cache.
1674   // A partially constructed cache should never be seen by a racing
1675   // thread. We also use release_store_ptr() to save a new jmethodID
1676   // in the cache so a partially constructed jmethodID should never be
1677   // seen either. Cache reads of existing jmethodIDs proceed without a
1678   // lock, but cache writes of a new jmethodID requires uniqueness and
1679   // creation of the cache itself requires no leaks so a lock is
1680   // generally acquired in those two cases.
1681   //
1682   // If the RedefineClasses() API has been used, then this cache can
1683   // grow and we'll have transitions from non-NULL to bigger non-NULL.
1684   // Cache creation requires no leaks and we require safety between all
1685   // cache accesses and freeing of the old cache so a lock is generally
1686   // acquired when the RedefineClasses() API has been used.
1687 
1688   if (jmeths != NULL) {
1689     // the cache already exists
1690     if (!ik_h-&gt;idnum_can_increment()) {
1691       // the cache can't grow so we can just get the current values
1692       get_jmethod_id_length_value(jmeths, idnum, &amp;length, &amp;id);
1693     } else {
1694       // cache can grow so we have to be more careful
1695       if (Threads::number_of_threads() == 0 ||
1696           SafepointSynchronize::is_at_safepoint()) {
1697         // we're single threaded or at a safepoint - no locking needed
1698         get_jmethod_id_length_value(jmeths, idnum, &amp;length, &amp;id);
1699       } else {
1700         MutexLocker ml(JmethodIdCreation_lock);
1701         get_jmethod_id_length_value(jmeths, idnum, &amp;length, &amp;id);
1702       }
1703     }
1704   }
1705   // implied else:
1706   // we need to allocate a cache so default length and id values are good
1707 
1708   if (jmeths == NULL ||   // no cache yet
1709       length &lt;= idnum ||  // cache is too short
1710       id == NULL) {       // cache doesn't contain entry
1711 
1712     // This function can be called by the VMThread so we have to do all
1713     // things that might block on a safepoint before grabbing the lock.
1714     // Otherwise, we can deadlock with the VMThread or have a cache
1715     // consistency issue. These vars keep track of what we might have
1716     // to free after the lock is dropped.
1717     jmethodID  to_dealloc_id     = NULL;
1718     jmethodID* to_dealloc_jmeths = NULL;
1719 
1720     // may not allocate new_jmeths or use it if we allocate it
1721     jmethodID* new_jmeths = NULL;
1722     if (length &lt;= idnum) {
1723       // allocate a new cache that might be used
1724       size_t size = MAX2(idnum+1, (size_t)ik_h-&gt;idnum_allocated_count());
1725       new_jmeths = NEW_C_HEAP_ARRAY(jmethodID, size+1, mtClass);
1726       memset(new_jmeths, 0, (size+1)*sizeof(jmethodID));
1727       // cache size is stored in element[0], other elements offset by one
1728       new_jmeths[0] = (jmethodID)size;
1729     }
1730 
1731     // allocate a new jmethodID that might be used
1732     jmethodID new_id = NULL;
1733     if (method_h-&gt;is_old() &amp;&amp; !method_h-&gt;is_obsolete()) {
1734       // The method passed in is old (but not obsolete), we need to use the current version
1735       Method* current_method = ik_h-&gt;method_with_idnum((int)idnum);
1736       assert(current_method != NULL, "old and but not obsolete, so should exist");
1737       new_id = Method::make_jmethod_id(ik_h-&gt;class_loader_data(), current_method);
1738     } else {
1739       // It is the current version of the method or an obsolete method,
1740       // use the version passed in
1741       new_id = Method::make_jmethod_id(ik_h-&gt;class_loader_data(), method_h());
1742     }
1743 
1744     if (Threads::number_of_threads() == 0 ||
1745         SafepointSynchronize::is_at_safepoint()) {
1746       // we're single threaded or at a safepoint - no locking needed
1747       id = get_jmethod_id_fetch_or_update(ik_h, idnum, new_id, new_jmeths,
1748                                           &amp;to_dealloc_id, &amp;to_dealloc_jmeths);
1749     } else {
1750       MutexLocker ml(JmethodIdCreation_lock);
1751       id = get_jmethod_id_fetch_or_update(ik_h, idnum, new_id, new_jmeths,
1752                                           &amp;to_dealloc_id, &amp;to_dealloc_jmeths);
1753     }
1754 
1755     // The lock has been dropped so we can free resources.
1756     // Free up either the old cache or the new cache if we allocated one.
1757     if (to_dealloc_jmeths != NULL) {
1758       FreeHeap(to_dealloc_jmeths);
1759     }
1760     // free up the new ID since it wasn't needed
1761     if (to_dealloc_id != NULL) {
1762       Method::destroy_jmethod_id(ik_h-&gt;class_loader_data(), to_dealloc_id);
1763     }
1764   }
1765   return id;
1766 }
1767 
1768 
1769 // Common code to fetch the jmethodID from the cache or update the
1770 // cache with the new jmethodID. This function should never do anything
1771 // that causes the caller to go to a safepoint or we can deadlock with
1772 // the VMThread or have cache consistency issues.
1773 //
1774 jmethodID InstanceKlass::get_jmethod_id_fetch_or_update(
1775             instanceKlassHandle ik_h, size_t idnum, jmethodID new_id,
1776             jmethodID* new_jmeths, jmethodID* to_dealloc_id_p,
1777             jmethodID** to_dealloc_jmeths_p) {
1778   assert(new_id != NULL, "sanity check");
1779   assert(to_dealloc_id_p != NULL, "sanity check");
1780   assert(to_dealloc_jmeths_p != NULL, "sanity check");
1781   assert(Threads::number_of_threads() == 0 ||
1782          SafepointSynchronize::is_at_safepoint() ||
1783          JmethodIdCreation_lock-&gt;owned_by_self(), "sanity check");
1784 
1785   // reacquire the cache - we are locked, single threaded or at a safepoint
1786   jmethodID* jmeths = ik_h-&gt;methods_jmethod_ids_acquire();
1787   jmethodID  id     = NULL;
1788   size_t     length = 0;
1789 
1790   if (jmeths == NULL ||                         // no cache yet
1791       (length = (size_t)jmeths[0]) &lt;= idnum) {  // cache is too short
1792     if (jmeths != NULL) {
1793       // copy any existing entries from the old cache
1794       for (size_t index = 0; index &lt; length; index++) {
1795         new_jmeths[index+1] = jmeths[index+1];
1796       }
1797       *to_dealloc_jmeths_p = jmeths;  // save old cache for later delete
1798     }
1799     ik_h-&gt;release_set_methods_jmethod_ids(jmeths = new_jmeths);
1800   } else {
1801     // fetch jmethodID (if any) from the existing cache
1802     id = jmeths[idnum+1];
1803     *to_dealloc_jmeths_p = new_jmeths;  // save new cache for later delete
1804   }
1805   if (id == NULL) {
1806     // No matching jmethodID in the existing cache or we have a new
1807     // cache or we just grew the cache. This cache write is done here
1808     // by the first thread to win the foot race because a jmethodID
1809     // needs to be unique once it is generally available.
1810     id = new_id;
1811 
1812     // The jmethodID cache can be read while unlocked so we have to
1813     // make sure the new jmethodID is complete before installing it
1814     // in the cache.
1815     OrderAccess::release_store_ptr(&amp;jmeths[idnum+1], id);
1816   } else {
1817     *to_dealloc_id_p = new_id; // save new id for later delete
1818   }
1819   return id;
1820 }
1821 
1822 
1823 // Common code to get the jmethodID cache length and the jmethodID
1824 // value at index idnum if there is one.
1825 //
1826 void InstanceKlass::get_jmethod_id_length_value(jmethodID* cache,
1827        size_t idnum, size_t *length_p, jmethodID* id_p) {
1828   assert(cache != NULL, "sanity check");
1829   assert(length_p != NULL, "sanity check");
1830   assert(id_p != NULL, "sanity check");
1831 
1832   // cache size is stored in element[0], other elements offset by one
1833   *length_p = (size_t)cache[0];
1834   if (*length_p &lt;= idnum) {  // cache is too short
1835     *id_p = NULL;
1836   } else {
1837     *id_p = cache[idnum+1];  // fetch jmethodID (if any)
1838   }
1839 }
1840 
1841 
1842 // Lookup a jmethodID, NULL if not found.  Do no blocking, no allocations, no handles
1843 jmethodID InstanceKlass::jmethod_id_or_null(Method* method) {
1844   size_t idnum = (size_t)method-&gt;method_idnum();
1845   jmethodID* jmeths = methods_jmethod_ids_acquire();
1846   size_t length;                                // length assigned as debugging crumb
1847   jmethodID id = NULL;
1848   if (jmeths != NULL &amp;&amp;                         // If there is a cache
1849       (length = (size_t)jmeths[0]) &gt; idnum) {   // and if it is long enough,
1850     id = jmeths[idnum+1];                       // Look up the id (may be NULL)
1851   }
1852   return id;
1853 }
1854 
1855 int nmethodBucket::decrement() {
1856   return Atomic::add(-1, (volatile int *)&amp;_count);
1857 }
1858 
1859 //
1860 // Walk the list of dependent nmethods searching for nmethods which
1861 // are dependent on the changes that were passed in and mark them for
1862 // deoptimization.  Returns the number of nmethods found.
1863 //
1864 int InstanceKlass::mark_dependent_nmethods(DepChange&amp; changes) {
1865   assert_locked_or_safepoint(CodeCache_lock);
1866   int found = 0;
1867   nmethodBucket* b = _dependencies;
1868   while (b != NULL) {
1869     nmethod* nm = b-&gt;get_nmethod();
1870     // since dependencies aren't removed until an nmethod becomes a zombie,
1871     // the dependency list may contain nmethods which aren't alive.
1872     if (b-&gt;count() &gt; 0 &amp;&amp; nm-&gt;is_alive() &amp;&amp; !nm-&gt;is_marked_for_deoptimization() &amp;&amp; nm-&gt;check_dependency_on(changes)) {
1873       if (TraceDependencies) {
1874         ResourceMark rm;
1875         tty-&gt;print_cr("Marked for deoptimization");
1876         tty-&gt;print_cr("  context = %s", this-&gt;external_name());
1877         changes.print();
1878         nm-&gt;print();
1879         nm-&gt;print_dependencies();
1880       }
1881       nm-&gt;mark_for_deoptimization();
1882       found++;
1883     }
1884     b = b-&gt;next();
1885   }
1886   return found;
1887 }
1888 
1889 void InstanceKlass::clean_dependent_nmethods() {
1890   assert_locked_or_safepoint(CodeCache_lock);
1891 
1892   if (has_unloaded_dependent()) {
1893     nmethodBucket* b = _dependencies;
1894     nmethodBucket* last = NULL;
1895     while (b != NULL) {
1896       assert(b-&gt;count() &gt;= 0, err_msg("bucket count: %d", b-&gt;count()));
1897 
1898       nmethodBucket* next = b-&gt;next();
1899 
1900       if (b-&gt;count() == 0) {
1901         if (last == NULL) {
1902           _dependencies = next;
1903         } else {
1904           last-&gt;set_next(next);
1905         }
1906         delete b;
1907         // last stays the same.
1908       } else {
1909         last = b;
1910       }
1911 
1912       b = next;
1913     }
1914     set_has_unloaded_dependent(false);
1915   }
1916 #ifdef ASSERT
1917   else {
1918     // Verification
1919     for (nmethodBucket* b = _dependencies; b != NULL; b = b-&gt;next()) {
1920       assert(b-&gt;count() &gt;= 0, err_msg("bucket count: %d", b-&gt;count()));
1921       assert(b-&gt;count() != 0, "empty buckets need to be cleaned");
1922     }
1923   }
1924 #endif
1925 }
1926 
1927 //
1928 // Add an nmethodBucket to the list of dependencies for this nmethod.
1929 // It's possible that an nmethod has multiple dependencies on this klass
1930 // so a count is kept for each bucket to guarantee that creation and
1931 // deletion of dependencies is consistent.
1932 //
1933 void InstanceKlass::add_dependent_nmethod(nmethod* nm) {
1934   assert_locked_or_safepoint(CodeCache_lock);
1935   nmethodBucket* b = _dependencies;
1936   nmethodBucket* last = NULL;
1937   while (b != NULL) {
1938     if (nm == b-&gt;get_nmethod()) {
1939       b-&gt;increment();
1940       return;
1941     }
1942     b = b-&gt;next();
1943   }
1944   _dependencies = new nmethodBucket(nm, _dependencies);
1945 }
1946 
1947 
1948 //
1949 // Decrement count of the nmethod in the dependency list and remove
1950 // the bucket competely when the count goes to 0.  This method must
1951 // find a corresponding bucket otherwise there's a bug in the
1952 // recording of dependecies.
1953 //
1954 void InstanceKlass::remove_dependent_nmethod(nmethod* nm) {
1955   assert_locked_or_safepoint(CodeCache_lock);
1956   nmethodBucket* b = _dependencies;
1957   nmethodBucket* last = NULL;
1958   while (b != NULL) {
1959     if (nm == b-&gt;get_nmethod()) {
1960       int val = b-&gt;decrement();
1961       guarantee(val &gt;= 0, err_msg("Underflow: %d", val));
1962       if (val == 0) {
1963         set_has_unloaded_dependent(true);
1964       }
1965       return;
1966     }
1967     last = b;
1968     b = b-&gt;next();
1969   }
1970 #ifdef ASSERT
1971   tty-&gt;print_cr("### %s can't find dependent nmethod:", this-&gt;external_name());
1972   nm-&gt;print();
1973 #endif // ASSERT
1974   ShouldNotReachHere();
1975 }
1976 
1977 
1978 #ifndef PRODUCT
1979 void InstanceKlass::print_dependent_nmethods(bool verbose) {
1980   nmethodBucket* b = _dependencies;
1981   int idx = 0;
1982   while (b != NULL) {
1983     nmethod* nm = b-&gt;get_nmethod();
1984     tty-&gt;print("[%d] count=%d { ", idx++, b-&gt;count());
1985     if (!verbose) {
1986       nm-&gt;print_on(tty, "nmethod");
1987       tty-&gt;print_cr(" } ");
1988     } else {
1989       nm-&gt;print();
1990       nm-&gt;print_dependencies();
1991       tty-&gt;print_cr("--- } ");
1992     }
1993     b = b-&gt;next();
1994   }
1995 }
1996 
1997 
1998 bool InstanceKlass::is_dependent_nmethod(nmethod* nm) {
1999   nmethodBucket* b = _dependencies;
2000   while (b != NULL) {
2001     if (nm == b-&gt;get_nmethod()) {
2002 #ifdef ASSERT
2003       int count = b-&gt;count();
2004       assert(count &gt;= 0, err_msg("count shouldn't be negative: %d", count));
2005 #endif
2006       return true;
2007     }
2008     b = b-&gt;next();
2009   }
2010   return false;
2011 }
2012 #endif //PRODUCT
2013 
2014 
2015 // Garbage collection
2016 
2017 #ifdef ASSERT
2018 template &lt;class T&gt; void assert_is_in(T *p) {
2019   T heap_oop = oopDesc::load_heap_oop(p);
2020   if (!oopDesc::is_null(heap_oop)) {
2021     oop o = oopDesc::decode_heap_oop_not_null(heap_oop);
2022     assert(Universe::heap()-&gt;is_in(o), "should be in heap");
2023   }
2024 }
2025 template &lt;class T&gt; void assert_is_in_closed_subset(T *p) {
2026   T heap_oop = oopDesc::load_heap_oop(p);
2027   if (!oopDesc::is_null(heap_oop)) {
2028     oop o = oopDesc::decode_heap_oop_not_null(heap_oop);
2029     assert(Universe::heap()-&gt;is_in_closed_subset(o),
2030            err_msg("should be in closed *p " INTPTR_FORMAT " " INTPTR_FORMAT, (address)p, (address)o));
2031   }
2032 }
2033 template &lt;class T&gt; void assert_is_in_reserved(T *p) {
2034   T heap_oop = oopDesc::load_heap_oop(p);
2035   if (!oopDesc::is_null(heap_oop)) {
2036     oop o = oopDesc::decode_heap_oop_not_null(heap_oop);
2037     assert(Universe::heap()-&gt;is_in_reserved(o), "should be in reserved");
2038   }
2039 }
2040 template &lt;class T&gt; void assert_nothing(T *p) {}
2041 
2042 #else
2043 template &lt;class T&gt; void assert_is_in(T *p) {}
2044 template &lt;class T&gt; void assert_is_in_closed_subset(T *p) {}
2045 template &lt;class T&gt; void assert_is_in_reserved(T *p) {}
2046 template &lt;class T&gt; void assert_nothing(T *p) {}
2047 #endif // ASSERT
2048 
2049 //
2050 // Macros that iterate over areas of oops which are specialized on type of
2051 // oop pointer either narrow or wide, depending on UseCompressedOops
2052 //
2053 // Parameters are:
2054 //   T         - type of oop to point to (either oop or narrowOop)
2055 //   start_p   - starting pointer for region to iterate over
2056 //   count     - number of oops or narrowOops to iterate over
2057 //   do_oop    - action to perform on each oop (it's arbitrary C code which
2058 //               makes it more efficient to put in a macro rather than making
2059 //               it a template function)
2060 //   assert_fn - assert function which is template function because performance
2061 //               doesn't matter when enabled.
2062 #define InstanceKlass_SPECIALIZED_OOP_ITERATE( \
2063   T, start_p, count, do_oop,                \
2064   assert_fn)                                \
2065 {                                           \
2066   T* p         = (T*)(start_p);             \
2067   T* const end = p + (count);               \
2068   while (p &lt; end) {                         \
2069     (assert_fn)(p);                         \
2070     do_oop;                                 \
2071     ++p;                                    \
2072   }                                         \
2073 }
2074 
2075 #define InstanceKlass_SPECIALIZED_OOP_REVERSE_ITERATE( \
2076   T, start_p, count, do_oop,                \
2077   assert_fn)                                \
2078 {                                           \
2079   T* const start = (T*)(start_p);           \
2080   T*       p     = start + (count);         \
2081   while (start &lt; p) {                       \
2082     --p;                                    \
2083     (assert_fn)(p);                         \
2084     do_oop;                                 \
2085   }                                         \
2086 }
2087 
2088 #define InstanceKlass_SPECIALIZED_BOUNDED_OOP_ITERATE( \
2089   T, start_p, count, low, high,             \
2090   do_oop, assert_fn)                        \
2091 {                                           \
2092   T* const l = (T*)(low);                   \
2093   T* const h = (T*)(high);                  \
2094   assert(mask_bits((intptr_t)l, sizeof(T)-1) == 0 &amp;&amp; \
2095          mask_bits((intptr_t)h, sizeof(T)-1) == 0,   \
2096          "bounded region must be properly aligned"); \
2097   T* p       = (T*)(start_p);               \
2098   T* end     = p + (count);                 \
2099   if (p &lt; l) p = l;                         \
2100   if (end &gt; h) end = h;                     \
2101   while (p &lt; end) {                         \
2102     (assert_fn)(p);                         \
2103     do_oop;                                 \
2104     ++p;                                    \
2105   }                                         \
2106 }
2107 
2108 
2109 // The following macros call specialized macros, passing either oop or
2110 // narrowOop as the specialization type.  These test the UseCompressedOops
2111 // flag.
2112 #define InstanceKlass_OOP_MAP_ITERATE(obj, do_oop, assert_fn)            \
2113 {                                                                        \
2114   /* Compute oopmap block range. The common case                         \
2115      is nonstatic_oop_map_size == 1. */                                  \
2116   OopMapBlock* map           = start_of_nonstatic_oop_maps();            \
2117   OopMapBlock* const end_map = map + nonstatic_oop_map_count();          \
2118   if (UseCompressedOops) {                                               \
2119     while (map &lt; end_map) {                                              \
2120       InstanceKlass_SPECIALIZED_OOP_ITERATE(narrowOop,                   \
2121         obj-&gt;obj_field_addr&lt;narrowOop&gt;(map-&gt;offset()), map-&gt;count(),     \
2122         do_oop, assert_fn)                                               \
2123       ++map;                                                             \
2124     }                                                                    \
2125   } else {                                                               \
2126     while (map &lt; end_map) {                                              \
2127       InstanceKlass_SPECIALIZED_OOP_ITERATE(oop,                         \
2128         obj-&gt;obj_field_addr&lt;oop&gt;(map-&gt;offset()), map-&gt;count(),           \
2129         do_oop, assert_fn)                                               \
2130       ++map;                                                             \
2131     }                                                                    \
2132   }                                                                      \
2133 }
2134 
2135 #define InstanceKlass_OOP_MAP_REVERSE_ITERATE(obj, do_oop, assert_fn)    \
2136 {                                                                        \
2137   OopMapBlock* const start_map = start_of_nonstatic_oop_maps();          \
2138   OopMapBlock* map             = start_map + nonstatic_oop_map_count();  \
2139   if (UseCompressedOops) {                                               \
2140     while (start_map &lt; map) {                                            \
2141       --map;                                                             \
2142       InstanceKlass_SPECIALIZED_OOP_REVERSE_ITERATE(narrowOop,           \
2143         obj-&gt;obj_field_addr&lt;narrowOop&gt;(map-&gt;offset()), map-&gt;count(),     \
2144         do_oop, assert_fn)                                               \
2145     }                                                                    \
2146   } else {                                                               \
2147     while (start_map &lt; map) {                                            \
2148       --map;                                                             \
2149       InstanceKlass_SPECIALIZED_OOP_REVERSE_ITERATE(oop,                 \
2150         obj-&gt;obj_field_addr&lt;oop&gt;(map-&gt;offset()), map-&gt;count(),           \
2151         do_oop, assert_fn)                                               \
2152     }                                                                    \
2153   }                                                                      \
2154 }
2155 
2156 #define InstanceKlass_BOUNDED_OOP_MAP_ITERATE(obj, low, high, do_oop,    \
2157                                               assert_fn)                 \
2158 {                                                                        \
2159   /* Compute oopmap block range. The common case is                      \
2160      nonstatic_oop_map_size == 1, so we accept the                       \
2161      usually non-existent extra overhead of examining                    \
2162      all the maps. */                                                    \
2163   OopMapBlock* map           = start_of_nonstatic_oop_maps();            \
2164   OopMapBlock* const end_map = map + nonstatic_oop_map_count();          \
2165   if (UseCompressedOops) {                                               \
2166     while (map &lt; end_map) {                                              \
2167       InstanceKlass_SPECIALIZED_BOUNDED_OOP_ITERATE(narrowOop,           \
2168         obj-&gt;obj_field_addr&lt;narrowOop&gt;(map-&gt;offset()), map-&gt;count(),     \
2169         low, high,                                                       \
2170         do_oop, assert_fn)                                               \
2171       ++map;                                                             \
2172     }                                                                    \
2173   } else {                                                               \
2174     while (map &lt; end_map) {                                              \
2175       InstanceKlass_SPECIALIZED_BOUNDED_OOP_ITERATE(oop,                 \
2176         obj-&gt;obj_field_addr&lt;oop&gt;(map-&gt;offset()), map-&gt;count(),           \
2177         low, high,                                                       \
2178         do_oop, assert_fn)                                               \
2179       ++map;                                                             \
2180     }                                                                    \
2181   }                                                                      \
2182 }
2183 
2184 void InstanceKlass::oop_follow_contents(oop obj) {
2185   assert(obj != NULL, "can't follow the content of NULL object");
2186   MarkSweep::follow_klass(obj-&gt;klass());
2187   InstanceKlass_OOP_MAP_ITERATE( \
2188     obj, \
2189     MarkSweep::mark_and_push(p), \
2190     assert_is_in_closed_subset)
2191 }
2192 
2193 #if INCLUDE_ALL_GCS
2194 void InstanceKlass::oop_follow_contents(ParCompactionManager* cm,
2195                                         oop obj) {
2196   assert(obj != NULL, "can't follow the content of NULL object");
2197   PSParallelCompact::follow_klass(cm, obj-&gt;klass());
2198   // Only mark the header and let the scan of the meta-data mark
2199   // everything else.
2200   InstanceKlass_OOP_MAP_ITERATE( \
2201     obj, \
2202     PSParallelCompact::mark_and_push(cm, p), \
2203     assert_is_in)
2204 }
2205 #endif // INCLUDE_ALL_GCS
2206 
2207 // closure's do_metadata() method dictates whether the given closure should be
2208 // applied to the klass ptr in the object header.
2209 
2210 #define InstanceKlass_OOP_OOP_ITERATE_DEFN(OopClosureType, nv_suffix)        \
2211                                                                              \
2212 int InstanceKlass::oop_oop_iterate##nv_suffix(oop obj, OopClosureType* closure) { \
2213   SpecializationStats::record_iterate_call##nv_suffix(SpecializationStats::ik);\
2214   /* header */                                                          \
2215   if_do_metadata_checked(closure, nv_suffix) {                          \
2216     closure-&gt;do_klass##nv_suffix(obj-&gt;klass());                         \
2217   }                                                                     \
2218   InstanceKlass_OOP_MAP_ITERATE(                                        \
2219     obj,                                                                \
2220     SpecializationStats::                                               \
2221       record_do_oop_call##nv_suffix(SpecializationStats::ik);           \
2222     (closure)-&gt;do_oop##nv_suffix(p),                                    \
2223     assert_is_in_closed_subset)                                         \
2224   return size_helper();                                                 \
2225 }
2226 
2227 #if INCLUDE_ALL_GCS
2228 #define InstanceKlass_OOP_OOP_ITERATE_BACKWARDS_DEFN(OopClosureType, nv_suffix) \
2229                                                                                 \
2230 int InstanceKlass::oop_oop_iterate_backwards##nv_suffix(oop obj,                \
2231                                               OopClosureType* closure) {        \
2232   SpecializationStats::record_iterate_call##nv_suffix(SpecializationStats::ik); \
2233                                                                                 \
2234   assert_should_ignore_metadata(closure, nv_suffix);                            \
2235                                                                                 \
2236   /* instance variables */                                                      \
2237   InstanceKlass_OOP_MAP_REVERSE_ITERATE(                                        \
2238     obj,                                                                        \
2239     SpecializationStats::record_do_oop_call##nv_suffix(SpecializationStats::ik);\
2240     (closure)-&gt;do_oop##nv_suffix(p),                                            \
2241     assert_is_in_closed_subset)                                                 \
2242    return size_helper();                                                        \
2243 }
2244 #endif // INCLUDE_ALL_GCS
2245 
2246 #define InstanceKlass_OOP_OOP_ITERATE_DEFN_m(OopClosureType, nv_suffix) \
2247                                                                         \
2248 int InstanceKlass::oop_oop_iterate##nv_suffix##_m(oop obj,              \
2249                                                   OopClosureType* closure, \
2250                                                   MemRegion mr) {          \
2251   SpecializationStats::record_iterate_call##nv_suffix(SpecializationStats::ik);\
2252   if_do_metadata_checked(closure, nv_suffix) {                           \
2253     if (mr.contains(obj)) {                                              \
2254       closure-&gt;do_klass##nv_suffix(obj-&gt;klass());                        \
2255     }                                                                    \
2256   }                                                                      \
2257   InstanceKlass_BOUNDED_OOP_MAP_ITERATE(                                 \
2258     obj, mr.start(), mr.end(),                                           \
2259     (closure)-&gt;do_oop##nv_suffix(p),                                     \
2260     assert_is_in_closed_subset)                                          \
2261   return size_helper();                                                  \
2262 }
2263 
2264 ALL_OOP_OOP_ITERATE_CLOSURES_1(InstanceKlass_OOP_OOP_ITERATE_DEFN)
2265 ALL_OOP_OOP_ITERATE_CLOSURES_2(InstanceKlass_OOP_OOP_ITERATE_DEFN)
2266 ALL_OOP_OOP_ITERATE_CLOSURES_1(InstanceKlass_OOP_OOP_ITERATE_DEFN_m)
2267 ALL_OOP_OOP_ITERATE_CLOSURES_2(InstanceKlass_OOP_OOP_ITERATE_DEFN_m)
2268 #if INCLUDE_ALL_GCS
2269 ALL_OOP_OOP_ITERATE_CLOSURES_1(InstanceKlass_OOP_OOP_ITERATE_BACKWARDS_DEFN)
2270 ALL_OOP_OOP_ITERATE_CLOSURES_2(InstanceKlass_OOP_OOP_ITERATE_BACKWARDS_DEFN)
2271 #endif // INCLUDE_ALL_GCS
2272 
2273 int InstanceKlass::oop_adjust_pointers(oop obj) {
2274   int size = size_helper();
2275   InstanceKlass_OOP_MAP_ITERATE( \
2276     obj, \
2277     MarkSweep::adjust_pointer(p), \
2278     assert_is_in)
2279   return size;
2280 }
2281 
2282 #if INCLUDE_ALL_GCS
2283 void InstanceKlass::oop_push_contents(PSPromotionManager* pm, oop obj) {
2284   InstanceKlass_OOP_MAP_REVERSE_ITERATE( \
2285     obj, \
2286     if (PSScavenge::should_scavenge(p)) { \
2287       pm-&gt;claim_or_forward_depth(p); \
2288     }, \
2289     assert_nothing )
2290 }
2291 
2292 int InstanceKlass::oop_update_pointers(ParCompactionManager* cm, oop obj) {
2293   int size = size_helper();
2294   InstanceKlass_OOP_MAP_ITERATE( \
2295     obj, \
2296     PSParallelCompact::adjust_pointer(p), \
2297     assert_is_in)
2298   return size;
2299 }
2300 
2301 #endif // INCLUDE_ALL_GCS
2302 
2303 void InstanceKlass::clean_implementors_list(BoolObjectClosure* is_alive) {
2304   assert(class_loader_data()-&gt;is_alive(is_alive), "this klass should be live");
2305   if (is_interface()) {
2306     if (ClassUnloading) {
2307       Klass* impl = implementor();
2308       if (impl != NULL) {
2309         if (!impl-&gt;is_loader_alive(is_alive)) {
2310           // remove this guy
2311           Klass** klass = adr_implementor();
2312           assert(klass != NULL, "null klass");
2313           if (klass != NULL) {
2314             *klass = NULL;
2315           }
2316         }
2317       }
2318     }
2319   }
2320 }
2321 
2322 void InstanceKlass::clean_method_data(BoolObjectClosure* is_alive) {
2323   for (int m = 0; m &lt; methods()-&gt;length(); m++) {
2324     MethodData* mdo = methods()-&gt;at(m)-&gt;method_data();
2325     if (mdo != NULL) {
2326       mdo-&gt;clean_method_data(is_alive);
2327     }
2328   }
2329 }
2330 
2331 
2332 static void remove_unshareable_in_class(Klass* k) {
2333   // remove klass's unshareable info
2334   k-&gt;remove_unshareable_info();
2335 }
2336 
2337 void InstanceKlass::remove_unshareable_info() {
2338   Klass::remove_unshareable_info();
2339   // Unlink the class
2340   if (is_linked()) {
2341     unlink_class();
2342   }
2343   init_implementor();
2344 
2345   constants()-&gt;remove_unshareable_info();
2346 
2347   for (int i = 0; i &lt; methods()-&gt;length(); i++) {
2348     Method* m = methods()-&gt;at(i);
2349     m-&gt;remove_unshareable_info();
2350   }
2351 
2352   // do array classes also.
2353   array_klasses_do(remove_unshareable_in_class);
2354 }
2355 
2356 static void restore_unshareable_in_class(Klass* k, TRAPS) {
2357   // Array classes have null protection domain.
2358   // --&gt; see ArrayKlass::complete_create_array_klass()
2359   k-&gt;restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);
2360 }
2361 
2362 void InstanceKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, TRAPS) {
2363   Klass::restore_unshareable_info(loader_data, protection_domain, CHECK);
2364   instanceKlassHandle ik(THREAD, this);
2365 
2366   Array&lt;Method*&gt;* methods = ik-&gt;methods();
2367   int num_methods = methods-&gt;length();
2368   for (int index2 = 0; index2 &lt; num_methods; ++index2) {
2369     methodHandle m(THREAD, methods-&gt;at(index2));
2370     m-&gt;restore_unshareable_info(CHECK);
2371   }
2372   if (JvmtiExport::has_redefined_a_class()) {
2373     // Reinitialize vtable because RedefineClasses may have changed some
2374     // entries in this vtable for super classes so the CDS vtable might
2375     // point to old or obsolete entries.  RedefineClasses doesn't fix up
2376     // vtables in the shared system dictionary, only the main one.
2377     // It also redefines the itable too so fix that too.
2378     ResourceMark rm(THREAD);
2379     ik-&gt;vtable()-&gt;initialize_vtable(false, CHECK);
2380     ik-&gt;itable()-&gt;initialize_itable(false, CHECK);
2381   }
2382 
2383   // restore constant pool resolved references
2384   ik-&gt;constants()-&gt;restore_unshareable_info(CHECK);
2385 
2386   ik-&gt;array_klasses_do(restore_unshareable_in_class, CHECK);
2387 }
2388 
2389 // returns true IFF is_in_error_state() has been changed as a result of this call.
2390 bool InstanceKlass::check_sharing_error_state() {
2391   assert(DumpSharedSpaces, "should only be called during dumping");
2392   bool old_state = is_in_error_state();
2393 
2394   if (!is_in_error_state()) {
2395     bool bad = false;
2396     for (InstanceKlass* sup = java_super(); sup; sup = sup-&gt;java_super()) {
2397       if (sup-&gt;is_in_error_state()) {
2398         bad = true;
2399         break;
2400       }
2401     }
2402     if (!bad) {
2403       Array&lt;Klass*&gt;* interfaces = transitive_interfaces();
2404       for (int i = 0; i &lt; interfaces-&gt;length(); i++) {
2405         Klass* iface = interfaces-&gt;at(i);
2406         if (InstanceKlass::cast(iface)-&gt;is_in_error_state()) {
2407           bad = true;
2408           break;
2409         }
2410       }
2411     }
2412 
2413     if (bad) {
2414       set_in_error_state();
2415     }
2416   }
2417 
2418   return (old_state != is_in_error_state());
2419 }
2420 
2421 static void clear_all_breakpoints(Method* m) {
2422   m-&gt;clear_all_breakpoints();
2423 }
2424 
2425 
2426 void InstanceKlass::notify_unload_class(InstanceKlass* ik) {
2427   // notify the debugger
2428   if (JvmtiExport::should_post_class_unload()) {
2429     JvmtiExport::post_class_unload(ik);
2430   }
2431 
2432   // notify ClassLoadingService of class unload
2433   ClassLoadingService::notify_class_unloaded(ik);
2434 }
2435 
2436 void InstanceKlass::release_C_heap_structures(InstanceKlass* ik) {
2437   // Clean up C heap
2438   ik-&gt;release_C_heap_structures();
2439   ik-&gt;constants()-&gt;release_C_heap_structures();
2440 }
2441 
2442 void InstanceKlass::release_C_heap_structures() {
2443 
2444   // Can't release the constant pool here because the constant pool can be
2445   // deallocated separately from the InstanceKlass for default methods and
2446   // redefine classes.
2447 
2448   // Deallocate oop map cache
2449   if (_oop_map_cache != NULL) {
2450     delete _oop_map_cache;
2451     _oop_map_cache = NULL;
2452   }
2453 
2454   // Deallocate JNI identifiers for jfieldIDs
2455   JNIid::deallocate(jni_ids());
2456   set_jni_ids(NULL);
2457 
2458   jmethodID* jmeths = methods_jmethod_ids_acquire();
2459   if (jmeths != (jmethodID*)NULL) {
2460     release_set_methods_jmethod_ids(NULL);
2461     FreeHeap(jmeths);
2462   }
2463 
2464   // Deallocate MemberNameTable
2465   {
2466     Mutex* lock_or_null = SafepointSynchronize::is_at_safepoint() ? NULL : MemberNameTable_lock;
2467     MutexLockerEx ml(lock_or_null, Mutex::_no_safepoint_check_flag);
2468     MemberNameTable* mnt = member_names();
2469     if (mnt != NULL) {
2470       delete mnt;
2471       set_member_names(NULL);
2472     }
2473   }
2474 
2475   // release dependencies
2476   nmethodBucket* b = _dependencies;
2477   _dependencies = NULL;
2478   while (b != NULL) {
2479     nmethodBucket* next = b-&gt;next();
2480     delete b;
2481     b = next;
2482   }
2483 
2484   // Deallocate breakpoint records
2485   if (breakpoints() != 0x0) {
2486     methods_do(clear_all_breakpoints);
2487     assert(breakpoints() == 0x0, "should have cleared breakpoints");
2488   }
2489 
2490   // deallocate information about previous versions
2491   if (_previous_versions != NULL) {
2492     for (int i = _previous_versions-&gt;length() - 1; i &gt;= 0; i--) {
2493       PreviousVersionNode * pv_node = _previous_versions-&gt;at(i);
2494       delete pv_node;
2495     }
2496     delete _previous_versions;
2497     _previous_versions = NULL;
2498   }
2499 
2500   // deallocate the cached class file
2501   if (_cached_class_file != NULL) {
2502     os::free(_cached_class_file, mtClass);
2503     _cached_class_file = NULL;
2504   }
2505 
2506   // Decrement symbol reference counts associated with the unloaded class.
2507   if (_name != NULL) _name-&gt;decrement_refcount();
2508   // unreference array name derived from this class name (arrays of an unloaded
2509   // class can't be referenced anymore).
2510   if (_array_name != NULL)  _array_name-&gt;decrement_refcount();
2511   if (_source_debug_extension != NULL) FREE_C_HEAP_ARRAY(char, _source_debug_extension, mtClass);
2512 
2513   assert(_total_instanceKlass_count &gt;= 1, "Sanity check");
2514   Atomic::dec(&amp;_total_instanceKlass_count);
2515 }
2516 
2517 void InstanceKlass::set_source_debug_extension(char* array, int length) {
2518   if (array == NULL) {
2519     _source_debug_extension = NULL;
2520   } else {
2521     // Adding one to the attribute length in order to store a null terminator
2522     // character could cause an overflow because the attribute length is
2523     // already coded with an u4 in the classfile, but in practice, it's
2524     // unlikely to happen.
2525     assert((length+1) &gt; length, "Overflow checking");
2526     char* sde = NEW_C_HEAP_ARRAY(char, (length + 1), mtClass);
2527     for (int i = 0; i &lt; length; i++) {
2528       sde[i] = array[i];
2529     }
2530     sde[length] = '\0';
2531     _source_debug_extension = sde;
2532   }
2533 }
2534 
2535 address InstanceKlass::static_field_addr(int offset) {
2536   return (address)(offset + InstanceMirrorKlass::offset_of_static_fields() + cast_from_oop&lt;intptr_t&gt;(java_mirror()));
2537 }
2538 
2539 
2540 const char* InstanceKlass::signature_name() const {
2541   int hash_len = 0;
2542   char hash_buf[40];
2543 
2544   // If this is an anonymous class, append a hash to make the name unique
2545   if (is_anonymous()) {
2546     assert(EnableInvokeDynamic, "EnableInvokeDynamic was not set.");
2547     intptr_t hash = (java_mirror() != NULL) ? java_mirror()-&gt;identity_hash() : 0;
2548     sprintf(hash_buf, "/" UINTX_FORMAT, (uintx)hash);
2549     hash_len = (int)strlen(hash_buf);
2550   }
2551 
2552   // Get the internal name as a c string
2553   const char* src = (const char*) (name()-&gt;as_C_string());
2554   const int src_length = (int)strlen(src);
2555 
2556   char* dest = NEW_RESOURCE_ARRAY(char, src_length + hash_len + 3);
2557 
2558   // Add L as type indicator
2559   int dest_index = 0;
2560   dest[dest_index++] = 'L';
2561 
2562   // Add the actual class name
2563   for (int src_index = 0; src_index &lt; src_length; ) {
2564     dest[dest_index++] = src[src_index++];
2565   }
2566 
2567   // If we have a hash, append it
2568   for (int hash_index = 0; hash_index &lt; hash_len; ) {
2569     dest[dest_index++] = hash_buf[hash_index++];
2570   }
2571 
2572   // Add the semicolon and the NULL
2573   dest[dest_index++] = ';';
2574   dest[dest_index] = '\0';
2575   return dest;
2576 }
2577 
2578 // different verisons of is_same_class_package
2579 bool InstanceKlass::is_same_class_package(Klass* class2) {
2580   Klass* class1 = this;
2581   oop classloader1 = InstanceKlass::cast(class1)-&gt;class_loader();
2582   Symbol* classname1 = class1-&gt;name();
2583 
2584   if (class2-&gt;oop_is_objArray()) {
2585     class2 = ObjArrayKlass::cast(class2)-&gt;bottom_klass();
2586   }
2587   oop classloader2;
2588   if (class2-&gt;oop_is_instance()) {
2589     classloader2 = InstanceKlass::cast(class2)-&gt;class_loader();
2590   } else {
2591     assert(class2-&gt;oop_is_typeArray(), "should be type array");
2592     classloader2 = NULL;
2593   }
2594   Symbol* classname2 = class2-&gt;name();
2595 
2596   return InstanceKlass::is_same_class_package(classloader1, classname1,
2597                                               classloader2, classname2);
2598 }
2599 
2600 bool InstanceKlass::is_same_class_package(oop classloader2, Symbol* classname2) {
2601   Klass* class1 = this;
2602   oop classloader1 = InstanceKlass::cast(class1)-&gt;class_loader();
2603   Symbol* classname1 = class1-&gt;name();
2604 
2605   return InstanceKlass::is_same_class_package(classloader1, classname1,
2606                                               classloader2, classname2);
2607 }
2608 
2609 // return true if two classes are in the same package, classloader
2610 // and classname information is enough to determine a class's package
2611 bool InstanceKlass::is_same_class_package(oop class_loader1, Symbol* class_name1,
2612                                           oop class_loader2, Symbol* class_name2) {
2613   if (class_loader1 != class_loader2) {
2614     return false;
2615   } else if (class_name1 == class_name2) {
2616     return true;                // skip painful bytewise comparison
2617   } else {
2618     ResourceMark rm;
2619 
2620     // The Symbol*'s are in UTF8 encoding. Since we only need to check explicitly
2621     // for ASCII characters ('/', 'L', '['), we can keep them in UTF8 encoding.
2622     // Otherwise, we just compare jbyte values between the strings.
2623     const jbyte *name1 = class_name1-&gt;base();
2624     const jbyte *name2 = class_name2-&gt;base();
2625 
2626     const jbyte *last_slash1 = UTF8::strrchr(name1, class_name1-&gt;utf8_length(), '/');
2627     const jbyte *last_slash2 = UTF8::strrchr(name2, class_name2-&gt;utf8_length(), '/');
2628 
2629     if ((last_slash1 == NULL) || (last_slash2 == NULL)) {
2630       // One of the two doesn't have a package.  Only return true
2631       // if the other one also doesn't have a package.
2632       return last_slash1 == last_slash2;
2633     } else {
2634       // Skip over '['s
2635       if (*name1 == '[') {
2636         do {
2637           name1++;
2638         } while (*name1 == '[');
2639         if (*name1 != 'L') {
2640           // Something is terribly wrong.  Shouldn't be here.
2641           return false;
2642         }
2643       }
2644       if (*name2 == '[') {
2645         do {
2646           name2++;
2647         } while (*name2 == '[');
2648         if (*name2 != 'L') {
2649           // Something is terribly wrong.  Shouldn't be here.
2650           return false;
2651         }
2652       }
2653 
2654       // Check that package part is identical
2655       int length1 = last_slash1 - name1;
2656       int length2 = last_slash2 - name2;
2657 
2658       return UTF8::equal(name1, length1, name2, length2);
2659     }
2660   }
2661 }
2662 
2663 // Returns true iff super_method can be overridden by a method in targetclassname
2664 // See JSL 3rd edition 8.4.6.1
2665 // Assumes name-signature match
2666 // "this" is InstanceKlass of super_method which must exist
2667 // note that the InstanceKlass of the method in the targetclassname has not always been created yet
2668 bool InstanceKlass::is_override(methodHandle super_method, Handle targetclassloader, Symbol* targetclassname, TRAPS) {
2669    // Private methods can not be overridden
2670    if (super_method-&gt;is_private()) {
2671      return false;
2672    }
2673    // If super method is accessible, then override
2674    if ((super_method-&gt;is_protected()) ||
2675        (super_method-&gt;is_public())) {
2676      return true;
2677    }
2678    // Package-private methods are not inherited outside of package
2679    assert(super_method-&gt;is_package_private(), "must be package private");
2680    return(is_same_class_package(targetclassloader(), targetclassname));
2681 }
2682 
2683 /* defined for now in jvm.cpp, for historical reasons *--
2684 Klass* InstanceKlass::compute_enclosing_class_impl(instanceKlassHandle self,
2685                                                      Symbol*&amp; simple_name_result, TRAPS) {
2686   ...
2687 }
2688 */
2689 
2690 // tell if two classes have the same enclosing class (at package level)
2691 bool InstanceKlass::is_same_package_member_impl(instanceKlassHandle class1,
2692                                                 Klass* class2_oop, TRAPS) {
2693   if (class2_oop == class1())                       return true;
2694   if (!class2_oop-&gt;oop_is_instance())  return false;
2695   instanceKlassHandle class2(THREAD, class2_oop);
2696 
2697   // must be in same package before we try anything else
2698   if (!class1-&gt;is_same_class_package(class2-&gt;class_loader(), class2-&gt;name()))
2699     return false;
2700 
2701   // As long as there is an outer1.getEnclosingClass,
2702   // shift the search outward.
2703   instanceKlassHandle outer1 = class1;
2704   for (;;) {
2705     // As we walk along, look for equalities between outer1 and class2.
2706     // Eventually, the walks will terminate as outer1 stops
2707     // at the top-level class around the original class.
2708     bool ignore_inner_is_member;
2709     Klass* next = outer1-&gt;compute_enclosing_class(&amp;ignore_inner_is_member,
2710                                                     CHECK_false);
2711     if (next == NULL)  break;
2712     if (next == class2())  return true;
2713     outer1 = instanceKlassHandle(THREAD, next);
2714   }
2715 
2716   // Now do the same for class2.
2717   instanceKlassHandle outer2 = class2;
2718   for (;;) {
2719     bool ignore_inner_is_member;
2720     Klass* next = outer2-&gt;compute_enclosing_class(&amp;ignore_inner_is_member,
2721                                                     CHECK_false);
2722     if (next == NULL)  break;
2723     // Might as well check the new outer against all available values.
2724     if (next == class1())  return true;
2725     if (next == outer1())  return true;
2726     outer2 = instanceKlassHandle(THREAD, next);
2727   }
2728 
2729   // If by this point we have not found an equality between the
2730   // two classes, we know they are in separate package members.
2731   return false;
2732 }
2733 
2734 
2735 jint InstanceKlass::compute_modifier_flags(TRAPS) const {
2736   jint access = access_flags().as_int();
2737 
2738   // But check if it happens to be member class.
2739   instanceKlassHandle ik(THREAD, this);
2740   InnerClassesIterator iter(ik);
2741   for (; !iter.done(); iter.next()) {
2742     int ioff = iter.inner_class_info_index();
2743     // Inner class attribute can be zero, skip it.
2744     // Strange but true:  JVM spec. allows null inner class refs.
2745     if (ioff == 0) continue;
2746 
2747     // only look at classes that are already loaded
2748     // since we are looking for the flags for our self.
2749     Symbol* inner_name = ik-&gt;constants()-&gt;klass_name_at(ioff);
2750     if ((ik-&gt;name() == inner_name)) {
2751       // This is really a member class.
2752       access = iter.inner_access_flags();
2753       break;
2754     }
2755   }
2756   // Remember to strip ACC_SUPER bit
2757   return (access &amp; (~JVM_ACC_SUPER)) &amp; JVM_ACC_WRITTEN_FLAGS;
2758 }
2759 
2760 jint InstanceKlass::jvmti_class_status() const {
2761   jint result = 0;
2762 
2763   if (is_linked()) {
2764     result |= JVMTI_CLASS_STATUS_VERIFIED | JVMTI_CLASS_STATUS_PREPARED;
2765   }
2766 
2767   if (is_initialized()) {
2768     assert(is_linked(), "Class status is not consistent");
2769     result |= JVMTI_CLASS_STATUS_INITIALIZED;
2770   }
2771   if (is_in_error_state()) {
2772     result |= JVMTI_CLASS_STATUS_ERROR;
2773   }
2774   return result;
2775 }
2776 
2777 Method* InstanceKlass::method_at_itable(Klass* holder, int index, TRAPS) {
2778   itableOffsetEntry* ioe = (itableOffsetEntry*)start_of_itable();
2779   int method_table_offset_in_words = ioe-&gt;offset()/wordSize;
2780   int nof_interfaces = (method_table_offset_in_words - itable_offset_in_words())
2781                        / itableOffsetEntry::size();
2782 
2783   for (int cnt = 0 ; ; cnt ++, ioe ++) {
2784     // If the interface isn't implemented by the receiver class,
2785     // the VM should throw IncompatibleClassChangeError.
2786     if (cnt &gt;= nof_interfaces) {
2787       THROW_NULL(vmSymbols::java_lang_IncompatibleClassChangeError());
2788     }
2789 
2790     Klass* ik = ioe-&gt;interface_klass();
2791     if (ik == holder) break;
2792   }
2793 
2794   itableMethodEntry* ime = ioe-&gt;first_method_entry(this);
2795   Method* m = ime[index].method();
2796   if (m == NULL) {
2797     THROW_NULL(vmSymbols::java_lang_AbstractMethodError());
2798   }
2799   return m;
2800 }
2801 
2802 
2803 #if INCLUDE_JVMTI
2804 // update default_methods for redefineclasses for methods that are
2805 // not yet in the vtable due to concurrent subclass define and superinterface
2806 // redefinition
2807 // Note: those in the vtable, should have been updated via adjust_method_entries
2808 void InstanceKlass::adjust_default_methods(InstanceKlass* holder, bool* trace_name_printed) {
2809   // search the default_methods for uses of either obsolete or EMCP methods
2810   if (default_methods() != NULL) {
2811     for (int index = 0; index &lt; default_methods()-&gt;length(); index ++) {
2812       Method* old_method = default_methods()-&gt;at(index);
2813       if (old_method == NULL || old_method-&gt;method_holder() != holder || !old_method-&gt;is_old()) {
2814         continue; // skip uninteresting entries
2815       }
2816       assert(!old_method-&gt;is_deleted(), "default methods may not be deleted");
2817 
2818       Method* new_method = holder-&gt;method_with_idnum(old_method-&gt;orig_method_idnum());
2819 
2820       assert(new_method != NULL, "method_with_idnum() should not be NULL");
2821       assert(old_method != new_method, "sanity check");
2822 
2823       default_methods()-&gt;at_put(index, new_method);
2824       if (RC_TRACE_IN_RANGE(0x00100000, 0x00400000)) {
2825         if (!(*trace_name_printed)) {
2826           // RC_TRACE_MESG macro has an embedded ResourceMark
2827           RC_TRACE_MESG(("adjust: klassname=%s default methods from name=%s",
2828                          external_name(),
2829                          old_method-&gt;method_holder()-&gt;external_name()));
2830           *trace_name_printed = true;
2831         }
2832         RC_TRACE(0x00100000, ("default method update: %s(%s) ",
2833                               new_method-&gt;name()-&gt;as_C_string(),
2834                               new_method-&gt;signature()-&gt;as_C_string()));
2835       }
2836     }
2837   }
2838 }
2839 #endif // INCLUDE_JVMTI
2840 
2841 // On-stack replacement stuff
2842 void InstanceKlass::add_osr_nmethod(nmethod* n) {
2843   // only one compilation can be active
2844   NEEDS_CLEANUP
2845   // This is a short non-blocking critical region, so the no safepoint check is ok.
2846   OsrList_lock-&gt;lock_without_safepoint_check();
2847   assert(n-&gt;is_osr_method(), "wrong kind of nmethod");
2848   n-&gt;set_osr_link(osr_nmethods_head());
2849   set_osr_nmethods_head(n);
2850   // Raise the highest osr level if necessary
2851   if (TieredCompilation) {
2852     Method* m = n-&gt;method();
2853     m-&gt;set_highest_osr_comp_level(MAX2(m-&gt;highest_osr_comp_level(), n-&gt;comp_level()));
2854   }
2855   // Remember to unlock again
2856   OsrList_lock-&gt;unlock();
2857 
2858   // Get rid of the osr methods for the same bci that have lower levels.
2859   if (TieredCompilation) {
2860     for (int l = CompLevel_limited_profile; l &lt; n-&gt;comp_level(); l++) {
2861       nmethod *inv = lookup_osr_nmethod(n-&gt;method(), n-&gt;osr_entry_bci(), l, true);
2862       if (inv != NULL &amp;&amp; inv-&gt;is_in_use()) {
2863         inv-&gt;make_not_entrant();
2864       }
2865     }
2866   }
2867 }
2868 
2869 
2870 void InstanceKlass::remove_osr_nmethod(nmethod* n) {
2871   // This is a short non-blocking critical region, so the no safepoint check is ok.
2872   OsrList_lock-&gt;lock_without_safepoint_check();
2873   assert(n-&gt;is_osr_method(), "wrong kind of nmethod");
2874   nmethod* last = NULL;
2875   nmethod* cur  = osr_nmethods_head();
2876   int max_level = CompLevel_none;  // Find the max comp level excluding n
2877   Method* m = n-&gt;method();
2878   // Search for match
2879   while(cur != NULL &amp;&amp; cur != n) {
2880     if (TieredCompilation &amp;&amp; m == cur-&gt;method()) {
2881       // Find max level before n
2882       max_level = MAX2(max_level, cur-&gt;comp_level());
2883     }
2884     last = cur;
2885     cur = cur-&gt;osr_link();
2886   }
2887   nmethod* next = NULL;
2888   if (cur == n) {
2889     next = cur-&gt;osr_link();
2890     if (last == NULL) {
2891       // Remove first element
2892       set_osr_nmethods_head(next);
2893     } else {
2894       last-&gt;set_osr_link(next);
2895     }
2896   }
2897   n-&gt;set_osr_link(NULL);
2898   if (TieredCompilation) {
2899     cur = next;
2900     while (cur != NULL) {
2901       // Find max level after n
2902       if (m == cur-&gt;method()) {
2903         max_level = MAX2(max_level, cur-&gt;comp_level());
2904       }
2905       cur = cur-&gt;osr_link();
2906     }
2907     m-&gt;set_highest_osr_comp_level(max_level);
2908   }
2909   // Remember to unlock again
2910   OsrList_lock-&gt;unlock();
2911 }
2912 
2913 int InstanceKlass::mark_osr_nmethods(const Method* m) {
2914   // This is a short non-blocking critical region, so the no safepoint check is ok.
2915   MutexLockerEx ml(OsrList_lock, Mutex::_no_safepoint_check_flag);
2916   nmethod* osr = osr_nmethods_head();
2917   int found = 0;
2918   while (osr != NULL) {
2919     assert(osr-&gt;is_osr_method(), "wrong kind of nmethod found in chain");
2920     if (osr-&gt;method() == m) {
2921       osr-&gt;mark_for_deoptimization();
2922       found++;
2923     }
2924     osr = osr-&gt;osr_link();
2925   }
2926   return found;
2927 }
2928 
2929 nmethod* InstanceKlass::lookup_osr_nmethod(const Method* m, int bci, int comp_level, bool match_level) const {
2930   // This is a short non-blocking critical region, so the no safepoint check is ok.
2931   OsrList_lock-&gt;lock_without_safepoint_check();
2932   nmethod* osr = osr_nmethods_head();
2933   nmethod* best = NULL;
2934   while (osr != NULL) {
2935     assert(osr-&gt;is_osr_method(), "wrong kind of nmethod found in chain");
2936     // There can be a time when a c1 osr method exists but we are waiting
2937     // for a c2 version. When c2 completes its osr nmethod we will trash
2938     // the c1 version and only be able to find the c2 version. However
2939     // while we overflow in the c1 code at back branches we don't want to
2940     // try and switch to the same code as we are already running
2941 
2942     if (osr-&gt;method() == m &amp;&amp;
2943         (bci == InvocationEntryBci || osr-&gt;osr_entry_bci() == bci)) {
2944       if (match_level) {
2945         if (osr-&gt;comp_level() == comp_level) {
2946           // Found a match - return it.
2947           OsrList_lock-&gt;unlock();
2948           return osr;
2949         }
2950       } else {
2951         if (best == NULL || (osr-&gt;comp_level() &gt; best-&gt;comp_level())) {
2952           if (osr-&gt;comp_level() == CompLevel_highest_tier) {
2953             // Found the best possible - return it.
2954             OsrList_lock-&gt;unlock();
2955             return osr;
2956           }
2957           best = osr;
2958         }
2959       }
2960     }
2961     osr = osr-&gt;osr_link();
2962   }
2963   OsrList_lock-&gt;unlock();
2964   if (best != NULL &amp;&amp; best-&gt;comp_level() &gt;= comp_level &amp;&amp; match_level == false) {
2965     return best;
2966   }
2967   return NULL;
2968 }
2969 
2970 bool InstanceKlass::add_member_name(Handle mem_name) {
2971   jweak mem_name_wref = JNIHandles::make_weak_global(mem_name);
2972   MutexLocker ml(MemberNameTable_lock);
2973   DEBUG_ONLY(No_Safepoint_Verifier nsv);
2974 
2975   // Check if method has been redefined while taking out MemberNameTable_lock, if so
2976   // return false.  We cannot cache obsolete methods. They will crash when the function
2977   // is called!
2978   Method* method = (Method*)java_lang_invoke_MemberName::vmtarget(mem_name());
2979   if (method-&gt;is_obsolete()) {
2980     return false;
2981   } else if (method-&gt;is_old()) {
2982     // Replace method with redefined version
2983     java_lang_invoke_MemberName::set_vmtarget(mem_name(), method_with_idnum(method-&gt;method_idnum()));
2984   }
2985 
2986   if (_member_names == NULL) {
2987     _member_names = new (ResourceObj::C_HEAP, mtClass) MemberNameTable(idnum_allocated_count());
2988   }
2989   _member_names-&gt;add_member_name(mem_name_wref);
2990   return true;
2991 }
2992 
2993 // -----------------------------------------------------------------------------------------------------
2994 // Printing
2995 
2996 #ifndef PRODUCT
2997 
2998 #define BULLET  " - "
2999 
3000 static const char* state_names[] = {
3001   "allocated", "loaded", "linked", "being_initialized", "fully_initialized", "initialization_error"
3002 };
3003 
3004 static void print_vtable(intptr_t* start, int len, outputStream* st) {
3005   for (int i = 0; i &lt; len; i++) {
3006     intptr_t e = start[i];
3007     st-&gt;print("%d : " INTPTR_FORMAT, i, e);
3008     if (e != 0 &amp;&amp; ((Metadata*)e)-&gt;is_metaspace_object()) {
3009       st-&gt;print(" ");
3010       ((Metadata*)e)-&gt;print_value_on(st);
3011     }
3012     st-&gt;cr();
3013   }
3014 }
3015 
3016 void InstanceKlass::print_on(outputStream* st) const {
3017   assert(is_klass(), "must be klass");
3018   Klass::print_on(st);
3019 
3020   st-&gt;print(BULLET"instance size:     %d", size_helper());                        st-&gt;cr();
3021   st-&gt;print(BULLET"klass size:        %d", size());                               st-&gt;cr();
3022   st-&gt;print(BULLET"access:            "); access_flags().print_on(st);            st-&gt;cr();
3023   st-&gt;print(BULLET"state:             "); st-&gt;print_cr("%s", state_names[_init_state]);
3024   st-&gt;print(BULLET"name:              "); name()-&gt;print_value_on(st);             st-&gt;cr();
3025   st-&gt;print(BULLET"super:             "); super()-&gt;print_value_on_maybe_null(st); st-&gt;cr();
3026   st-&gt;print(BULLET"sub:               ");
3027   Klass* sub = subklass();
3028   int n;
3029   for (n = 0; sub != NULL; n++, sub = sub-&gt;next_sibling()) {
3030     if (n &lt; MaxSubklassPrintSize) {
3031       sub-&gt;print_value_on(st);
3032       st-&gt;print("   ");
3033     }
3034   }
3035   if (n &gt;= MaxSubklassPrintSize) st-&gt;print("(%d more klasses...)", n - MaxSubklassPrintSize);
3036   st-&gt;cr();
3037 
3038   if (is_interface()) {
3039     st-&gt;print_cr(BULLET"nof implementors:  %d", nof_implementors());
3040     if (nof_implementors() == 1) {
3041       st-&gt;print_cr(BULLET"implementor:    ");
3042       st-&gt;print("   ");
3043       implementor()-&gt;print_value_on(st);
3044       st-&gt;cr();
3045     }
3046   }
3047 
3048   st-&gt;print(BULLET"arrays:            "); array_klasses()-&gt;print_value_on_maybe_null(st); st-&gt;cr();
3049   st-&gt;print(BULLET"methods:           "); methods()-&gt;print_value_on(st);                  st-&gt;cr();
3050   if (Verbose || WizardMode) {
3051     Array&lt;Method*&gt;* method_array = methods();
3052     for (int i = 0; i &lt; method_array-&gt;length(); i++) {
3053       st-&gt;print("%d : ", i); method_array-&gt;at(i)-&gt;print_value(); st-&gt;cr();
3054     }
3055   }
3056   st-&gt;print(BULLET"method ordering:   "); method_ordering()-&gt;print_value_on(st);      st-&gt;cr();
3057   st-&gt;print(BULLET"default_methods:   "); default_methods()-&gt;print_value_on(st);      st-&gt;cr();
3058   if (Verbose &amp;&amp; default_methods() != NULL) {
3059     Array&lt;Method*&gt;* method_array = default_methods();
3060     for (int i = 0; i &lt; method_array-&gt;length(); i++) {
3061       st-&gt;print("%d : ", i); method_array-&gt;at(i)-&gt;print_value(); st-&gt;cr();
3062     }
3063   }
3064   if (default_vtable_indices() != NULL) {
3065     st-&gt;print(BULLET"default vtable indices:   "); default_vtable_indices()-&gt;print_value_on(st);       st-&gt;cr();
3066   }
3067   st-&gt;print(BULLET"local interfaces:  "); local_interfaces()-&gt;print_value_on(st);      st-&gt;cr();
3068   st-&gt;print(BULLET"trans. interfaces: "); transitive_interfaces()-&gt;print_value_on(st); st-&gt;cr();
3069   st-&gt;print(BULLET"constants:         "); constants()-&gt;print_value_on(st);         st-&gt;cr();
3070   if (class_loader_data() != NULL) {
3071     st-&gt;print(BULLET"class loader data:  ");
3072     class_loader_data()-&gt;print_value_on(st);
3073     st-&gt;cr();
3074   }
3075   st-&gt;print(BULLET"host class:        "); host_klass()-&gt;print_value_on_maybe_null(st); st-&gt;cr();
3076   if (source_file_name() != NULL) {
3077     st-&gt;print(BULLET"source file:       ");
3078     source_file_name()-&gt;print_value_on(st);
3079     st-&gt;cr();
3080   }
3081   if (source_debug_extension() != NULL) {
3082     st-&gt;print(BULLET"source debug extension:       ");
3083     st-&gt;print("%s", source_debug_extension());
3084     st-&gt;cr();
3085   }
3086   st-&gt;print(BULLET"class annotations:       "); class_annotations()-&gt;print_value_on(st); st-&gt;cr();
3087   st-&gt;print(BULLET"class type annotations:  "); class_type_annotations()-&gt;print_value_on(st); st-&gt;cr();
3088   st-&gt;print(BULLET"field annotations:       "); fields_annotations()-&gt;print_value_on(st); st-&gt;cr();
3089   st-&gt;print(BULLET"field type annotations:  "); fields_type_annotations()-&gt;print_value_on(st); st-&gt;cr();
3090   {
3091     bool have_pv = false;
3092     PreviousVersionWalker pvw(Thread::current(), (InstanceKlass*)this);
3093     for (PreviousVersionNode * pv_node = pvw.next_previous_version();
3094          pv_node != NULL; pv_node = pvw.next_previous_version()) {
3095       if (!have_pv)
3096         st-&gt;print(BULLET"previous version:  ");
3097       have_pv = true;
3098       pv_node-&gt;prev_constant_pool()-&gt;print_value_on(st);
3099     }
3100     if (have_pv) st-&gt;cr();
3101   } // pvw is cleaned up
3102 
3103   if (generic_signature() != NULL) {
3104     st-&gt;print(BULLET"generic signature: ");
3105     generic_signature()-&gt;print_value_on(st);
3106     st-&gt;cr();
3107   }
3108   st-&gt;print(BULLET"inner classes:     "); inner_classes()-&gt;print_value_on(st);     st-&gt;cr();
3109   st-&gt;print(BULLET"java mirror:       "); java_mirror()-&gt;print_value_on(st);       st-&gt;cr();
3110   st-&gt;print(BULLET"vtable length      %d  (start addr: " INTPTR_FORMAT ")", vtable_length(), start_of_vtable());  st-&gt;cr();
3111   if (vtable_length() &gt; 0 &amp;&amp; (Verbose || WizardMode))  print_vtable(start_of_vtable(), vtable_length(), st);
3112   st-&gt;print(BULLET"itable length      %d (start addr: " INTPTR_FORMAT ")", itable_length(), start_of_itable()); st-&gt;cr();
3113   if (itable_length() &gt; 0 &amp;&amp; (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);
3114   st-&gt;print_cr(BULLET"---- static fields (%d words):", static_field_size());
3115   FieldPrinter print_static_field(st);
3116   ((InstanceKlass*)this)-&gt;do_local_static_fields(&amp;print_static_field);
3117   st-&gt;print_cr(BULLET"---- non-static fields (%d words):", nonstatic_field_size());
3118   FieldPrinter print_nonstatic_field(st);
3119   ((InstanceKlass*)this)-&gt;do_nonstatic_fields(&amp;print_nonstatic_field);
3120 
3121   st-&gt;print(BULLET"non-static oop maps: ");
3122   OopMapBlock* map     = start_of_nonstatic_oop_maps();
3123   OopMapBlock* end_map = map + nonstatic_oop_map_count();
3124   while (map &lt; end_map) {
3125     st-&gt;print("%d-%d ", map-&gt;offset(), map-&gt;offset() + heapOopSize*(map-&gt;count() - 1));
3126     map++;
3127   }
3128   st-&gt;cr();
3129 }
3130 
3131 #endif //PRODUCT
3132 
3133 void InstanceKlass::print_value_on(outputStream* st) const {
3134   assert(is_klass(), "must be klass");
3135   if (Verbose || WizardMode)  access_flags().print_on(st);
3136   name()-&gt;print_value_on(st);
3137 }
3138 
3139 #ifndef PRODUCT
3140 
3141 void FieldPrinter::do_field(fieldDescriptor* fd) {
3142   _st-&gt;print(BULLET);
3143    if (_obj == NULL) {
3144      fd-&gt;print_on(_st);
3145      _st-&gt;cr();
3146    } else {
3147      fd-&gt;print_on_for(_st, _obj);
3148      _st-&gt;cr();
3149    }
3150 }
3151 
3152 
3153 void InstanceKlass::oop_print_on(oop obj, outputStream* st) {
3154   Klass::oop_print_on(obj, st);
3155 
3156   if (this == SystemDictionary::String_klass()) {
3157     typeArrayOop value  = java_lang_String::value(obj);
3158     juint        offset = java_lang_String::offset(obj);
3159     juint        length = java_lang_String::length(obj);
3160     if (value != NULL &amp;&amp;
3161         value-&gt;is_typeArray() &amp;&amp;
3162         offset          &lt;= (juint) value-&gt;length() &amp;&amp;
3163         offset + length &lt;= (juint) value-&gt;length()) {
3164       st-&gt;print(BULLET"string: ");
3165       java_lang_String::print(obj, st);
3166       st-&gt;cr();
3167       if (!WizardMode)  return;  // that is enough
3168     }
3169   }
3170 
3171   st-&gt;print_cr(BULLET"---- fields (total size %d words):", oop_size(obj));
3172   FieldPrinter print_field(st, obj);
3173   do_nonstatic_fields(&amp;print_field);
3174 
3175   if (this == SystemDictionary::Class_klass()) {
3176     st-&gt;print(BULLET"signature: ");
3177     java_lang_Class::print_signature(obj, st);
3178     st-&gt;cr();
3179     Klass* mirrored_klass = java_lang_Class::as_Klass(obj);
3180     st-&gt;print(BULLET"fake entry for mirror: ");
3181     mirrored_klass-&gt;print_value_on_maybe_null(st);
3182     st-&gt;cr();
3183     Klass* array_klass = java_lang_Class::array_klass(obj);
3184     st-&gt;print(BULLET"fake entry for array: ");
3185     array_klass-&gt;print_value_on_maybe_null(st);
3186     st-&gt;cr();
3187     st-&gt;print_cr(BULLET"fake entry for oop_size: %d", java_lang_Class::oop_size(obj));
3188     st-&gt;print_cr(BULLET"fake entry for static_oop_field_count: %d", java_lang_Class::static_oop_field_count(obj));
3189     Klass* real_klass = java_lang_Class::as_Klass(obj);
3190     if (real_klass != NULL &amp;&amp; real_klass-&gt;oop_is_instance()) {
3191       InstanceKlass::cast(real_klass)-&gt;do_local_static_fields(&amp;print_field);
3192     }
3193   } else if (this == SystemDictionary::MethodType_klass()) {
3194     st-&gt;print(BULLET"signature: ");
3195     java_lang_invoke_MethodType::print_signature(obj, st);
3196     st-&gt;cr();
3197   }
3198 }
3199 
3200 #endif //PRODUCT
3201 
3202 void InstanceKlass::oop_print_value_on(oop obj, outputStream* st) {
3203   st-&gt;print("a ");
3204   name()-&gt;print_value_on(st);
3205   obj-&gt;print_address_on(st);
3206   if (this == SystemDictionary::String_klass()
3207       &amp;&amp; java_lang_String::value(obj) != NULL) {
3208     ResourceMark rm;
3209     int len = java_lang_String::length(obj);
3210     int plen = (len &lt; 24 ? len : 12);
3211     char* str = java_lang_String::as_utf8_string(obj, 0, plen);
3212     st-&gt;print(" = \"%s\"", str);
3213     if (len &gt; plen)
3214       st-&gt;print("...[%d]", len);
3215   } else if (this == SystemDictionary::Class_klass()) {
3216     Klass* k = java_lang_Class::as_Klass(obj);
3217     st-&gt;print(" = ");
3218     if (k != NULL) {
3219       k-&gt;print_value_on(st);
3220     } else {
3221       const char* tname = type2name(java_lang_Class::primitive_type(obj));
3222       st-&gt;print("%s", tname ? tname : "type?");
3223     }
3224   } else if (this == SystemDictionary::MethodType_klass()) {
3225     st-&gt;print(" = ");
3226     java_lang_invoke_MethodType::print_signature(obj, st);
3227   } else if (java_lang_boxing_object::is_instance(obj)) {
3228     st-&gt;print(" = ");
3229     java_lang_boxing_object::print(obj, st);
3230   } else if (this == SystemDictionary::LambdaForm_klass()) {
3231     oop vmentry = java_lang_invoke_LambdaForm::vmentry(obj);
3232     if (vmentry != NULL) {
3233       st-&gt;print(" =&gt; ");
3234       vmentry-&gt;print_value_on(st);
3235     }
3236   } else if (this == SystemDictionary::MemberName_klass()) {
3237     Metadata* vmtarget = java_lang_invoke_MemberName::vmtarget(obj);
3238     if (vmtarget != NULL) {
3239       st-&gt;print(" = ");
3240       vmtarget-&gt;print_value_on(st);
3241     } else {
3242       java_lang_invoke_MemberName::clazz(obj)-&gt;print_value_on(st);
3243       st-&gt;print(".");
3244       java_lang_invoke_MemberName::name(obj)-&gt;print_value_on(st);
3245     }
3246   }
3247 }
3248 
3249 const char* InstanceKlass::internal_name() const {
3250   return external_name();
3251 }
3252 
3253 #if INCLUDE_SERVICES
3254 // Size Statistics
3255 void InstanceKlass::collect_statistics(KlassSizeStats *sz) const {
3256   Klass::collect_statistics(sz);
3257 
3258   sz-&gt;_inst_size  = HeapWordSize * size_helper();
3259   sz-&gt;_vtab_bytes = HeapWordSize * align_object_offset(vtable_length());
3260   sz-&gt;_itab_bytes = HeapWordSize * align_object_offset(itable_length());
3261   sz-&gt;_nonstatic_oopmap_bytes = HeapWordSize *
3262         ((is_interface() || is_anonymous()) ?
3263          align_object_offset(nonstatic_oop_map_size()) :
3264          nonstatic_oop_map_size());
3265 
3266   int n = 0;
3267   n += (sz-&gt;_methods_array_bytes         = sz-&gt;count_array(methods()));
3268   n += (sz-&gt;_method_ordering_bytes       = sz-&gt;count_array(method_ordering()));
3269   n += (sz-&gt;_local_interfaces_bytes      = sz-&gt;count_array(local_interfaces()));
3270   n += (sz-&gt;_transitive_interfaces_bytes = sz-&gt;count_array(transitive_interfaces()));
3271   n += (sz-&gt;_fields_bytes                = sz-&gt;count_array(fields()));
3272   n += (sz-&gt;_inner_classes_bytes         = sz-&gt;count_array(inner_classes()));
3273   sz-&gt;_ro_bytes += n;
3274 
3275   const ConstantPool* cp = constants();
3276   if (cp) {
3277     cp-&gt;collect_statistics(sz);
3278   }
3279 
3280   const Annotations* anno = annotations();
3281   if (anno) {
3282     anno-&gt;collect_statistics(sz);
3283   }
3284 
3285   const Array&lt;Method*&gt;* methods_array = methods();
3286   if (methods()) {
3287     for (int i = 0; i &lt; methods_array-&gt;length(); i++) {
3288       Method* method = methods_array-&gt;at(i);
3289       if (method) {
3290         sz-&gt;_method_count ++;
3291         method-&gt;collect_statistics(sz);
3292       }
3293     }
3294   }
3295 }
3296 #endif // INCLUDE_SERVICES
3297 
3298 // Verification
3299 
3300 class VerifyFieldClosure: public OopClosure {
3301  protected:
3302   template &lt;class T&gt; void do_oop_work(T* p) {
3303     oop obj = oopDesc::load_decode_heap_oop(p);
3304     if (!obj-&gt;is_oop_or_null()) {
3305       tty-&gt;print_cr("Failed: " PTR_FORMAT " -&gt; " PTR_FORMAT, p, (address)obj);
3306       Universe::print();
3307       guarantee(false, "boom");
3308     }
3309   }
3310  public:
3311   virtual void do_oop(oop* p)       { VerifyFieldClosure::do_oop_work(p); }
3312   virtual void do_oop(narrowOop* p) { VerifyFieldClosure::do_oop_work(p); }
3313 };
3314 
3315 void InstanceKlass::verify_on(outputStream* st) {
3316 #ifndef PRODUCT
3317   // Avoid redundant verifies, this really should be in product.
3318   if (_verify_count == Universe::verify_count()) return;
3319   _verify_count = Universe::verify_count();
3320 #endif
3321 
3322   // Verify Klass
3323   Klass::verify_on(st);
3324 
3325   // Verify that klass is present in ClassLoaderData
3326   guarantee(class_loader_data()-&gt;contains_klass(this),
3327             "this class isn't found in class loader data");
3328 
3329   // Verify vtables
3330   if (is_linked()) {
3331     ResourceMark rm;
3332     // $$$ This used to be done only for m/s collections.  Doing it
3333     // always seemed a valid generalization.  (DLD -- 6/00)
3334     vtable()-&gt;verify(st);
3335   }
3336 
3337   // Verify first subklass
3338   if (subklass_oop() != NULL) {
3339     guarantee(subklass_oop()-&gt;is_klass(), "should be klass");
3340   }
3341 
3342   // Verify siblings
3343   Klass* super = this-&gt;super();
3344   Klass* sib = next_sibling();
3345   if (sib != NULL) {
3346     if (sib == this) {
3347       fatal(err_msg("subclass points to itself " PTR_FORMAT, sib));
3348     }
3349 
3350     guarantee(sib-&gt;is_klass(), "should be klass");
3351     guarantee(sib-&gt;super() == super, "siblings should have same superklass");
3352   }
3353 
3354   // Verify implementor fields
3355   Klass* im = implementor();
3356   if (im != NULL) {
3357     guarantee(is_interface(), "only interfaces should have implementor set");
3358     guarantee(im-&gt;is_klass(), "should be klass");
3359     guarantee(!im-&gt;is_interface() || im == this,
3360       "implementors cannot be interfaces");
3361   }
3362 
3363   // Verify local interfaces
3364   if (local_interfaces()) {
3365     Array&lt;Klass*&gt;* local_interfaces = this-&gt;local_interfaces();
3366     for (int j = 0; j &lt; local_interfaces-&gt;length(); j++) {
3367       Klass* e = local_interfaces-&gt;at(j);
3368       guarantee(e-&gt;is_klass() &amp;&amp; e-&gt;is_interface(), "invalid local interface");
3369     }
3370   }
3371 
3372   // Verify transitive interfaces
3373   if (transitive_interfaces() != NULL) {
3374     Array&lt;Klass*&gt;* transitive_interfaces = this-&gt;transitive_interfaces();
3375     for (int j = 0; j &lt; transitive_interfaces-&gt;length(); j++) {
3376       Klass* e = transitive_interfaces-&gt;at(j);
3377       guarantee(e-&gt;is_klass() &amp;&amp; e-&gt;is_interface(), "invalid transitive interface");
3378     }
3379   }
3380 
3381   // Verify methods
3382   if (methods() != NULL) {
3383     Array&lt;Method*&gt;* methods = this-&gt;methods();
3384     for (int j = 0; j &lt; methods-&gt;length(); j++) {
3385       guarantee(methods-&gt;at(j)-&gt;is_method(), "non-method in methods array");
3386     }
3387     for (int j = 0; j &lt; methods-&gt;length() - 1; j++) {
3388       Method* m1 = methods-&gt;at(j);
3389       Method* m2 = methods-&gt;at(j + 1);
3390       guarantee(m1-&gt;name()-&gt;fast_compare(m2-&gt;name()) &lt;= 0, "methods not sorted correctly");
3391     }
3392   }
3393 
3394   // Verify method ordering
3395   if (method_ordering() != NULL) {
3396     Array&lt;int&gt;* method_ordering = this-&gt;method_ordering();
3397     int length = method_ordering-&gt;length();
3398     if (JvmtiExport::can_maintain_original_method_order() ||
3399         ((UseSharedSpaces || DumpSharedSpaces) &amp;&amp; length != 0)) {
3400       guarantee(length == methods()-&gt;length(), "invalid method ordering length");
3401       jlong sum = 0;
3402       for (int j = 0; j &lt; length; j++) {
3403         int original_index = method_ordering-&gt;at(j);
3404         guarantee(original_index &gt;= 0, "invalid method ordering index");
3405         guarantee(original_index &lt; length, "invalid method ordering index");
3406         sum += original_index;
3407       }
3408       // Verify sum of indices 0,1,...,length-1
3409       guarantee(sum == ((jlong)length*(length-1))/2, "invalid method ordering sum");
3410     } else {
3411       guarantee(length == 0, "invalid method ordering length");
3412     }
3413   }
3414 
3415   // Verify default methods
3416   if (default_methods() != NULL) {
3417     Array&lt;Method*&gt;* methods = this-&gt;default_methods();
3418     for (int j = 0; j &lt; methods-&gt;length(); j++) {
3419       guarantee(methods-&gt;at(j)-&gt;is_method(), "non-method in methods array");
3420     }
3421     for (int j = 0; j &lt; methods-&gt;length() - 1; j++) {
3422       Method* m1 = methods-&gt;at(j);
3423       Method* m2 = methods-&gt;at(j + 1);
3424       guarantee(m1-&gt;name()-&gt;fast_compare(m2-&gt;name()) &lt;= 0, "methods not sorted correctly");
3425     }
3426   }
3427 
3428   // Verify JNI static field identifiers
3429   if (jni_ids() != NULL) {
3430     jni_ids()-&gt;verify(this);
3431   }
3432 
3433   // Verify other fields
3434   if (array_klasses() != NULL) {
3435     guarantee(array_klasses()-&gt;is_klass(), "should be klass");
3436   }
3437   if (constants() != NULL) {
3438     guarantee(constants()-&gt;is_constantPool(), "should be constant pool");
3439   }
3440   const Klass* host = host_klass();
3441   if (host != NULL) {
3442     guarantee(host-&gt;is_klass(), "should be klass");
3443   }
3444 }
3445 
3446 void InstanceKlass::oop_verify_on(oop obj, outputStream* st) {
3447   Klass::oop_verify_on(obj, st);
3448   VerifyFieldClosure blk;
3449   obj-&gt;oop_iterate_no_header(&amp;blk);
3450 }
3451 
3452 
3453 // JNIid class for jfieldIDs only
3454 // Note to reviewers:
3455 // These JNI functions are just moved over to column 1 and not changed
3456 // in the compressed oops workspace.
3457 JNIid::JNIid(Klass* holder, int offset, JNIid* next) {
3458   _holder = holder;
3459   _offset = offset;
3460   _next = next;
3461   debug_only(_is_static_field_id = false;)
3462 }
3463 
3464 
3465 JNIid* JNIid::find(int offset) {
3466   JNIid* current = this;
3467   while (current != NULL) {
3468     if (current-&gt;offset() == offset) return current;
3469     current = current-&gt;next();
3470   }
3471   return NULL;
3472 }
3473 
3474 void JNIid::deallocate(JNIid* current) {
3475   while (current != NULL) {
3476     JNIid* next = current-&gt;next();
3477     delete current;
3478     current = next;
3479   }
3480 }
3481 
3482 
3483 void JNIid::verify(Klass* holder) {
3484   int first_field_offset  = InstanceMirrorKlass::offset_of_static_fields();
3485   int end_field_offset;
3486   end_field_offset = first_field_offset + (InstanceKlass::cast(holder)-&gt;static_field_size() * wordSize);
3487 
3488   JNIid* current = this;
3489   while (current != NULL) {
3490     guarantee(current-&gt;holder() == holder, "Invalid klass in JNIid");
3491 #ifdef ASSERT
3492     int o = current-&gt;offset();
3493     if (current-&gt;is_static_field_id()) {
3494       guarantee(o &gt;= first_field_offset  &amp;&amp; o &lt; end_field_offset,  "Invalid static field offset in JNIid");
3495     }
3496 #endif
3497     current = current-&gt;next();
3498   }
3499 }
3500 
3501 
3502 #ifdef ASSERT
3503 void InstanceKlass::set_init_state(ClassState state) {
3504   bool good_state = is_shared() ? (_init_state &lt;= state)
3505                                                : (_init_state &lt; state);
3506   assert(good_state || state == allocated, "illegal state transition");
3507   _init_state = (u1)state;
3508 }
3509 #endif
3510 
3511 
3512 // RedefineClasses() support for previous versions:
3513 
3514 // Purge previous versions
3515 static void purge_previous_versions_internal(InstanceKlass* ik, int emcp_method_count) {
3516   if (ik-&gt;previous_versions() != NULL) {
3517     // This klass has previous versions so see what we can cleanup
3518     // while it is safe to do so.
3519 
3520     int deleted_count = 0;    // leave debugging breadcrumbs
3521     int live_count = 0;
3522     ClassLoaderData* loader_data = ik-&gt;class_loader_data() == NULL ?
3523                        ClassLoaderData::the_null_class_loader_data() :
3524                        ik-&gt;class_loader_data();
3525 
3526     // RC_TRACE macro has an embedded ResourceMark
3527     RC_TRACE(0x00000200, ("purge: %s: previous version length=%d",
3528       ik-&gt;external_name(), ik-&gt;previous_versions()-&gt;length()));
3529 
3530     for (int i = ik-&gt;previous_versions()-&gt;length() - 1; i &gt;= 0; i--) {
3531       // check the previous versions array
3532       PreviousVersionNode * pv_node = ik-&gt;previous_versions()-&gt;at(i);
3533       ConstantPool* cp_ref = pv_node-&gt;prev_constant_pool();
3534       assert(cp_ref != NULL, "cp ref was unexpectedly cleared");
3535 
3536       ConstantPool* pvcp = cp_ref;
3537       if (!pvcp-&gt;on_stack()) {
3538         // If the constant pool isn't on stack, none of the methods
3539         // are executing.  Delete all the methods, the constant pool and
3540         // and this previous version node.
3541         GrowableArray&lt;Method*&gt;* method_refs = pv_node-&gt;prev_EMCP_methods();
3542         if (method_refs != NULL) {
3543           for (int j = method_refs-&gt;length() - 1; j &gt;= 0; j--) {
3544             Method* method = method_refs-&gt;at(j);
3545             assert(method != NULL, "method ref was unexpectedly cleared");
3546             method_refs-&gt;remove_at(j);
3547             // method will be freed with associated class.
3548           }
3549         }
3550         // Remove the constant pool
3551         delete pv_node;
3552         // Since we are traversing the array backwards, we don't have to
3553         // do anything special with the index.
3554         ik-&gt;previous_versions()-&gt;remove_at(i);
3555         deleted_count++;
3556         continue;
3557       } else {
3558         RC_TRACE(0x00000200, ("purge: previous version @%d is alive", i));
3559         assert(pvcp-&gt;pool_holder() != NULL, "Constant pool with no holder");
3560         guarantee (!loader_data-&gt;is_unloading(), "unloaded classes can't be on the stack");
3561         live_count++;
3562       }
3563 
3564       // At least one method is live in this previous version, clean out
3565       // the others or mark them as obsolete.
3566       GrowableArray&lt;Method*&gt;* method_refs = pv_node-&gt;prev_EMCP_methods();
3567       if (method_refs != NULL) {
3568         RC_TRACE(0x00000200, ("purge: previous methods length=%d",
3569           method_refs-&gt;length()));
3570         for (int j = method_refs-&gt;length() - 1; j &gt;= 0; j--) {
3571           Method* method = method_refs-&gt;at(j);
3572           assert(method != NULL, "method ref was unexpectedly cleared");
3573 
3574           // Remove the emcp method if it's not executing
3575           // If it's been made obsolete by a redefinition of a non-emcp
3576           // method, mark it as obsolete but leave it to clean up later.
3577           if (!method-&gt;on_stack()) {
3578             method_refs-&gt;remove_at(j);
3579           } else if (emcp_method_count == 0) {
3580             method-&gt;set_is_obsolete();
3581           } else {
3582             // RC_TRACE macro has an embedded ResourceMark
3583             RC_TRACE(0x00000200,
3584               ("purge: %s(%s): prev method @%d in version @%d is alive",
3585               method-&gt;name()-&gt;as_C_string(),
3586               method-&gt;signature()-&gt;as_C_string(), j, i));
3587           }
3588         }
3589       }
3590     }
3591     assert(ik-&gt;previous_versions()-&gt;length() == live_count, "sanity check");
3592     RC_TRACE(0x00000200,
3593       ("purge: previous version stats: live=%d, deleted=%d", live_count,
3594       deleted_count));
3595   }
3596 }
3597 
3598 // External interface for use during class unloading.
3599 void InstanceKlass::purge_previous_versions(InstanceKlass* ik) {
3600   // Call with &gt;0 emcp methods since they are not currently being redefined.
3601   purge_previous_versions_internal(ik, 1);
3602 }
3603 
3604 
3605 // Potentially add an information node that contains pointers to the
3606 // interesting parts of the previous version of the_class.
3607 // This is also where we clean out any unused references.
3608 // Note that while we delete nodes from the _previous_versions
3609 // array, we never delete the array itself until the klass is
3610 // unloaded. The has_been_redefined() query depends on that fact.
3611 //
3612 void InstanceKlass::add_previous_version(instanceKlassHandle ikh,
3613        BitMap* emcp_methods, int emcp_method_count) {
3614   assert(Thread::current()-&gt;is_VM_thread(),
3615          "only VMThread can add previous versions");
3616 
3617   if (_previous_versions == NULL) {
3618     // This is the first previous version so make some space.
3619     // Start with 2 elements under the assumption that the class
3620     // won't be redefined much.
3621     _previous_versions =  new (ResourceObj::C_HEAP, mtClass)
3622                             GrowableArray&lt;PreviousVersionNode *&gt;(2, true);
3623   }
3624 
3625   ConstantPool* cp_ref = ikh-&gt;constants();
3626 
3627   // RC_TRACE macro has an embedded ResourceMark
3628   RC_TRACE(0x00000400, ("adding previous version ref for %s @%d, EMCP_cnt=%d "
3629                         "on_stack=%d",
3630     ikh-&gt;external_name(), _previous_versions-&gt;length(), emcp_method_count,
3631     cp_ref-&gt;on_stack()));
3632 
3633   // If the constant pool for this previous version of the class
3634   // is not marked as being on the stack, then none of the methods
3635   // in this previous version of the class are on the stack so
3636   // we don't need to create a new PreviousVersionNode. However,
3637   // we still need to examine older previous versions below.
3638   Array&lt;Method*&gt;* old_methods = ikh-&gt;methods();
3639 
3640   if (cp_ref-&gt;on_stack()) {
3641     PreviousVersionNode * pv_node = NULL;
3642     if (emcp_method_count == 0) {
3643       // non-shared ConstantPool gets a reference
3644       pv_node = new PreviousVersionNode(cp_ref, NULL);
3645       RC_TRACE(0x00000400,
3646           ("add: all methods are obsolete; flushing any EMCP refs"));
3647     } else {
3648       int local_count = 0;
3649       GrowableArray&lt;Method*&gt;* method_refs = new (ResourceObj::C_HEAP, mtClass)
3650           GrowableArray&lt;Method*&gt;(emcp_method_count, true);
3651       for (int i = 0; i &lt; old_methods-&gt;length(); i++) {
3652         if (emcp_methods-&gt;at(i)) {
3653             // this old method is EMCP. Save it only if it's on the stack
3654             Method* old_method = old_methods-&gt;at(i);
3655             if (old_method-&gt;on_stack()) {
3656               method_refs-&gt;append(old_method);
3657             }
3658           if (++local_count &gt;= emcp_method_count) {
3659             // no more EMCP methods so bail out now
3660             break;
3661           }
3662         }
3663       }
3664       // non-shared ConstantPool gets a reference
3665       pv_node = new PreviousVersionNode(cp_ref, method_refs);
3666     }
3667     // append new previous version.
3668     _previous_versions-&gt;append(pv_node);
3669   }
3670 
3671   // Since the caller is the VMThread and we are at a safepoint, this
3672   // is a good time to clear out unused references.
3673 
3674   RC_TRACE(0x00000400, ("add: previous version length=%d",
3675     _previous_versions-&gt;length()));
3676 
3677   // Purge previous versions not executing on the stack
3678   purge_previous_versions_internal(this, emcp_method_count);
3679 
3680   int obsolete_method_count = old_methods-&gt;length() - emcp_method_count;
3681 
3682   if (emcp_method_count != 0 &amp;&amp; obsolete_method_count != 0 &amp;&amp;
3683       _previous_versions-&gt;length() &gt; 0) {
3684     // We have a mix of obsolete and EMCP methods so we have to
3685     // clear out any matching EMCP method entries the hard way.
3686     int local_count = 0;
3687     for (int i = 0; i &lt; old_methods-&gt;length(); i++) {
3688       if (!emcp_methods-&gt;at(i)) {
3689         // only obsolete methods are interesting
3690         Method* old_method = old_methods-&gt;at(i);
3691         Symbol* m_name = old_method-&gt;name();
3692         Symbol* m_signature = old_method-&gt;signature();
3693 
3694         // we might not have added the last entry
3695         for (int j = _previous_versions-&gt;length() - 1; j &gt;= 0; j--) {
3696           // check the previous versions array for non executing obsolete methods
3697           PreviousVersionNode * pv_node = _previous_versions-&gt;at(j);
3698 
3699           GrowableArray&lt;Method*&gt;* method_refs = pv_node-&gt;prev_EMCP_methods();
3700           if (method_refs == NULL) {
3701             // We have run into a PreviousVersion generation where
3702             // all methods were made obsolete during that generation's
3703             // RedefineClasses() operation. At the time of that
3704             // operation, all EMCP methods were flushed so we don't
3705             // have to go back any further.
3706             //
3707             // A NULL method_refs is different than an empty method_refs.
3708             // We cannot infer any optimizations about older generations
3709             // from an empty method_refs for the current generation.
3710             break;
3711           }
3712 
3713           for (int k = method_refs-&gt;length() - 1; k &gt;= 0; k--) {
3714             Method* method = method_refs-&gt;at(k);
3715 
3716             if (!method-&gt;is_obsolete() &amp;&amp;
3717                 method-&gt;name() == m_name &amp;&amp;
3718                 method-&gt;signature() == m_signature) {
3719               // The current RedefineClasses() call has made all EMCP
3720               // versions of this method obsolete so mark it as obsolete
3721               // and remove the reference.
3722               RC_TRACE(0x00000400,
3723                 ("add: %s(%s): flush obsolete method @%d in version @%d",
3724                 m_name-&gt;as_C_string(), m_signature-&gt;as_C_string(), k, j));
3725 
3726               method-&gt;set_is_obsolete();
3727               // Leave obsolete methods on the previous version list to
3728               // clean up later.
3729               break;
3730             }
3731           }
3732 
3733           // The previous loop may not find a matching EMCP method, but
3734           // that doesn't mean that we can optimize and not go any
3735           // further back in the PreviousVersion generations. The EMCP
3736           // method for this generation could have already been deleted,
3737           // but there still may be an older EMCP method that has not
3738           // been deleted.
3739         }
3740 
3741         if (++local_count &gt;= obsolete_method_count) {
3742           // no more obsolete methods so bail out now
3743           break;
3744         }
3745       }
3746     }
3747   }
3748 } // end add_previous_version()
3749 
3750 
3751 // Determine if InstanceKlass has a previous version.
3752 bool InstanceKlass::has_previous_version() const {
3753   return (_previous_versions != NULL &amp;&amp; _previous_versions-&gt;length() &gt; 0);
3754 } // end has_previous_version()
3755 
3756 
3757 InstanceKlass* InstanceKlass::get_klass_version(int version) {
3758   if (constants()-&gt;version() == version) {
3759     return this;
3760   }
3761   PreviousVersionWalker pvw(Thread::current(), (InstanceKlass*)this);
3762   for (PreviousVersionNode * pv_node = pvw.next_previous_version();
3763        pv_node != NULL; pv_node = pvw.next_previous_version()) {
3764     ConstantPool* prev_cp = pv_node-&gt;prev_constant_pool();
3765     if (prev_cp-&gt;version() == version) {
3766       return prev_cp-&gt;pool_holder();
3767     }
3768   }
3769   return NULL; // None found
3770 }
3771 
3772 
3773 Method* InstanceKlass::method_with_idnum(int idnum) {
3774   Method* m = NULL;
3775   if (idnum &lt; methods()-&gt;length()) {
3776     m = methods()-&gt;at(idnum);
3777   }
3778   if (m == NULL || m-&gt;method_idnum() != idnum) {
3779     for (int index = 0; index &lt; methods()-&gt;length(); ++index) {
3780       m = methods()-&gt;at(index);
3781       if (m-&gt;method_idnum() == idnum) {
3782         return m;
3783       }
3784     }
3785     // None found, return null for the caller to handle.
3786     return NULL;
3787   }
3788   return m;
3789 }
3790 
3791 
3792 Method* InstanceKlass::method_with_orig_idnum(int idnum) {
3793   if (idnum &gt;= methods()-&gt;length()) {
3794     return NULL;
3795   }
3796   Method* m = methods()-&gt;at(idnum);
3797   if (m != NULL &amp;&amp; m-&gt;orig_method_idnum() == idnum) {
3798     return m;
3799   }
3800   // Obsolete method idnum does not match the original idnum
3801   for (int index = 0; index &lt; methods()-&gt;length(); ++index) {
3802     m = methods()-&gt;at(index);
3803     if (m-&gt;orig_method_idnum() == idnum) {
3804       return m;
3805     }
3806   }
3807   // None found, return null for the caller to handle.
3808   return NULL;
3809 }
3810 
3811 
3812 Method* InstanceKlass::method_with_orig_idnum(int idnum, int version) {
3813   InstanceKlass* holder = get_klass_version(version);
3814   if (holder == NULL) {
3815     return NULL; // The version of klass is gone, no method is found
3816   }
3817   Method* method = holder-&gt;method_with_orig_idnum(idnum);
3818   return method;
3819 }
3820 
3821 
3822 jint InstanceKlass::get_cached_class_file_len() {
3823   return VM_RedefineClasses::get_cached_class_file_len(_cached_class_file);
3824 }
3825 
3826 unsigned char * InstanceKlass::get_cached_class_file_bytes() {
3827   return VM_RedefineClasses::get_cached_class_file_bytes(_cached_class_file);
3828 }
3829 
3830 
3831 // Construct a PreviousVersionNode entry for the array hung off
3832 // the InstanceKlass.
3833 PreviousVersionNode::PreviousVersionNode(ConstantPool* prev_constant_pool,
3834   GrowableArray&lt;Method*&gt;* prev_EMCP_methods) {
3835 
3836   _prev_constant_pool = prev_constant_pool;
3837   _prev_EMCP_methods = prev_EMCP_methods;
3838 }
3839 
3840 
3841 // Destroy a PreviousVersionNode
3842 PreviousVersionNode::~PreviousVersionNode() {
3843   if (_prev_constant_pool != NULL) {
3844     _prev_constant_pool = NULL;
3845   }
3846 
3847   if (_prev_EMCP_methods != NULL) {
3848     delete _prev_EMCP_methods;
3849   }
3850 }
3851 
3852 // Construct a helper for walking the previous versions array
3853 PreviousVersionWalker::PreviousVersionWalker(Thread* thread, InstanceKlass *ik) {
3854   _thread = thread;
3855   _previous_versions = ik-&gt;previous_versions();
3856   _current_index = 0;
3857   _current_p = NULL;
3858   _current_constant_pool_handle = constantPoolHandle(thread, ik-&gt;constants());
3859 }
3860 
3861 
3862 // Return the interesting information for the next previous version
3863 // of the klass. Returns NULL if there are no more previous versions.
3864 PreviousVersionNode* PreviousVersionWalker::next_previous_version() {
3865   if (_previous_versions == NULL) {
3866     // no previous versions so nothing to return
3867     return NULL;
3868   }
3869 
3870   _current_p = NULL;  // reset to NULL
3871   _current_constant_pool_handle = NULL;
3872 
3873   int length = _previous_versions-&gt;length();
3874 
3875   while (_current_index &lt; length) {
3876     PreviousVersionNode * pv_node = _previous_versions-&gt;at(_current_index++);
3877 
3878     // Save a handle to the constant pool for this previous version,
3879     // which keeps all the methods from being deallocated.
3880     _current_constant_pool_handle = constantPoolHandle(_thread, pv_node-&gt;prev_constant_pool());
3881     _current_p = pv_node;
3882     return pv_node;
3883   }
3884 
3885   return NULL;
3886 } // end next_previous_version()
</pre></body></html>
