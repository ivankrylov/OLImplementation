<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/c1/c1_GraphBuilder.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2013, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "c1/c1_CFGPrinter.hpp"
  27 #include "c1/c1_Canonicalizer.hpp"
  28 #include "c1/c1_Compilation.hpp"
  29 #include "c1/c1_GraphBuilder.hpp"
  30 #include "c1/c1_InstructionPrinter.hpp"
  31 #include "ci/ciCallSite.hpp"
  32 #include "ci/ciField.hpp"
  33 #include "ci/ciKlass.hpp"
  34 #include "ci/ciMemberName.hpp"
  35 #include "compiler/compileBroker.hpp"
  36 #include "interpreter/bytecode.hpp"
  37 #include "runtime/sharedRuntime.hpp"
  38 #include "runtime/compilationPolicy.hpp"
  39 #include "utilities/bitMap.inline.hpp"
  40 
  41 class BlockListBuilder VALUE_OBJ_CLASS_SPEC {
  42  private:
  43   Compilation* _compilation;
  44   IRScope*     _scope;
  45 
  46   BlockList    _blocks;                // internal list of all blocks
  47   BlockList*   _bci2block;             // mapping from bci to blocks for GraphBuilder
  48 
  49   // fields used by mark_loops
  50   BitMap       _active;                // for iteration of control flow graph
  51   BitMap       _visited;               // for iteration of control flow graph
  52   intArray     _loop_map;              // caches the information if a block is contained in a loop
  53   int          _next_loop_index;       // next free loop number
  54   int          _next_block_number;     // for reverse postorder numbering of blocks
  55 
  56   // accessors
  57   Compilation*  compilation() const              { return _compilation; }
  58   IRScope*      scope() const                    { return _scope; }
  59   ciMethod*     method() const                   { return scope()-&gt;method(); }
  60   XHandlers*    xhandlers() const                { return scope()-&gt;xhandlers(); }
  61 
  62   // unified bailout support
  63   void          bailout(const char* msg) const   { compilation()-&gt;bailout(msg); }
  64   bool          bailed_out() const               { return compilation()-&gt;bailed_out(); }
  65 
  66   // helper functions
  67   BlockBegin* make_block_at(int bci, BlockBegin* predecessor);
  68   void handle_exceptions(BlockBegin* current, int cur_bci);
  69   void handle_jsr(BlockBegin* current, int sr_bci, int next_bci);
  70   void store_one(BlockBegin* current, int local);
  71   void store_two(BlockBegin* current, int local);
  72   void set_entries(int osr_bci);
  73   void set_leaders();
  74 
  75   void make_loop_header(BlockBegin* block);
  76   void mark_loops();
  77   int  mark_loops(BlockBegin* b, bool in_subroutine);
  78 
  79   // debugging
  80 #ifndef PRODUCT
  81   void print();
  82 #endif
  83 
  84  public:
  85   // creation
  86   BlockListBuilder(Compilation* compilation, IRScope* scope, int osr_bci);
  87 
  88   // accessors for GraphBuilder
  89   BlockList*    bci2block() const                { return _bci2block; }
  90 };
  91 
  92 
  93 // Implementation of BlockListBuilder
  94 
  95 BlockListBuilder::BlockListBuilder(Compilation* compilation, IRScope* scope, int osr_bci)
  96  : _compilation(compilation)
  97  , _scope(scope)
  98  , _blocks(16)
  99  , _bci2block(new BlockList(scope-&gt;method()-&gt;code_size(), NULL))
 100  , _next_block_number(0)
 101  , _active()         // size not known yet
 102  , _visited()        // size not known yet
 103  , _next_loop_index(0)
 104  , _loop_map() // size not known yet
 105 {
 106   set_entries(osr_bci);
 107   set_leaders();
 108   CHECK_BAILOUT();
 109 
 110   mark_loops();
 111   NOT_PRODUCT(if (PrintInitialBlockList) print());
 112 
 113 #ifndef PRODUCT
 114   if (PrintCFGToFile) {
 115     stringStream title;
 116     title.print("BlockListBuilder ");
 117     scope-&gt;method()-&gt;print_name(&amp;title);
 118     CFGPrinter::print_cfg(_bci2block, title.as_string(), false, false);
 119   }
 120 #endif
 121 }
 122 
 123 
 124 void BlockListBuilder::set_entries(int osr_bci) {
 125   // generate start blocks
 126   BlockBegin* std_entry = make_block_at(0, NULL);
 127   if (scope()-&gt;caller() == NULL) {
 128     std_entry-&gt;set(BlockBegin::std_entry_flag);
 129   }
 130   if (osr_bci != -1) {
 131     BlockBegin* osr_entry = make_block_at(osr_bci, NULL);
 132     osr_entry-&gt;set(BlockBegin::osr_entry_flag);
 133   }
 134 
 135   // generate exception entry blocks
 136   XHandlers* list = xhandlers();
 137   const int n = list-&gt;length();
 138   for (int i = 0; i &lt; n; i++) {
 139     XHandler* h = list-&gt;handler_at(i);
 140     BlockBegin* entry = make_block_at(h-&gt;handler_bci(), NULL);
 141     entry-&gt;set(BlockBegin::exception_entry_flag);
 142     h-&gt;set_entry_block(entry);
 143   }
 144 }
 145 
 146 
 147 BlockBegin* BlockListBuilder::make_block_at(int cur_bci, BlockBegin* predecessor) {
 148   assert(method()-&gt;bci_block_start().at(cur_bci), "wrong block starts of MethodLivenessAnalyzer");
 149 
 150   BlockBegin* block = _bci2block-&gt;at(cur_bci);
 151   if (block == NULL) {
 152     block = new BlockBegin(cur_bci);
 153     block-&gt;init_stores_to_locals(method()-&gt;max_locals());
 154     _bci2block-&gt;at_put(cur_bci, block);
 155     _blocks.append(block);
 156 
 157     assert(predecessor == NULL || predecessor-&gt;bci() &lt; cur_bci, "targets for backward branches must already exist");
 158   }
 159 
 160   if (predecessor != NULL) {
 161     if (block-&gt;is_set(BlockBegin::exception_entry_flag)) {
 162       BAILOUT_("Exception handler can be reached by both normal and exceptional control flow", block);
 163     }
 164 
 165     predecessor-&gt;add_successor(block);
 166     block-&gt;increment_total_preds();
 167   }
 168 
 169   return block;
 170 }
 171 
 172 
 173 inline void BlockListBuilder::store_one(BlockBegin* current, int local) {
 174   current-&gt;stores_to_locals().set_bit(local);
 175 }
 176 inline void BlockListBuilder::store_two(BlockBegin* current, int local) {
 177   store_one(current, local);
 178   store_one(current, local + 1);
 179 }
 180 
 181 
 182 void BlockListBuilder::handle_exceptions(BlockBegin* current, int cur_bci) {
 183   // Draws edges from a block to its exception handlers
 184   XHandlers* list = xhandlers();
 185   const int n = list-&gt;length();
 186 
 187   for (int i = 0; i &lt; n; i++) {
 188     XHandler* h = list-&gt;handler_at(i);
 189 
 190     if (h-&gt;covers(cur_bci)) {
 191       BlockBegin* entry = h-&gt;entry_block();
 192       assert(entry != NULL &amp;&amp; entry == _bci2block-&gt;at(h-&gt;handler_bci()), "entry must be set");
 193       assert(entry-&gt;is_set(BlockBegin::exception_entry_flag), "flag must be set");
 194 
 195       // add each exception handler only once
 196       if (!current-&gt;is_successor(entry)) {
 197         current-&gt;add_successor(entry);
 198         entry-&gt;increment_total_preds();
 199       }
 200 
 201       // stop when reaching catchall
 202       if (h-&gt;catch_type() == 0) break;
 203     }
 204   }
 205 }
 206 
 207 void BlockListBuilder::handle_jsr(BlockBegin* current, int sr_bci, int next_bci) {
 208   // start a new block after jsr-bytecode and link this block into cfg
 209   make_block_at(next_bci, current);
 210 
 211   // start a new block at the subroutine entry at mark it with special flag
 212   BlockBegin* sr_block = make_block_at(sr_bci, current);
 213   if (!sr_block-&gt;is_set(BlockBegin::subroutine_entry_flag)) {
 214     sr_block-&gt;set(BlockBegin::subroutine_entry_flag);
 215   }
 216 }
 217 
 218 
 219 void BlockListBuilder::set_leaders() {
 220   bool has_xhandlers = xhandlers()-&gt;has_handlers();
 221   BlockBegin* current = NULL;
 222 
 223   // The information which bci starts a new block simplifies the analysis
 224   // Without it, backward branches could jump to a bci where no block was created
 225   // during bytecode iteration. This would require the creation of a new block at the
 226   // branch target and a modification of the successor lists.
 227   BitMap bci_block_start = method()-&gt;bci_block_start();
 228 
 229   ciBytecodeStream s(method());
 230   while (s.next() != ciBytecodeStream::EOBC()) {
 231     int cur_bci = s.cur_bci();
 232 
 233     if (bci_block_start.at(cur_bci)) {
 234       current = make_block_at(cur_bci, current);
 235     }
 236     assert(current != NULL, "must have current block");
 237 
 238     if (has_xhandlers &amp;&amp; GraphBuilder::can_trap(method(), s.cur_bc())) {
 239       handle_exceptions(current, cur_bci);
 240     }
 241 
 242     switch (s.cur_bc()) {
 243       // track stores to local variables for selective creation of phi functions
 244       case Bytecodes::_iinc:     store_one(current, s.get_index()); break;
 245       case Bytecodes::_istore:   store_one(current, s.get_index()); break;
 246       case Bytecodes::_lstore:   store_two(current, s.get_index()); break;
 247       case Bytecodes::_fstore:   store_one(current, s.get_index()); break;
 248       case Bytecodes::_dstore:   store_two(current, s.get_index()); break;
 249       case Bytecodes::_astore:   store_one(current, s.get_index()); break;
 250       case Bytecodes::_istore_0: store_one(current, 0); break;
 251       case Bytecodes::_istore_1: store_one(current, 1); break;
 252       case Bytecodes::_istore_2: store_one(current, 2); break;
 253       case Bytecodes::_istore_3: store_one(current, 3); break;
 254       case Bytecodes::_lstore_0: store_two(current, 0); break;
 255       case Bytecodes::_lstore_1: store_two(current, 1); break;
 256       case Bytecodes::_lstore_2: store_two(current, 2); break;
 257       case Bytecodes::_lstore_3: store_two(current, 3); break;
 258       case Bytecodes::_fstore_0: store_one(current, 0); break;
 259       case Bytecodes::_fstore_1: store_one(current, 1); break;
 260       case Bytecodes::_fstore_2: store_one(current, 2); break;
 261       case Bytecodes::_fstore_3: store_one(current, 3); break;
 262       case Bytecodes::_dstore_0: store_two(current, 0); break;
 263       case Bytecodes::_dstore_1: store_two(current, 1); break;
 264       case Bytecodes::_dstore_2: store_two(current, 2); break;
 265       case Bytecodes::_dstore_3: store_two(current, 3); break;
 266       case Bytecodes::_astore_0: store_one(current, 0); break;
 267       case Bytecodes::_astore_1: store_one(current, 1); break;
 268       case Bytecodes::_astore_2: store_one(current, 2); break;
 269       case Bytecodes::_astore_3: store_one(current, 3); break;
 270 
 271       // track bytecodes that affect the control flow
 272       case Bytecodes::_athrow:  // fall through
 273       case Bytecodes::_ret:     // fall through
 274       case Bytecodes::_ireturn: // fall through
 275       case Bytecodes::_lreturn: // fall through
 276       case Bytecodes::_freturn: // fall through
 277       case Bytecodes::_dreturn: // fall through
 278       case Bytecodes::_areturn: // fall through
 279       case Bytecodes::_return:
 280         current = NULL;
 281         break;
 282 
 283       case Bytecodes::_ifeq:      // fall through
 284       case Bytecodes::_ifne:      // fall through
 285       case Bytecodes::_iflt:      // fall through
 286       case Bytecodes::_ifge:      // fall through
 287       case Bytecodes::_ifgt:      // fall through
 288       case Bytecodes::_ifle:      // fall through
 289       case Bytecodes::_if_icmpeq: // fall through
 290       case Bytecodes::_if_icmpne: // fall through
 291       case Bytecodes::_if_icmplt: // fall through
 292       case Bytecodes::_if_icmpge: // fall through
 293       case Bytecodes::_if_icmpgt: // fall through
 294       case Bytecodes::_if_icmple: // fall through
 295       case Bytecodes::_if_acmpeq: // fall through
 296       case Bytecodes::_if_acmpne: // fall through
 297       case Bytecodes::_ifnull:    // fall through
 298       case Bytecodes::_ifnonnull:
 299         make_block_at(s.next_bci(), current);
 300         make_block_at(s.get_dest(), current);
 301         current = NULL;
 302         break;
 303 
 304       case Bytecodes::_goto:
 305         make_block_at(s.get_dest(), current);
 306         current = NULL;
 307         break;
 308 
 309       case Bytecodes::_goto_w:
 310         make_block_at(s.get_far_dest(), current);
 311         current = NULL;
 312         break;
 313 
 314       case Bytecodes::_jsr:
 315         handle_jsr(current, s.get_dest(), s.next_bci());
 316         current = NULL;
 317         break;
 318 
 319       case Bytecodes::_jsr_w:
 320         handle_jsr(current, s.get_far_dest(), s.next_bci());
 321         current = NULL;
 322         break;
 323 
 324       case Bytecodes::_tableswitch: {
 325         // set block for each case
 326         Bytecode_tableswitch sw(&amp;s);
 327         int l = sw.length();
 328         for (int i = 0; i &lt; l; i++) {
 329           make_block_at(cur_bci + sw.dest_offset_at(i), current);
 330         }
 331         make_block_at(cur_bci + sw.default_offset(), current);
 332         current = NULL;
 333         break;
 334       }
 335 
 336       case Bytecodes::_lookupswitch: {
 337         // set block for each case
 338         Bytecode_lookupswitch sw(&amp;s);
 339         int l = sw.number_of_pairs();
 340         for (int i = 0; i &lt; l; i++) {
 341           make_block_at(cur_bci + sw.pair_at(i).offset(), current);
 342         }
 343         make_block_at(cur_bci + sw.default_offset(), current);
 344         current = NULL;
 345         break;
 346       }
 347     }
 348   }
 349 }
 350 
 351 
 352 void BlockListBuilder::mark_loops() {
 353   ResourceMark rm;
 354 
 355   _active = BitMap(BlockBegin::number_of_blocks());         _active.clear();
 356   _visited = BitMap(BlockBegin::number_of_blocks());        _visited.clear();
 357   _loop_map = intArray(BlockBegin::number_of_blocks(), 0);
 358   _next_loop_index = 0;
 359   _next_block_number = _blocks.length();
 360 
 361   // recursively iterate the control flow graph
 362   mark_loops(_bci2block-&gt;at(0), false);
 363   assert(_next_block_number &gt;= 0, "invalid block numbers");
 364 }
 365 
 366 void BlockListBuilder::make_loop_header(BlockBegin* block) {
 367   if (block-&gt;is_set(BlockBegin::exception_entry_flag)) {
 368     // exception edges may look like loops but don't mark them as such
 369     // since it screws up block ordering.
 370     return;
 371   }
 372   if (!block-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
 373     block-&gt;set(BlockBegin::parser_loop_header_flag);
 374 
 375     assert(_loop_map.at(block-&gt;block_id()) == 0, "must not be set yet");
 376     assert(0 &lt;= _next_loop_index &amp;&amp; _next_loop_index &lt; BitsPerInt, "_next_loop_index is used as a bit-index in integer");
 377     _loop_map.at_put(block-&gt;block_id(), 1 &lt;&lt; _next_loop_index);
 378     if (_next_loop_index &lt; 31) _next_loop_index++;
 379   } else {
 380     // block already marked as loop header
 381     assert(is_power_of_2((unsigned int)_loop_map.at(block-&gt;block_id())), "exactly one bit must be set");
 382   }
 383 }
 384 
 385 int BlockListBuilder::mark_loops(BlockBegin* block, bool in_subroutine) {
 386   int block_id = block-&gt;block_id();
 387 
 388   if (_visited.at(block_id)) {
 389     if (_active.at(block_id)) {
 390       // reached block via backward branch
 391       make_loop_header(block);
 392     }
 393     // return cached loop information for this block
 394     return _loop_map.at(block_id);
 395   }
 396 
 397   if (block-&gt;is_set(BlockBegin::subroutine_entry_flag)) {
 398     in_subroutine = true;
 399   }
 400 
 401   // set active and visited bits before successors are processed
 402   _visited.set_bit(block_id);
 403   _active.set_bit(block_id);
 404 
 405   intptr_t loop_state = 0;
 406   for (int i = block-&gt;number_of_sux() - 1; i &gt;= 0; i--) {
 407     // recursively process all successors
 408     loop_state |= mark_loops(block-&gt;sux_at(i), in_subroutine);
 409   }
 410 
 411   // clear active-bit after all successors are processed
 412   _active.clear_bit(block_id);
 413 
 414   // reverse-post-order numbering of all blocks
 415   block-&gt;set_depth_first_number(_next_block_number);
 416   _next_block_number--;
 417 
 418   if (loop_state != 0 || in_subroutine ) {
 419     // block is contained at least in one loop, so phi functions are necessary
 420     // phi functions are also necessary for all locals stored in a subroutine
 421     scope()-&gt;requires_phi_function().set_union(block-&gt;stores_to_locals());
 422   }
 423 
 424   if (block-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
 425     int header_loop_state = _loop_map.at(block_id);
 426     assert(is_power_of_2((unsigned)header_loop_state), "exactly one bit must be set");
 427 
 428     // If the highest bit is set (i.e. when integer value is negative), the method
 429     // has 32 or more loops. This bit is never cleared because it is used for multiple loops
 430     if (header_loop_state &gt;= 0) {
 431       clear_bits(loop_state, header_loop_state);
 432     }
 433   }
 434 
 435   // cache and return loop information for this block
 436   _loop_map.at_put(block_id, loop_state);
 437   return loop_state;
 438 }
 439 
 440 
 441 #ifndef PRODUCT
 442 
 443 int compare_depth_first(BlockBegin** a, BlockBegin** b) {
 444   return (*a)-&gt;depth_first_number() - (*b)-&gt;depth_first_number();
 445 }
 446 
 447 void BlockListBuilder::print() {
 448   tty-&gt;print("----- initial block list of BlockListBuilder for method ");
 449   method()-&gt;print_short_name();
 450   tty-&gt;cr();
 451 
 452   // better readability if blocks are sorted in processing order
 453   _blocks.sort(compare_depth_first);
 454 
 455   for (int i = 0; i &lt; _blocks.length(); i++) {
 456     BlockBegin* cur = _blocks.at(i);
 457     tty-&gt;print("%4d: B%-4d bci: %-4d  preds: %-4d ", cur-&gt;depth_first_number(), cur-&gt;block_id(), cur-&gt;bci(), cur-&gt;total_preds());
 458 
 459     tty-&gt;print(cur-&gt;is_set(BlockBegin::std_entry_flag)               ? " std" : "    ");
 460     tty-&gt;print(cur-&gt;is_set(BlockBegin::osr_entry_flag)               ? " osr" : "    ");
 461     tty-&gt;print(cur-&gt;is_set(BlockBegin::exception_entry_flag)         ? " ex" : "   ");
 462     tty-&gt;print(cur-&gt;is_set(BlockBegin::subroutine_entry_flag)        ? " sr" : "   ");
 463     tty-&gt;print(cur-&gt;is_set(BlockBegin::parser_loop_header_flag)      ? " lh" : "   ");
 464 
 465     if (cur-&gt;number_of_sux() &gt; 0) {
 466       tty-&gt;print("    sux: ");
 467       for (int j = 0; j &lt; cur-&gt;number_of_sux(); j++) {
 468         BlockBegin* sux = cur-&gt;sux_at(j);
 469         tty-&gt;print("B%d ", sux-&gt;block_id());
 470       }
 471     }
 472     tty-&gt;cr();
 473   }
 474 }
 475 
 476 #endif
 477 
 478 
 479 // A simple growable array of Values indexed by ciFields
 480 class FieldBuffer: public CompilationResourceObj {
 481  private:
 482   GrowableArray&lt;Value&gt; _values;
 483 
 484  public:
 485   FieldBuffer() {}
 486 
 487   void kill() {
 488     _values.trunc_to(0);
 489   }
 490 
 491   Value at(ciField* field) {
 492     assert(field-&gt;holder()-&gt;is_loaded(), "must be a loaded field");
 493     int offset = field-&gt;offset();
 494     if (offset &lt; _values.length()) {
 495       return _values.at(offset);
 496     } else {
 497       return NULL;
 498     }
 499   }
 500 
 501   void at_put(ciField* field, Value value) {
 502     assert(field-&gt;holder()-&gt;is_loaded(), "must be a loaded field");
 503     int offset = field-&gt;offset();
 504     _values.at_put_grow(offset, value, NULL);
 505   }
 506 
 507 };
 508 
 509 
 510 // MemoryBuffer is fairly simple model of the current state of memory.
 511 // It partitions memory into several pieces.  The first piece is
 512 // generic memory where little is known about the owner of the memory.
 513 // This is conceptually represented by the tuple &lt;O, F, V&gt; which says
 514 // that the field F of object O has value V.  This is flattened so
 515 // that F is represented by the offset of the field and the parallel
 516 // arrays _objects and _values are used for O and V.  Loads of O.F can
 517 // simply use V.  Newly allocated objects are kept in a separate list
 518 // along with a parallel array for each object which represents the
 519 // current value of its fields.  Stores of the default value to fields
 520 // which have never been stored to before are eliminated since they
 521 // are redundant.  Once newly allocated objects are stored into
 522 // another object or they are passed out of the current compile they
 523 // are treated like generic memory.
 524 
 525 class MemoryBuffer: public CompilationResourceObj {
 526  private:
 527   FieldBuffer                 _values;
 528   GrowableArray&lt;Value&gt;        _objects;
 529   GrowableArray&lt;Value&gt;        _newobjects;
 530   GrowableArray&lt;FieldBuffer*&gt; _fields;
 531 
 532  public:
 533   MemoryBuffer() {}
 534 
 535   StoreField* store(StoreField* st) {
 536     if (!EliminateFieldAccess) {
 537       return st;
 538     }
 539 
 540     Value object = st-&gt;obj();
 541     Value value = st-&gt;value();
 542     ciField* field = st-&gt;field();
 543     if (field-&gt;holder()-&gt;is_loaded()) {
 544       int offset = field-&gt;offset();
 545       int index = _newobjects.find(object);
 546       if (index != -1) {
 547         // newly allocated object with no other stores performed on this field
 548         FieldBuffer* buf = _fields.at(index);
 549         if (buf-&gt;at(field) == NULL &amp;&amp; is_default_value(value)) {
 550 #ifndef PRODUCT
 551           if (PrintIRDuringConstruction &amp;&amp; Verbose) {
 552             tty-&gt;print_cr("Eliminated store for object %d:", index);
 553             st-&gt;print_line();
 554           }
 555 #endif
 556           return NULL;
 557         } else {
 558           buf-&gt;at_put(field, value);
 559         }
 560       } else {
 561         _objects.at_put_grow(offset, object, NULL);
 562         _values.at_put(field, value);
 563       }
 564 
 565       store_value(value);
 566     } else {
 567       // if we held onto field names we could alias based on names but
 568       // we don't know what's being stored to so kill it all.
 569       kill();
 570     }
 571     return st;
 572   }
 573 
 574 
 575   // return true if this value correspond to the default value of a field.
 576   bool is_default_value(Value value) {
 577     Constant* con = value-&gt;as_Constant();
 578     if (con) {
 579       switch (con-&gt;type()-&gt;tag()) {
 580         case intTag:    return con-&gt;type()-&gt;as_IntConstant()-&gt;value() == 0;
 581         case longTag:   return con-&gt;type()-&gt;as_LongConstant()-&gt;value() == 0;
 582         case floatTag:  return jint_cast(con-&gt;type()-&gt;as_FloatConstant()-&gt;value()) == 0;
 583         case doubleTag: return jlong_cast(con-&gt;type()-&gt;as_DoubleConstant()-&gt;value()) == jlong_cast(0);
 584         case objectTag: return con-&gt;type() == objectNull;
 585         default:  ShouldNotReachHere();
 586       }
 587     }
 588     return false;
 589   }
 590 
 591 
 592   // return either the actual value of a load or the load itself
 593   Value load(LoadField* load) {
 594     if (!EliminateFieldAccess) {
 595       return load;
 596     }
 597 
 598     if (RoundFPResults &amp;&amp; UseSSE &lt; 2 &amp;&amp; load-&gt;type()-&gt;is_float_kind()) {
 599       // can't skip load since value might get rounded as a side effect
 600       return load;
 601     }
 602 
 603     ciField* field = load-&gt;field();
 604     Value object   = load-&gt;obj();
 605     if (field-&gt;holder()-&gt;is_loaded() &amp;&amp; !field-&gt;is_volatile()) {
 606       int offset = field-&gt;offset();
 607       Value result = NULL;
 608       int index = _newobjects.find(object);
 609       if (index != -1) {
 610         result = _fields.at(index)-&gt;at(field);
 611       } else if (_objects.at_grow(offset, NULL) == object) {
 612         result = _values.at(field);
 613       }
 614       if (result != NULL) {
 615 #ifndef PRODUCT
 616         if (PrintIRDuringConstruction &amp;&amp; Verbose) {
 617           tty-&gt;print_cr("Eliminated load: ");
 618           load-&gt;print_line();
 619         }
 620 #endif
 621         assert(result-&gt;type()-&gt;tag() == load-&gt;type()-&gt;tag(), "wrong types");
 622         return result;
 623       }
 624     }
 625     return load;
 626   }
 627 
 628   // Record this newly allocated object
 629   void new_instance(NewInstance* object) {
 630     int index = _newobjects.length();
 631     _newobjects.append(object);
 632     if (_fields.at_grow(index, NULL) == NULL) {
 633       _fields.at_put(index, new FieldBuffer());
 634     } else {
 635       _fields.at(index)-&gt;kill();
 636     }
 637   }
 638 
 639   void store_value(Value value) {
 640     int index = _newobjects.find(value);
 641     if (index != -1) {
 642       // stored a newly allocated object into another object.
 643       // Assume we've lost track of it as separate slice of memory.
 644       // We could do better by keeping track of whether individual
 645       // fields could alias each other.
 646       _newobjects.remove_at(index);
 647       // pull out the field info and store it at the end up the list
 648       // of field info list to be reused later.
 649       _fields.append(_fields.at(index));
 650       _fields.remove_at(index);
 651     }
 652   }
 653 
 654   void kill() {
 655     _newobjects.trunc_to(0);
 656     _objects.trunc_to(0);
 657     _values.kill();
 658   }
 659 };
 660 
 661 
 662 // Implementation of GraphBuilder's ScopeData
 663 
 664 GraphBuilder::ScopeData::ScopeData(ScopeData* parent)
 665   : _parent(parent)
 666   , _bci2block(NULL)
 667   , _scope(NULL)
 668   , _has_handler(false)
 669   , _stream(NULL)
 670   , _work_list(NULL)
 671   , _parsing_jsr(false)
 672   , _jsr_xhandlers(NULL)
 673   , _caller_stack_size(-1)
 674   , _continuation(NULL)
 675   , _num_returns(0)
 676   , _cleanup_block(NULL)
 677   , _cleanup_return_prev(NULL)
 678   , _cleanup_state(NULL)
 679 {
 680   if (parent != NULL) {
 681     _max_inline_size = (intx) ((float) NestedInliningSizeRatio * (float) parent-&gt;max_inline_size() / 100.0f);
 682   } else {
 683     _max_inline_size = MaxInlineSize;
 684   }
 685   if (_max_inline_size &lt; MaxTrivialSize) {
 686     _max_inline_size = MaxTrivialSize;
 687   }
 688 }
 689 
 690 
 691 void GraphBuilder::kill_all() {
 692   if (UseLocalValueNumbering) {
 693     vmap()-&gt;kill_all();
 694   }
 695   _memory-&gt;kill();
 696 }
 697 
 698 
 699 BlockBegin* GraphBuilder::ScopeData::block_at(int bci) {
 700   if (parsing_jsr()) {
 701     // It is necessary to clone all blocks associated with a
 702     // subroutine, including those for exception handlers in the scope
 703     // of the method containing the jsr (because those exception
 704     // handlers may contain ret instructions in some cases).
 705     BlockBegin* block = bci2block()-&gt;at(bci);
 706     if (block != NULL &amp;&amp; block == parent()-&gt;bci2block()-&gt;at(bci)) {
 707       BlockBegin* new_block = new BlockBegin(block-&gt;bci());
 708 #ifndef PRODUCT
 709       if (PrintInitialBlockList) {
 710         tty-&gt;print_cr("CFG: cloned block %d (bci %d) as block %d for jsr",
 711                       block-&gt;block_id(), block-&gt;bci(), new_block-&gt;block_id());
 712       }
 713 #endif
 714       // copy data from cloned blocked
 715       new_block-&gt;set_depth_first_number(block-&gt;depth_first_number());
 716       if (block-&gt;is_set(BlockBegin::parser_loop_header_flag)) new_block-&gt;set(BlockBegin::parser_loop_header_flag);
 717       // Preserve certain flags for assertion checking
 718       if (block-&gt;is_set(BlockBegin::subroutine_entry_flag)) new_block-&gt;set(BlockBegin::subroutine_entry_flag);
 719       if (block-&gt;is_set(BlockBegin::exception_entry_flag))  new_block-&gt;set(BlockBegin::exception_entry_flag);
 720 
 721       // copy was_visited_flag to allow early detection of bailouts
 722       // if a block that is used in a jsr has already been visited before,
 723       // it is shared between the normal control flow and a subroutine
 724       // BlockBegin::try_merge returns false when the flag is set, this leads
 725       // to a compilation bailout
 726       if (block-&gt;is_set(BlockBegin::was_visited_flag))  new_block-&gt;set(BlockBegin::was_visited_flag);
 727 
 728       bci2block()-&gt;at_put(bci, new_block);
 729       block = new_block;
 730     }
 731     return block;
 732   } else {
 733     return bci2block()-&gt;at(bci);
 734   }
 735 }
 736 
 737 
 738 XHandlers* GraphBuilder::ScopeData::xhandlers() const {
 739   if (_jsr_xhandlers == NULL) {
 740     assert(!parsing_jsr(), "");
 741     return scope()-&gt;xhandlers();
 742   }
 743   assert(parsing_jsr(), "");
 744   return _jsr_xhandlers;
 745 }
 746 
 747 
 748 void GraphBuilder::ScopeData::set_scope(IRScope* scope) {
 749   _scope = scope;
 750   bool parent_has_handler = false;
 751   if (parent() != NULL) {
 752     parent_has_handler = parent()-&gt;has_handler();
 753   }
 754   _has_handler = parent_has_handler || scope-&gt;xhandlers()-&gt;has_handlers();
 755 }
 756 
 757 
 758 void GraphBuilder::ScopeData::set_inline_cleanup_info(BlockBegin* block,
 759                                                       Instruction* return_prev,
 760                                                       ValueStack* return_state) {
 761   _cleanup_block       = block;
 762   _cleanup_return_prev = return_prev;
 763   _cleanup_state       = return_state;
 764 }
 765 
 766 
 767 void GraphBuilder::ScopeData::add_to_work_list(BlockBegin* block) {
 768   if (_work_list == NULL) {
 769     _work_list = new BlockList();
 770   }
 771 
 772   if (!block-&gt;is_set(BlockBegin::is_on_work_list_flag)) {
 773     // Do not start parsing the continuation block while in a
 774     // sub-scope
 775     if (parsing_jsr()) {
 776       if (block == jsr_continuation()) {
 777         return;
 778       }
 779     } else {
 780       if (block == continuation()) {
 781         return;
 782       }
 783     }
 784     block-&gt;set(BlockBegin::is_on_work_list_flag);
 785     _work_list-&gt;push(block);
 786 
 787     sort_top_into_worklist(_work_list, block);
 788   }
 789 }
 790 
 791 
 792 void GraphBuilder::sort_top_into_worklist(BlockList* worklist, BlockBegin* top) {
 793   assert(worklist-&gt;top() == top, "");
 794   // sort block descending into work list
 795   const int dfn = top-&gt;depth_first_number();
 796   assert(dfn != -1, "unknown depth first number");
 797   int i = worklist-&gt;length()-2;
 798   while (i &gt;= 0) {
 799     BlockBegin* b = worklist-&gt;at(i);
 800     if (b-&gt;depth_first_number() &lt; dfn) {
 801       worklist-&gt;at_put(i+1, b);
 802     } else {
 803       break;
 804     }
 805     i --;
 806   }
 807   if (i &gt;= -1) worklist-&gt;at_put(i + 1, top);
 808 }
 809 
 810 
 811 BlockBegin* GraphBuilder::ScopeData::remove_from_work_list() {
 812   if (is_work_list_empty()) {
 813     return NULL;
 814   }
 815   return _work_list-&gt;pop();
 816 }
 817 
 818 
 819 bool GraphBuilder::ScopeData::is_work_list_empty() const {
 820   return (_work_list == NULL || _work_list-&gt;length() == 0);
 821 }
 822 
 823 
 824 void GraphBuilder::ScopeData::setup_jsr_xhandlers() {
 825   assert(parsing_jsr(), "");
 826   // clone all the exception handlers from the scope
 827   XHandlers* handlers = new XHandlers(scope()-&gt;xhandlers());
 828   const int n = handlers-&gt;length();
 829   for (int i = 0; i &lt; n; i++) {
 830     // The XHandlers need to be adjusted to dispatch to the cloned
 831     // handler block instead of the default one but the synthetic
 832     // unlocker needs to be handled specially.  The synthetic unlocker
 833     // should be left alone since there can be only one and all code
 834     // should dispatch to the same one.
 835     XHandler* h = handlers-&gt;handler_at(i);
 836     assert(h-&gt;handler_bci() != SynchronizationEntryBCI, "must be real");
 837     h-&gt;set_entry_block(block_at(h-&gt;handler_bci()));
 838   }
 839   _jsr_xhandlers = handlers;
 840 }
 841 
 842 
 843 int GraphBuilder::ScopeData::num_returns() {
 844   if (parsing_jsr()) {
 845     return parent()-&gt;num_returns();
 846   }
 847   return _num_returns;
 848 }
 849 
 850 
 851 void GraphBuilder::ScopeData::incr_num_returns() {
 852   if (parsing_jsr()) {
 853     parent()-&gt;incr_num_returns();
 854   } else {
 855     ++_num_returns;
 856   }
 857 }
 858 
 859 
 860 // Implementation of GraphBuilder
 861 
 862 #define INLINE_BAILOUT(msg)        { inline_bailout(msg); return false; }
 863 
 864 
 865 void GraphBuilder::load_constant() {
 866   ciConstant con = stream()-&gt;get_constant();
 867   if (con.basic_type() == T_ILLEGAL) {
 868     BAILOUT("could not resolve a constant");
 869   } else {
 870     ValueType* t = illegalType;
 871     ValueStack* patch_state = NULL;
 872     switch (con.basic_type()) {
 873       case T_BOOLEAN: t = new IntConstant     (con.as_boolean()); break;
 874       case T_BYTE   : t = new IntConstant     (con.as_byte   ()); break;
 875       case T_CHAR   : t = new IntConstant     (con.as_char   ()); break;
 876       case T_SHORT  : t = new IntConstant     (con.as_short  ()); break;
 877       case T_INT    : t = new IntConstant     (con.as_int    ()); break;
 878       case T_LONG   : t = new LongConstant    (con.as_long   ()); break;
 879       case T_FLOAT  : t = new FloatConstant   (con.as_float  ()); break;
 880       case T_DOUBLE : t = new DoubleConstant  (con.as_double ()); break;
 881       case T_ARRAY  : t = new ArrayConstant   (con.as_object ()-&gt;as_array   ()); break;
 882       case T_OBJECT :
 883        {
 884         ciObject* obj = con.as_object();
 885         if (!obj-&gt;is_loaded()
 886             || (PatchALot &amp;&amp; obj-&gt;klass() != ciEnv::current()-&gt;String_klass())) {
 887           patch_state = copy_state_before();
 888           t = new ObjectConstant(obj);
 889         } else {
 890           assert(obj-&gt;is_instance(), "must be java_mirror of klass");
 891           t = new InstanceConstant(obj-&gt;as_instance());
 892         }
 893         break;
 894        }
 895       default       : ShouldNotReachHere();
 896     }
 897     Value x;
 898     if (patch_state != NULL) {
 899       x = new Constant(t, patch_state);
 900     } else {
 901       x = new Constant(t);
 902     }
 903     push(t, append(x));
 904   }
 905 }
 906 
 907 
 908 void GraphBuilder::load_local(ValueType* type, int index) {
 909   Value x = state()-&gt;local_at(index);
 910   assert(x != NULL &amp;&amp; !x-&gt;type()-&gt;is_illegal(), "access of illegal local variable");
 911   push(type, x);
 912 }
 913 
 914 
 915 void GraphBuilder::store_local(ValueType* type, int index) {
 916   Value x = pop(type);
 917   store_local(state(), x, index);
 918 }
 919 
 920 
 921 void GraphBuilder::store_local(ValueStack* state, Value x, int index) {
 922   if (parsing_jsr()) {
 923     // We need to do additional tracking of the location of the return
 924     // address for jsrs since we don't handle arbitrary jsr/ret
 925     // constructs. Here we are figuring out in which circumstances we
 926     // need to bail out.
 927     if (x-&gt;type()-&gt;is_address()) {
 928       scope_data()-&gt;set_jsr_return_address_local(index);
 929 
 930       // Also check parent jsrs (if any) at this time to see whether
 931       // they are using this local. We don't handle skipping over a
 932       // ret.
 933       for (ScopeData* cur_scope_data = scope_data()-&gt;parent();
 934            cur_scope_data != NULL &amp;&amp; cur_scope_data-&gt;parsing_jsr() &amp;&amp; cur_scope_data-&gt;scope() == scope();
 935            cur_scope_data = cur_scope_data-&gt;parent()) {
 936         if (cur_scope_data-&gt;jsr_return_address_local() == index) {
 937           BAILOUT("subroutine overwrites return address from previous subroutine");
 938         }
 939       }
 940     } else if (index == scope_data()-&gt;jsr_return_address_local()) {
 941       scope_data()-&gt;set_jsr_return_address_local(-1);
 942     }
 943   }
 944 
 945   state-&gt;store_local(index, round_fp(x));
 946 }
 947 
 948 
 949 void GraphBuilder::load_indexed(BasicType type) {
 950   // In case of in block code motion in range check elimination
 951   ValueStack* state_before = copy_state_indexed_access();
 952   compilation()-&gt;set_has_access_indexed(true);
 953   Value index = ipop();
 954   Value array = apop();
 955   Value length = NULL;
 956   if (CSEArrayLength ||
 957       (array-&gt;as_AccessField() &amp;&amp; array-&gt;as_AccessField()-&gt;field()-&gt;is_constant()) ||
 958       (array-&gt;as_NewArray() &amp;&amp; array-&gt;as_NewArray()-&gt;length() &amp;&amp; array-&gt;as_NewArray()-&gt;length()-&gt;type()-&gt;is_constant())) {
 959     length = append(new ArrayLength(array, state_before));
 960   }
 961   push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));
 962 }
 963 
 964 
 965 void GraphBuilder::store_indexed(BasicType type) {
 966   // In case of in block code motion in range check elimination
 967   ValueStack* state_before = copy_state_indexed_access();
 968   compilation()-&gt;set_has_access_indexed(true);
 969   Value value = pop(as_ValueType(type));
 970   Value index = ipop();
 971   Value array = apop();
 972   Value length = NULL;
 973   if (CSEArrayLength ||
 974       (array-&gt;as_AccessField() &amp;&amp; array-&gt;as_AccessField()-&gt;field()-&gt;is_constant()) ||
 975       (array-&gt;as_NewArray() &amp;&amp; array-&gt;as_NewArray()-&gt;length() &amp;&amp; array-&gt;as_NewArray()-&gt;length()-&gt;type()-&gt;is_constant())) {
 976     length = append(new ArrayLength(array, state_before));
 977   }
 978   StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before);
 979   append(result);
 980   _memory-&gt;store_value(value);
 981 
 982   if (type == T_OBJECT &amp;&amp; is_profiling()) {
 983     // Note that we'd collect profile data in this method if we wanted it.
 984     compilation()-&gt;set_would_profile(true);
 985 
 986     if (profile_checkcasts()) {
 987       result-&gt;set_profiled_method(method());
 988       result-&gt;set_profiled_bci(bci());
 989       result-&gt;set_should_profile(true);
 990     }
 991   }
 992 }
 993 
 994 
 995 void GraphBuilder::stack_op(Bytecodes::Code code) {
 996   switch (code) {
 997     case Bytecodes::_pop:
 998       { state()-&gt;raw_pop();
 999       }
1000       break;
1001     case Bytecodes::_pop2:
1002       { state()-&gt;raw_pop();
1003         state()-&gt;raw_pop();
1004       }
1005       break;
1006     case Bytecodes::_dup:
1007       { Value w = state()-&gt;raw_pop();
1008         state()-&gt;raw_push(w);
1009         state()-&gt;raw_push(w);
1010       }
1011       break;
1012     case Bytecodes::_dup_x1:
1013       { Value w1 = state()-&gt;raw_pop();
1014         Value w2 = state()-&gt;raw_pop();
1015         state()-&gt;raw_push(w1);
1016         state()-&gt;raw_push(w2);
1017         state()-&gt;raw_push(w1);
1018       }
1019       break;
1020     case Bytecodes::_dup_x2:
1021       { Value w1 = state()-&gt;raw_pop();
1022         Value w2 = state()-&gt;raw_pop();
1023         Value w3 = state()-&gt;raw_pop();
1024         state()-&gt;raw_push(w1);
1025         state()-&gt;raw_push(w3);
1026         state()-&gt;raw_push(w2);
1027         state()-&gt;raw_push(w1);
1028       }
1029       break;
1030     case Bytecodes::_dup2:
1031       { Value w1 = state()-&gt;raw_pop();
1032         Value w2 = state()-&gt;raw_pop();
1033         state()-&gt;raw_push(w2);
1034         state()-&gt;raw_push(w1);
1035         state()-&gt;raw_push(w2);
1036         state()-&gt;raw_push(w1);
1037       }
1038       break;
1039     case Bytecodes::_dup2_x1:
1040       { Value w1 = state()-&gt;raw_pop();
1041         Value w2 = state()-&gt;raw_pop();
1042         Value w3 = state()-&gt;raw_pop();
1043         state()-&gt;raw_push(w2);
1044         state()-&gt;raw_push(w1);
1045         state()-&gt;raw_push(w3);
1046         state()-&gt;raw_push(w2);
1047         state()-&gt;raw_push(w1);
1048       }
1049       break;
1050     case Bytecodes::_dup2_x2:
1051       { Value w1 = state()-&gt;raw_pop();
1052         Value w2 = state()-&gt;raw_pop();
1053         Value w3 = state()-&gt;raw_pop();
1054         Value w4 = state()-&gt;raw_pop();
1055         state()-&gt;raw_push(w2);
1056         state()-&gt;raw_push(w1);
1057         state()-&gt;raw_push(w4);
1058         state()-&gt;raw_push(w3);
1059         state()-&gt;raw_push(w2);
1060         state()-&gt;raw_push(w1);
1061       }
1062       break;
1063     case Bytecodes::_swap:
1064       { Value w1 = state()-&gt;raw_pop();
1065         Value w2 = state()-&gt;raw_pop();
1066         state()-&gt;raw_push(w1);
1067         state()-&gt;raw_push(w2);
1068       }
1069       break;
1070     default:
1071       ShouldNotReachHere();
1072       break;
1073   }
1074 }
1075 
1076 
1077 void GraphBuilder::arithmetic_op(ValueType* type, Bytecodes::Code code, ValueStack* state_before) {
1078   Value y = pop(type);
1079   Value x = pop(type);
1080   // NOTE: strictfp can be queried from current method since we don't
1081   // inline methods with differing strictfp bits
1082   Value res = new ArithmeticOp(code, x, y, method()-&gt;is_strict(), state_before);
1083   // Note: currently single-precision floating-point rounding on Intel is handled at the LIRGenerator level
1084   res = append(res);
1085   if (method()-&gt;is_strict()) {
1086     res = round_fp(res);
1087   }
1088   push(type, res);
1089 }
1090 
1091 
1092 void GraphBuilder::negate_op(ValueType* type) {
1093   push(type, append(new NegateOp(pop(type))));
1094 }
1095 
1096 
1097 void GraphBuilder::shift_op(ValueType* type, Bytecodes::Code code) {
1098   Value s = ipop();
1099   Value x = pop(type);
1100   // try to simplify
1101   // Note: This code should go into the canonicalizer as soon as it can
1102   //       can handle canonicalized forms that contain more than one node.
1103   if (CanonicalizeNodes &amp;&amp; code == Bytecodes::_iushr) {
1104     // pattern: x &gt;&gt;&gt; s
1105     IntConstant* s1 = s-&gt;type()-&gt;as_IntConstant();
1106     if (s1 != NULL) {
1107       // pattern: x &gt;&gt;&gt; s1, with s1 constant
1108       ShiftOp* l = x-&gt;as_ShiftOp();
1109       if (l != NULL &amp;&amp; l-&gt;op() == Bytecodes::_ishl) {
1110         // pattern: (a &lt;&lt; b) &gt;&gt;&gt; s1
1111         IntConstant* s0 = l-&gt;y()-&gt;type()-&gt;as_IntConstant();
1112         if (s0 != NULL) {
1113           // pattern: (a &lt;&lt; s0) &gt;&gt;&gt; s1
1114           const int s0c = s0-&gt;value() &amp; 0x1F; // only the low 5 bits are significant for shifts
1115           const int s1c = s1-&gt;value() &amp; 0x1F; // only the low 5 bits are significant for shifts
1116           if (s0c == s1c) {
1117             if (s0c == 0) {
1118               // pattern: (a &lt;&lt; 0) &gt;&gt;&gt; 0 =&gt; simplify to: a
1119               ipush(l-&gt;x());
1120             } else {
1121               // pattern: (a &lt;&lt; s0c) &gt;&gt;&gt; s0c =&gt; simplify to: a &amp; m, with m constant
1122               assert(0 &lt; s0c &amp;&amp; s0c &lt; BitsPerInt, "adjust code below to handle corner cases");
1123               const int m = (1 &lt;&lt; (BitsPerInt - s0c)) - 1;
1124               Value s = append(new Constant(new IntConstant(m)));
1125               ipush(append(new LogicOp(Bytecodes::_iand, l-&gt;x(), s)));
1126             }
1127             return;
1128           }
1129         }
1130       }
1131     }
1132   }
1133   // could not simplify
1134   push(type, append(new ShiftOp(code, x, s)));
1135 }
1136 
1137 
1138 void GraphBuilder::logic_op(ValueType* type, Bytecodes::Code code) {
1139   Value y = pop(type);
1140   Value x = pop(type);
1141   push(type, append(new LogicOp(code, x, y)));
1142 }
1143 
1144 
1145 void GraphBuilder::compare_op(ValueType* type, Bytecodes::Code code) {
1146   ValueStack* state_before = copy_state_before();
1147   Value y = pop(type);
1148   Value x = pop(type);
1149   ipush(append(new CompareOp(code, x, y, state_before)));
1150 }
1151 
1152 
1153 void GraphBuilder::convert(Bytecodes::Code op, BasicType from, BasicType to) {
1154   push(as_ValueType(to), append(new Convert(op, pop(as_ValueType(from)), as_ValueType(to))));
1155 }
1156 
1157 
1158 void GraphBuilder::increment() {
1159   int index = stream()-&gt;get_index();
1160   int delta = stream()-&gt;is_wide() ? (signed short)Bytes::get_Java_u2(stream()-&gt;cur_bcp() + 4) : (signed char)(stream()-&gt;cur_bcp()[2]);
1161   load_local(intType, index);
1162   ipush(append(new Constant(new IntConstant(delta))));
1163   arithmetic_op(intType, Bytecodes::_iadd);
1164   store_local(intType, index);
1165 }
1166 
1167 
1168 void GraphBuilder::_goto(int from_bci, int to_bci) {
1169   Goto *x = new Goto(block_at(to_bci), to_bci &lt;= from_bci);
1170   if (is_profiling()) {
1171     compilation()-&gt;set_would_profile(true);
1172     x-&gt;set_profiled_bci(bci());
1173     if (profile_branches()) {
1174       x-&gt;set_profiled_method(method());
1175       x-&gt;set_should_profile(true);
1176     }
1177   }
1178   append(x);
1179 }
1180 
1181 
1182 void GraphBuilder::if_node(Value x, If::Condition cond, Value y, ValueStack* state_before) {
1183   BlockBegin* tsux = block_at(stream()-&gt;get_dest());
1184   BlockBegin* fsux = block_at(stream()-&gt;next_bci());
1185   bool is_bb = tsux-&gt;bci() &lt; stream()-&gt;cur_bci() || fsux-&gt;bci() &lt; stream()-&gt;cur_bci();
1186   // In case of loop invariant code motion or predicate insertion
1187   // before the body of a loop the state is needed
1188   Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()-&gt;is_optimistic()) ? state_before : NULL, is_bb));
1189 
1190   assert(i-&gt;as_Goto() == NULL ||
1191          (i-&gt;as_Goto()-&gt;sux_at(0) == tsux  &amp;&amp; i-&gt;as_Goto()-&gt;is_safepoint() == tsux-&gt;bci() &lt; stream()-&gt;cur_bci()) ||
1192          (i-&gt;as_Goto()-&gt;sux_at(0) == fsux  &amp;&amp; i-&gt;as_Goto()-&gt;is_safepoint() == fsux-&gt;bci() &lt; stream()-&gt;cur_bci()),
1193          "safepoint state of Goto returned by canonicalizer incorrect");
1194 
1195   if (is_profiling()) {
1196     If* if_node = i-&gt;as_If();
1197     if (if_node != NULL) {
1198       // Note that we'd collect profile data in this method if we wanted it.
1199       compilation()-&gt;set_would_profile(true);
1200       // At level 2 we need the proper bci to count backedges
1201       if_node-&gt;set_profiled_bci(bci());
1202       if (profile_branches()) {
1203         // Successors can be rotated by the canonicalizer, check for this case.
1204         if_node-&gt;set_profiled_method(method());
1205         if_node-&gt;set_should_profile(true);
1206         if (if_node-&gt;tsux() == fsux) {
1207           if_node-&gt;set_swapped(true);
1208         }
1209       }
1210       return;
1211     }
1212 
1213     // Check if this If was reduced to Goto.
1214     Goto *goto_node = i-&gt;as_Goto();
1215     if (goto_node != NULL) {
1216       compilation()-&gt;set_would_profile(true);
1217       goto_node-&gt;set_profiled_bci(bci());
1218       if (profile_branches()) {
1219         goto_node-&gt;set_profiled_method(method());
1220         goto_node-&gt;set_should_profile(true);
1221         // Find out which successor is used.
1222         if (goto_node-&gt;default_sux() == tsux) {
1223           goto_node-&gt;set_direction(Goto::taken);
1224         } else if (goto_node-&gt;default_sux() == fsux) {
1225           goto_node-&gt;set_direction(Goto::not_taken);
1226         } else {
1227           ShouldNotReachHere();
1228         }
1229       }
1230       return;
1231     }
1232   }
1233 }
1234 
1235 
1236 void GraphBuilder::if_zero(ValueType* type, If::Condition cond) {
1237   Value y = append(new Constant(intZero));
1238   ValueStack* state_before = copy_state_before();
1239   Value x = ipop();
1240   if_node(x, cond, y, state_before);
1241 }
1242 
1243 
1244 void GraphBuilder::if_null(ValueType* type, If::Condition cond) {
1245   Value y = append(new Constant(objectNull));
1246   ValueStack* state_before = copy_state_before();
1247   Value x = apop();
1248   if_node(x, cond, y, state_before);
1249 }
1250 
1251 
1252 void GraphBuilder::if_same(ValueType* type, If::Condition cond) {
1253   ValueStack* state_before = copy_state_before();
1254   Value y = pop(type);
1255   Value x = pop(type);
1256   if_node(x, cond, y, state_before);
1257 }
1258 
1259 
1260 void GraphBuilder::jsr(int dest) {
1261   // We only handle well-formed jsrs (those which are "block-structured").
1262   // If the bytecodes are strange (jumping out of a jsr block) then we
1263   // might end up trying to re-parse a block containing a jsr which
1264   // has already been activated. Watch for this case and bail out.
1265   for (ScopeData* cur_scope_data = scope_data();
1266        cur_scope_data != NULL &amp;&amp; cur_scope_data-&gt;parsing_jsr() &amp;&amp; cur_scope_data-&gt;scope() == scope();
1267        cur_scope_data = cur_scope_data-&gt;parent()) {
1268     if (cur_scope_data-&gt;jsr_entry_bci() == dest) {
1269       BAILOUT("too-complicated jsr/ret structure");
1270     }
1271   }
1272 
1273   push(addressType, append(new Constant(new AddressConstant(next_bci()))));
1274   if (!try_inline_jsr(dest)) {
1275     return; // bailed out while parsing and inlining subroutine
1276   }
1277 }
1278 
1279 
1280 void GraphBuilder::ret(int local_index) {
1281   if (!parsing_jsr()) BAILOUT("ret encountered while not parsing subroutine");
1282 
1283   if (local_index != scope_data()-&gt;jsr_return_address_local()) {
1284     BAILOUT("can not handle complicated jsr/ret constructs");
1285   }
1286 
1287   // Rets simply become (NON-SAFEPOINT) gotos to the jsr continuation
1288   append(new Goto(scope_data()-&gt;jsr_continuation(), false));
1289 }
1290 
1291 
1292 void GraphBuilder::table_switch() {
1293   Bytecode_tableswitch sw(stream());
1294   const int l = sw.length();
1295   if (CanonicalizeNodes &amp;&amp; l == 1) {
1296     // total of 2 successors =&gt; use If instead of switch
1297     // Note: This code should go into the canonicalizer as soon as it can
1298     //       can handle canonicalized forms that contain more than one node.
1299     Value key = append(new Constant(new IntConstant(sw.low_key())));
1300     BlockBegin* tsux = block_at(bci() + sw.dest_offset_at(0));
1301     BlockBegin* fsux = block_at(bci() + sw.default_offset());
1302     bool is_bb = tsux-&gt;bci() &lt; bci() || fsux-&gt;bci() &lt; bci();
1303     // In case of loop invariant code motion or predicate insertion
1304     // before the body of a loop the state is needed
1305     ValueStack* state_before = copy_state_if_bb(is_bb);
1306     append(new If(ipop(), If::eql, true, key, tsux, fsux, state_before, is_bb));
1307   } else {
1308     // collect successors
1309     BlockList* sux = new BlockList(l + 1, NULL);
1310     int i;
1311     bool has_bb = false;
1312     for (i = 0; i &lt; l; i++) {
1313       sux-&gt;at_put(i, block_at(bci() + sw.dest_offset_at(i)));
1314       if (sw.dest_offset_at(i) &lt; 0) has_bb = true;
1315     }
1316     // add default successor
1317     if (sw.default_offset() &lt; 0) has_bb = true;
1318     sux-&gt;at_put(i, block_at(bci() + sw.default_offset()));
1319     // In case of loop invariant code motion or predicate insertion
1320     // before the body of a loop the state is needed
1321     ValueStack* state_before = copy_state_if_bb(has_bb);
1322     Instruction* res = append(new TableSwitch(ipop(), sux, sw.low_key(), state_before, has_bb));
1323 #ifdef ASSERT
1324     if (res-&gt;as_Goto()) {
1325       for (i = 0; i &lt; l; i++) {
1326         if (sux-&gt;at(i) == res-&gt;as_Goto()-&gt;sux_at(0)) {
1327           assert(res-&gt;as_Goto()-&gt;is_safepoint() == sw.dest_offset_at(i) &lt; 0, "safepoint state of Goto returned by canonicalizer incorrect");
1328         }
1329       }
1330     }
1331 #endif
1332   }
1333 }
1334 
1335 
1336 void GraphBuilder::lookup_switch() {
1337   Bytecode_lookupswitch sw(stream());
1338   const int l = sw.number_of_pairs();
1339   if (CanonicalizeNodes &amp;&amp; l == 1) {
1340     // total of 2 successors =&gt; use If instead of switch
1341     // Note: This code should go into the canonicalizer as soon as it can
1342     //       can handle canonicalized forms that contain more than one node.
1343     // simplify to If
1344     LookupswitchPair pair = sw.pair_at(0);
1345     Value key = append(new Constant(new IntConstant(pair.match())));
1346     BlockBegin* tsux = block_at(bci() + pair.offset());
1347     BlockBegin* fsux = block_at(bci() + sw.default_offset());
1348     bool is_bb = tsux-&gt;bci() &lt; bci() || fsux-&gt;bci() &lt; bci();
1349     // In case of loop invariant code motion or predicate insertion
1350     // before the body of a loop the state is needed
1351     ValueStack* state_before = copy_state_if_bb(is_bb);;
1352     append(new If(ipop(), If::eql, true, key, tsux, fsux, state_before, is_bb));
1353   } else {
1354     // collect successors &amp; keys
1355     BlockList* sux = new BlockList(l + 1, NULL);
1356     intArray* keys = new intArray(l, 0);
1357     int i;
1358     bool has_bb = false;
1359     for (i = 0; i &lt; l; i++) {
1360       LookupswitchPair pair = sw.pair_at(i);
1361       if (pair.offset() &lt; 0) has_bb = true;
1362       sux-&gt;at_put(i, block_at(bci() + pair.offset()));
1363       keys-&gt;at_put(i, pair.match());
1364     }
1365     // add default successor
1366     if (sw.default_offset() &lt; 0) has_bb = true;
1367     sux-&gt;at_put(i, block_at(bci() + sw.default_offset()));
1368     // In case of loop invariant code motion or predicate insertion
1369     // before the body of a loop the state is needed
1370     ValueStack* state_before = copy_state_if_bb(has_bb);
1371     Instruction* res = append(new LookupSwitch(ipop(), sux, keys, state_before, has_bb));
1372 #ifdef ASSERT
1373     if (res-&gt;as_Goto()) {
1374       for (i = 0; i &lt; l; i++) {
1375         if (sux-&gt;at(i) == res-&gt;as_Goto()-&gt;sux_at(0)) {
1376           assert(res-&gt;as_Goto()-&gt;is_safepoint() == sw.pair_at(i).offset() &lt; 0, "safepoint state of Goto returned by canonicalizer incorrect");
1377         }
1378       }
1379     }
1380 #endif
1381   }
1382 }
1383 
1384 void GraphBuilder::call_register_finalizer() {
1385   // If the receiver requires finalization then emit code to perform
1386   // the registration on return.
1387 
1388   // Gather some type information about the receiver
1389   Value receiver = state()-&gt;local_at(0);
1390   assert(receiver != NULL, "must have a receiver");
1391   ciType* declared_type = receiver-&gt;declared_type();
1392   ciType* exact_type = receiver-&gt;exact_type();
1393   if (exact_type == NULL &amp;&amp;
1394       receiver-&gt;as_Local() &amp;&amp;
1395       receiver-&gt;as_Local()-&gt;java_index() == 0) {
1396     ciInstanceKlass* ik = compilation()-&gt;method()-&gt;holder();
1397     if (ik-&gt;is_final()) {
1398       exact_type = ik;
1399     } else if (UseCHA &amp;&amp; !(ik-&gt;has_subklass() || ik-&gt;is_interface())) {
1400       // test class is leaf class
1401       compilation()-&gt;dependency_recorder()-&gt;assert_leaf_type(ik);
1402       exact_type = ik;
1403     } else {
1404       declared_type = ik;
1405     }
1406   }
1407 
1408   // see if we know statically that registration isn't required
1409   bool needs_check = true;
1410   if (exact_type != NULL) {
1411     needs_check = exact_type-&gt;as_instance_klass()-&gt;has_finalizer();
1412   } else if (declared_type != NULL) {
1413     ciInstanceKlass* ik = declared_type-&gt;as_instance_klass();
1414     if (!Dependencies::has_finalizable_subclass(ik)) {
1415       compilation()-&gt;dependency_recorder()-&gt;assert_has_no_finalizable_subclasses(ik);
1416       needs_check = false;
1417     }
1418   }
1419 
1420   if (needs_check) {
1421     // Perform the registration of finalizable objects.
1422     ValueStack* state_before = copy_state_for_exception();
1423     load_local(objectType, 0);
1424     append_split(new Intrinsic(voidType, vmIntrinsics::_Object_init,
1425                                state()-&gt;pop_arguments(1),
1426                                true, state_before, true));
1427   }
1428 }
1429 
1430 
1431 void GraphBuilder::method_return(Value x) {
1432   if (RegisterFinalizersAtInit &amp;&amp;
1433       method()-&gt;intrinsic_id() == vmIntrinsics::_Object_init) {
1434     call_register_finalizer();
1435   }
1436 
1437   bool need_mem_bar = false;
1438   if (method()-&gt;name() == ciSymbol::object_initializer_name() &amp;&amp;
1439       scope()-&gt;wrote_final()) {
1440     need_mem_bar = true;
1441   }
1442 
1443   // Check to see whether we are inlining. If so, Return
1444   // instructions become Gotos to the continuation point.
1445   if (continuation() != NULL) {
1446     assert(!method()-&gt;is_synchronized() || InlineSynchronizedMethods, "can not inline synchronized methods yet");
1447 
1448     if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
1449       // Report exit from inline methods
1450       Values* args = new Values(1);
1451       args-&gt;push(append(new Constant(new MethodConstant(method()))));
1452       append(new RuntimeCall(voidType, "dtrace_method_exit", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), args));
1453     }
1454 
1455     // If the inlined method is synchronized, the monitor must be
1456     // released before we jump to the continuation block.
1457     if (method()-&gt;is_synchronized()) {
1458       assert(state()-&gt;locks_size() == 1, "receiver must be locked here");
1459       monitorexit(state()-&gt;lock_at(0), SynchronizationEntryBCI);
1460     }
1461 
1462     if (need_mem_bar) {
1463       append(new MemBar(lir_membar_storestore));
1464     }
1465 
1466     // State at end of inlined method is the state of the caller
1467     // without the method parameters on stack, including the
1468     // return value, if any, of the inlined method on operand stack.
1469     int invoke_bci = state()-&gt;caller_state()-&gt;bci();
1470     set_state(state()-&gt;caller_state()-&gt;copy_for_parsing());
1471     if (x != NULL) {
1472       state()-&gt;push(x-&gt;type(), x);
1473       if (profile_return() &amp;&amp; x-&gt;type()-&gt;is_object_kind()) {
1474         ciMethod* caller = state()-&gt;scope()-&gt;method();
1475         ciMethodData* md = caller-&gt;method_data_or_null();
1476         ciProfileData* data = md-&gt;bci_to_data(invoke_bci);
1477         if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
1478           bool has_return = data-&gt;is_CallTypeData() ? ((ciCallTypeData*)data)-&gt;has_return() : ((ciVirtualCallTypeData*)data)-&gt;has_return();
1479           // May not be true in case of an inlined call through a method handle intrinsic.
1480           if (has_return) {
1481             profile_return_type(x, method(), caller, invoke_bci);
1482           }
1483         }
1484       }
1485     }
1486     Goto* goto_callee = new Goto(continuation(), false);
1487 
1488     // See whether this is the first return; if so, store off some
1489     // of the state for later examination
1490     if (num_returns() == 0) {
1491       set_inline_cleanup_info();
1492     }
1493 
1494     // The current bci() is in the wrong scope, so use the bci() of
1495     // the continuation point.
1496     append_with_bci(goto_callee, scope_data()-&gt;continuation()-&gt;bci());
1497     incr_num_returns();
1498     return;
1499   }
1500 
1501   state()-&gt;truncate_stack(0);
1502   if (method()-&gt;is_synchronized()) {
1503     // perform the unlocking before exiting the method
1504     Value receiver;
1505     if (!method()-&gt;is_static()) {
1506       receiver = _initial_state-&gt;local_at(0);
1507     } else {
1508       receiver = append(new Constant(new ClassConstant(method()-&gt;holder())));
1509     }
1510     append_split(new MonitorExit(receiver, state()-&gt;unlock()));
1511   }
1512 
1513   if (need_mem_bar) {
1514       append(new MemBar(lir_membar_storestore));
1515   }
1516 
1517   append(new Return(x));
1518 }
1519 
1520 
1521 void GraphBuilder::access_field(Bytecodes::Code code) {
1522   bool will_link;
1523   ciField* field = stream()-&gt;get_field(will_link);
1524   ciInstanceKlass* holder = field-&gt;holder();
1525   BasicType field_type = field-&gt;type()-&gt;basic_type();
1526   ValueType* type = as_ValueType(field_type);
1527   // call will_link again to determine if the field is valid.
1528   const bool needs_patching = !holder-&gt;is_loaded() ||
1529                               !field-&gt;will_link(method()-&gt;holder(), code) ||
1530                               PatchALot;
1531 
1532   ValueStack* state_before = NULL;
1533   if (!holder-&gt;is_initialized() || needs_patching) {
1534     // save state before instruction for debug info when
1535     // deoptimization happens during patching
1536     state_before = copy_state_before();
1537   }
1538 
1539   Value obj = NULL;
1540   if (code == Bytecodes::_getstatic || code == Bytecodes::_putstatic) {
1541     if (state_before != NULL) {
1542       // build a patching constant
1543       obj = new Constant(new InstanceConstant(holder-&gt;java_mirror()), state_before);
1544     } else {
1545       obj = new Constant(new InstanceConstant(holder-&gt;java_mirror()));
1546     }
1547   }
1548 
1549   if (field-&gt;is_final() &amp;&amp; (code == Bytecodes::_putfield)) {
1550     scope()-&gt;set_wrote_final();
1551   }
1552 
1553   const int offset = !needs_patching ? field-&gt;offset() : -1;
1554   switch (code) {
1555     case Bytecodes::_getstatic: {
1556       // check for compile-time constants, i.e., initialized static final fields
1557       Instruction* constant = NULL;
1558       if (field-&gt;is_constant() &amp;&amp; !PatchALot) {
1559         ciConstant field_val = field-&gt;constant_value();
1560         BasicType field_type = field_val.basic_type();
1561         switch (field_type) {
1562         case T_ARRAY:
1563         case T_OBJECT:
1564           if (field_val.as_object()-&gt;should_be_constant()) {
1565             constant = new Constant(as_ValueType(field_val));
1566           }
1567           break;
1568 
1569         default:
1570           constant = new Constant(as_ValueType(field_val));
1571         }
1572         // Stable static fields are checked for non-default values in ciField::initialize_from().
1573       }
1574       if (constant != NULL) {
1575         push(type, append(constant));
1576       } else {
1577         if (state_before == NULL) {
1578           state_before = copy_state_for_exception();
1579         }
1580         push(type, append(new LoadField(append(obj), offset, field, true,
1581                                         state_before, needs_patching)));
1582       }
1583       break;
1584     }
1585     case Bytecodes::_putstatic:
1586       { Value val = pop(type);
1587         if (state_before == NULL) {
1588           state_before = copy_state_for_exception();
1589         }
1590         append(new StoreField(append(obj), offset, field, val, true, state_before, needs_patching));
1591       }
1592       break;
1593     case Bytecodes::_getfield: {
1594       // Check for compile-time constants, i.e., trusted final non-static fields.
1595       Instruction* constant = NULL;
1596       obj = apop();
1597       ObjectType* obj_type = obj-&gt;type()-&gt;as_ObjectType();
1598       if (obj_type-&gt;is_constant() &amp;&amp; !PatchALot) {
1599         ciObject* const_oop = obj_type-&gt;constant_value();
1600         if (!const_oop-&gt;is_null_object() &amp;&amp; const_oop-&gt;is_loaded()) {
1601           if (field-&gt;is_constant()) {
1602             ciConstant field_val = field-&gt;constant_value_of(const_oop);
1603             BasicType field_type = field_val.basic_type();
1604             switch (field_type) {
1605             case T_ARRAY:
1606             case T_OBJECT:
1607               if (field_val.as_object()-&gt;should_be_constant()) {
1608                 constant = new Constant(as_ValueType(field_val));
1609               }
1610               break;
1611             default:
1612               constant = new Constant(as_ValueType(field_val));
1613             }
1614             if (FoldStableValues &amp;&amp; field-&gt;is_stable() &amp;&amp; field_val.is_null_or_zero()) {
1615               // Stable field with default value can't be constant.
1616               constant = NULL;
1617             }
1618           } else {
1619             // For CallSite objects treat the target field as a compile time constant.
1620             if (const_oop-&gt;is_call_site()) {
1621               ciCallSite* call_site = const_oop-&gt;as_call_site();
1622               if (field-&gt;is_call_site_target()) {
1623                 ciMethodHandle* target = call_site-&gt;get_target();
1624                 if (target != NULL) {  // just in case
1625                   ciConstant field_val(T_OBJECT, target);
1626                   constant = new Constant(as_ValueType(field_val));
1627                   // Add a dependence for invalidation of the optimization.
1628                   if (!call_site-&gt;is_constant_call_site()) {
1629                     dependency_recorder()-&gt;assert_call_site_target_value(call_site, target);
1630                   }
1631                 }
1632               }
1633             }
1634           }
1635         }
1636       }
1637       if (constant != NULL) {
1638         push(type, append(constant));
1639       } else {
1640         if (state_before == NULL) {
1641           state_before = copy_state_for_exception();
1642         }
1643         LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);
1644         Value replacement = !needs_patching ? _memory-&gt;load(load) : load;
1645         if (replacement != load) {
1646           assert(replacement-&gt;is_linked() || !replacement-&gt;can_be_linked(), "should already by linked");
1647           push(type, replacement);
1648         } else {
1649           push(type, append(load));
1650         }
1651       }
1652       break;
1653     }
1654     case Bytecodes::_putfield: {
1655       Value val = pop(type);
1656       obj = apop();
1657       if (state_before == NULL) {
1658         state_before = copy_state_for_exception();
1659       }
1660       StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);
1661       if (!needs_patching) store = _memory-&gt;store(store);
1662       if (store != NULL) {
1663         append(store);
1664       }
1665       break;
1666     }
1667     default:
1668       ShouldNotReachHere();
1669       break;
1670   }
1671 }
1672 
1673 
1674 Dependencies* GraphBuilder::dependency_recorder() const {
1675   assert(DeoptC1, "need debug information");
1676   return compilation()-&gt;dependency_recorder();
1677 }
1678 
1679 // How many arguments do we want to profile?
1680 Values* GraphBuilder::args_list_for_profiling(ciMethod* target, int&amp; start, bool may_have_receiver) {
1681   int n = 0;
1682   bool has_receiver = may_have_receiver &amp;&amp; Bytecodes::has_receiver(method()-&gt;java_code_at_bci(bci()));
1683   start = has_receiver ? 1 : 0;
1684   if (profile_arguments()) {
1685     ciProfileData* data = method()-&gt;method_data()-&gt;bci_to_data(bci());
1686     if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
1687       n = data-&gt;is_CallTypeData() ? data-&gt;as_CallTypeData()-&gt;number_of_arguments() : data-&gt;as_VirtualCallTypeData()-&gt;number_of_arguments();
1688     }
1689   }
1690   // If we are inlining then we need to collect arguments to profile parameters for the target
1691   if (profile_parameters() &amp;&amp; target != NULL) {
1692     if (target-&gt;method_data() != NULL &amp;&amp; target-&gt;method_data()-&gt;parameters_type_data() != NULL) {
1693       // The receiver is profiled on method entry so it's included in
1694       // the number of parameters but here we're only interested in
1695       // actual arguments.
1696       n = MAX2(n, target-&gt;method_data()-&gt;parameters_type_data()-&gt;number_of_parameters() - start);
1697     }
1698   }
1699   if (n &gt; 0) {
1700     return new Values(n);
1701   }
1702   return NULL;
1703 }
1704 
1705 void GraphBuilder::check_args_for_profiling(Values* obj_args, int expected) {
1706 #ifdef ASSERT
1707   bool ignored_will_link;
1708   ciSignature* declared_signature = NULL;
1709   ciMethod* real_target = method()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
1710   assert(expected == obj_args-&gt;length() || real_target-&gt;is_method_handle_intrinsic(), "missed on arg?");
1711 #endif
1712 }
1713 
1714 // Collect arguments that we want to profile in a list
1715 Values* GraphBuilder::collect_args_for_profiling(Values* args, ciMethod* target, bool may_have_receiver) {
1716   int start = 0;
1717   Values* obj_args = args_list_for_profiling(target, start, may_have_receiver);
1718   if (obj_args == NULL) {
1719     return NULL;
1720   }
1721   int s = obj_args-&gt;size();
1722   // if called through method handle invoke, some arguments may have been popped
1723   for (int i = start, j = 0; j &lt; s &amp;&amp; i &lt; args-&gt;length(); i++) {
1724     if (args-&gt;at(i)-&gt;type()-&gt;is_object_kind()) {
1725       obj_args-&gt;push(args-&gt;at(i));
1726       j++;
1727     }
1728   }
1729   check_args_for_profiling(obj_args, s);
1730   return obj_args;
1731 }
1732 
1733 
1734 void GraphBuilder::invoke(Bytecodes::Code code) {
1735   bool will_link;
1736   ciSignature* declared_signature = NULL;
1737   ciMethod*             target = stream()-&gt;get_method(will_link, &amp;declared_signature);
1738   ciKlass*              holder = stream()-&gt;get_declared_method_holder();
1739   const Bytecodes::Code bc_raw = stream()-&gt;cur_bc_raw();
1740   assert(declared_signature != NULL, "cannot be null");
1741 
1742   if (!C1PatchInvokeDynamic &amp;&amp; Bytecodes::has_optional_appendix(bc_raw) &amp;&amp; !will_link) {
1743     BAILOUT("unlinked call site (C1PatchInvokeDynamic is off)");
1744   }
1745 
1746   // we have to make sure the argument size (incl. the receiver)
1747   // is correct for compilation (the call would fail later during
1748   // linkage anyway) - was bug (gri 7/28/99)
1749   {
1750     // Use raw to get rewritten bytecode.
1751     const bool is_invokestatic = bc_raw == Bytecodes::_invokestatic;
1752     const bool allow_static =
1753           is_invokestatic ||
1754           bc_raw == Bytecodes::_invokehandle ||
1755           bc_raw == Bytecodes::_invokedynamic;
1756     if (target-&gt;is_loaded()) {
1757       if (( target-&gt;is_static() &amp;&amp; !allow_static) ||
1758           (!target-&gt;is_static() &amp;&amp;  is_invokestatic)) {
1759         BAILOUT("will cause link error");
1760       }
1761     }
1762   }
1763   ciInstanceKlass* klass = target-&gt;holder();
1764 
1765   // check if CHA possible: if so, change the code to invoke_special
1766   ciInstanceKlass* calling_klass = method()-&gt;holder();
1767   ciInstanceKlass* callee_holder = ciEnv::get_instance_klass_for_declared_method_holder(holder);
1768   ciInstanceKlass* actual_recv = callee_holder;
1769 
1770   CompileLog* log = compilation()-&gt;log();
1771   if (log != NULL)
1772       log-&gt;elem("call method='%d' instr='%s'",
1773                 log-&gt;identify(target),
1774                 Bytecodes::name(code));
1775 
1776   // Some methods are obviously bindable without any type checks so
1777   // convert them directly to an invokespecial or invokestatic.
1778   if (target-&gt;is_loaded() &amp;&amp; !target-&gt;is_abstract() &amp;&amp; target-&gt;can_be_statically_bound()) {
1779     switch (bc_raw) {
1780     case Bytecodes::_invokevirtual:
1781       code = Bytecodes::_invokespecial;
1782       break;
1783     case Bytecodes::_invokehandle:
1784       code = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokespecial;
1785       break;
1786     }
1787   } else {
1788     if (bc_raw == Bytecodes::_invokehandle) {
1789       assert(!will_link, "should come here only for unlinked call");
1790       code = Bytecodes::_invokespecial;
1791     }
1792   }
1793 
1794   // Push appendix argument (MethodType, CallSite, etc.), if one.
1795   bool patch_for_appendix = false;
1796   int patching_appendix_arg = 0;
1797   if (C1PatchInvokeDynamic &amp;&amp;
1798       (Bytecodes::has_optional_appendix(bc_raw) &amp;&amp; (!will_link || PatchALot))) {
1799     Value arg = append(new Constant(new ObjectConstant(compilation()-&gt;env()-&gt;unloaded_ciinstance()), copy_state_before()));
1800     apush(arg);
1801     patch_for_appendix = true;
1802     patching_appendix_arg = (will_link &amp;&amp; stream()-&gt;has_appendix()) ? 0 : 1;
1803   } else if (stream()-&gt;has_appendix()) {
1804     ciObject* appendix = stream()-&gt;get_appendix();
1805     Value arg = append(new Constant(new ObjectConstant(appendix)));
1806     apush(arg);
1807   }
1808 
1809   // NEEDS_CLEANUP
1810   // I've added the target-&gt;is_loaded() test below but I don't really understand
1811   // how klass-&gt;is_loaded() can be true and yet target-&gt;is_loaded() is false.
1812   // this happened while running the JCK invokevirtual tests under doit.  TKR
1813   ciMethod* cha_monomorphic_target = NULL;
1814   ciMethod* exact_target = NULL;
1815   Value better_receiver = NULL;
1816   if (UseCHA &amp;&amp; DeoptC1 &amp;&amp; klass-&gt;is_loaded() &amp;&amp; target-&gt;is_loaded() &amp;&amp;
1817       !(// %%% FIXME: Are both of these relevant?
1818         target-&gt;is_method_handle_intrinsic() ||
1819         target-&gt;is_compiled_lambda_form()) &amp;&amp;
1820       !patch_for_appendix) {
1821     Value receiver = NULL;
1822     ciInstanceKlass* receiver_klass = NULL;
1823     bool type_is_exact = false;
1824     // try to find a precise receiver type
1825     if (will_link &amp;&amp; !target-&gt;is_static()) {
1826       int index = state()-&gt;stack_size() - (target-&gt;arg_size_no_receiver() + 1);
1827       receiver = state()-&gt;stack_at(index);
1828       ciType* type = receiver-&gt;exact_type();
1829       if (type != NULL &amp;&amp; type-&gt;is_loaded() &amp;&amp;
1830           type-&gt;is_instance_klass() &amp;&amp; !type-&gt;as_instance_klass()-&gt;is_interface()) {
1831         receiver_klass = (ciInstanceKlass*) type;
1832         type_is_exact = true;
1833       }
1834       if (type == NULL) {
1835         type = receiver-&gt;declared_type();
1836         if (type != NULL &amp;&amp; type-&gt;is_loaded() &amp;&amp;
1837             type-&gt;is_instance_klass() &amp;&amp; !type-&gt;as_instance_klass()-&gt;is_interface()) {
1838           receiver_klass = (ciInstanceKlass*) type;
1839           if (receiver_klass-&gt;is_leaf_type() &amp;&amp; !receiver_klass-&gt;is_final()) {
1840             // Insert a dependency on this type since
1841             // find_monomorphic_target may assume it's already done.
1842             dependency_recorder()-&gt;assert_leaf_type(receiver_klass);
1843             type_is_exact = true;
1844           }
1845         }
1846       }
1847     }
1848     if (receiver_klass != NULL &amp;&amp; type_is_exact &amp;&amp;
1849         receiver_klass-&gt;is_loaded() &amp;&amp; code != Bytecodes::_invokespecial) {
1850       // If we have the exact receiver type we can bind directly to
1851       // the method to call.
1852       exact_target = target-&gt;resolve_invoke(calling_klass, receiver_klass);
1853       if (exact_target != NULL) {
1854         target = exact_target;
1855         code = Bytecodes::_invokespecial;
1856       }
1857     }
1858     if (receiver_klass != NULL &amp;&amp;
1859         receiver_klass-&gt;is_subtype_of(actual_recv) &amp;&amp;
1860         actual_recv-&gt;is_initialized()) {
1861       actual_recv = receiver_klass;
1862     }
1863 
1864     if ((code == Bytecodes::_invokevirtual &amp;&amp; callee_holder-&gt;is_initialized()) ||
1865         (code == Bytecodes::_invokeinterface &amp;&amp; callee_holder-&gt;is_initialized() &amp;&amp; !actual_recv-&gt;is_interface())) {
1866       // Use CHA on the receiver to select a more precise method.
1867       cha_monomorphic_target = target-&gt;find_monomorphic_target(calling_klass, callee_holder, actual_recv);
1868     } else if (code == Bytecodes::_invokeinterface &amp;&amp; callee_holder-&gt;is_loaded() &amp;&amp; receiver != NULL) {
1869       // if there is only one implementor of this interface then we
1870       // may be able bind this invoke directly to the implementing
1871       // klass but we need both a dependence on the single interface
1872       // and on the method we bind to.  Additionally since all we know
1873       // about the receiver type is the it's supposed to implement the
1874       // interface we have to insert a check that it's the class we
1875       // expect.  Interface types are not checked by the verifier so
1876       // they are roughly equivalent to Object.
1877       ciInstanceKlass* singleton = NULL;
1878       if (target-&gt;holder()-&gt;nof_implementors() == 1) {
1879         singleton = target-&gt;holder()-&gt;implementor();
1880         assert(singleton != NULL &amp;&amp; singleton != target-&gt;holder(),
1881                "just checking");
1882 
1883         assert(holder-&gt;is_interface(), "invokeinterface to non interface?");
1884         ciInstanceKlass* decl_interface = (ciInstanceKlass*)holder;
1885         // the number of implementors for decl_interface is less or
1886         // equal to the number of implementors for target-&gt;holder() so
1887         // if number of implementors of target-&gt;holder() == 1 then
1888         // number of implementors for decl_interface is 0 or 1. If
1889         // it's 0 then no class implements decl_interface and there's
1890         // no point in inlining.
1891         if (!holder-&gt;is_loaded() || decl_interface-&gt;nof_implementors() != 1 || decl_interface-&gt;has_default_methods()) {
1892           singleton = NULL;
1893         }
1894       }
1895       if (singleton) {
1896         cha_monomorphic_target = target-&gt;find_monomorphic_target(calling_klass, target-&gt;holder(), singleton);
1897         if (cha_monomorphic_target != NULL) {
1898           // If CHA is able to bind this invoke then update the class
1899           // to match that class, otherwise klass will refer to the
1900           // interface.
1901           klass = cha_monomorphic_target-&gt;holder();
1902           actual_recv = target-&gt;holder();
1903 
1904           // insert a check it's really the expected class.
1905           CheckCast* c = new CheckCast(klass, receiver, copy_state_for_exception());
1906           c-&gt;set_incompatible_class_change_check();
1907           c-&gt;set_direct_compare(klass-&gt;is_final());
1908           // pass the result of the checkcast so that the compiler has
1909           // more accurate type info in the inlinee
1910           better_receiver = append_split(c);
1911         }
1912       }
1913     }
1914   }
1915 
1916   if (cha_monomorphic_target != NULL) {
1917     if (cha_monomorphic_target-&gt;is_abstract()) {
1918       // Do not optimize for abstract methods
1919       cha_monomorphic_target = NULL;
1920     }
1921   }
1922 
1923   if (cha_monomorphic_target != NULL) {
1924     if (!(target-&gt;is_final_method())) {
1925       // If we inlined because CHA revealed only a single target method,
1926       // then we are dependent on that target method not getting overridden
1927       // by dynamic class loading.  Be sure to test the "static" receiver
1928       // dest_method here, as opposed to the actual receiver, which may
1929       // falsely lead us to believe that the receiver is final or private.
1930       dependency_recorder()-&gt;assert_unique_concrete_method(actual_recv, cha_monomorphic_target);
1931     }
1932     code = Bytecodes::_invokespecial;
1933   }
1934 
1935   // check if we could do inlining
1936   if (!PatchALot &amp;&amp; Inline &amp;&amp; klass-&gt;is_loaded() &amp;&amp;
1937       (klass-&gt;is_initialized() || klass-&gt;is_interface() &amp;&amp; target-&gt;holder()-&gt;is_initialized())
1938       &amp;&amp; target-&gt;is_loaded()
1939       &amp;&amp; !patch_for_appendix) {
1940     // callee is known =&gt; check if we have static binding
1941     assert(target-&gt;is_loaded(), "callee must be known");
1942     if (code == Bytecodes::_invokestatic  ||
1943         code == Bytecodes::_invokespecial ||
1944         code == Bytecodes::_invokevirtual &amp;&amp; target-&gt;is_final_method() ||
1945         code == Bytecodes::_invokedynamic) {
1946       ciMethod* inline_target = (cha_monomorphic_target != NULL) ? cha_monomorphic_target : target;
1947       // static binding =&gt; check if callee is ok
1948       bool success = try_inline(inline_target, (cha_monomorphic_target != NULL) || (exact_target != NULL), code, better_receiver);
1949 
1950       CHECK_BAILOUT();
1951       clear_inline_bailout();
1952 
1953       if (success) {
1954         // Register dependence if JVMTI has either breakpoint
1955         // setting or hotswapping of methods capabilities since they may
1956         // cause deoptimization.
1957         if (compilation()-&gt;env()-&gt;jvmti_can_hotswap_or_post_breakpoint()) {
1958           dependency_recorder()-&gt;assert_evol_method(inline_target);
1959         }
1960         return;
1961       }
1962     } else {
1963       print_inlining(target, "no static binding", /*success*/ false);
1964     }
1965   } else {
1966     print_inlining(target, "not inlineable", /*success*/ false);
1967   }
1968 
1969   // If we attempted an inline which did not succeed because of a
1970   // bailout during construction of the callee graph, the entire
1971   // compilation has to be aborted. This is fairly rare and currently
1972   // seems to only occur for jasm-generated classes which contain
1973   // jsr/ret pairs which are not associated with finally clauses and
1974   // do not have exception handlers in the containing method, and are
1975   // therefore not caught early enough to abort the inlining without
1976   // corrupting the graph. (We currently bail out with a non-empty
1977   // stack at a ret in these situations.)
1978   CHECK_BAILOUT();
1979 
1980   // inlining not successful =&gt; standard invoke
1981   bool is_loaded = target-&gt;is_loaded();
1982   ValueType* result_type = as_ValueType(declared_signature-&gt;return_type());
1983   ValueStack* state_before = copy_state_exhandling();
1984 
1985   // The bytecode (code) might change in this method so we are checking this very late.
1986   const bool has_receiver =
1987     code == Bytecodes::_invokespecial   ||
1988     code == Bytecodes::_invokevirtual   ||
1989     code == Bytecodes::_invokeinterface;
1990   Values* args = state()-&gt;pop_arguments(target-&gt;arg_size_no_receiver() + patching_appendix_arg);
1991   Value recv = has_receiver ? apop() : NULL;
1992   int vtable_index = Method::invalid_vtable_index;
1993 
1994 #ifdef SPARC
1995   // Currently only supported on Sparc.
1996   // The UseInlineCaches only controls dispatch to invokevirtuals for
1997   // loaded classes which we weren't able to statically bind.
1998   if (!UseInlineCaches &amp;&amp; is_loaded &amp;&amp; code == Bytecodes::_invokevirtual
1999       &amp;&amp; !target-&gt;can_be_statically_bound()) {
2000     // Find a vtable index if one is available
2001     // For arrays, callee_holder is Object. Resolving the call with
2002     // Object would allow an illegal call to finalize() on an
2003     // array. We use holder instead: illegal calls to finalize() won't
2004     // be compiled as vtable calls (IC call resolution will catch the
2005     // illegal call) and the few legal calls on array types won't be
2006     // either.
2007     vtable_index = target-&gt;resolve_vtable_index(calling_klass, holder);
2008   }
2009 #endif
2010 
2011   if (recv != NULL &amp;&amp;
2012       (code == Bytecodes::_invokespecial ||
2013        !is_loaded || target-&gt;is_final())) {
2014     // invokespecial always needs a NULL check.  invokevirtual where
2015     // the target is final or where it's not known that whether the
2016     // target is final requires a NULL check.  Otherwise normal
2017     // invokevirtual will perform the null check during the lookup
2018     // logic or the unverified entry point.  Profiling of calls
2019     // requires that the null check is performed in all cases.
2020     null_check(recv);
2021   }
2022 
2023   if (is_profiling()) {
2024     if (recv != NULL &amp;&amp; profile_calls()) {
2025       null_check(recv);
2026     }
2027     // Note that we'd collect profile data in this method if we wanted it.
2028     compilation()-&gt;set_would_profile(true);
2029 
2030     if (profile_calls()) {
2031       assert(cha_monomorphic_target == NULL || exact_target == NULL, "both can not be set");
2032       ciKlass* target_klass = NULL;
2033       if (cha_monomorphic_target != NULL) {
2034         target_klass = cha_monomorphic_target-&gt;holder();
2035       } else if (exact_target != NULL) {
2036         target_klass = exact_target-&gt;holder();
2037       }
2038       profile_call(target, recv, target_klass, collect_args_for_profiling(args, NULL, false), false);
2039     }
2040   }
2041 
2042   Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before);
2043   // push result
2044   append_split(result);
2045 
2046   if (result_type != voidType) {
2047     if (method()-&gt;is_strict()) {
2048       push(result_type, round_fp(result));
2049     } else {
2050       push(result_type, result);
2051     }
2052   }
2053   if (profile_return() &amp;&amp; result_type-&gt;is_object_kind()) {
2054     profile_return_type(result, target);
2055   }
2056 }
2057 
2058 
2059 void GraphBuilder::new_instance(int klass_index) {
2060   ValueStack* state_before = copy_state_exhandling();
2061   bool will_link;
2062   ciKlass* klass = stream()-&gt;get_klass(will_link);
2063   assert(klass-&gt;is_instance_klass(), "must be an instance klass");
2064   NewInstance* new_instance = new NewInstance(klass-&gt;as_instance_klass(), state_before, stream()-&gt;is_unresolved_klass());
2065   _memory-&gt;new_instance(new_instance);
2066   apush(append_split(new_instance));
2067 }
2068 
2069 
2070 void GraphBuilder::new_type_array() {
2071   ValueStack* state_before = copy_state_exhandling();
2072   apush(append_split(new NewTypeArray(ipop(), (BasicType)stream()-&gt;get_index(), state_before)));
2073 }
2074 
2075 
2076 void GraphBuilder::new_object_array() {
2077   bool will_link;
2078   ciKlass* klass = stream()-&gt;get_klass(will_link);
2079   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2080   NewArray* n = new NewObjectArray(klass, ipop(), state_before);
2081   apush(append_split(n));
2082 }
2083 
2084 
2085 bool GraphBuilder::direct_compare(ciKlass* k) {
2086   if (k-&gt;is_loaded() &amp;&amp; k-&gt;is_instance_klass() &amp;&amp; !UseSlowPath) {
2087     ciInstanceKlass* ik = k-&gt;as_instance_klass();
2088     if (ik-&gt;is_final()) {
2089       return true;
2090     } else {
2091       if (DeoptC1 &amp;&amp; UseCHA &amp;&amp; !(ik-&gt;has_subklass() || ik-&gt;is_interface())) {
2092         // test class is leaf class
2093         dependency_recorder()-&gt;assert_leaf_type(ik);
2094         return true;
2095       }
2096     }
2097   }
2098   return false;
2099 }
2100 
2101 
2102 void GraphBuilder::check_cast(int klass_index) {
2103   bool will_link;
2104   ciKlass* klass = stream()-&gt;get_klass(will_link);
2105   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_for_exception();
2106   CheckCast* c = new CheckCast(klass, apop(), state_before);
2107   apush(append_split(c));
2108   c-&gt;set_direct_compare(direct_compare(klass));
2109 
2110   if (is_profiling()) {
2111     // Note that we'd collect profile data in this method if we wanted it.
2112     compilation()-&gt;set_would_profile(true);
2113 
2114     if (profile_checkcasts()) {
2115       c-&gt;set_profiled_method(method());
2116       c-&gt;set_profiled_bci(bci());
2117       c-&gt;set_should_profile(true);
2118     }
2119   }
2120 }
2121 
2122 
2123 void GraphBuilder::instance_of(int klass_index) {
2124   bool will_link;
2125   ciKlass* klass = stream()-&gt;get_klass(will_link);
2126   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2127   InstanceOf* i = new InstanceOf(klass, apop(), state_before);
2128   ipush(append_split(i));
2129   i-&gt;set_direct_compare(direct_compare(klass));
2130 
2131   if (is_profiling()) {
2132     // Note that we'd collect profile data in this method if we wanted it.
2133     compilation()-&gt;set_would_profile(true);
2134 
2135     if (profile_checkcasts()) {
2136       i-&gt;set_profiled_method(method());
2137       i-&gt;set_profiled_bci(bci());
2138       i-&gt;set_should_profile(true);
2139     }
2140   }
2141 }
2142 
2143 
2144 void GraphBuilder::monitorenter(Value x, int bci) {
2145   // save state before locking in case of deoptimization after a NullPointerException
2146   ValueStack* state_before = copy_state_for_exception_with_bci(bci);
2147   append_with_bci(new MonitorEnter(x, state()-&gt;lock(x), state_before), bci);
2148   kill_all();
2149 }
2150 
2151 
2152 void GraphBuilder::monitorexit(Value x, int bci) {
2153   append_with_bci(new MonitorExit(x, state()-&gt;unlock()), bci);
2154   kill_all();
2155 }
2156 
2157 
2158 void GraphBuilder::new_multi_array(int dimensions) {
2159   bool will_link;
2160   ciKlass* klass = stream()-&gt;get_klass(will_link);
2161   ValueStack* state_before = !klass-&gt;is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
2162 
2163   Values* dims = new Values(dimensions, NULL);
2164   // fill in all dimensions
2165   int i = dimensions;
2166   while (i-- &gt; 0) dims-&gt;at_put(i, ipop());
2167   // create array
2168   NewArray* n = new NewMultiArray(klass, dims, state_before);
2169   apush(append_split(n));
2170 }
2171 
2172 
2173 void GraphBuilder::throw_op(int bci) {
2174   // We require that the debug info for a Throw be the "state before"
2175   // the Throw (i.e., exception oop is still on TOS)
2176   ValueStack* state_before = copy_state_before_with_bci(bci);
2177   Throw* t = new Throw(apop(), state_before);
2178   // operand stack not needed after a throw
2179   state()-&gt;truncate_stack(0);
2180   append_with_bci(t, bci);
2181 }
2182 
2183 
2184 Value GraphBuilder::round_fp(Value fp_value) {
2185   // no rounding needed if SSE2 is used
2186   if (RoundFPResults &amp;&amp; UseSSE &lt; 2) {
2187     // Must currently insert rounding node for doubleword values that
2188     // are results of expressions (i.e., not loads from memory or
2189     // constants)
2190     if (fp_value-&gt;type()-&gt;tag() == doubleTag &amp;&amp;
2191         fp_value-&gt;as_Constant() == NULL &amp;&amp;
2192         fp_value-&gt;as_Local() == NULL &amp;&amp;       // method parameters need no rounding
2193         fp_value-&gt;as_RoundFP() == NULL) {
2194       return append(new RoundFP(fp_value));
2195     }
2196   }
2197   return fp_value;
2198 }
2199 
2200 
2201 Instruction* GraphBuilder::append_with_bci(Instruction* instr, int bci) {
2202   Canonicalizer canon(compilation(), instr, bci);
2203   Instruction* i1 = canon.canonical();
2204   if (i1-&gt;is_linked() || !i1-&gt;can_be_linked()) {
2205     // Canonicalizer returned an instruction which was already
2206     // appended so simply return it.
2207     return i1;
2208   }
2209 
2210   if (UseLocalValueNumbering) {
2211     // Lookup the instruction in the ValueMap and add it to the map if
2212     // it's not found.
2213     Instruction* i2 = vmap()-&gt;find_insert(i1);
2214     if (i2 != i1) {
2215       // found an entry in the value map, so just return it.
2216       assert(i2-&gt;is_linked(), "should already be linked");
2217       return i2;
2218     }
2219     ValueNumberingEffects vne(vmap());
2220     i1-&gt;visit(&amp;vne);
2221   }
2222 
2223   // i1 was not eliminated =&gt; append it
2224   assert(i1-&gt;next() == NULL, "shouldn't already be linked");
2225   _last = _last-&gt;set_next(i1, canon.bci());
2226 
2227   if (++_instruction_count &gt;= InstructionCountCutoff &amp;&amp; !bailed_out()) {
2228     // set the bailout state but complete normal processing.  We
2229     // might do a little more work before noticing the bailout so we
2230     // want processing to continue normally until it's noticed.
2231     bailout("Method and/or inlining is too large");
2232   }
2233 
2234 #ifndef PRODUCT
2235   if (PrintIRDuringConstruction) {
2236     InstructionPrinter ip;
2237     ip.print_line(i1);
2238     if (Verbose) {
2239       state()-&gt;print();
2240     }
2241   }
2242 #endif
2243 
2244   // save state after modification of operand stack for StateSplit instructions
2245   StateSplit* s = i1-&gt;as_StateSplit();
2246   if (s != NULL) {
2247     if (EliminateFieldAccess) {
2248       Intrinsic* intrinsic = s-&gt;as_Intrinsic();
2249       if (s-&gt;as_Invoke() != NULL || (intrinsic &amp;&amp; !intrinsic-&gt;preserves_state())) {
2250         _memory-&gt;kill();
2251       }
2252     }
2253     s-&gt;set_state(state()-&gt;copy(ValueStack::StateAfter, canon.bci()));
2254   }
2255 
2256   // set up exception handlers for this instruction if necessary
2257   if (i1-&gt;can_trap()) {
2258     i1-&gt;set_exception_handlers(handle_exception(i1));
2259     assert(i1-&gt;exception_state() != NULL || !i1-&gt;needs_exception_state() || bailed_out(), "handle_exception must set exception state");
2260   }
2261   return i1;
2262 }
2263 
2264 
2265 Instruction* GraphBuilder::append(Instruction* instr) {
2266   assert(instr-&gt;as_StateSplit() == NULL || instr-&gt;as_BlockEnd() != NULL, "wrong append used");
2267   return append_with_bci(instr, bci());
2268 }
2269 
2270 
2271 Instruction* GraphBuilder::append_split(StateSplit* instr) {
2272   return append_with_bci(instr, bci());
2273 }
2274 
2275 
2276 void GraphBuilder::null_check(Value value) {
2277   if (value-&gt;as_NewArray() != NULL || value-&gt;as_NewInstance() != NULL) {
2278     return;
2279   } else {
2280     Constant* con = value-&gt;as_Constant();
2281     if (con) {
2282       ObjectType* c = con-&gt;type()-&gt;as_ObjectType();
2283       if (c &amp;&amp; c-&gt;is_loaded()) {
2284         ObjectConstant* oc = c-&gt;as_ObjectConstant();
2285         if (!oc || !oc-&gt;value()-&gt;is_null_object()) {
2286           return;
2287         }
2288       }
2289     }
2290   }
2291   append(new NullCheck(value, copy_state_for_exception()));
2292 }
2293 
2294 
2295 
2296 XHandlers* GraphBuilder::handle_exception(Instruction* instruction) {
2297   if (!has_handler() &amp;&amp; (!instruction-&gt;needs_exception_state() || instruction-&gt;exception_state() != NULL)) {
2298     assert(instruction-&gt;exception_state() == NULL
2299            || instruction-&gt;exception_state()-&gt;kind() == ValueStack::EmptyExceptionState
2300            || (instruction-&gt;exception_state()-&gt;kind() == ValueStack::ExceptionState &amp;&amp; _compilation-&gt;env()-&gt;jvmti_can_access_local_variables()),
2301            "exception_state should be of exception kind");
2302     return new XHandlers();
2303   }
2304 
2305   XHandlers*  exception_handlers = new XHandlers();
2306   ScopeData*  cur_scope_data = scope_data();
2307   ValueStack* cur_state = instruction-&gt;state_before();
2308   ValueStack* prev_state = NULL;
2309   int scope_count = 0;
2310 
2311   assert(cur_state != NULL, "state_before must be set");
2312   do {
2313     int cur_bci = cur_state-&gt;bci();
2314     assert(cur_scope_data-&gt;scope() == cur_state-&gt;scope(), "scopes do not match");
2315     assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data-&gt;stream()-&gt;cur_bci(), "invalid bci");
2316 
2317     // join with all potential exception handlers
2318     XHandlers* list = cur_scope_data-&gt;xhandlers();
2319     const int n = list-&gt;length();
2320     for (int i = 0; i &lt; n; i++) {
2321       XHandler* h = list-&gt;handler_at(i);
2322       if (h-&gt;covers(cur_bci)) {
2323         // h is a potential exception handler =&gt; join it
2324         compilation()-&gt;set_has_exception_handlers(true);
2325 
2326         BlockBegin* entry = h-&gt;entry_block();
2327         if (entry == block()) {
2328           // It's acceptable for an exception handler to cover itself
2329           // but we don't handle that in the parser currently.  It's
2330           // very rare so we bailout instead of trying to handle it.
2331           BAILOUT_("exception handler covers itself", exception_handlers);
2332         }
2333         assert(entry-&gt;bci() == h-&gt;handler_bci(), "must match");
2334         assert(entry-&gt;bci() == -1 || entry == cur_scope_data-&gt;block_at(entry-&gt;bci()), "blocks must correspond");
2335 
2336         // previously this was a BAILOUT, but this is not necessary
2337         // now because asynchronous exceptions are not handled this way.
2338         assert(entry-&gt;state() == NULL || cur_state-&gt;total_locks_size() == entry-&gt;state()-&gt;total_locks_size(), "locks do not match");
2339 
2340         // xhandler start with an empty expression stack
2341         if (cur_state-&gt;stack_size() != 0) {
2342           cur_state = cur_state-&gt;copy(ValueStack::ExceptionState, cur_state-&gt;bci());
2343         }
2344         if (instruction-&gt;exception_state() == NULL) {
2345           instruction-&gt;set_exception_state(cur_state);
2346         }
2347 
2348         // Note: Usually this join must work. However, very
2349         // complicated jsr-ret structures where we don't ret from
2350         // the subroutine can cause the objects on the monitor
2351         // stacks to not match because blocks can be parsed twice.
2352         // The only test case we've seen so far which exhibits this
2353         // problem is caught by the infinite recursion test in
2354         // GraphBuilder::jsr() if the join doesn't work.
2355         if (!entry-&gt;try_merge(cur_state)) {
2356           BAILOUT_("error while joining with exception handler, prob. due to complicated jsr/rets", exception_handlers);
2357         }
2358 
2359         // add current state for correct handling of phi functions at begin of xhandler
2360         int phi_operand = entry-&gt;add_exception_state(cur_state);
2361 
2362         // add entry to the list of xhandlers of this block
2363         _block-&gt;add_exception_handler(entry);
2364 
2365         // add back-edge from xhandler entry to this block
2366         if (!entry-&gt;is_predecessor(_block)) {
2367           entry-&gt;add_predecessor(_block);
2368         }
2369 
2370         // clone XHandler because phi_operand and scope_count can not be shared
2371         XHandler* new_xhandler = new XHandler(h);
2372         new_xhandler-&gt;set_phi_operand(phi_operand);
2373         new_xhandler-&gt;set_scope_count(scope_count);
2374         exception_handlers-&gt;append(new_xhandler);
2375 
2376         // fill in exception handler subgraph lazily
2377         assert(!entry-&gt;is_set(BlockBegin::was_visited_flag), "entry must not be visited yet");
2378         cur_scope_data-&gt;add_to_work_list(entry);
2379 
2380         // stop when reaching catchall
2381         if (h-&gt;catch_type() == 0) {
2382           return exception_handlers;
2383         }
2384       }
2385     }
2386 
2387     if (exception_handlers-&gt;length() == 0) {
2388       // This scope and all callees do not handle exceptions, so the local
2389       // variables of this scope are not needed. However, the scope itself is
2390       // required for a correct exception stack trace -&gt; clear out the locals.
2391       if (_compilation-&gt;env()-&gt;jvmti_can_access_local_variables()) {
2392         cur_state = cur_state-&gt;copy(ValueStack::ExceptionState, cur_state-&gt;bci());
2393       } else {
2394         cur_state = cur_state-&gt;copy(ValueStack::EmptyExceptionState, cur_state-&gt;bci());
2395       }
2396       if (prev_state != NULL) {
2397         prev_state-&gt;set_caller_state(cur_state);
2398       }
2399       if (instruction-&gt;exception_state() == NULL) {
2400         instruction-&gt;set_exception_state(cur_state);
2401       }
2402     }
2403 
2404     // Set up iteration for next time.
2405     // If parsing a jsr, do not grab exception handlers from the
2406     // parent scopes for this method (already got them, and they
2407     // needed to be cloned)
2408 
2409     while (cur_scope_data-&gt;parsing_jsr()) {
2410       cur_scope_data = cur_scope_data-&gt;parent();
2411     }
2412 
2413     assert(cur_scope_data-&gt;scope() == cur_state-&gt;scope(), "scopes do not match");
2414     assert(cur_state-&gt;locks_size() == 0 || cur_state-&gt;locks_size() == 1, "unlocking must be done in a catchall exception handler");
2415 
2416     prev_state = cur_state;
2417     cur_state = cur_state-&gt;caller_state();
2418     cur_scope_data = cur_scope_data-&gt;parent();
2419     scope_count++;
2420   } while (cur_scope_data != NULL);
2421 
2422   return exception_handlers;
2423 }
2424 
2425 
2426 // Helper class for simplifying Phis.
2427 class PhiSimplifier : public BlockClosure {
2428  private:
2429   bool _has_substitutions;
2430   Value simplify(Value v);
2431 
2432  public:
2433   PhiSimplifier(BlockBegin* start) : _has_substitutions(false) {
2434     start-&gt;iterate_preorder(this);
2435     if (_has_substitutions) {
2436       SubstitutionResolver sr(start);
2437     }
2438   }
2439   void block_do(BlockBegin* b);
2440   bool has_substitutions() const { return _has_substitutions; }
2441 };
2442 
2443 
2444 Value PhiSimplifier::simplify(Value v) {
2445   Phi* phi = v-&gt;as_Phi();
2446 
2447   if (phi == NULL) {
2448     // no phi function
2449     return v;
2450   } else if (v-&gt;has_subst()) {
2451     // already substituted; subst can be phi itself -&gt; simplify
2452     return simplify(v-&gt;subst());
2453   } else if (phi-&gt;is_set(Phi::cannot_simplify)) {
2454     // already tried to simplify phi before
2455     return phi;
2456   } else if (phi-&gt;is_set(Phi::visited)) {
2457     // break cycles in phi functions
2458     return phi;
2459   } else if (phi-&gt;type()-&gt;is_illegal()) {
2460     // illegal phi functions are ignored anyway
2461     return phi;
2462 
2463   } else {
2464     // mark phi function as processed to break cycles in phi functions
2465     phi-&gt;set(Phi::visited);
2466 
2467     // simplify x = [y, x] and x = [y, y] to y
2468     Value subst = NULL;
2469     int opd_count = phi-&gt;operand_count();
2470     for (int i = 0; i &lt; opd_count; i++) {
2471       Value opd = phi-&gt;operand_at(i);
2472       assert(opd != NULL, "Operand must exist!");
2473 
2474       if (opd-&gt;type()-&gt;is_illegal()) {
2475         // if one operand is illegal, the entire phi function is illegal
2476         phi-&gt;make_illegal();
2477         phi-&gt;clear(Phi::visited);
2478         return phi;
2479       }
2480 
2481       Value new_opd = simplify(opd);
2482       assert(new_opd != NULL, "Simplified operand must exist!");
2483 
2484       if (new_opd != phi &amp;&amp; new_opd != subst) {
2485         if (subst == NULL) {
2486           subst = new_opd;
2487         } else {
2488           // no simplification possible
2489           phi-&gt;set(Phi::cannot_simplify);
2490           phi-&gt;clear(Phi::visited);
2491           return phi;
2492         }
2493       }
2494     }
2495 
2496     // sucessfully simplified phi function
2497     assert(subst != NULL, "illegal phi function");
2498     _has_substitutions = true;
2499     phi-&gt;clear(Phi::visited);
2500     phi-&gt;set_subst(subst);
2501 
2502 #ifndef PRODUCT
2503     if (PrintPhiFunctions) {
2504       tty-&gt;print_cr("simplified phi function %c%d to %c%d (Block B%d)", phi-&gt;type()-&gt;tchar(), phi-&gt;id(), subst-&gt;type()-&gt;tchar(), subst-&gt;id(), phi-&gt;block()-&gt;block_id());
2505     }
2506 #endif
2507 
2508     return subst;
2509   }
2510 }
2511 
2512 
2513 void PhiSimplifier::block_do(BlockBegin* b) {
2514   for_each_phi_fun(b, phi,
2515     simplify(phi);
2516   );
2517 
2518 #ifdef ASSERT
2519   for_each_phi_fun(b, phi,
2520                    assert(phi-&gt;operand_count() != 1 || phi-&gt;subst() != phi, "missed trivial simplification");
2521   );
2522 
2523   ValueStack* state = b-&gt;state()-&gt;caller_state();
2524   for_each_state_value(state, value,
2525     Phi* phi = value-&gt;as_Phi();
2526     assert(phi == NULL || phi-&gt;block() != b, "must not have phi function to simplify in caller state");
2527   );
2528 #endif
2529 }
2530 
2531 // This method is called after all blocks are filled with HIR instructions
2532 // It eliminates all Phi functions of the form x = [y, y] and x = [y, x]
2533 void GraphBuilder::eliminate_redundant_phis(BlockBegin* start) {
2534   PhiSimplifier simplifier(start);
2535 }
2536 
2537 
2538 void GraphBuilder::connect_to_end(BlockBegin* beg) {
2539   // setup iteration
2540   kill_all();
2541   _block = beg;
2542   _state = beg-&gt;state()-&gt;copy_for_parsing();
2543   _last  = beg;
2544   iterate_bytecodes_for_block(beg-&gt;bci());
2545 }
2546 
2547 
2548 BlockEnd* GraphBuilder::iterate_bytecodes_for_block(int bci) {
2549 #ifndef PRODUCT
2550   if (PrintIRDuringConstruction) {
2551     tty-&gt;cr();
2552     InstructionPrinter ip;
2553     ip.print_instr(_block); tty-&gt;cr();
2554     ip.print_stack(_block-&gt;state()); tty-&gt;cr();
2555     ip.print_inline_level(_block);
2556     ip.print_head();
2557     tty-&gt;print_cr("locals size: %d stack size: %d", state()-&gt;locals_size(), state()-&gt;stack_size());
2558   }
2559 #endif
2560   _skip_block = false;
2561   assert(state() != NULL, "ValueStack missing!");
2562   CompileLog* log = compilation()-&gt;log();
2563   ciBytecodeStream s(method());
2564   s.reset_to_bci(bci);
2565   int prev_bci = bci;
2566   scope_data()-&gt;set_stream(&amp;s);
2567   // iterate
2568   Bytecodes::Code code = Bytecodes::_illegal;
2569   bool push_exception = false;
2570 
2571   if (block()-&gt;is_set(BlockBegin::exception_entry_flag) &amp;&amp; block()-&gt;next() == NULL) {
2572     // first thing in the exception entry block should be the exception object.
2573     push_exception = true;
2574   }
2575 
2576   while (!bailed_out() &amp;&amp; last()-&gt;as_BlockEnd() == NULL &amp;&amp;
2577          (code = stream()-&gt;next()) != ciBytecodeStream::EOBC() &amp;&amp;
2578          (block_at(s.cur_bci()) == NULL || block_at(s.cur_bci()) == block())) {
2579     assert(state()-&gt;kind() == ValueStack::Parsing, "invalid state kind");
2580 
2581     if (log != NULL)
2582       log-&gt;set_context("bc code='%d' bci='%d'", (int)code, s.cur_bci());
2583 
2584     // Check for active jsr during OSR compilation
2585     if (compilation()-&gt;is_osr_compile()
2586         &amp;&amp; scope()-&gt;is_top_scope()
2587         &amp;&amp; parsing_jsr()
2588         &amp;&amp; s.cur_bci() == compilation()-&gt;osr_bci()) {
2589       bailout("OSR not supported while a jsr is active");
2590     }
2591 
2592     if (push_exception) {
2593       apush(append(new ExceptionObject()));
2594       push_exception = false;
2595     }
2596 
2597     // handle bytecode
2598     switch (code) {
2599       case Bytecodes::_nop            : /* nothing to do */ break;
2600       case Bytecodes::_aconst_null    : apush(append(new Constant(objectNull            ))); break;
2601       case Bytecodes::_iconst_m1      : ipush(append(new Constant(new IntConstant   (-1)))); break;
2602       case Bytecodes::_iconst_0       : ipush(append(new Constant(intZero               ))); break;
2603       case Bytecodes::_iconst_1       : ipush(append(new Constant(intOne                ))); break;
2604       case Bytecodes::_iconst_2       : ipush(append(new Constant(new IntConstant   ( 2)))); break;
2605       case Bytecodes::_iconst_3       : ipush(append(new Constant(new IntConstant   ( 3)))); break;
2606       case Bytecodes::_iconst_4       : ipush(append(new Constant(new IntConstant   ( 4)))); break;
2607       case Bytecodes::_iconst_5       : ipush(append(new Constant(new IntConstant   ( 5)))); break;
2608       case Bytecodes::_lconst_0       : lpush(append(new Constant(new LongConstant  ( 0)))); break;
2609       case Bytecodes::_lconst_1       : lpush(append(new Constant(new LongConstant  ( 1)))); break;
2610       case Bytecodes::_fconst_0       : fpush(append(new Constant(new FloatConstant ( 0)))); break;
2611       case Bytecodes::_fconst_1       : fpush(append(new Constant(new FloatConstant ( 1)))); break;
2612       case Bytecodes::_fconst_2       : fpush(append(new Constant(new FloatConstant ( 2)))); break;
2613       case Bytecodes::_dconst_0       : dpush(append(new Constant(new DoubleConstant( 0)))); break;
2614       case Bytecodes::_dconst_1       : dpush(append(new Constant(new DoubleConstant( 1)))); break;
2615       case Bytecodes::_bipush         : ipush(append(new Constant(new IntConstant(((signed char*)s.cur_bcp())[1])))); break;
2616       case Bytecodes::_sipush         : ipush(append(new Constant(new IntConstant((short)Bytes::get_Java_u2(s.cur_bcp()+1))))); break;
2617       case Bytecodes::_ldc            : // fall through
2618       case Bytecodes::_ldc_w          : // fall through
2619       case Bytecodes::_ldc2_w         : load_constant(); break;
2620       case Bytecodes::_iload          : load_local(intType     , s.get_index()); break;
2621       case Bytecodes::_lload          : load_local(longType    , s.get_index()); break;
2622       case Bytecodes::_fload          : load_local(floatType   , s.get_index()); break;
2623       case Bytecodes::_dload          : load_local(doubleType  , s.get_index()); break;
2624       case Bytecodes::_aload          : load_local(instanceType, s.get_index()); break;
2625       case Bytecodes::_iload_0        : load_local(intType   , 0); break;
2626       case Bytecodes::_iload_1        : load_local(intType   , 1); break;
2627       case Bytecodes::_iload_2        : load_local(intType   , 2); break;
2628       case Bytecodes::_iload_3        : load_local(intType   , 3); break;
2629       case Bytecodes::_lload_0        : load_local(longType  , 0); break;
2630       case Bytecodes::_lload_1        : load_local(longType  , 1); break;
2631       case Bytecodes::_lload_2        : load_local(longType  , 2); break;
2632       case Bytecodes::_lload_3        : load_local(longType  , 3); break;
2633       case Bytecodes::_fload_0        : load_local(floatType , 0); break;
2634       case Bytecodes::_fload_1        : load_local(floatType , 1); break;
2635       case Bytecodes::_fload_2        : load_local(floatType , 2); break;
2636       case Bytecodes::_fload_3        : load_local(floatType , 3); break;
2637       case Bytecodes::_dload_0        : load_local(doubleType, 0); break;
2638       case Bytecodes::_dload_1        : load_local(doubleType, 1); break;
2639       case Bytecodes::_dload_2        : load_local(doubleType, 2); break;
2640       case Bytecodes::_dload_3        : load_local(doubleType, 3); break;
2641       case Bytecodes::_aload_0        : load_local(objectType, 0); break;
2642       case Bytecodes::_aload_1        : load_local(objectType, 1); break;
2643       case Bytecodes::_aload_2        : load_local(objectType, 2); break;
2644       case Bytecodes::_aload_3        : load_local(objectType, 3); break;
2645       case Bytecodes::_iaload         : load_indexed(T_INT   ); break;
2646       case Bytecodes::_laload         : load_indexed(T_LONG  ); break;
2647       case Bytecodes::_faload         : load_indexed(T_FLOAT ); break;
2648       case Bytecodes::_daload         : load_indexed(T_DOUBLE); break;
2649       case Bytecodes::_aaload         : load_indexed(T_OBJECT); break;
2650       case Bytecodes::_baload         : load_indexed(T_BYTE  ); break;
2651       case Bytecodes::_caload         : load_indexed(T_CHAR  ); break;
2652       case Bytecodes::_saload         : load_indexed(T_SHORT ); break;
2653       case Bytecodes::_istore         : store_local(intType   , s.get_index()); break;
2654       case Bytecodes::_lstore         : store_local(longType  , s.get_index()); break;
2655       case Bytecodes::_fstore         : store_local(floatType , s.get_index()); break;
2656       case Bytecodes::_dstore         : store_local(doubleType, s.get_index()); break;
2657       case Bytecodes::_astore         : store_local(objectType, s.get_index()); break;
2658       case Bytecodes::_istore_0       : store_local(intType   , 0); break;
2659       case Bytecodes::_istore_1       : store_local(intType   , 1); break;
2660       case Bytecodes::_istore_2       : store_local(intType   , 2); break;
2661       case Bytecodes::_istore_3       : store_local(intType   , 3); break;
2662       case Bytecodes::_lstore_0       : store_local(longType  , 0); break;
2663       case Bytecodes::_lstore_1       : store_local(longType  , 1); break;
2664       case Bytecodes::_lstore_2       : store_local(longType  , 2); break;
2665       case Bytecodes::_lstore_3       : store_local(longType  , 3); break;
2666       case Bytecodes::_fstore_0       : store_local(floatType , 0); break;
2667       case Bytecodes::_fstore_1       : store_local(floatType , 1); break;
2668       case Bytecodes::_fstore_2       : store_local(floatType , 2); break;
2669       case Bytecodes::_fstore_3       : store_local(floatType , 3); break;
2670       case Bytecodes::_dstore_0       : store_local(doubleType, 0); break;
2671       case Bytecodes::_dstore_1       : store_local(doubleType, 1); break;
2672       case Bytecodes::_dstore_2       : store_local(doubleType, 2); break;
2673       case Bytecodes::_dstore_3       : store_local(doubleType, 3); break;
2674       case Bytecodes::_astore_0       : store_local(objectType, 0); break;
2675       case Bytecodes::_astore_1       : store_local(objectType, 1); break;
2676       case Bytecodes::_astore_2       : store_local(objectType, 2); break;
2677       case Bytecodes::_astore_3       : store_local(objectType, 3); break;
2678       case Bytecodes::_iastore        : store_indexed(T_INT   ); break;
2679       case Bytecodes::_lastore        : store_indexed(T_LONG  ); break;
2680       case Bytecodes::_fastore        : store_indexed(T_FLOAT ); break;
2681       case Bytecodes::_dastore        : store_indexed(T_DOUBLE); break;
2682       case Bytecodes::_aastore        : store_indexed(T_OBJECT); break;
2683       case Bytecodes::_bastore        : store_indexed(T_BYTE  ); break;
2684       case Bytecodes::_castore        : store_indexed(T_CHAR  ); break;
2685       case Bytecodes::_sastore        : store_indexed(T_SHORT ); break;
2686       case Bytecodes::_pop            : // fall through
2687       case Bytecodes::_pop2           : // fall through
2688       case Bytecodes::_dup            : // fall through
2689       case Bytecodes::_dup_x1         : // fall through
2690       case Bytecodes::_dup_x2         : // fall through
2691       case Bytecodes::_dup2           : // fall through
2692       case Bytecodes::_dup2_x1        : // fall through
2693       case Bytecodes::_dup2_x2        : // fall through
2694       case Bytecodes::_swap           : stack_op(code); break;
2695       case Bytecodes::_iadd           : arithmetic_op(intType   , code); break;
2696       case Bytecodes::_ladd           : arithmetic_op(longType  , code); break;
2697       case Bytecodes::_fadd           : arithmetic_op(floatType , code); break;
2698       case Bytecodes::_dadd           : arithmetic_op(doubleType, code); break;
2699       case Bytecodes::_isub           : arithmetic_op(intType   , code); break;
2700       case Bytecodes::_lsub           : arithmetic_op(longType  , code); break;
2701       case Bytecodes::_fsub           : arithmetic_op(floatType , code); break;
2702       case Bytecodes::_dsub           : arithmetic_op(doubleType, code); break;
2703       case Bytecodes::_imul           : arithmetic_op(intType   , code); break;
2704       case Bytecodes::_lmul           : arithmetic_op(longType  , code); break;
2705       case Bytecodes::_fmul           : arithmetic_op(floatType , code); break;
2706       case Bytecodes::_dmul           : arithmetic_op(doubleType, code); break;
2707       case Bytecodes::_idiv           : arithmetic_op(intType   , code, copy_state_for_exception()); break;
2708       case Bytecodes::_ldiv           : arithmetic_op(longType  , code, copy_state_for_exception()); break;
2709       case Bytecodes::_fdiv           : arithmetic_op(floatType , code); break;
2710       case Bytecodes::_ddiv           : arithmetic_op(doubleType, code); break;
2711       case Bytecodes::_irem           : arithmetic_op(intType   , code, copy_state_for_exception()); break;
2712       case Bytecodes::_lrem           : arithmetic_op(longType  , code, copy_state_for_exception()); break;
2713       case Bytecodes::_frem           : arithmetic_op(floatType , code); break;
2714       case Bytecodes::_drem           : arithmetic_op(doubleType, code); break;
2715       case Bytecodes::_ineg           : negate_op(intType   ); break;
2716       case Bytecodes::_lneg           : negate_op(longType  ); break;
2717       case Bytecodes::_fneg           : negate_op(floatType ); break;
2718       case Bytecodes::_dneg           : negate_op(doubleType); break;
2719       case Bytecodes::_ishl           : shift_op(intType , code); break;
2720       case Bytecodes::_lshl           : shift_op(longType, code); break;
2721       case Bytecodes::_ishr           : shift_op(intType , code); break;
2722       case Bytecodes::_lshr           : shift_op(longType, code); break;
2723       case Bytecodes::_iushr          : shift_op(intType , code); break;
2724       case Bytecodes::_lushr          : shift_op(longType, code); break;
2725       case Bytecodes::_iand           : logic_op(intType , code); break;
2726       case Bytecodes::_land           : logic_op(longType, code); break;
2727       case Bytecodes::_ior            : logic_op(intType , code); break;
2728       case Bytecodes::_lor            : logic_op(longType, code); break;
2729       case Bytecodes::_ixor           : logic_op(intType , code); break;
2730       case Bytecodes::_lxor           : logic_op(longType, code); break;
2731       case Bytecodes::_iinc           : increment(); break;
2732       case Bytecodes::_i2l            : convert(code, T_INT   , T_LONG  ); break;
2733       case Bytecodes::_i2f            : convert(code, T_INT   , T_FLOAT ); break;
2734       case Bytecodes::_i2d            : convert(code, T_INT   , T_DOUBLE); break;
2735       case Bytecodes::_l2i            : convert(code, T_LONG  , T_INT   ); break;
2736       case Bytecodes::_l2f            : convert(code, T_LONG  , T_FLOAT ); break;
2737       case Bytecodes::_l2d            : convert(code, T_LONG  , T_DOUBLE); break;
2738       case Bytecodes::_f2i            : convert(code, T_FLOAT , T_INT   ); break;
2739       case Bytecodes::_f2l            : convert(code, T_FLOAT , T_LONG  ); break;
2740       case Bytecodes::_f2d            : convert(code, T_FLOAT , T_DOUBLE); break;
2741       case Bytecodes::_d2i            : convert(code, T_DOUBLE, T_INT   ); break;
2742       case Bytecodes::_d2l            : convert(code, T_DOUBLE, T_LONG  ); break;
2743       case Bytecodes::_d2f            : convert(code, T_DOUBLE, T_FLOAT ); break;
2744       case Bytecodes::_i2b            : convert(code, T_INT   , T_BYTE  ); break;
2745       case Bytecodes::_i2c            : convert(code, T_INT   , T_CHAR  ); break;
2746       case Bytecodes::_i2s            : convert(code, T_INT   , T_SHORT ); break;
2747       case Bytecodes::_lcmp           : compare_op(longType  , code); break;
2748       case Bytecodes::_fcmpl          : compare_op(floatType , code); break;
2749       case Bytecodes::_fcmpg          : compare_op(floatType , code); break;
2750       case Bytecodes::_dcmpl          : compare_op(doubleType, code); break;
2751       case Bytecodes::_dcmpg          : compare_op(doubleType, code); break;
2752       case Bytecodes::_ifeq           : if_zero(intType   , If::eql); break;
2753       case Bytecodes::_ifne           : if_zero(intType   , If::neq); break;
2754       case Bytecodes::_iflt           : if_zero(intType   , If::lss); break;
2755       case Bytecodes::_ifge           : if_zero(intType   , If::geq); break;
2756       case Bytecodes::_ifgt           : if_zero(intType   , If::gtr); break;
2757       case Bytecodes::_ifle           : if_zero(intType   , If::leq); break;
2758       case Bytecodes::_if_icmpeq      : if_same(intType   , If::eql); break;
2759       case Bytecodes::_if_icmpne      : if_same(intType   , If::neq); break;
2760       case Bytecodes::_if_icmplt      : if_same(intType   , If::lss); break;
2761       case Bytecodes::_if_icmpge      : if_same(intType   , If::geq); break;
2762       case Bytecodes::_if_icmpgt      : if_same(intType   , If::gtr); break;
2763       case Bytecodes::_if_icmple      : if_same(intType   , If::leq); break;
2764       case Bytecodes::_if_acmpeq      : if_same(objectType, If::eql); break;
2765       case Bytecodes::_if_acmpne      : if_same(objectType, If::neq); break;
2766       case Bytecodes::_goto           : _goto(s.cur_bci(), s.get_dest()); break;
2767       case Bytecodes::_jsr            : jsr(s.get_dest()); break;
2768       case Bytecodes::_ret            : ret(s.get_index()); break;
2769       case Bytecodes::_tableswitch    : table_switch(); break;
2770       case Bytecodes::_lookupswitch   : lookup_switch(); break;
2771       case Bytecodes::_ireturn        : method_return(ipop()); break;
2772       case Bytecodes::_lreturn        : method_return(lpop()); break;
2773       case Bytecodes::_freturn        : method_return(fpop()); break;
2774       case Bytecodes::_dreturn        : method_return(dpop()); break;
2775       case Bytecodes::_areturn        : method_return(apop()); break;
2776       case Bytecodes::_return         : method_return(NULL  ); break;
2777       case Bytecodes::_getstatic      : // fall through
2778       case Bytecodes::_putstatic      : // fall through
2779       case Bytecodes::_getfield       : // fall through
2780       case Bytecodes::_putfield       : access_field(code); break;
2781       case Bytecodes::_invokevirtual  : // fall through
2782       case Bytecodes::_invokespecial  : // fall through
2783       case Bytecodes::_invokestatic   : // fall through
2784       case Bytecodes::_invokedynamic  : // fall through
2785       case Bytecodes::_invokeinterface: invoke(code); break;
2786       case Bytecodes::_new            : new_instance(s.get_index_u2()); break;
2787       case Bytecodes::_newarray       : new_type_array(); break;
2788       case Bytecodes::_anewarray      : new_object_array(); break;
2789       case Bytecodes::_arraylength    : { ValueStack* state_before = copy_state_for_exception(); ipush(append(new ArrayLength(apop(), state_before))); break; }
2790       case Bytecodes::_athrow         : throw_op(s.cur_bci()); break;
2791       case Bytecodes::_checkcast      : check_cast(s.get_index_u2()); break;
2792       case Bytecodes::_instanceof     : instance_of(s.get_index_u2()); break;
2793       case Bytecodes::_monitorenter   : monitorenter(apop(), s.cur_bci()); break;
2794       case Bytecodes::_monitorexit    : monitorexit (apop(), s.cur_bci()); break;
2795       case Bytecodes::_wide           : ShouldNotReachHere(); break;
2796       case Bytecodes::_multianewarray : new_multi_array(s.cur_bcp()[3]); break;
2797       case Bytecodes::_ifnull         : if_null(objectType, If::eql); break;
2798       case Bytecodes::_ifnonnull      : if_null(objectType, If::neq); break;
2799       case Bytecodes::_goto_w         : _goto(s.cur_bci(), s.get_far_dest()); break;
2800       case Bytecodes::_jsr_w          : jsr(s.get_far_dest()); break;
2801       case Bytecodes::_breakpoint     : BAILOUT_("concurrent setting of breakpoint", NULL);
2802       default                         : ShouldNotReachHere(); break;
2803     }
2804 
2805     if (log != NULL)
2806       log-&gt;clear_context(); // skip marker if nothing was printed
2807 
2808     // save current bci to setup Goto at the end
2809     prev_bci = s.cur_bci();
2810 
2811   }
2812   CHECK_BAILOUT_(NULL);
2813   // stop processing of this block (see try_inline_full)
2814   if (_skip_block) {
2815     _skip_block = false;
2816     assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
2817     return _last-&gt;as_BlockEnd();
2818   }
2819   // if there are any, check if last instruction is a BlockEnd instruction
2820   BlockEnd* end = last()-&gt;as_BlockEnd();
2821   if (end == NULL) {
2822     // all blocks must end with a BlockEnd instruction =&gt; add a Goto
2823     end = new Goto(block_at(s.cur_bci()), false);
2824     append(end);
2825   }
2826   assert(end == last()-&gt;as_BlockEnd(), "inconsistency");
2827 
2828   assert(end-&gt;state() != NULL, "state must already be present");
2829   assert(end-&gt;as_Return() == NULL || end-&gt;as_Throw() == NULL || end-&gt;state()-&gt;stack_size() == 0, "stack not needed for return and throw");
2830 
2831   // connect to begin &amp; set state
2832   // NOTE that inlining may have changed the block we are parsing
2833   block()-&gt;set_end(end);
2834   // propagate state
2835   for (int i = end-&gt;number_of_sux() - 1; i &gt;= 0; i--) {
2836     BlockBegin* sux = end-&gt;sux_at(i);
2837     assert(sux-&gt;is_predecessor(block()), "predecessor missing");
2838     // be careful, bailout if bytecodes are strange
2839     if (!sux-&gt;try_merge(end-&gt;state())) BAILOUT_("block join failed", NULL);
2840     scope_data()-&gt;add_to_work_list(end-&gt;sux_at(i));
2841   }
2842 
2843   scope_data()-&gt;set_stream(NULL);
2844 
2845   // done
2846   return end;
2847 }
2848 
2849 
2850 void GraphBuilder::iterate_all_blocks(bool start_in_current_block_for_inlining) {
2851   do {
2852     if (start_in_current_block_for_inlining &amp;&amp; !bailed_out()) {
2853       iterate_bytecodes_for_block(0);
2854       start_in_current_block_for_inlining = false;
2855     } else {
2856       BlockBegin* b;
2857       while ((b = scope_data()-&gt;remove_from_work_list()) != NULL) {
2858         if (!b-&gt;is_set(BlockBegin::was_visited_flag)) {
2859           if (b-&gt;is_set(BlockBegin::osr_entry_flag)) {
2860             // we're about to parse the osr entry block, so make sure
2861             // we setup the OSR edge leading into this block so that
2862             // Phis get setup correctly.
2863             setup_osr_entry_block();
2864             // this is no longer the osr entry block, so clear it.
2865             b-&gt;clear(BlockBegin::osr_entry_flag);
2866           }
2867           b-&gt;set(BlockBegin::was_visited_flag);
2868           connect_to_end(b);
2869         }
2870       }
2871     }
2872   } while (!bailed_out() &amp;&amp; !scope_data()-&gt;is_work_list_empty());
2873 }
2874 
2875 
2876 bool GraphBuilder::_can_trap      [Bytecodes::number_of_java_codes];
2877 
2878 void GraphBuilder::initialize() {
2879   // the following bytecodes are assumed to potentially
2880   // throw exceptions in compiled code - note that e.g.
2881   // monitorexit &amp; the return bytecodes do not throw
2882   // exceptions since monitor pairing proved that they
2883   // succeed (if monitor pairing succeeded)
2884   Bytecodes::Code can_trap_list[] =
2885     { Bytecodes::_ldc
2886     , Bytecodes::_ldc_w
2887     , Bytecodes::_ldc2_w
2888     , Bytecodes::_iaload
2889     , Bytecodes::_laload
2890     , Bytecodes::_faload
2891     , Bytecodes::_daload
2892     , Bytecodes::_aaload
2893     , Bytecodes::_baload
2894     , Bytecodes::_caload
2895     , Bytecodes::_saload
2896     , Bytecodes::_iastore
2897     , Bytecodes::_lastore
2898     , Bytecodes::_fastore
2899     , Bytecodes::_dastore
2900     , Bytecodes::_aastore
2901     , Bytecodes::_bastore
2902     , Bytecodes::_castore
2903     , Bytecodes::_sastore
2904     , Bytecodes::_idiv
2905     , Bytecodes::_ldiv
2906     , Bytecodes::_irem
2907     , Bytecodes::_lrem
2908     , Bytecodes::_getstatic
2909     , Bytecodes::_putstatic
2910     , Bytecodes::_getfield
2911     , Bytecodes::_putfield
2912     , Bytecodes::_invokevirtual
2913     , Bytecodes::_invokespecial
2914     , Bytecodes::_invokestatic
2915     , Bytecodes::_invokedynamic
2916     , Bytecodes::_invokeinterface
2917     , Bytecodes::_new
2918     , Bytecodes::_newarray
2919     , Bytecodes::_anewarray
2920     , Bytecodes::_arraylength
2921     , Bytecodes::_athrow
2922     , Bytecodes::_checkcast
2923     , Bytecodes::_instanceof
2924     , Bytecodes::_monitorenter
2925     , Bytecodes::_multianewarray
2926     };
2927 
2928   // inititialize trap tables
2929   for (int i = 0; i &lt; Bytecodes::number_of_java_codes; i++) {
2930     _can_trap[i] = false;
2931   }
2932   // set standard trap info
2933   for (uint j = 0; j &lt; ARRAY_SIZE(can_trap_list); j++) {
2934     _can_trap[can_trap_list[j]] = true;
2935   }
2936 }
2937 
2938 
2939 BlockBegin* GraphBuilder::header_block(BlockBegin* entry, BlockBegin::Flag f, ValueStack* state) {
2940   assert(entry-&gt;is_set(f), "entry/flag mismatch");
2941   // create header block
2942   BlockBegin* h = new BlockBegin(entry-&gt;bci());
2943   h-&gt;set_depth_first_number(0);
2944 
2945   Value l = h;
2946   BlockEnd* g = new Goto(entry, false);
2947   l-&gt;set_next(g, entry-&gt;bci());
2948   h-&gt;set_end(g);
2949   h-&gt;set(f);
2950   // setup header block end state
2951   ValueStack* s = state-&gt;copy(ValueStack::StateAfter, entry-&gt;bci()); // can use copy since stack is empty (=&gt; no phis)
2952   assert(s-&gt;stack_is_empty(), "must have empty stack at entry point");
2953   g-&gt;set_state(s);
2954   return h;
2955 }
2956 
2957 
2958 
2959 BlockBegin* GraphBuilder::setup_start_block(int osr_bci, BlockBegin* std_entry, BlockBegin* osr_entry, ValueStack* state) {
2960   BlockBegin* start = new BlockBegin(0);
2961 
2962   // This code eliminates the empty start block at the beginning of
2963   // each method.  Previously, each method started with the
2964   // start-block created below, and this block was followed by the
2965   // header block that was always empty.  This header block is only
2966   // necesary if std_entry is also a backward branch target because
2967   // then phi functions may be necessary in the header block.  It's
2968   // also necessary when profiling so that there's a single block that
2969   // can increment the interpreter_invocation_count.
2970   BlockBegin* new_header_block;
2971   if (std_entry-&gt;number_of_preds() &gt; 0 || count_invocations() || count_backedges()) {
2972     new_header_block = header_block(std_entry, BlockBegin::std_entry_flag, state);
2973   } else {
2974     new_header_block = std_entry;
2975   }
2976 
2977   // setup start block (root for the IR graph)
2978   Base* base =
2979     new Base(
2980       new_header_block,
2981       osr_entry
2982     );
2983   start-&gt;set_next(base, 0);
2984   start-&gt;set_end(base);
2985   // create &amp; setup state for start block
2986   start-&gt;set_state(state-&gt;copy(ValueStack::StateAfter, std_entry-&gt;bci()));
2987   base-&gt;set_state(state-&gt;copy(ValueStack::StateAfter, std_entry-&gt;bci()));
2988 
2989   if (base-&gt;std_entry()-&gt;state() == NULL) {
2990     // setup states for header blocks
2991     base-&gt;std_entry()-&gt;merge(state);
2992   }
2993 
2994   assert(base-&gt;std_entry()-&gt;state() != NULL, "");
2995   return start;
2996 }
2997 
2998 
2999 void GraphBuilder::setup_osr_entry_block() {
3000   assert(compilation()-&gt;is_osr_compile(), "only for osrs");
3001 
3002   int osr_bci = compilation()-&gt;osr_bci();
3003   ciBytecodeStream s(method());
3004   s.reset_to_bci(osr_bci);
3005   s.next();
3006   scope_data()-&gt;set_stream(&amp;s);
3007 
3008   // create a new block to be the osr setup code
3009   _osr_entry = new BlockBegin(osr_bci);
3010   _osr_entry-&gt;set(BlockBegin::osr_entry_flag);
3011   _osr_entry-&gt;set_depth_first_number(0);
3012   BlockBegin* target = bci2block()-&gt;at(osr_bci);
3013   assert(target != NULL &amp;&amp; target-&gt;is_set(BlockBegin::osr_entry_flag), "must be there");
3014   // the osr entry has no values for locals
3015   ValueStack* state = target-&gt;state()-&gt;copy();
3016   _osr_entry-&gt;set_state(state);
3017 
3018   kill_all();
3019   _block = _osr_entry;
3020   _state = _osr_entry-&gt;state()-&gt;copy();
3021   assert(_state-&gt;bci() == osr_bci, "mismatch");
3022   _last  = _osr_entry;
3023   Value e = append(new OsrEntry());
3024   e-&gt;set_needs_null_check(false);
3025 
3026   // OSR buffer is
3027   //
3028   // locals[nlocals-1..0]
3029   // monitors[number_of_locks-1..0]
3030   //
3031   // locals is a direct copy of the interpreter frame so in the osr buffer
3032   // so first slot in the local array is the last local from the interpreter
3033   // and last slot is local[0] (receiver) from the interpreter
3034   //
3035   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
3036   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
3037   // in the interpreter frame (the method lock if a sync method)
3038 
3039   // Initialize monitors in the compiled activation.
3040 
3041   int index;
3042   Value local;
3043 
3044   // find all the locals that the interpreter thinks contain live oops
3045   const BitMap live_oops = method()-&gt;live_local_oops_at_bci(osr_bci);
3046 
3047   // compute the offset into the locals so that we can treat the buffer
3048   // as if the locals were still in the interpreter frame
3049   int locals_offset = BytesPerWord * (method()-&gt;max_locals() - 1);
3050   for_each_local_value(state, index, local) {
3051     int offset = locals_offset - (index + local-&gt;type()-&gt;size() - 1) * BytesPerWord;
3052     Value get;
3053     if (local-&gt;type()-&gt;is_object_kind() &amp;&amp; !live_oops.at(index)) {
3054       // The interpreter thinks this local is dead but the compiler
3055       // doesn't so pretend that the interpreter passed in null.
3056       get = append(new Constant(objectNull));
3057     } else {
3058       get = append(new UnsafeGetRaw(as_BasicType(local-&gt;type()), e,
3059                                     append(new Constant(new IntConstant(offset))),
3060                                     0,
3061                                     true /*unaligned*/, true /*wide*/));
3062     }
3063     _state-&gt;store_local(index, get);
3064   }
3065 
3066   // the storage for the OSR buffer is freed manually in the LIRGenerator.
3067 
3068   assert(state-&gt;caller_state() == NULL, "should be top scope");
3069   state-&gt;clear_locals();
3070   Goto* g = new Goto(target, false);
3071   append(g);
3072   _osr_entry-&gt;set_end(g);
3073   target-&gt;merge(_osr_entry-&gt;end()-&gt;state());
3074 
3075   scope_data()-&gt;set_stream(NULL);
3076 }
3077 
3078 
3079 ValueStack* GraphBuilder::state_at_entry() {
3080   ValueStack* state = new ValueStack(scope(), NULL);
3081 
3082   // Set up locals for receiver
3083   int idx = 0;
3084   if (!method()-&gt;is_static()) {
3085     // we should always see the receiver
3086     state-&gt;store_local(idx, new Local(method()-&gt;holder(), objectType, idx));
3087     idx = 1;
3088   }
3089 
3090   // Set up locals for incoming arguments
3091   ciSignature* sig = method()-&gt;signature();
3092   for (int i = 0; i &lt; sig-&gt;count(); i++) {
3093     ciType* type = sig-&gt;type_at(i);
3094     BasicType basic_type = type-&gt;basic_type();
3095     // don't allow T_ARRAY to propagate into locals types
3096     if (basic_type == T_ARRAY) basic_type = T_OBJECT;
3097     ValueType* vt = as_ValueType(basic_type);
3098     state-&gt;store_local(idx, new Local(type, vt, idx));
3099     idx += type-&gt;size();
3100   }
3101 
3102   // lock synchronized method
3103   if (method()-&gt;is_synchronized()) {
3104     state-&gt;lock(NULL);
3105   }
3106 
3107   return state;
3108 }
3109 
3110 
3111 GraphBuilder::GraphBuilder(Compilation* compilation, IRScope* scope)
3112   : _scope_data(NULL)
3113   , _instruction_count(0)
3114   , _osr_entry(NULL)
3115   , _memory(new MemoryBuffer())
3116   , _compilation(compilation)
3117   , _inline_bailout_msg(NULL)
3118 {
3119   int osr_bci = compilation-&gt;osr_bci();
3120 
3121   // determine entry points and bci2block mapping
3122   BlockListBuilder blm(compilation, scope, osr_bci);
3123   CHECK_BAILOUT();
3124 
3125   BlockList* bci2block = blm.bci2block();
3126   BlockBegin* start_block = bci2block-&gt;at(0);
3127 
3128   push_root_scope(scope, bci2block, start_block);
3129 
3130   // setup state for std entry
3131   _initial_state = state_at_entry();
3132   start_block-&gt;merge(_initial_state);
3133 
3134   // complete graph
3135   _vmap        = new ValueMap();
3136   switch (scope-&gt;method()-&gt;intrinsic_id()) {
3137   case vmIntrinsics::_dabs          : // fall through
3138   case vmIntrinsics::_dsqrt         : // fall through
3139   case vmIntrinsics::_dsin          : // fall through
3140   case vmIntrinsics::_dcos          : // fall through
3141   case vmIntrinsics::_dtan          : // fall through
3142   case vmIntrinsics::_dlog          : // fall through
3143   case vmIntrinsics::_dlog10        : // fall through
3144   case vmIntrinsics::_dexp          : // fall through
3145   case vmIntrinsics::_dpow          : // fall through
3146     {
3147       // Compiles where the root method is an intrinsic need a special
3148       // compilation environment because the bytecodes for the method
3149       // shouldn't be parsed during the compilation, only the special
3150       // Intrinsic node should be emitted.  If this isn't done the the
3151       // code for the inlined version will be different than the root
3152       // compiled version which could lead to monotonicity problems on
3153       // intel.
3154 
3155       // Set up a stream so that appending instructions works properly.
3156       ciBytecodeStream s(scope-&gt;method());
3157       s.reset_to_bci(0);
3158       scope_data()-&gt;set_stream(&amp;s);
3159       s.next();
3160 
3161       // setup the initial block state
3162       _block = start_block;
3163       _state = start_block-&gt;state()-&gt;copy_for_parsing();
3164       _last  = start_block;
3165       load_local(doubleType, 0);
3166       if (scope-&gt;method()-&gt;intrinsic_id() == vmIntrinsics::_dpow) {
3167         load_local(doubleType, 2);
3168       }
3169 
3170       // Emit the intrinsic node.
3171       bool result = try_inline_intrinsics(scope-&gt;method());
3172       if (!result) BAILOUT("failed to inline intrinsic");
3173       method_return(dpop());
3174 
3175       // connect the begin and end blocks and we're all done.
3176       BlockEnd* end = last()-&gt;as_BlockEnd();
3177       block()-&gt;set_end(end);
3178       break;
3179     }
3180 
3181   case vmIntrinsics::_Reference_get:
3182     {
3183       {
3184         // With java.lang.ref.reference.get() we must go through the
3185         // intrinsic - when G1 is enabled - even when get() is the root
3186         // method of the compile so that, if necessary, the value in
3187         // the referent field of the reference object gets recorded by
3188         // the pre-barrier code.
3189         // Specifically, if G1 is enabled, the value in the referent
3190         // field is recorded by the G1 SATB pre barrier. This will
3191         // result in the referent being marked live and the reference
3192         // object removed from the list of discovered references during
3193         // reference processing.
3194 
3195         // Also we need intrinsic to prevent commoning reads from this field
3196         // across safepoint since GC can change its value.
3197 
3198         // Set up a stream so that appending instructions works properly.
3199         ciBytecodeStream s(scope-&gt;method());
3200         s.reset_to_bci(0);
3201         scope_data()-&gt;set_stream(&amp;s);
3202         s.next();
3203 
3204         // setup the initial block state
3205         _block = start_block;
3206         _state = start_block-&gt;state()-&gt;copy_for_parsing();
3207         _last  = start_block;
3208         load_local(objectType, 0);
3209 
3210         // Emit the intrinsic node.
3211         bool result = try_inline_intrinsics(scope-&gt;method());
3212         if (!result) BAILOUT("failed to inline intrinsic");
3213         method_return(apop());
3214 
3215         // connect the begin and end blocks and we're all done.
3216         BlockEnd* end = last()-&gt;as_BlockEnd();
3217         block()-&gt;set_end(end);
3218         break;
3219       }
3220       // Otherwise, fall thru
3221     }
3222 
3223   default:
3224     scope_data()-&gt;add_to_work_list(start_block);
3225     iterate_all_blocks();
3226     break;
3227   }
3228   CHECK_BAILOUT();
3229 
3230   _start = setup_start_block(osr_bci, start_block, _osr_entry, _initial_state);
3231 
3232   eliminate_redundant_phis(_start);
3233 
3234   NOT_PRODUCT(if (PrintValueNumbering &amp;&amp; Verbose) print_stats());
3235   // for osr compile, bailout if some requirements are not fulfilled
3236   if (osr_bci != -1) {
3237     BlockBegin* osr_block = blm.bci2block()-&gt;at(osr_bci);
3238     assert(osr_block-&gt;is_set(BlockBegin::was_visited_flag),"osr entry must have been visited for osr compile");
3239 
3240     // check if osr entry point has empty stack - we cannot handle non-empty stacks at osr entry points
3241     if (!osr_block-&gt;state()-&gt;stack_is_empty()) {
3242       BAILOUT("stack not empty at OSR entry point");
3243     }
3244   }
3245 #ifndef PRODUCT
3246   if (PrintCompilation &amp;&amp; Verbose) tty-&gt;print_cr("Created %d Instructions", _instruction_count);
3247 #endif
3248 }
3249 
3250 
3251 ValueStack* GraphBuilder::copy_state_before() {
3252   return copy_state_before_with_bci(bci());
3253 }
3254 
3255 ValueStack* GraphBuilder::copy_state_exhandling() {
3256   return copy_state_exhandling_with_bci(bci());
3257 }
3258 
3259 ValueStack* GraphBuilder::copy_state_for_exception() {
3260   return copy_state_for_exception_with_bci(bci());
3261 }
3262 
3263 ValueStack* GraphBuilder::copy_state_before_with_bci(int bci) {
3264   return state()-&gt;copy(ValueStack::StateBefore, bci);
3265 }
3266 
3267 ValueStack* GraphBuilder::copy_state_exhandling_with_bci(int bci) {
3268   if (!has_handler()) return NULL;
3269   return state()-&gt;copy(ValueStack::StateBefore, bci);
3270 }
3271 
3272 ValueStack* GraphBuilder::copy_state_for_exception_with_bci(int bci) {
3273   ValueStack* s = copy_state_exhandling_with_bci(bci);
3274   if (s == NULL) {
3275     if (_compilation-&gt;env()-&gt;jvmti_can_access_local_variables()) {
3276       s = state()-&gt;copy(ValueStack::ExceptionState, bci);
3277     } else {
3278       s = state()-&gt;copy(ValueStack::EmptyExceptionState, bci);
3279     }
3280   }
3281   return s;
3282 }
3283 
3284 int GraphBuilder::recursive_inline_level(ciMethod* cur_callee) const {
3285   int recur_level = 0;
3286   for (IRScope* s = scope(); s != NULL; s = s-&gt;caller()) {
3287     if (s-&gt;method() == cur_callee) {
3288       ++recur_level;
3289     }
3290   }
3291   return recur_level;
3292 }
3293 
3294 
3295 bool GraphBuilder::try_inline(ciMethod* callee, bool holder_known, Bytecodes::Code bc, Value receiver) {
3296   const char* msg = NULL;
3297 
3298   // clear out any existing inline bailout condition
3299   clear_inline_bailout();
3300 
3301   // exclude methods we don't want to inline
3302   msg = should_not_inline(callee);
3303   if (msg != NULL) {
3304     print_inlining(callee, msg, /*success*/ false);
3305     return false;
3306   }
3307 
3308   // method handle invokes
3309   if (callee-&gt;is_method_handle_intrinsic()) {
3310     return try_method_handle_inline(callee);
3311   }
3312 
3313   // handle intrinsics
3314   if (callee-&gt;intrinsic_id() != vmIntrinsics::_none) {
3315     if (try_inline_intrinsics(callee)) {
3316       print_inlining(callee, "intrinsic");
3317       return true;
3318     }
3319     // try normal inlining
3320   }
3321 
3322   // certain methods cannot be parsed at all
3323   msg = check_can_parse(callee);
3324   if (msg != NULL) {
3325     print_inlining(callee, msg, /*success*/ false);
3326     return false;
3327   }
3328 
3329   // If bytecode not set use the current one.
3330   if (bc == Bytecodes::_illegal) {
3331     bc = code();
3332   }
3333   if (try_inline_full(callee, holder_known, bc, receiver))
3334     return true;
3335 
3336   // Entire compilation could fail during try_inline_full call.
3337   // In that case printing inlining decision info is useless.
3338   if (!bailed_out())
3339     print_inlining(callee, _inline_bailout_msg, /*success*/ false);
3340 
3341   return false;
3342 }
3343 
3344 
3345 const char* GraphBuilder::check_can_parse(ciMethod* callee) const {
3346   // Certain methods cannot be parsed at all:
3347   if ( callee-&gt;is_native())            return "native method";
3348   if ( callee-&gt;is_abstract())          return "abstract method";
3349   if (!callee-&gt;can_be_compiled())      return "not compilable (disabled)";
3350   return NULL;
3351 }
3352 
3353 
3354 // negative filter: should callee NOT be inlined?  returns NULL, ok to inline, or rejection msg
3355 const char* GraphBuilder::should_not_inline(ciMethod* callee) const {
3356   if ( callee-&gt;should_exclude())       return "excluded by CompilerOracle";
3357   if ( callee-&gt;should_not_inline())    return "disallowed by CompilerOracle";
3358   if ( callee-&gt;dont_inline())          return "don't inline by annotation";
3359   return NULL;
3360 }
3361 
3362 
3363 bool GraphBuilder::try_inline_intrinsics(ciMethod* callee) {
3364   if (callee-&gt;is_synchronized()) {
3365     // We don't currently support any synchronized intrinsics
3366     return false;
3367   }
3368 
3369   // callee seems like a good candidate
3370   // determine id
3371   vmIntrinsics::ID id = callee-&gt;intrinsic_id();
3372   if (!InlineNatives &amp;&amp; id != vmIntrinsics::_Reference_get) {
3373     // InlineNatives does not control Reference.get
3374     INLINE_BAILOUT("intrinsic method inlining disabled");
3375   }
3376   bool preserves_state = false;
3377   bool cantrap = true;
3378   switch (id) {
3379     case vmIntrinsics::_arraycopy:
3380       if (!InlineArrayCopy) return false;
3381       break;
3382 
3383 #ifdef TRACE_HAVE_INTRINSICS
3384     case vmIntrinsics::_classID:
3385     case vmIntrinsics::_threadID:
3386       preserves_state = true;
3387       cantrap = true;
3388       break;
3389 
3390     case vmIntrinsics::_counterTime:
3391       preserves_state = true;
3392       cantrap = false;
3393       break;
3394 #endif
3395 
3396     case vmIntrinsics::_currentTimeMillis:
3397     case vmIntrinsics::_nanoTime:
3398       preserves_state = true;
3399       cantrap = false;
3400       break;
3401 
3402     case vmIntrinsics::_floatToRawIntBits   :
3403     case vmIntrinsics::_intBitsToFloat      :
3404     case vmIntrinsics::_doubleToRawLongBits :
3405     case vmIntrinsics::_longBitsToDouble    :
3406       if (!InlineMathNatives) return false;
3407       preserves_state = true;
3408       cantrap = false;
3409       break;
3410 
3411     case vmIntrinsics::_getClass      :
3412     case vmIntrinsics::_isInstance    :
3413       if (!InlineClassNatives) return false;
3414       preserves_state = true;
3415       break;
3416 
3417     case vmIntrinsics::_currentThread :
3418       if (!InlineThreadNatives) return false;
3419       preserves_state = true;
3420       cantrap = false;
3421       break;
3422 
3423     case vmIntrinsics::_dabs          : // fall through
3424     case vmIntrinsics::_dsqrt         : // fall through
3425     case vmIntrinsics::_dsin          : // fall through
3426     case vmIntrinsics::_dcos          : // fall through
3427     case vmIntrinsics::_dtan          : // fall through
3428     case vmIntrinsics::_dlog          : // fall through
3429     case vmIntrinsics::_dlog10        : // fall through
3430     case vmIntrinsics::_dexp          : // fall through
3431     case vmIntrinsics::_dpow          : // fall through
3432       if (!InlineMathNatives) return false;
3433       cantrap = false;
3434       preserves_state = true;
3435       break;
3436 
3437     // Use special nodes for Unsafe instructions so we can more easily
3438     // perform an address-mode optimization on the raw variants
3439     case vmIntrinsics::_getObject : return append_unsafe_get_obj(callee, T_OBJECT,  false);
3440     case vmIntrinsics::_getBoolean: return append_unsafe_get_obj(callee, T_BOOLEAN, false);
3441     case vmIntrinsics::_getByte   : return append_unsafe_get_obj(callee, T_BYTE,    false);
3442     case vmIntrinsics::_getShort  : return append_unsafe_get_obj(callee, T_SHORT,   false);
3443     case vmIntrinsics::_getChar   : return append_unsafe_get_obj(callee, T_CHAR,    false);
3444     case vmIntrinsics::_getInt    : return append_unsafe_get_obj(callee, T_INT,     false);
3445     case vmIntrinsics::_getLong   : return append_unsafe_get_obj(callee, T_LONG,    false);
3446     case vmIntrinsics::_getFloat  : return append_unsafe_get_obj(callee, T_FLOAT,   false);
3447     case vmIntrinsics::_getDouble : return append_unsafe_get_obj(callee, T_DOUBLE,  false);
3448 
3449     case vmIntrinsics::_putObject : return append_unsafe_put_obj(callee, T_OBJECT,  false);
3450     case vmIntrinsics::_putBoolean: return append_unsafe_put_obj(callee, T_BOOLEAN, false);
3451     case vmIntrinsics::_putByte   : return append_unsafe_put_obj(callee, T_BYTE,    false);
3452     case vmIntrinsics::_putShort  : return append_unsafe_put_obj(callee, T_SHORT,   false);
3453     case vmIntrinsics::_putChar   : return append_unsafe_put_obj(callee, T_CHAR,    false);
3454     case vmIntrinsics::_putInt    : return append_unsafe_put_obj(callee, T_INT,     false);
3455     case vmIntrinsics::_putLong   : return append_unsafe_put_obj(callee, T_LONG,    false);
3456     case vmIntrinsics::_putFloat  : return append_unsafe_put_obj(callee, T_FLOAT,   false);
3457     case vmIntrinsics::_putDouble : return append_unsafe_put_obj(callee, T_DOUBLE,  false);
3458 
3459     case vmIntrinsics::_getObjectVolatile : return append_unsafe_get_obj(callee, T_OBJECT,  true);
3460     case vmIntrinsics::_getBooleanVolatile: return append_unsafe_get_obj(callee, T_BOOLEAN, true);
3461     case vmIntrinsics::_getByteVolatile   : return append_unsafe_get_obj(callee, T_BYTE,    true);
3462     case vmIntrinsics::_getShortVolatile  : return append_unsafe_get_obj(callee, T_SHORT,   true);
3463     case vmIntrinsics::_getCharVolatile   : return append_unsafe_get_obj(callee, T_CHAR,    true);
3464     case vmIntrinsics::_getIntVolatile    : return append_unsafe_get_obj(callee, T_INT,     true);
3465     case vmIntrinsics::_getLongVolatile   : return append_unsafe_get_obj(callee, T_LONG,    true);
3466     case vmIntrinsics::_getFloatVolatile  : return append_unsafe_get_obj(callee, T_FLOAT,   true);
3467     case vmIntrinsics::_getDoubleVolatile : return append_unsafe_get_obj(callee, T_DOUBLE,  true);
3468 
3469     case vmIntrinsics::_putObjectVolatile : return append_unsafe_put_obj(callee, T_OBJECT,  true);
3470     case vmIntrinsics::_putBooleanVolatile: return append_unsafe_put_obj(callee, T_BOOLEAN, true);
3471     case vmIntrinsics::_putByteVolatile   : return append_unsafe_put_obj(callee, T_BYTE,    true);
3472     case vmIntrinsics::_putShortVolatile  : return append_unsafe_put_obj(callee, T_SHORT,   true);
3473     case vmIntrinsics::_putCharVolatile   : return append_unsafe_put_obj(callee, T_CHAR,    true);
3474     case vmIntrinsics::_putIntVolatile    : return append_unsafe_put_obj(callee, T_INT,     true);
3475     case vmIntrinsics::_putLongVolatile   : return append_unsafe_put_obj(callee, T_LONG,    true);
3476     case vmIntrinsics::_putFloatVolatile  : return append_unsafe_put_obj(callee, T_FLOAT,   true);
3477     case vmIntrinsics::_putDoubleVolatile : return append_unsafe_put_obj(callee, T_DOUBLE,  true);
3478 
3479     case vmIntrinsics::_getByte_raw   : return append_unsafe_get_raw(callee, T_BYTE);
3480     case vmIntrinsics::_getShort_raw  : return append_unsafe_get_raw(callee, T_SHORT);
3481     case vmIntrinsics::_getChar_raw   : return append_unsafe_get_raw(callee, T_CHAR);
3482     case vmIntrinsics::_getInt_raw    : return append_unsafe_get_raw(callee, T_INT);
3483     case vmIntrinsics::_getLong_raw   : return append_unsafe_get_raw(callee, T_LONG);
3484     case vmIntrinsics::_getFloat_raw  : return append_unsafe_get_raw(callee, T_FLOAT);
3485     case vmIntrinsics::_getDouble_raw : return append_unsafe_get_raw(callee, T_DOUBLE);
3486 
3487     case vmIntrinsics::_putByte_raw   : return append_unsafe_put_raw(callee, T_BYTE);
3488     case vmIntrinsics::_putShort_raw  : return append_unsafe_put_raw(callee, T_SHORT);
3489     case vmIntrinsics::_putChar_raw   : return append_unsafe_put_raw(callee, T_CHAR);
3490     case vmIntrinsics::_putInt_raw    : return append_unsafe_put_raw(callee, T_INT);
3491     case vmIntrinsics::_putLong_raw   : return append_unsafe_put_raw(callee, T_LONG);
3492     case vmIntrinsics::_putFloat_raw  : return append_unsafe_put_raw(callee, T_FLOAT);
3493     case vmIntrinsics::_putDouble_raw : return append_unsafe_put_raw(callee, T_DOUBLE);
3494 
3495     case vmIntrinsics::_prefetchRead        : return append_unsafe_prefetch(callee, false, false);
3496     case vmIntrinsics::_prefetchWrite       : return append_unsafe_prefetch(callee, false, true);
3497     case vmIntrinsics::_prefetchReadStatic  : return append_unsafe_prefetch(callee, true,  false);
3498     case vmIntrinsics::_prefetchWriteStatic : return append_unsafe_prefetch(callee, true,  true);
3499 
3500     case vmIntrinsics::_checkIndex    :
3501       if (!InlineNIOCheckIndex) return false;
3502       preserves_state = true;
3503       break;
3504     case vmIntrinsics::_putOrderedObject : return append_unsafe_put_obj(callee, T_OBJECT,  true);
3505     case vmIntrinsics::_putOrderedInt    : return append_unsafe_put_obj(callee, T_INT,     true);
3506     case vmIntrinsics::_putOrderedLong   : return append_unsafe_put_obj(callee, T_LONG,    true);
3507 
3508     case vmIntrinsics::_compareAndSwapLong:
3509       if (!VM_Version::supports_cx8()) return false;
3510       // fall through
3511     case vmIntrinsics::_compareAndSwapInt:
3512     case vmIntrinsics::_compareAndSwapObject:
3513       append_unsafe_CAS(callee);
3514       return true;
3515 
3516     case vmIntrinsics::_getAndAddInt:
3517       if (!VM_Version::supports_atomic_getadd4()) {
3518         return false;
3519       }
3520       return append_unsafe_get_and_set_obj(callee, true);
3521     case vmIntrinsics::_getAndAddLong:
3522       if (!VM_Version::supports_atomic_getadd8()) {
3523         return false;
3524       }
3525       return append_unsafe_get_and_set_obj(callee, true);
3526     case vmIntrinsics::_getAndSetInt:
3527       if (!VM_Version::supports_atomic_getset4()) {
3528         return false;
3529       }
3530       return append_unsafe_get_and_set_obj(callee, false);
3531     case vmIntrinsics::_getAndSetLong:
3532       if (!VM_Version::supports_atomic_getset8()) {
3533         return false;
3534       }
3535       return append_unsafe_get_and_set_obj(callee, false);
3536     case vmIntrinsics::_getAndSetObject:
3537 #ifdef _LP64
3538       if (!UseCompressedOops &amp;&amp; !VM_Version::supports_atomic_getset8()) {
3539         return false;
3540       }
3541       if (UseCompressedOops &amp;&amp; !VM_Version::supports_atomic_getset4()) {
3542         return false;
3543       }
3544 #else
3545       if (!VM_Version::supports_atomic_getset4()) {
3546         return false;
3547       }
3548 #endif
3549       return append_unsafe_get_and_set_obj(callee, false);
3550 
3551     case vmIntrinsics::_Reference_get:
3552       // Use the intrinsic version of Reference.get() so that the value in
3553       // the referent field can be registered by the G1 pre-barrier code.
3554       // Also to prevent commoning reads from this field across safepoint
3555       // since GC can change its value.
3556       preserves_state = true;
3557       break;
3558 
3559     case vmIntrinsics::_updateCRC32:
3560     case vmIntrinsics::_updateBytesCRC32:
3561     case vmIntrinsics::_updateByteBufferCRC32:
3562       if (!UseCRC32Intrinsics) return false;
3563       cantrap = false;
3564       preserves_state = true;
3565       break;
3566 
3567     case vmIntrinsics::_loadFence :
3568     case vmIntrinsics::_storeFence:
3569     case vmIntrinsics::_fullFence :
3570       break;
3571 
3572     default                       : return false; // do not inline
3573   }
3574   // create intrinsic node
3575   const bool has_receiver = !callee-&gt;is_static();
3576   ValueType* result_type = as_ValueType(callee-&gt;return_type());
3577   ValueStack* state_before = copy_state_for_exception();
3578 
3579   Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
3580 
3581   if (is_profiling()) {
3582     // Don't profile in the special case where the root method
3583     // is the intrinsic
3584     if (callee != method()) {
3585       // Note that we'd collect profile data in this method if we wanted it.
3586       compilation()-&gt;set_would_profile(true);
3587       if (profile_calls()) {
3588         Value recv = NULL;
3589         if (has_receiver) {
3590           recv = args-&gt;at(0);
3591           null_check(recv);
3592         }
3593         profile_call(callee, recv, NULL, collect_args_for_profiling(args, callee, true), true);
3594       }
3595     }
3596   }
3597 
3598   Intrinsic* result = new Intrinsic(result_type, id, args, has_receiver, state_before,
3599                                     preserves_state, cantrap);
3600   // append instruction &amp; push result
3601   Value value = append_split(result);
3602   if (result_type != voidType) push(result_type, value);
3603 
3604   if (callee != method() &amp;&amp; profile_return() &amp;&amp; result_type-&gt;is_object_kind()) {
3605     profile_return_type(result, callee);
3606   }
3607 
3608   // done
3609   return true;
3610 }
3611 
3612 
3613 bool GraphBuilder::try_inline_jsr(int jsr_dest_bci) {
3614   // Introduce a new callee continuation point - all Ret instructions
3615   // will be replaced with Gotos to this point.
3616   BlockBegin* cont = block_at(next_bci());
3617   assert(cont != NULL, "continuation must exist (BlockListBuilder starts a new block after a jsr");
3618 
3619   // Note: can not assign state to continuation yet, as we have to
3620   // pick up the state from the Ret instructions.
3621 
3622   // Push callee scope
3623   push_scope_for_jsr(cont, jsr_dest_bci);
3624 
3625   // Temporarily set up bytecode stream so we can append instructions
3626   // (only using the bci of this stream)
3627   scope_data()-&gt;set_stream(scope_data()-&gt;parent()-&gt;stream());
3628 
3629   BlockBegin* jsr_start_block = block_at(jsr_dest_bci);
3630   assert(jsr_start_block != NULL, "jsr start block must exist");
3631   assert(!jsr_start_block-&gt;is_set(BlockBegin::was_visited_flag), "should not have visited jsr yet");
3632   Goto* goto_sub = new Goto(jsr_start_block, false);
3633   // Must copy state to avoid wrong sharing when parsing bytecodes
3634   assert(jsr_start_block-&gt;state() == NULL, "should have fresh jsr starting block");
3635   jsr_start_block-&gt;set_state(copy_state_before_with_bci(jsr_dest_bci));
3636   append(goto_sub);
3637   _block-&gt;set_end(goto_sub);
3638   _last = _block = jsr_start_block;
3639 
3640   // Clear out bytecode stream
3641   scope_data()-&gt;set_stream(NULL);
3642 
3643   scope_data()-&gt;add_to_work_list(jsr_start_block);
3644 
3645   // Ready to resume parsing in subroutine
3646   iterate_all_blocks();
3647 
3648   // If we bailed out during parsing, return immediately (this is bad news)
3649   CHECK_BAILOUT_(false);
3650 
3651   // Detect whether the continuation can actually be reached. If not,
3652   // it has not had state set by the join() operations in
3653   // iterate_bytecodes_for_block()/ret() and we should not touch the
3654   // iteration state. The calling activation of
3655   // iterate_bytecodes_for_block will then complete normally.
3656   if (cont-&gt;state() != NULL) {
3657     if (!cont-&gt;is_set(BlockBegin::was_visited_flag)) {
3658       // add continuation to work list instead of parsing it immediately
3659       scope_data()-&gt;parent()-&gt;add_to_work_list(cont);
3660     }
3661   }
3662 
3663   assert(jsr_continuation() == cont, "continuation must not have changed");
3664   assert(!jsr_continuation()-&gt;is_set(BlockBegin::was_visited_flag) ||
3665          jsr_continuation()-&gt;is_set(BlockBegin::parser_loop_header_flag),
3666          "continuation can only be visited in case of backward branches");
3667   assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "block must have end");
3668 
3669   // continuation is in work list, so end iteration of current block
3670   _skip_block = true;
3671   pop_scope_for_jsr();
3672 
3673   return true;
3674 }
3675 
3676 
3677 // Inline the entry of a synchronized method as a monitor enter and
3678 // register the exception handler which releases the monitor if an
3679 // exception is thrown within the callee. Note that the monitor enter
3680 // cannot throw an exception itself, because the receiver is
3681 // guaranteed to be non-null by the explicit null check at the
3682 // beginning of inlining.
3683 void GraphBuilder::inline_sync_entry(Value lock, BlockBegin* sync_handler) {
3684   assert(lock != NULL &amp;&amp; sync_handler != NULL, "lock or handler missing");
3685 
3686   monitorenter(lock, SynchronizationEntryBCI);
3687   assert(_last-&gt;as_MonitorEnter() != NULL, "monitor enter expected");
3688   _last-&gt;set_needs_null_check(false);
3689 
3690   sync_handler-&gt;set(BlockBegin::exception_entry_flag);
3691   sync_handler-&gt;set(BlockBegin::is_on_work_list_flag);
3692 
3693   ciExceptionHandler* desc = new ciExceptionHandler(method()-&gt;holder(), 0, method()-&gt;code_size(), -1, 0);
3694   XHandler* h = new XHandler(desc);
3695   h-&gt;set_entry_block(sync_handler);
3696   scope_data()-&gt;xhandlers()-&gt;append(h);
3697   scope_data()-&gt;set_has_handler();
3698 }
3699 
3700 
3701 // If an exception is thrown and not handled within an inlined
3702 // synchronized method, the monitor must be released before the
3703 // exception is rethrown in the outer scope. Generate the appropriate
3704 // instructions here.
3705 void GraphBuilder::fill_sync_handler(Value lock, BlockBegin* sync_handler, bool default_handler) {
3706   BlockBegin* orig_block = _block;
3707   ValueStack* orig_state = _state;
3708   Instruction* orig_last = _last;
3709   _last = _block = sync_handler;
3710   _state = sync_handler-&gt;state()-&gt;copy();
3711 
3712   assert(sync_handler != NULL, "handler missing");
3713   assert(!sync_handler-&gt;is_set(BlockBegin::was_visited_flag), "is visited here");
3714 
3715   assert(lock != NULL || default_handler, "lock or handler missing");
3716 
3717   XHandler* h = scope_data()-&gt;xhandlers()-&gt;remove_last();
3718   assert(h-&gt;entry_block() == sync_handler, "corrupt list of handlers");
3719 
3720   block()-&gt;set(BlockBegin::was_visited_flag);
3721   Value exception = append_with_bci(new ExceptionObject(), SynchronizationEntryBCI);
3722   assert(exception-&gt;is_pinned(), "must be");
3723 
3724   int bci = SynchronizationEntryBCI;
3725   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
3726     // Report exit from inline methods.  We don't have a stream here
3727     // so pass an explicit bci of SynchronizationEntryBCI.
3728     Values* args = new Values(1);
3729     args-&gt;push(append_with_bci(new Constant(new MethodConstant(method())), bci));
3730     append_with_bci(new RuntimeCall(voidType, "dtrace_method_exit", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), args), bci);
3731   }
3732 
3733   if (lock) {
3734     assert(state()-&gt;locks_size() &gt; 0 &amp;&amp; state()-&gt;lock_at(state()-&gt;locks_size() - 1) == lock, "lock is missing");
3735     if (!lock-&gt;is_linked()) {
3736       lock = append_with_bci(lock, bci);
3737     }
3738 
3739     // exit the monitor in the context of the synchronized method
3740     monitorexit(lock, bci);
3741 
3742     // exit the context of the synchronized method
3743     if (!default_handler) {
3744       pop_scope();
3745       bci = _state-&gt;caller_state()-&gt;bci();
3746       _state = _state-&gt;caller_state()-&gt;copy_for_parsing();
3747     }
3748   }
3749 
3750   // perform the throw as if at the the call site
3751   apush(exception);
3752   throw_op(bci);
3753 
3754   BlockEnd* end = last()-&gt;as_BlockEnd();
3755   block()-&gt;set_end(end);
3756 
3757   _block = orig_block;
3758   _state = orig_state;
3759   _last = orig_last;
3760 }
3761 
3762 
3763 bool GraphBuilder::try_inline_full(ciMethod* callee, bool holder_known, Bytecodes::Code bc, Value receiver) {
3764   assert(!callee-&gt;is_native(), "callee must not be native");
3765   if (CompilationPolicy::policy()-&gt;should_not_inline(compilation()-&gt;env(), callee)) {
3766     INLINE_BAILOUT("inlining prohibited by policy");
3767   }
3768   // first perform tests of things it's not possible to inline
3769   if (callee-&gt;has_exception_handlers() &amp;&amp;
3770       !InlineMethodsWithExceptionHandlers) INLINE_BAILOUT("callee has exception handlers");
3771   if (callee-&gt;is_synchronized() &amp;&amp;
3772       !InlineSynchronizedMethods         ) INLINE_BAILOUT("callee is synchronized");
3773   if (!callee-&gt;holder()-&gt;is_initialized()) INLINE_BAILOUT("callee's klass not initialized yet");
3774   if (!callee-&gt;has_balanced_monitors())    INLINE_BAILOUT("callee's monitors do not match");
3775 
3776   // Proper inlining of methods with jsrs requires a little more work.
3777   if (callee-&gt;has_jsrs()                 ) INLINE_BAILOUT("jsrs not handled properly by inliner yet");
3778 
3779   // When SSE2 is used on intel, then no special handling is needed
3780   // for strictfp because the enum-constant is fixed at compile time,
3781   // the check for UseSSE2 is needed here
3782   if (strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt; 2 &amp;&amp; method()-&gt;is_strict() != callee-&gt;is_strict()) {
3783     INLINE_BAILOUT("caller and callee have different strict fp requirements");
3784   }
3785 
3786   if (is_profiling() &amp;&amp; !callee-&gt;ensure_method_data()) {
3787     INLINE_BAILOUT("mdo allocation failed");
3788   }
3789 
3790   // now perform tests that are based on flag settings
3791   if (callee-&gt;force_inline() || callee-&gt;should_inline()) {
3792     if (inline_level() &gt; MaxForceInlineLevel                    ) INLINE_BAILOUT("MaxForceInlineLevel");
3793     if (recursive_inline_level(callee) &gt; MaxRecursiveInlineLevel) INLINE_BAILOUT("recursive inlining too deep");
3794 
3795     const char* msg = "";
3796     if (callee-&gt;force_inline())  msg = "force inline by annotation";
3797     if (callee-&gt;should_inline()) msg = "force inline by CompileOracle";
3798     print_inlining(callee, msg);
3799   } else {
3800     // use heuristic controls on inlining
3801     if (inline_level() &gt; MaxInlineLevel                         ) INLINE_BAILOUT("inlining too deep");
3802     if (recursive_inline_level(callee) &gt; MaxRecursiveInlineLevel) INLINE_BAILOUT("recursive inlining too deep");
3803     if (callee-&gt;code_size_for_inlining() &gt; max_inline_size()    ) INLINE_BAILOUT("callee is too large");
3804 
3805     // don't inline throwable methods unless the inlining tree is rooted in a throwable class
3806     if (callee-&gt;name() == ciSymbol::object_initializer_name() &amp;&amp;
3807         callee-&gt;holder()-&gt;is_subclass_of(ciEnv::current()-&gt;Throwable_klass())) {
3808       // Throwable constructor call
3809       IRScope* top = scope();
3810       while (top-&gt;caller() != NULL) {
3811         top = top-&gt;caller();
3812       }
3813       if (!top-&gt;method()-&gt;holder()-&gt;is_subclass_of(ciEnv::current()-&gt;Throwable_klass())) {
3814         INLINE_BAILOUT("don't inline Throwable constructors");
3815       }
3816     }
3817 
3818     if (compilation()-&gt;env()-&gt;num_inlined_bytecodes() &gt; DesiredMethodLimit) {
3819       INLINE_BAILOUT("total inlining greater than DesiredMethodLimit");
3820     }
3821     // printing
3822     print_inlining(callee);
3823   }
3824 
3825   // NOTE: Bailouts from this point on, which occur at the
3826   // GraphBuilder level, do not cause bailout just of the inlining but
3827   // in fact of the entire compilation.
3828 
3829   BlockBegin* orig_block = block();
3830 
3831   const bool is_invokedynamic = bc == Bytecodes::_invokedynamic;
3832   const bool has_receiver = (bc != Bytecodes::_invokestatic &amp;&amp; !is_invokedynamic);
3833 
3834   const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
3835   assert(args_base &gt;= 0, "stack underflow during inlining");
3836 
3837   // Insert null check if necessary
3838   Value recv = NULL;
3839   if (has_receiver) {
3840     // note: null check must happen even if first instruction of callee does
3841     //       an implicit null check since the callee is in a different scope
3842     //       and we must make sure exception handling does the right thing
3843     assert(!callee-&gt;is_static(), "callee must not be static");
3844     assert(callee-&gt;arg_size() &gt; 0, "must have at least a receiver");
3845     recv = state()-&gt;stack_at(args_base);
3846     null_check(recv);
3847   }
3848 
3849   if (is_profiling()) {
3850     // Note that we'd collect profile data in this method if we wanted it.
3851     // this may be redundant here...
3852     compilation()-&gt;set_would_profile(true);
3853 
3854     if (profile_calls()) {
3855       int start = 0;
3856       Values* obj_args = args_list_for_profiling(callee, start, has_receiver);
3857       if (obj_args != NULL) {
3858         int s = obj_args-&gt;size();
3859         // if called through method handle invoke, some arguments may have been popped
3860         for (int i = args_base+start, j = 0; j &lt; obj_args-&gt;size() &amp;&amp; i &lt; state()-&gt;stack_size(); ) {
3861           Value v = state()-&gt;stack_at_inc(i);
3862           if (v-&gt;type()-&gt;is_object_kind()) {
3863             obj_args-&gt;push(v);
3864             j++;
3865           }
3866         }
3867         check_args_for_profiling(obj_args, s);
3868       }
3869       profile_call(callee, recv, holder_known ? callee-&gt;holder() : NULL, obj_args, true);
3870     }
3871   }
3872 
3873   // Introduce a new callee continuation point - if the callee has
3874   // more than one return instruction or the return does not allow
3875   // fall-through of control flow, all return instructions of the
3876   // callee will need to be replaced by Goto's pointing to this
3877   // continuation point.
3878   BlockBegin* cont = block_at(next_bci());
3879   bool continuation_existed = true;
3880   if (cont == NULL) {
3881     cont = new BlockBegin(next_bci());
3882     // low number so that continuation gets parsed as early as possible
3883     cont-&gt;set_depth_first_number(0);
3884 #ifndef PRODUCT
3885     if (PrintInitialBlockList) {
3886       tty-&gt;print_cr("CFG: created block %d (bci %d) as continuation for inline at bci %d",
3887                     cont-&gt;block_id(), cont-&gt;bci(), bci());
3888     }
3889 #endif
3890     continuation_existed = false;
3891   }
3892   // Record number of predecessors of continuation block before
3893   // inlining, to detect if inlined method has edges to its
3894   // continuation after inlining.
3895   int continuation_preds = cont-&gt;number_of_preds();
3896 
3897   // Push callee scope
3898   push_scope(callee, cont);
3899 
3900   // the BlockListBuilder for the callee could have bailed out
3901   if (bailed_out())
3902       return false;
3903 
3904   // Temporarily set up bytecode stream so we can append instructions
3905   // (only using the bci of this stream)
3906   scope_data()-&gt;set_stream(scope_data()-&gt;parent()-&gt;stream());
3907 
3908   // Pass parameters into callee state: add assignments
3909   // note: this will also ensure that all arguments are computed before being passed
3910   ValueStack* callee_state = state();
3911   ValueStack* caller_state = state()-&gt;caller_state();
3912   for (int i = args_base; i &lt; caller_state-&gt;stack_size(); ) {
3913     const int arg_no = i - args_base;
3914     Value arg = caller_state-&gt;stack_at_inc(i);
3915     store_local(callee_state, arg, arg_no);
3916   }
3917 
3918   // Remove args from stack.
3919   // Note that we preserve locals state in case we can use it later
3920   // (see use of pop_scope() below)
3921   caller_state-&gt;truncate_stack(args_base);
3922   assert(callee_state-&gt;stack_size() == 0, "callee stack must be empty");
3923 
3924   Value lock;
3925   BlockBegin* sync_handler;
3926 
3927   // Inline the locking of the receiver if the callee is synchronized
3928   if (callee-&gt;is_synchronized()) {
3929     lock = callee-&gt;is_static() ? append(new Constant(new InstanceConstant(callee-&gt;holder()-&gt;java_mirror())))
3930                                : state()-&gt;local_at(0);
3931     sync_handler = new BlockBegin(SynchronizationEntryBCI);
3932     inline_sync_entry(lock, sync_handler);
3933   }
3934 
3935   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
3936     Values* args = new Values(1);
3937     args-&gt;push(append(new Constant(new MethodConstant(method()))));
3938     append(new RuntimeCall(voidType, "dtrace_method_entry", CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), args));
3939   }
3940 
3941   if (profile_inlined_calls()) {
3942     profile_invocation(callee, copy_state_before_with_bci(SynchronizationEntryBCI));
3943   }
3944 
3945   BlockBegin* callee_start_block = block_at(0);
3946   if (callee_start_block != NULL) {
3947     assert(callee_start_block-&gt;is_set(BlockBegin::parser_loop_header_flag), "must be loop header");
3948     Goto* goto_callee = new Goto(callee_start_block, false);
3949     // The state for this goto is in the scope of the callee, so use
3950     // the entry bci for the callee instead of the call site bci.
3951     append_with_bci(goto_callee, 0);
3952     _block-&gt;set_end(goto_callee);
3953     callee_start_block-&gt;merge(callee_state);
3954 
3955     _last = _block = callee_start_block;
3956 
3957     scope_data()-&gt;add_to_work_list(callee_start_block);
3958   }
3959 
3960   // Clear out bytecode stream
3961   scope_data()-&gt;set_stream(NULL);
3962 
3963   CompileLog* log = compilation()-&gt;log();
3964   if (log != NULL) log-&gt;head("parse method='%d'", log-&gt;identify(callee));
3965 
3966   // Ready to resume parsing in callee (either in the same block we
3967   // were in before or in the callee's start block)
3968   iterate_all_blocks(callee_start_block == NULL);
3969 
3970   if (log != NULL) log-&gt;done("parse");
3971 
3972   // If we bailed out during parsing, return immediately (this is bad news)
3973   if (bailed_out())
3974       return false;
3975 
3976   // iterate_all_blocks theoretically traverses in random order; in
3977   // practice, we have only traversed the continuation if we are
3978   // inlining into a subroutine
3979   assert(continuation_existed ||
3980          !continuation()-&gt;is_set(BlockBegin::was_visited_flag),
3981          "continuation should not have been parsed yet if we created it");
3982 
3983   // At this point we are almost ready to return and resume parsing of
3984   // the caller back in the GraphBuilder. The only thing we want to do
3985   // first is an optimization: during parsing of the callee we
3986   // generated at least one Goto to the continuation block. If we
3987   // generated exactly one, and if the inlined method spanned exactly
3988   // one block (and we didn't have to Goto its entry), then we snip
3989   // off the Goto to the continuation, allowing control to fall
3990   // through back into the caller block and effectively performing
3991   // block merging. This allows load elimination and CSE to take place
3992   // across multiple callee scopes if they are relatively simple, and
3993   // is currently essential to making inlining profitable.
3994   if (num_returns() == 1
3995       &amp;&amp; block() == orig_block
3996       &amp;&amp; block() == inline_cleanup_block()) {
3997     _last  = inline_cleanup_return_prev();
3998     _state = inline_cleanup_state();
3999   } else if (continuation_preds == cont-&gt;number_of_preds()) {
4000     // Inlining caused that the instructions after the invoke in the
4001     // caller are not reachable any more. So skip filling this block
4002     // with instructions!
4003     assert(cont == continuation(), "");
4004     assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
4005     _skip_block = true;
4006   } else {
4007     // Resume parsing in continuation block unless it was already parsed.
4008     // Note that if we don't change _last here, iteration in
4009     // iterate_bytecodes_for_block will stop when we return.
4010     if (!continuation()-&gt;is_set(BlockBegin::was_visited_flag)) {
4011       // add continuation to work list instead of parsing it immediately
4012       assert(_last &amp;&amp; _last-&gt;as_BlockEnd(), "");
4013       scope_data()-&gt;parent()-&gt;add_to_work_list(continuation());
4014       _skip_block = true;
4015     }
4016   }
4017 
4018   // Fill the exception handler for synchronized methods with instructions
4019   if (callee-&gt;is_synchronized() &amp;&amp; sync_handler-&gt;state() != NULL) {
4020     fill_sync_handler(lock, sync_handler);
4021   } else {
4022     pop_scope();
4023   }
4024 
4025   compilation()-&gt;notice_inlined_method(callee);
4026 
4027   return true;
4028 }
4029 
4030 
4031 bool GraphBuilder::try_method_handle_inline(ciMethod* callee) {
4032   ValueStack* state_before = state()-&gt;copy_for_parsing();
4033   vmIntrinsics::ID iid = callee-&gt;intrinsic_id();
4034   switch (iid) {
4035   case vmIntrinsics::_invokeBasic:
4036     {
4037       // get MethodHandle receiver
4038       const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
4039       ValueType* type = state()-&gt;stack_at(args_base)-&gt;type();
4040       if (type-&gt;is_constant()) {
4041         ciMethod* target = type-&gt;as_ObjectType()-&gt;constant_value()-&gt;as_method_handle()-&gt;get_vmtarget();
4042         // We don't do CHA here so only inline static and statically bindable methods.
4043         if (target-&gt;is_static() || target-&gt;can_be_statically_bound()) {
4044           Bytecodes::Code bc = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;
4045           if (try_inline(target, /*holder_known*/ true, bc)) {
4046             return true;
4047           }
4048         } else {
4049           print_inlining(target, "not static or statically bindable", /*success*/ false);
4050         }
4051       } else {
4052         print_inlining(callee, "receiver not constant", /*success*/ false);
4053       }
4054     }
4055     break;
4056 
4057   case vmIntrinsics::_linkToVirtual:
4058   case vmIntrinsics::_linkToStatic:
4059   case vmIntrinsics::_linkToSpecial:
4060   case vmIntrinsics::_linkToInterface:
4061     {
4062       // pop MemberName argument
4063       const int args_base = state()-&gt;stack_size() - callee-&gt;arg_size();
4064       ValueType* type = apop()-&gt;type();
4065       if (type-&gt;is_constant()) {
4066         ciMethod* target = type-&gt;as_ObjectType()-&gt;constant_value()-&gt;as_member_name()-&gt;get_vmtarget();
4067         // If the target is another method handle invoke try recursivly to get
4068         // a better target.
4069         if (target-&gt;is_method_handle_intrinsic()) {
4070           if (try_method_handle_inline(target)) {
4071             return true;
4072           }
4073         } else {
4074           ciSignature* signature = target-&gt;signature();
4075           const int receiver_skip = target-&gt;is_static() ? 0 : 1;
4076           // Cast receiver to its type.
4077           if (!target-&gt;is_static()) {
4078             ciKlass* tk = signature-&gt;accessing_klass();
4079             Value obj = state()-&gt;stack_at(args_base);
4080             if (obj-&gt;exact_type() == NULL &amp;&amp;
4081                 obj-&gt;declared_type() != tk &amp;&amp; tk != compilation()-&gt;env()-&gt;Object_klass()) {
4082               TypeCast* c = new TypeCast(tk, obj, state_before);
4083               append(c);
4084               state()-&gt;stack_at_put(args_base, c);
4085             }
4086           }
4087           // Cast reference arguments to its type.
4088           for (int i = 0, j = 0; i &lt; signature-&gt;count(); i++) {
4089             ciType* t = signature-&gt;type_at(i);
4090             if (t-&gt;is_klass()) {
4091               ciKlass* tk = t-&gt;as_klass();
4092               Value obj = state()-&gt;stack_at(args_base + receiver_skip + j);
4093               if (obj-&gt;exact_type() == NULL &amp;&amp;
4094                   obj-&gt;declared_type() != tk &amp;&amp; tk != compilation()-&gt;env()-&gt;Object_klass()) {
4095                 TypeCast* c = new TypeCast(t, obj, state_before);
4096                 append(c);
4097                 state()-&gt;stack_at_put(args_base + receiver_skip + j, c);
4098               }
4099             }
4100             j += t-&gt;size();  // long and double take two slots
4101           }
4102           // We don't do CHA here so only inline static and statically bindable methods.
4103           if (target-&gt;is_static() || target-&gt;can_be_statically_bound()) {
4104             Bytecodes::Code bc = target-&gt;is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;
4105             if (try_inline(target, /*holder_known*/ true, bc)) {
4106               return true;
4107             }
4108           } else {
4109             print_inlining(target, "not static or statically bindable", /*success*/ false);
4110           }
4111         }
4112       } else {
4113         print_inlining(callee, "MemberName not constant", /*success*/ false);
4114       }
4115     }
4116     break;
4117 
4118   default:
4119     fatal(err_msg("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid)));
4120     break;
4121   }
4122   set_state(state_before);
4123   return false;
4124 }
4125 
4126 
4127 void GraphBuilder::inline_bailout(const char* msg) {
4128   assert(msg != NULL, "inline bailout msg must exist");
4129   _inline_bailout_msg = msg;
4130 }
4131 
4132 
4133 void GraphBuilder::clear_inline_bailout() {
4134   _inline_bailout_msg = NULL;
4135 }
4136 
4137 
4138 void GraphBuilder::push_root_scope(IRScope* scope, BlockList* bci2block, BlockBegin* start) {
4139   ScopeData* data = new ScopeData(NULL);
4140   data-&gt;set_scope(scope);
4141   data-&gt;set_bci2block(bci2block);
4142   _scope_data = data;
4143   _block = start;
4144 }
4145 
4146 
4147 void GraphBuilder::push_scope(ciMethod* callee, BlockBegin* continuation) {
4148   IRScope* callee_scope = new IRScope(compilation(), scope(), bci(), callee, -1, false);
4149   scope()-&gt;add_callee(callee_scope);
4150 
4151   BlockListBuilder blb(compilation(), callee_scope, -1);
4152   CHECK_BAILOUT();
4153 
4154   if (!blb.bci2block()-&gt;at(0)-&gt;is_set(BlockBegin::parser_loop_header_flag)) {
4155     // this scope can be inlined directly into the caller so remove
4156     // the block at bci 0.
4157     blb.bci2block()-&gt;at_put(0, NULL);
4158   }
4159 
4160   set_state(new ValueStack(callee_scope, state()-&gt;copy(ValueStack::CallerState, bci())));
4161 
4162   ScopeData* data = new ScopeData(scope_data());
4163   data-&gt;set_scope(callee_scope);
4164   data-&gt;set_bci2block(blb.bci2block());
4165   data-&gt;set_continuation(continuation);
4166   _scope_data = data;
4167 }
4168 
4169 
4170 void GraphBuilder::push_scope_for_jsr(BlockBegin* jsr_continuation, int jsr_dest_bci) {
4171   ScopeData* data = new ScopeData(scope_data());
4172   data-&gt;set_parsing_jsr();
4173   data-&gt;set_jsr_entry_bci(jsr_dest_bci);
4174   data-&gt;set_jsr_return_address_local(-1);
4175   // Must clone bci2block list as we will be mutating it in order to
4176   // properly clone all blocks in jsr region as well as exception
4177   // handlers containing rets
4178   BlockList* new_bci2block = new BlockList(bci2block()-&gt;length());
4179   new_bci2block-&gt;push_all(bci2block());
4180   data-&gt;set_bci2block(new_bci2block);
4181   data-&gt;set_scope(scope());
4182   data-&gt;setup_jsr_xhandlers();
4183   data-&gt;set_continuation(continuation());
4184   data-&gt;set_jsr_continuation(jsr_continuation);
4185   _scope_data = data;
4186 }
4187 
4188 
4189 void GraphBuilder::pop_scope() {
4190   int number_of_locks = scope()-&gt;number_of_locks();
4191   _scope_data = scope_data()-&gt;parent();
4192   // accumulate minimum number of monitor slots to be reserved
4193   scope()-&gt;set_min_number_of_locks(number_of_locks);
4194 }
4195 
4196 
4197 void GraphBuilder::pop_scope_for_jsr() {
4198   _scope_data = scope_data()-&gt;parent();
4199 }
4200 
4201 bool GraphBuilder::append_unsafe_get_obj(ciMethod* callee, BasicType t, bool is_volatile) {
4202   if (InlineUnsafeOps) {
4203     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4204     null_check(args-&gt;at(0));
4205     Instruction* offset = args-&gt;at(2);
4206 #ifndef _LP64
4207     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4208 #endif
4209     Instruction* op = append(new UnsafeGetObject(t, args-&gt;at(1), offset, is_volatile));
4210     push(op-&gt;type(), op);
4211     compilation()-&gt;set_has_unsafe_access(true);
4212   }
4213   return InlineUnsafeOps;
4214 }
4215 
4216 
4217 bool GraphBuilder::append_unsafe_put_obj(ciMethod* callee, BasicType t, bool is_volatile) {
4218   if (InlineUnsafeOps) {
4219     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4220     null_check(args-&gt;at(0));
4221     Instruction* offset = args-&gt;at(2);
4222 #ifndef _LP64
4223     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4224 #endif
4225     Instruction* op = append(new UnsafePutObject(t, args-&gt;at(1), offset, args-&gt;at(3), is_volatile));
4226     compilation()-&gt;set_has_unsafe_access(true);
4227     kill_all();
4228   }
4229   return InlineUnsafeOps;
4230 }
4231 
4232 
4233 bool GraphBuilder::append_unsafe_get_raw(ciMethod* callee, BasicType t) {
4234   if (InlineUnsafeOps) {
4235     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4236     null_check(args-&gt;at(0));
4237     Instruction* op = append(new UnsafeGetRaw(t, args-&gt;at(1), false));
4238     push(op-&gt;type(), op);
4239     compilation()-&gt;set_has_unsafe_access(true);
4240   }
4241   return InlineUnsafeOps;
4242 }
4243 
4244 
4245 bool GraphBuilder::append_unsafe_put_raw(ciMethod* callee, BasicType t) {
4246   if (InlineUnsafeOps) {
4247     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4248     null_check(args-&gt;at(0));
4249     Instruction* op = append(new UnsafePutRaw(t, args-&gt;at(1), args-&gt;at(2)));
4250     compilation()-&gt;set_has_unsafe_access(true);
4251   }
4252   return InlineUnsafeOps;
4253 }
4254 
4255 
4256 bool GraphBuilder::append_unsafe_prefetch(ciMethod* callee, bool is_static, bool is_store) {
4257   if (InlineUnsafeOps) {
4258     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4259     int obj_arg_index = 1; // Assume non-static case
4260     if (is_static) {
4261       obj_arg_index = 0;
4262     } else {
4263       null_check(args-&gt;at(0));
4264     }
4265     Instruction* offset = args-&gt;at(obj_arg_index + 1);
4266 #ifndef _LP64
4267     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4268 #endif
4269     Instruction* op = is_store ? append(new UnsafePrefetchWrite(args-&gt;at(obj_arg_index), offset))
4270                                : append(new UnsafePrefetchRead (args-&gt;at(obj_arg_index), offset));
4271     compilation()-&gt;set_has_unsafe_access(true);
4272   }
4273   return InlineUnsafeOps;
4274 }
4275 
4276 
4277 void GraphBuilder::append_unsafe_CAS(ciMethod* callee) {
4278   ValueStack* state_before = copy_state_for_exception();
4279   ValueType* result_type = as_ValueType(callee-&gt;return_type());
4280   assert(result_type-&gt;is_int(), "int result");
4281   Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4282 
4283   // Pop off some args to speically handle, then push back
4284   Value newval = args-&gt;pop();
4285   Value cmpval = args-&gt;pop();
4286   Value offset = args-&gt;pop();
4287   Value src = args-&gt;pop();
4288   Value unsafe_obj = args-&gt;pop();
4289 
4290   // Separately handle the unsafe arg. It is not needed for code
4291   // generation, but must be null checked
4292   null_check(unsafe_obj);
4293 
4294 #ifndef _LP64
4295   offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4296 #endif
4297 
4298   args-&gt;push(src);
4299   args-&gt;push(offset);
4300   args-&gt;push(cmpval);
4301   args-&gt;push(newval);
4302 
4303   // An unsafe CAS can alias with other field accesses, but we don't
4304   // know which ones so mark the state as no preserved.  This will
4305   // cause CSE to invalidate memory across it.
4306   bool preserves_state = false;
4307   Intrinsic* result = new Intrinsic(result_type, callee-&gt;intrinsic_id(), args, false, state_before, preserves_state);
4308   append_split(result);
4309   push(result_type, result);
4310   compilation()-&gt;set_has_unsafe_access(true);
4311 }
4312 
4313 
4314 void GraphBuilder::print_inlining(ciMethod* callee, const char* msg, bool success) {
4315   CompileLog* log = compilation()-&gt;log();
4316   if (log != NULL) {
4317     if (success) {
4318       if (msg != NULL)
4319         log-&gt;inline_success(msg);
4320       else
4321         log-&gt;inline_success("receiver is statically known");
4322     } else {
4323       if (msg != NULL)
4324         log-&gt;inline_fail(msg);
4325       else
4326         log-&gt;inline_fail("reason unknown");
4327     }
4328   }
4329 
4330   if (!PrintInlining &amp;&amp; !compilation()-&gt;method()-&gt;has_option("PrintInlining")) {
4331     return;
4332   }
4333   CompileTask::print_inlining(callee, scope()-&gt;level(), bci(), msg);
4334   if (success &amp;&amp; CIPrintMethodCodes) {
4335     callee-&gt;print_codes();
4336   }
4337 }
4338 
4339 bool GraphBuilder::append_unsafe_get_and_set_obj(ciMethod* callee, bool is_add) {
4340   if (InlineUnsafeOps) {
4341     Values* args = state()-&gt;pop_arguments(callee-&gt;arg_size());
4342     BasicType t = callee-&gt;return_type()-&gt;basic_type();
4343     null_check(args-&gt;at(0));
4344     Instruction* offset = args-&gt;at(2);
4345 #ifndef _LP64
4346     offset = append(new Convert(Bytecodes::_l2i, offset, as_ValueType(T_INT)));
4347 #endif
4348     Instruction* op = append(new UnsafeGetAndSetObject(t, args-&gt;at(1), offset, args-&gt;at(3), is_add));
4349     compilation()-&gt;set_has_unsafe_access(true);
4350     kill_all();
4351     push(op-&gt;type(), op);
4352   }
4353   return InlineUnsafeOps;
4354 }
4355 
4356 #ifndef PRODUCT
4357 void GraphBuilder::print_stats() {
4358   vmap()-&gt;print();
4359 }
4360 #endif // PRODUCT
4361 
4362 void GraphBuilder::profile_call(ciMethod* callee, Value recv, ciKlass* known_holder, Values* obj_args, bool inlined) {
4363   assert(known_holder == NULL || (known_holder-&gt;is_instance_klass() &amp;&amp;
4364                                   (!known_holder-&gt;is_interface() ||
4365                                    ((ciInstanceKlass*)known_holder)-&gt;has_default_methods())), "should be default method");
4366   if (known_holder != NULL) {
4367     if (known_holder-&gt;exact_klass() == NULL) {
4368       known_holder = compilation()-&gt;cha_exact_type(known_holder);
4369     }
4370   }
4371 
4372   append(new ProfileCall(method(), bci(), callee, recv, known_holder, obj_args, inlined));
4373 }
4374 
4375 void GraphBuilder::profile_return_type(Value ret, ciMethod* callee, ciMethod* m, int invoke_bci) {
4376   assert((m == NULL) == (invoke_bci &lt; 0), "invalid method and invalid bci together");
4377   if (m == NULL) {
4378     m = method();
4379   }
4380   if (invoke_bci &lt; 0) {
4381     invoke_bci = bci();
4382   }
4383   ciMethodData* md = m-&gt;method_data_or_null();
4384   ciProfileData* data = md-&gt;bci_to_data(invoke_bci);
4385   if (data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData()) {
4386     append(new ProfileReturnType(m , invoke_bci, callee, ret));
4387   }
4388 }
4389 
4390 void GraphBuilder::profile_invocation(ciMethod* callee, ValueStack* state) {
4391   append(new ProfileInvoke(callee, state));
4392 }
</pre></body></html>
