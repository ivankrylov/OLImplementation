--- old/src/cpu/ppc/vm/stubGenerator_ppc.cpp	2015-06-16 10:25:38.678571782 -0700
+++ new/src/cpu/ppc/vm/stubGenerator_ppc.cpp	2015-06-16 10:25:38.610573386 -0700
@@ -2087,6 +2087,10 @@
       guarantee(!UseAESIntrinsics, "not yet implemented.");
     }
 
+    if (UseObjectLayoutIntrinsics) {
+      guarantee(!UseObjectLayoutIntrinsics, "not yet implemented.");
+    }
+
     // Safefetch stubs.
     generate_safefetch("SafeFetch32", sizeof(int),     &StubRoutines::_safefetch32_entry,
                                                        &StubRoutines::_safefetch32_fault_pc,
--- old/src/cpu/sparc/vm/stubGenerator_sparc.cpp	2015-06-16 10:25:38.974564796 -0700
+++ new/src/cpu/sparc/vm/stubGenerator_sparc.cpp	2015-06-16 10:25:38.894566684 -0700
@@ -4874,6 +4874,10 @@
       StubRoutines::_sha512_implCompress   = generate_sha512_implCompress(false, "sha512_implCompress");
       StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true,  "sha512_implCompressMB");
     }
+
+    if (UseObjectLayoutIntrinsics) {
+      guarantee(!UseObjectLayoutIntrinsics, "not yet implemented.");
+    }
   }
 
 
--- old/src/cpu/x86/vm/x86_64.ad	2015-06-16 10:25:39.290557318 -0700
+++ new/src/cpu/x86/vm/x86_64.ad	2015-06-16 10:25:39.218559022 -0700
@@ -6962,6 +6962,28 @@
   ins_pipe( ialu_reg );
 %}
 
+instruct castDerived_rReg(rRegP dst, rRegL src, rFlagsReg cr)
+%{
+  match(Set dst (CastDerived (AddP dst src)));
+  effect(KILL cr);
+
+  format %{ "addq    $dst, $src\t# cast derived ptr" %}
+  opcode(0x03);
+  ins_encode(REX_reg_reg_wide(dst, src), OpcP, reg_reg(dst, src));
+  ins_pipe(ialu_reg_reg);
+%}
+
+instruct castDerived_rReg_imm(rRegP dst, immL32 src, rFlagsReg cr)
+%{
+  match(Set dst (CastDerived (AddP dst src)));
+  effect(KILL cr);
+
+  format %{ "addq    $dst, $src\t# cast derived ptr" %}
+  opcode(0x81, 0x00); /* /0 id */
+  ins_encode(OpcSErm_wide(dst, src), Con8or32(src));
+  ins_pipe( ialu_reg );
+%}
+
 // XXX addP mem ops ????
 
 instruct leaP_rReg_imm(rRegP dst, rRegP src0, immL32 src1)
--- old/src/share/vm/adlc/formssel.cpp	2015-06-16 10:25:39.646548891 -0700
+++ new/src/share/vm/adlc/formssel.cpp	2015-06-16 10:25:39.578550501 -0700
@@ -772,6 +772,7 @@
   if( _matrule && _matrule->_rChild &&
        (!strcmp(_matrule->_rChild->_opType,"CastPP")       ||  // new result type
         !strcmp(_matrule->_rChild->_opType,"CastX2P")      ||  // new result type
+        !strcmp(_matrule->_rChild->_opType,"CastDerived")  ||  // new result type
         !strcmp(_matrule->_rChild->_opType,"DecodeN")      ||
         !strcmp(_matrule->_rChild->_opType,"EncodeP")      ||
         !strcmp(_matrule->_rChild->_opType,"DecodeNKlass") ||
--- old/src/share/vm/c1/c1_GraphBuilder.cpp	2015-06-16 10:25:39.954541601 -0700
+++ new/src/share/vm/c1/c1_GraphBuilder.cpp	2015-06-16 10:25:39.882543305 -0700
@@ -3569,6 +3569,11 @@
     case vmIntrinsics::_fullFence :
       break;
 
+    // ObjectLayout intrinsics support
+    case vmIntrinsics::_deriveContainedObjectAtOffset:
+      if (!UseObjectLayoutIntrinsics) return false;
+      return false; // FIXME: not yet implemented
+
     default                       : return false; // do not inline
   }
   // create intrinsic node
--- old/src/share/vm/classfile/classFileParser.cpp	2015-06-16 10:25:40.258534387 -0700
+++ new/src/share/vm/classfile/classFileParser.cpp	2015-06-16 10:25:40.182536190 -0700
@@ -3669,7 +3669,7 @@
     compute_oop_map_count(_super_klass, nonstatic_oop_map_count,
                           first_nonstatic_oop_offset);
 
-#ifndef PRODUCT
+//#ifndef PRODUCT
   if (PrintFieldLayout) {
     print_field_layout(_class_name,
           _fields,
@@ -3680,7 +3680,7 @@
           static_fields_end);
   }
 
-#endif
+//#endif
   // Pass back information needed for InstanceKlass creation
   info->nonstatic_oop_offsets = nonstatic_oop_offsets;
   info->nonstatic_oop_counts = nonstatic_oop_counts;
--- old/src/share/vm/classfile/javaClasses.cpp	2015-06-16 10:25:40.586526599 -0700
+++ new/src/share/vm/classfile/javaClasses.cpp	2015-06-16 10:25:40.510528405 -0700
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
@@ -124,7 +124,7 @@
       tty->print_cr("  name: %s, sig: %s, flags: %08x", fs.name()->as_C_string(), fs.signature()->as_C_string(), fs.access_flags().as_int());
     }
 #endif //PRODUCT
-    vm_exit_during_initialization("Invalid layout of preloaded class: use -XX:+TraceClassLoading to see the origin of the problem class");
+    fatal("Invalid layout of preloaded class");
   }
   dest_offset = fd.offset();
 }
@@ -1278,8 +1278,7 @@
 }
 
 static inline bool version_matches(Method* method, int version) {
-  assert(version < MAX_VERSION, "version is too big");
-  return method != NULL && (method->constants()->version() == version);
+  return (method->constants()->version() == version && version < MAX_VERSION);
 }
 
 static inline int get_line_number(Method* method, int bci) {
@@ -1309,7 +1308,6 @@
   typeArrayOop    _methods;
   typeArrayOop    _bcis;
   objArrayOop     _mirrors;
-  typeArrayOop    _cprefs; // needed to insulate method name against redefinition
   int             _index;
   No_Safepoint_Verifier _nsv;
 
@@ -1317,9 +1315,8 @@
 
   enum {
     trace_methods_offset = java_lang_Throwable::trace_methods_offset,
-    trace_bcis_offset    = java_lang_Throwable::trace_bcis_offset,
+    trace_bcis_offset = java_lang_Throwable::trace_bcis_offset,
     trace_mirrors_offset = java_lang_Throwable::trace_mirrors_offset,
-    trace_cprefs_offset  = java_lang_Throwable::trace_cprefs_offset,
     trace_next_offset    = java_lang_Throwable::trace_next_offset,
     trace_size           = java_lang_Throwable::trace_size,
     trace_chunk_size     = java_lang_Throwable::trace_chunk_size
@@ -1341,14 +1338,9 @@
     assert(mirrors != NULL, "mirror array should be initialized in backtrace");
     return mirrors;
   }
-  static typeArrayOop get_cprefs(objArrayHandle chunk) {
-    typeArrayOop cprefs = typeArrayOop(chunk->obj_at(trace_cprefs_offset));
-    assert(cprefs != NULL, "cprefs array should be initialized in backtrace");
-    return cprefs;
-  }
 
   // constructor for new backtrace
-  BacktraceBuilder(TRAPS): _methods(NULL), _bcis(NULL), _head(NULL), _mirrors(NULL), _cprefs(NULL) {
+  BacktraceBuilder(TRAPS): _methods(NULL), _bcis(NULL), _head(NULL), _mirrors(NULL) {
     expand(CHECK);
     _backtrace = _head;
     _index = 0;
@@ -1358,7 +1350,6 @@
     _methods = get_methods(backtrace);
     _bcis = get_bcis(backtrace);
     _mirrors = get_mirrors(backtrace);
-    _cprefs = get_cprefs(backtrace);
     assert(_methods->length() == _bcis->length() &&
            _methods->length() == _mirrors->length(),
            "method and source information arrays should match");
@@ -1384,22 +1375,17 @@
     objArrayOop mirrors = oopFactory::new_objectArray(trace_chunk_size, CHECK);
     objArrayHandle new_mirrors(THREAD, mirrors);
 
-    typeArrayOop cprefs = oopFactory::new_shortArray(trace_chunk_size, CHECK);
-    typeArrayHandle new_cprefs(THREAD, cprefs);
-
     if (!old_head.is_null()) {
       old_head->obj_at_put(trace_next_offset, new_head());
     }
     new_head->obj_at_put(trace_methods_offset, new_methods());
     new_head->obj_at_put(trace_bcis_offset, new_bcis());
     new_head->obj_at_put(trace_mirrors_offset, new_mirrors());
-    new_head->obj_at_put(trace_cprefs_offset, new_cprefs());
 
     _head    = new_head();
     _methods = new_methods();
     _bcis = new_bcis();
     _mirrors = new_mirrors();
-    _cprefs  = new_cprefs();
     _index = 0;
   }
 
@@ -1419,9 +1405,8 @@
       method = mhandle();
     }
 
-    _methods->short_at_put(_index, method->orig_method_idnum());
+    _methods->short_at_put(_index, method->method_idnum());
     _bcis->int_at_put(_index, merge_bci_and_version(bci, method->constants()->version()));
-    _cprefs->short_at_put(_index, method->name_index());
 
     // We need to save the mirrors in the backtrace to keep the class
     // from being unloaded while we still have this stack trace.
@@ -1434,26 +1419,27 @@
 
 // Print stack trace element to resource allocated buffer
 char* java_lang_Throwable::print_stack_element_to_buffer(Handle mirror,
-                                  int method_id, int version, int bci, int cpref) {
+                                  int method_id, int version, int bci) {
 
   // Get strings and string lengths
   InstanceKlass* holder = InstanceKlass::cast(java_lang_Class::as_Klass(mirror()));
   const char* klass_name  = holder->external_name();
   int buf_len = (int)strlen(klass_name);
 
-  Method* method = holder->method_with_orig_idnum(method_id, version);
+  // The method id may point to an obsolete method, can't get more stack information
+  Method* method = holder->method_with_idnum(method_id);
+  if (method == NULL) {
+    char* buf = NEW_RESOURCE_ARRAY(char, buf_len + 64);
+    // This is what the java code prints in this case - added Redefined
+    sprintf(buf, "\tat %s.null (Redefined)", klass_name);
+    return buf;
+  }
 
-  // The method can be NULL if the requested class version is gone
-  Symbol* sym = (method != NULL) ? method->name() : holder->constants()->symbol_at(cpref);
-  char* method_name = sym->as_C_string();
+  char* method_name = method->name()->as_C_string();
   buf_len += (int)strlen(method_name);
 
-  // Use a specific ik version as a holder since the mirror might
-  // refer to a version that is now obsolete and no longer accessible
-  // via the previous versions list.
-  holder = holder->get_klass_version(version);
   char* source_file_name = NULL;
-  if (holder != NULL) {
+  if (version_matches(method, version)) {
     Symbol* source = holder->source_file_name();
     if (source != NULL) {
       source_file_name = source->as_C_string();
@@ -1495,18 +1481,17 @@
 }
 
 void java_lang_Throwable::print_stack_element(outputStream *st, Handle mirror,
-                                              int method_id, int version, int bci, int cpref) {
+                                              int method_id, int version, int bci) {
   ResourceMark rm;
-  char* buf = print_stack_element_to_buffer(mirror, method_id, version, bci, cpref);
+  char* buf = print_stack_element_to_buffer(mirror, method_id, version, bci);
   st->print_cr("%s", buf);
 }
 
 void java_lang_Throwable::print_stack_element(outputStream *st, methodHandle method, int bci) {
   Handle mirror = method->method_holder()->java_mirror();
-  int method_id = method->orig_method_idnum();
+  int method_id = method->method_idnum();
   int version = method->constants()->version();
-  int cpref = method->name_index();
-  print_stack_element(st, mirror, method_id, version, bci, cpref);
+  print_stack_element(st, mirror, method_id, version, bci);
 }
 
 const char* java_lang_Throwable::no_stack_trace_message() {
@@ -1531,7 +1516,6 @@
       typeArrayHandle methods (THREAD, BacktraceBuilder::get_methods(result));
       typeArrayHandle bcis (THREAD, BacktraceBuilder::get_bcis(result));
       objArrayHandle mirrors (THREAD, BacktraceBuilder::get_mirrors(result));
-      typeArrayHandle cprefs (THREAD, BacktraceBuilder::get_cprefs(result));
 
       int length = methods()->length();
       for (int index = 0; index < length; index++) {
@@ -1541,8 +1525,7 @@
         int method = methods->short_at(index);
         int version = version_at(bcis->int_at(index));
         int bci = bci_at(bcis->int_at(index));
-        int cpref = cprefs->short_at(index);
-        print_stack_element(st, mirror, method, version, bci, cpref);
+        print_stack_element(st, mirror, method, version, bci);
       }
       result = objArrayHandle(THREAD, objArrayOop(result->obj_at(trace_next_offset)));
     }
@@ -1826,30 +1809,29 @@
   if (chunk == NULL) {
     THROW_(vmSymbols::java_lang_IndexOutOfBoundsException(), NULL);
   }
-  // Get method id, bci, version, mirror and cpref from chunk
+  // Get method id, bci, version and mirror from chunk
   typeArrayOop methods = BacktraceBuilder::get_methods(chunk);
   typeArrayOop bcis = BacktraceBuilder::get_bcis(chunk);
   objArrayOop mirrors = BacktraceBuilder::get_mirrors(chunk);
-  typeArrayOop cprefs = BacktraceBuilder::get_cprefs(chunk);
 
   assert(methods != NULL && bcis != NULL && mirrors != NULL, "sanity check");
 
   int method = methods->short_at(chunk_index);
   int version = version_at(bcis->int_at(chunk_index));
   int bci = bci_at(bcis->int_at(chunk_index));
-  int cpref = cprefs->short_at(chunk_index);
   Handle mirror(THREAD, mirrors->obj_at(chunk_index));
 
   // Chunk can be partial full
   if (mirror.is_null()) {
     THROW_(vmSymbols::java_lang_IndexOutOfBoundsException(), NULL);
   }
-  oop element = java_lang_StackTraceElement::create(mirror, method, version, bci, cpref, CHECK_0);
+
+  oop element = java_lang_StackTraceElement::create(mirror, method, version, bci, CHECK_0);
   return element;
 }
 
 oop java_lang_StackTraceElement::create(Handle mirror, int method_id,
-                                        int version, int bci, int cpref, TRAPS) {
+                                        int version, int bci, TRAPS) {
   // Allocate java.lang.StackTraceElement instance
   Klass* k = SystemDictionary::StackTraceElement_klass();
   assert(k != NULL, "must be loaded in 1.4+");
@@ -1866,13 +1848,17 @@
   oop classname = StringTable::intern((char*) str, CHECK_0);
   java_lang_StackTraceElement::set_declaringClass(element(), classname);
 
-  Method* method = holder->method_with_orig_idnum(method_id, version);
-
-  // The method can be NULL if the requested class version is gone
-  Symbol* sym = (method != NULL) ? method->name() : holder->constants()->symbol_at(cpref);
+  Method* method = holder->method_with_idnum(method_id);
+  // Method on stack may be obsolete because it was redefined so cannot be
+  // found by idnum.
+  if (method == NULL) {
+    // leave name and fileName null
+    java_lang_StackTraceElement::set_lineNumber(element(), -1);
+    return element();
+  }
 
   // Fill in method name
-  oop methodname = StringTable::intern(sym, CHECK_0);
+  oop methodname = StringTable::intern(method->name(), CHECK_0);
   java_lang_StackTraceElement::set_methodName(element(), methodname);
 
   if (!version_matches(method, version)) {
@@ -1881,11 +1867,6 @@
     java_lang_StackTraceElement::set_lineNumber(element(), -1);
   } else {
     // Fill in source file name and line number.
-    // Use a specific ik version as a holder since the mirror might
-    // refer to a version that is now obsolete and no longer accessible
-    // via the previous versions list.
-    holder = holder->get_klass_version(version);
-    assert(holder != NULL, "sanity check");
     Symbol* source = holder->source_file_name();
     if (ShowHiddenFrames && source == NULL)
       source = vmSymbols::unknown_class_name();
@@ -1900,9 +1881,8 @@
 
 oop java_lang_StackTraceElement::create(methodHandle method, int bci, TRAPS) {
   Handle mirror (THREAD, method->method_holder()->java_mirror());
-  int method_id = method->orig_method_idnum();
-  int cpref = method->name_index();
-  return create(mirror, method_id, method->constants()->version(), bci, cpref, THREAD);
+  int method_id = method->method_idnum();
+  return create(mirror, method_id, method->constants()->version(), bci, THREAD);
 }
 
 void java_lang_reflect_AccessibleObject::compute_offsets() {
@@ -2801,6 +2781,33 @@
   return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;
 }
 
+#if INCLUDE_JVMTI
+// Can be executed on VM thread only
+void java_lang_invoke_MemberName::adjust_vmtarget(oop mname, Method* old_method,
+                                                  Method* new_method, bool* trace_name_printed) {
+  assert(is_method(mname), "wrong type");
+  assert(Thread::current()->is_VM_thread(), "not VM thread");
+
+  Method* target = (Method*)mname->address_field(_vmtarget_offset);
+  if (target == old_method) {
+    mname->address_field_put(_vmtarget_offset, (address)new_method);
+
+    if (RC_TRACE_IN_RANGE(0x00100000, 0x00400000)) {
+      if (!(*trace_name_printed)) {
+        // RC_TRACE_MESG macro has an embedded ResourceMark
+        RC_TRACE_MESG(("adjust: name=%s",
+                       old_method->method_holder()->external_name()));
+        *trace_name_printed = true;
+      }
+      // RC_TRACE macro has an embedded ResourceMark
+      RC_TRACE(0x00400000, ("MemberName method update: %s(%s)",
+                            new_method->name()->as_C_string(),
+                            new_method->signature()->as_C_string()));
+    }
+  }
+}
+#endif // INCLUDE_JVMTI
+
 void java_lang_invoke_MemberName::set_vmtarget(oop mname, Metadata* ref) {
   assert(is_instance(mname), "wrong type");
   // check the type of the vmtarget
@@ -3251,6 +3258,37 @@
   compute_offset(_limit_offset, k, vmSymbols::limit_name(), vmSymbols::int_signature());
 }
 
+int org_ObjectLayout_AbstractStructuredArray::_bodySize_offset = 0;
+int org_ObjectLayout_AbstractStructuredArray::_length_offset = 0;
+int org_ObjectLayout_AbstractStructuredArray::_elementSize_offset = 0;
+int org_ObjectLayout_AbstractStructuredArray::_paddingSize_offset = 0;
+int org_ObjectLayout_AbstractStructuredArray::_elementClass_offset = 0;
+
+void org_ObjectLayout_AbstractStructuredArray::compute_offsets() {
+  Klass* k = SystemDictionary::AbstractStructuredArray_klass();
+  if (k != NULL) {
+    compute_offset(_bodySize_offset, k,
+        vmSymbols::bodySize_name(), vmSymbols::int_signature());
+    compute_offset(_length_offset, k,
+        vmSymbols::length_name(), vmSymbols::long_signature());
+    compute_offset(_elementSize_offset, k,
+        vmSymbols::elementSize_name(), vmSymbols::long_signature());
+    compute_offset(_paddingSize_offset, k,
+        vmSymbols::paddingSize_name(), vmSymbols::long_signature());
+    compute_offset(_elementClass_offset, k,
+        vmSymbols::elementClass_name(), vmSymbols::class_signature());
+
+    if (TraceObjectLayoutIntrinsics) {
+      tty->print_cr(
+          "org_ObjectLayout_AbstractStructuredArray::compute_offsets: "
+          "_bodySize_offset=%d, _length_offset=%d, _elementSize_offset=%d, "
+          "_paddingSize_offset=%d, _elementClass_offset=%d",
+          _bodySize_offset, _length_offset, _elementSize_offset,
+          _paddingSize_offset, _elementClass_offset);
+    }
+  }
+}
+
 void java_util_concurrent_locks_AbstractOwnableSynchronizer::initialize(TRAPS) {
   if (_owner_offset != 0) return;
 
@@ -3355,6 +3393,9 @@
   }
   if (JDK_Version::is_jdk18x_version())
     java_lang_reflect_Parameter::compute_offsets();
+  if (JDK_Version::is_gte_jdk18x_version()) {
+    org_ObjectLayout_AbstractStructuredArray::compute_offsets();
+  }
 
   // generated interpreter code wants to know about the offsets we just computed:
   AbstractAssembler::update_delayed_values();
@@ -3574,7 +3615,7 @@
     tty->print_cr("  name: %s, sig: %s, flags: %08x", fs.name()->as_C_string(), fs.signature()->as_C_string(), fs.access_flags().as_int());
   }
 #endif //PRODUCT
-  vm_exit_during_initialization("Invalid layout of preloaded class: use -XX:+TraceClassLoading to see the origin of the problem class");
+  fatal("Invalid layout of preloaded class");
   return -1;
 }
 
--- old/src/share/vm/classfile/javaClasses.hpp	2015-06-16 10:25:40.922518623 -0700
+++ new/src/share/vm/classfile/javaClasses.hpp	2015-06-16 10:25:40.846520428 -0700
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
@@ -484,9 +484,8 @@
     trace_methods_offset = 0,
     trace_bcis_offset    = 1,
     trace_mirrors_offset = 2,
-    trace_cprefs_offset  = 3,
-    trace_next_offset    = 4,
-    trace_size           = 5,
+    trace_next_offset    = 3,
+    trace_size           = 4,
     trace_chunk_size     = 32
   };
 
@@ -497,7 +496,7 @@
   static int static_unassigned_stacktrace_offset;
 
   // Printing
-  static char* print_stack_element_to_buffer(Handle mirror, int method, int version, int bci, int cpref);
+  static char* print_stack_element_to_buffer(Handle mirror, int method, int version, int bci);
   // StackTrace (programmatic access, new since 1.4)
   static void clear_stacktrace(oop throwable);
   // No stack trace available
@@ -518,7 +517,7 @@
   static oop message(Handle throwable);
   static void set_message(oop throwable, oop value);
   static void print_stack_element(outputStream *st, Handle mirror, int method,
-                                  int version, int bci, int cpref);
+                                  int version, int bci);
   static void print_stack_element(outputStream *st, methodHandle method, int bci);
   static void print_stack_usage(Handle stream);
 
@@ -1096,6 +1095,10 @@
 
   static Metadata*      vmtarget(oop mname);
   static void       set_vmtarget(oop mname, Metadata* target);
+#if INCLUDE_JVMTI
+  static void       adjust_vmtarget(oop mname, Method* old_method, Method* new_method,
+                                    bool* trace_name_printed);
+#endif // INCLUDE_JVMTI
 
   static intptr_t       vmindex(oop mname);
   static void       set_vmindex(oop mname, intptr_t index);
@@ -1323,7 +1326,7 @@
   static void set_lineNumber(oop element, int value);
 
   // Create an instance of StackTraceElement
-  static oop create(Handle mirror, int method, int version, int bci, int cpref, TRAPS);
+  static oop create(Handle mirror, int method, int version, int bci, TRAPS);
   static oop create(methodHandle method, int bci, TRAPS);
 
   // Debugging
@@ -1378,6 +1381,78 @@
   static oop  get_owner_threadObj(oop obj);
 };
 
+
+// Interface to org.ObjectLayout.AbstractStructuredArray objects
+
+class org_ObjectLayout_AbstractStructuredArray : AllStatic {
+  friend class JavaClasses;
+
+ private:
+  static int _bodySize_offset;
+  static int _length_offset;
+  static int _elementSize_offset;
+  static int _paddingSize_offset;
+  static int _elementClass_offset;
+
+  static void compute_offsets();
+
+ public:
+  static int bodySize_offset() {
+    return _bodySize_offset;
+  }
+  static int length_offset() {
+    return _length_offset;
+  }
+  static int elementSize_offset() {
+    return _elementSize_offset;
+  }
+  static int paddingSize_offset() {
+    return _paddingSize_offset;
+  }
+  static int elementClass_offset() {
+    return _elementClass_offset;
+  }
+
+  static jint bodySize(oop asa) {
+    return asa->int_field(_bodySize_offset);
+  }
+  static void set_bodySize(oop asa, jint value) {
+    return asa->int_field_put(_bodySize_offset, value);
+  }
+
+  static jlong length(oop asa) {
+    return asa->long_field(_length_offset);
+  }
+  static void set_length(oop asa, jlong value) {
+    return asa->long_field_put(_length_offset, value);
+  }
+
+  static jlong elementSize(oop asa) {
+    return asa->long_field(_elementSize_offset);
+  }
+  static void set_elementSize(oop asa, jlong value) {
+    return asa->long_field_put(_elementSize_offset, value);
+  }
+
+  static jlong paddingSize(oop asa) {
+    return asa->long_field(_paddingSize_offset);
+  }
+  static void set_paddingSize(oop asa, jlong value) {
+    return asa->long_field_put(_paddingSize_offset, value);
+  }
+
+  static oop elementClass(oop asa) {
+    return asa->obj_field(_elementClass_offset);
+  }
+  static void set_elementClass(oop asa, oop value) {
+    return asa->obj_field_put(_elementClass_offset, value);
+  }
+
+  static jlong footprint_with_contained_objects(oop asa) {
+    return bodySize(asa) + length(asa) * elementSize(asa);
+  }
+};
+
 // Use to declare fields that need to be injected into Java classes
 // for the JVM to use.  The name_index and signature_index are
 // declared in vmSymbols.  The may_be_java flag is used to declare
--- old/src/share/vm/classfile/systemDictionary.hpp	2015-06-16 10:25:41.210511772 -0700
+++ new/src/share/vm/classfile/systemDictionary.hpp	2015-06-16 10:25:41.138513486 -0700
@@ -195,8 +195,11 @@
   do_klass(Short_klass,                                 java_lang_Short,                           Pre                 ) \
   do_klass(Integer_klass,                               java_lang_Integer,                         Pre                 ) \
   do_klass(Long_klass,                                  java_lang_Long,                            Pre                 ) \
-  /*end*/
-
+                                                                                                                         \
+  /* ObjectLayout support */                                                                                             \
+  do_klass(AbstractStructuredArray_klass,               org_ObjectLayout_AbstractStructuredArray,  Opt                 ) \
+                                                                                                                         \
+  /* End */
 
 class SystemDictionary : AllStatic {
   friend class VMStructs;
--- old/src/share/vm/classfile/vmSymbols.hpp	2015-06-16 10:25:41.478505391 -0700
+++ new/src/share/vm/classfile/vmSymbols.hpp	2015-06-16 10:25:41.406507106 -0700
@@ -244,6 +244,7 @@
   template(returnType_name,                           "returnType")                               \
   template(signature_name,                            "signature")                                \
   template(slot_name,                                 "slot")                                     \
+  template(selectAlternative_name,                    "selectAlternative")                        \
                                                                                                   \
   /* Support for annotations (JDK 1.5 and above) */                                               \
                                                                                                   \
@@ -295,7 +296,8 @@
   template(setTarget_signature,                       "(Ljava/lang/invoke/MethodHandle;)V")       \
   NOT_LP64(  do_alias(intptr_signature,               int_signature)  )                           \
   LP64_ONLY( do_alias(intptr_signature,               long_signature) )                           \
-                                                                                                  \
+  template(selectAlternative_signature, "(ZLjava/lang/invoke/MethodHandle;Ljava/lang/invoke/MethodHandle;)Ljava/lang/invoke/MethodHandle;") \
+                                                                      \
   /* common method and field names */                                                             \
   template(object_initializer_name,                   "<init>")                                   \
   template(class_initializer_name,                    "<clinit>")                                 \
@@ -475,6 +477,7 @@
   template(threadgroup_string_void_signature,         "(Ljava/lang/ThreadGroup;Ljava/lang/String;)V")             \
   template(string_class_signature,                    "(Ljava/lang/String;)Ljava/lang/Class;")                    \
   template(object_object_object_signature,            "(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;") \
+  template(object_long_object_signature,              "(Ljava/lang/Object;J)Ljava/lang/Object;")                  \
   template(string_string_string_signature,            "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;") \
   template(string_string_signature,                   "(Ljava/lang/String;)Ljava/lang/String;")                   \
   template(classloader_string_long_signature,         "(Ljava/lang/ClassLoader;Ljava/lang/String;)J")             \
@@ -589,6 +592,15 @@
   template(classRedefinedCount_name,                   "classRedefinedCount")                                     \
   template(classLoader_name,                           "classLoader")                                             \
                                                                                                                   \
+  /* ObjectLayout support */                                                                                      \
+  template(org_ObjectLayout_AbstractStructuredArray,   "org/ObjectLayout/AbstractStructuredArray")                \
+  template(existenceIndicatesIntrinsic_name,           "existenceIndicatesIntrinsic")                             \
+  template(bodySize_name,                              "bodySize")                                                \
+  template(length_name,                                "length")                                                  \
+  template(elementSize_name,                           "elementSize")                                             \
+  template(paddingSize_name,                           "paddingSize")                                             \
+  template(elementClass_name,                          "elementClass")                                            \
+                                                                                                                  \
   /* trace signatures */                                                                                          \
   TRACE_TEMPLATES(template)                                                                                       \
                                                                                                                   \
@@ -866,14 +878,29 @@
    do_name(     fullFence_name,                                  "fullFence")                                           \
    do_alias(    fullFence_signature,                              void_method_signature)                                \
                                                                                                                         \
-  /* Custom branch frequencies profiling support for JSR292 */                                                          \
-  do_class(java_lang_invoke_MethodHandleImpl,               "java/lang/invoke/MethodHandleImpl")                        \
-  do_intrinsic(_profileBoolean, java_lang_invoke_MethodHandleImpl, profileBoolean_name, profileBoolean_signature,    F_S)  \
-   do_name(     profileBoolean_name,                               "profileBoolean")                                     \
-   do_signature(profileBoolean_signature,                           "(Z[I)Z")                                            \
+  /* ObjectLayout intrinsics */                                                                                         \
+   do_signature(class_long_long_long_signature,                  "(Ljava/lang/Class;JJ)J")                              \
+  do_intrinsic(_getInstanceSize,                                  sun_misc_Unsafe, getInstanceSize_name, class_long_signature, F_RN) \
+   do_name(     getInstanceSize_name,                            "getInstanceSize")                                     \
+  do_intrinsic(_getInstanceFootprintWhenContained,                sun_misc_Unsafe, getInstanceFootprintWhenContained_name, class_long_signature, F_RN) \
+   do_name(     getInstanceFootprintWhenContained_name,          "getInstanceFootprintWhenContained")                   \
+  do_intrinsic(_getContainingObjectFootprint,                     sun_misc_Unsafe, getContainingObjectFootprint_name, class_long_long_long_signature, F_RN) \
+   do_name(     getContainingObjectFootprint_name,               "getContainingObjectFootprint")                        \
+  do_intrinsic(_getContainingObjectFootprintWhenContained,        sun_misc_Unsafe, getContainingObjectFootprintWhenContained_name, class_long_long_long_signature, F_RN) \
+   do_name(     getContainingObjectFootprintWhenContained_name,  "getContainingObjectFootprintWhenContained")           \
+  do_intrinsic(_getPrePaddingInObjectFootprint,                   sun_misc_Unsafe, getPrePaddingInObjectFootprint_name, long_long_signature, F_RN) \
+   do_name(     getPrePaddingInObjectFootprint_name,             "getPrePaddingInObjectFootprint")                      \
+  do_intrinsic(_allocateHeapForElementArrayClass,                 sun_misc_Unsafe, allocateHeapForElementArrayClass_name, allocateHeapForElementArrayClass_signature, F_RN) \
+   do_name(     allocateHeapForElementArrayClass_name,           "allocateHeapForElementArrayClass")                    \
+   do_signature(allocateHeapForElementArrayClass_signature,      "(Ljava/lang/Class;[J[Ljava/lang/Class;)Ljava/lang/Object;") \
+  do_intrinsic(_constructObjectAtOffset,                          sun_misc_Unsafe, constructObjectAtOffset_name, constructObjectAtOffset_signature, F_RN) \
+   do_name(     constructObjectAtOffset_name,                    "constructObjectAtOffset")                             \
+   do_signature(constructObjectAtOffset_signature,               "(Ljava/lang/Object;JJZZJLjava/lang/reflect/Constructor;[Ljava/lang/Object;)V") \
+  do_intrinsic(_deriveContainedObjectAtOffset,                    sun_misc_Unsafe, deriveContainedObjectAtOffset_name, object_long_object_signature, F_RN) \
+   do_name(     deriveContainedObjectAtOffset_name,              "deriveContainedObjectAtOffset")                       \
                                                                                                                         \
   /* unsafe memory references (there are a lot of them...) */                                                           \
-  do_signature(getObject_signature,       "(Ljava/lang/Object;J)Ljava/lang/Object;")                                    \
+  do_alias(    getObject_signature,        object_long_object_signature)                                                \
   do_signature(putObject_signature,       "(Ljava/lang/Object;JLjava/lang/Object;)V")                                   \
   do_signature(getBoolean_signature,      "(Ljava/lang/Object;J)Z")                                                     \
   do_signature(putBoolean_signature,      "(Ljava/lang/Object;JZ)V")                                                    \
--- old/src/share/vm/gc_implementation/parallelScavenge/psMarkSweep.cpp	2015-06-16 10:25:41.754498821 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psMarkSweep.cpp	2015-06-16 10:25:41.686500439 -0700
@@ -167,12 +167,19 @@
   {
     HandleMark hm;
 
+    gclog_or_tty->date_stamp(PrintGC && PrintGCDateStamps);
     TraceCPUTime tcpu(PrintGCDetails, true, gclog_or_tty);
     GCTraceTime t1(GCCauseString("Full GC", gc_cause), PrintGC, !PrintGCDetails, NULL, _gc_tracer->gc_id());
     TraceCollectorStats tcs(counters());
     TraceMemoryManagerStats tms(true /* Full GC */,gc_cause);
 
-    if (TraceGen1Time) accumulated_time()->start();
+    if (TraceGen1Time) {
+      accumulated_time()->start();
+    }
+
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSMarkSweep::invoke_no_policy: Starting full GC...");
+    }
 
     // Let the size policy know we're starting
     size_policy->major_collection_begin();
@@ -200,16 +207,32 @@
 
     mark_sweep_phase1(clear_all_softrefs);
 
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSMarkSweep::invoke_no_policy: Finished phase 1...");
+    }
+
     mark_sweep_phase2();
 
-    // Don't add any more derived pointers during phase3
-    COMPILER2_PRESENT(assert(DerivedPointerTable::is_active(), "Sanity"));
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSMarkSweep::invoke_no_policy: Finished phase 2...");
+    }
+
+    // Don't add any more derived pointers during phase 3
+    COMPILER2_PRESENT(assert(DerivedPointerTable::is_active(), "sanity check"));
     COMPILER2_PRESENT(DerivedPointerTable::set_active(false));
 
     mark_sweep_phase3();
 
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSMarkSweep::invoke_no_policy: Finished phase 3...");
+    }
+
     mark_sweep_phase4();
 
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSMarkSweep::invoke_no_policy: Finished phase 4...");
+    }
+
     restore_marks();
 
     deallocate_stacks();
@@ -345,7 +368,13 @@
     // We collected the heap, recalculate the metaspace capacity
     MetaspaceGC::compute_new_size();
 
-    if (TraceGen1Time) accumulated_time()->stop();
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSMarkSweep::invoke_no_policy: Finished full GC");
+    }
+
+    if (TraceGen1Time) {
+      accumulated_time()->stop();
+    }
 
     if (PrintGC) {
       if (PrintGCDetails) {
--- old/src/share/vm/gc_implementation/parallelScavenge/psMarkSweepDecorator.cpp	2015-06-16 10:25:42.018492533 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psMarkSweepDecorator.cpp	2015-06-16 10:25:41.950494153 -0700
@@ -72,6 +72,60 @@
   return _destination_decorator;
 }
 
+void PSMarkSweepDecorator::forward_container(address src, address dst, uint nesting_level) {
+  if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+    tty->print_cr("PSMarkSweepDecorator::forward_container: src=0x%p, dst=0x%p, nesting_level=%u",
+        (void*) src, (void*) dst, nesting_level);
+  }
+
+  assert(((oop) src)->is_container(), "src not container");
+
+  jint body_size = org_ObjectLayout_AbstractStructuredArray::bodySize((oop) src);
+  jlong length = org_ObjectLayout_AbstractStructuredArray::length((oop) src);
+  jlong padding_size = org_ObjectLayout_AbstractStructuredArray::paddingSize((oop) src);
+  jlong element_size = org_ObjectLayout_AbstractStructuredArray::elementSize((oop) src);
+
+  markOop mark_before = ((oop) src)->mark();
+  ((oop) src)->forward_to((oop) dst);
+  if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+    tty->print_cr("  Forwarding container 0x%p to 0x%p, nesting_level=%u, mark before=0x%p, mark after=0x%p",
+        (void*) src, (void*) dst, nesting_level, mark_before, ((oop) src)->mark());
+  }
+  assert(((oop) src)->is_gc_marked(), "encoding the pointer should preserve the mark");
+  src += body_size;
+  dst += body_size;
+
+  for (jlong i = 0; i < length; i++) {
+    oop prepadding_obj = (oop) src;
+    assert(!prepadding_obj->is_contained(), "sanity check");
+    assert(!prepadding_obj->is_container(), "sanity check");
+    markOop mark_before = prepadding_obj->mark();
+    prepadding_obj->forward_to((oop) dst);
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+      tty->print_cr("  Forwarding prepadding 0x%p to 0x%p, nesting_level=%u, mark before=0x%p, mark after=0x%p",
+          (void*) src, (void*) dst, nesting_level, mark_before, prepadding_obj->mark());
+    }
+    assert(prepadding_obj->is_gc_marked(), "encoding the pointer should preserve the mark");
+
+    oop element_obj = (oop) (src + padding_size);
+    assert(element_obj->is_contained(), "sanity check");
+    if (element_obj->is_container()) {
+      forward_container(src + padding_size, dst + padding_size, nesting_level + 1);
+    } else {
+      markOop mark_before = element_obj->mark();
+      element_obj->forward_to((oop) (dst + padding_size));
+      if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+        tty->print_cr("  Forwarding element 0x%p to 0x%p, nesting_level=%u, mark before=0x%p, mark after=0x%p",
+            (void*) (src + padding_size), (void*) (dst + padding_size), nesting_level,mark_before, element_obj->mark());
+      }
+      assert(element_obj->is_gc_marked(), "encoding the pointer should preserve the mark");
+    }
+
+    src += element_size;
+    dst += element_size;
+  }
+}
+
 // FIX ME FIX ME FIX ME FIX ME!!!!!!!!!
 // The object forwarding code is duplicated. Factor this out!!!!!
 //
@@ -125,6 +179,21 @@
       Prefetch::write(q, interval);
       size_t size = oop(q)->size();
 
+      if (oop(q)->is_contained()) {
+        if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+          tty->print_cr("PSMarkSweepDecorator::precompact: 0x%p belongs to dead container, mark is 0x%p",
+              (void*) q, ((oop) q)->mark());
+        }
+        oop(q)->clear_contained();
+      }
+      if (oop(q)->is_container()) {
+        size = org_ObjectLayout_AbstractStructuredArray::footprint_with_contained_objects((oop) q) >> LogHeapWordSize;
+        if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+          tty->print_cr("PSMarkSweepDecorator::precompact: 0x%p is container of size %ld, mark is 0x%p",
+              (void*) q, size << LogHeapWordSize, ((oop) q)->mark());
+        }
+      }
+
       size_t compaction_max_size = pointer_delta(compact_end, compact_top);
 
       // This should only happen if a space in the young gen overflows the
@@ -149,15 +218,28 @@
       }
 
       // store the forwarding pointer into the mark word
-      if (q != compact_top) {
-        oop(q)->forward_to(oop(compact_top));
-        assert(oop(q)->is_gc_marked(), "encoding the pointer should preserve the mark");
-      } else {
+//      if (q != compact_top) {
+        if (oop(q)->is_container()) {
+          forward_container((address) q, (address) compact_top, 0);
+        } else {
+          markOop mark_before = ((oop) q)->mark();
+          oop(q)->forward_to(oop(compact_top));
+          assert(oop(q)->is_gc_marked(), "encoding the pointer should preserve the mark");
+          if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+            tty->print_cr("Forwarding (1) object 0x%p to 0x%p, mark before=0x%p, mark after=0x%p",
+                (void*) q, (void*) compact_top,mark_before,  ((oop) q)->mark());
+          }
+        }
+/*      } else {
         // if the object isn't moving we can just set the mark to the default
         // mark and handle it specially later on.
-        oop(q)->init_mark();
-        assert(oop(q)->forwardee() == NULL, "should be forwarded to NULL");
-      }
+//        if (oop(q)->is_container()) {
+//          init_marks_in_container(oop(q));
+//        } else {
+          oop(q)->init_mark();
+          assert(oop(q)->forwardee() == NULL, "should be forwarded to NULL");
+//        }
+      }*/
 
       // Update object start array
       if (start_array) {
@@ -209,15 +291,24 @@
           }
 
           // store the forwarding pointer into the mark word
-          if (q != compact_top) {
-            oop(q)->forward_to(oop(compact_top));
-            assert(oop(q)->is_gc_marked(), "encoding the pointer should preserve the mark");
-          } else {
+//          if (q != compact_top) {
+            if (oop(q)->is_container()) {
+              forward_container((address) q, (address) compact_top, 0);
+            } else {
+              markOop mark_before = ((oop) q)->mark();
+              oop(q)->forward_to(oop(compact_top));
+              assert(oop(q)->is_gc_marked(), "encoding the pointer should preserve the mark");
+              if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+                tty->print_cr("Forwarding (2) object 0x%p to 0x%p, mark before=0x%p, mark after=0x%p",
+                    (void*) q, (void*) compact_top,  mark_before, ((oop) q)->mark());
+              }
+            }
+/*          } else {
             // if the object isn't moving we can just set the mark to the default
             // mark and handle it specially later on.
             oop(q)->init_mark();
             assert(oop(q)->forwardee() == NULL, "should be forwarded to NULL");
-          }
+          }*/
 
           // Update object start array
           if (start_array) {
@@ -303,6 +394,10 @@
 
     while (q < end) {
       // point all the oops to the new location
+      if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+        tty->print_cr("PSMarkSweepDecorator::adjust_pointers: Special case 0x%p 0x%p",
+            (void*) q, ((oop) q)->mark());
+      }
       size_t size = oop(q)->adjust_pointers();
       q += size;
     }
@@ -324,6 +419,10 @@
     if (oop(q)->is_gc_marked()) {
       // q is alive
       // point all the oops to the new location
+      if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+        tty->print_cr("PSMarkSweepDecorator::adjust_pointers: Normal case 0x%p 0x%p",
+            (void*) q, ((oop) q)->mark());
+      }
       size_t size = oop(q)->adjust_pointers();
       debug_only(prev_q = q);
       q += size;
@@ -392,11 +491,16 @@
       Prefetch::write(compaction_top, copy_interval);
 
       // copy object and reinit its mark
-      assert(q != compaction_top, "everything in this pass should be moving");
+      if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+        tty->print_cr("PSMarkSweepDecorator::compact: Copying 0x%p with mark 0x%p",
+            (void*) q,((oop) q)->mark());
+      }
+//      assert(q != compaction_top, "everything in this pass should be moving");
       Copy::aligned_conjoint_words(q, compaction_top, size);
-      oop(compaction_top)->init_mark();
       assert(oop(compaction_top)->klass() != NULL, "should have a class");
 
+      ((oop) compaction_top)->convert_to_unmarked();
+
       debug_only(prev_q = q);
       q += size;
     }
--- old/src/share/vm/gc_implementation/parallelScavenge/psMarkSweepDecorator.hpp	2015-06-16 10:25:42.286486135 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psMarkSweepDecorator.hpp	2015-06-16 10:25:42.214487854 -0700
@@ -48,6 +48,7 @@
 
   bool insert_deadspace(size_t& allowed_deadspace_words, HeapWord* q,
                         size_t word_len);
+  void forward_container(address src, address dst, uint nesting_level);
 
  public:
   PSMarkSweepDecorator(MutableSpace* space, ObjectStartArray* start_array,
--- old/src/share/vm/gc_implementation/parallelScavenge/psPromotionManager.cpp	2015-06-16 10:25:42.538480118 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psPromotionManager.cpp	2015-06-16 10:25:42.470481741 -0700
@@ -305,40 +305,78 @@
 }
 
 oop PSPromotionManager::oop_promotion_failed(oop obj, markOop obj_mark) {
-  assert(_old_gen_is_full || PromotionFailureALot, "Sanity");
+  assert(_old_gen_is_full || PromotionFailureALot, "sanity");
 
-  // Attempt to CAS in the header.
-  // This tests if the header is still the same as when
-  // this started.  If it is the same (i.e., no forwarding
-  // pointer has been installed), then this thread owns
-  // it.
+  // This attempt to CAS in the header tests if the header is still the same as
+  // when this started. If it is the same (i.e., no forwarding pointer has been
+  // installed), then this thread owns it.
   if (obj->cas_forward_to(obj, obj_mark)) {
     // We won any races, we "own" this object.
-    assert(obj == obj->forwardee(), "Sanity");
-
-    _promotion_failed_info.register_copy_failure(obj->size());
-
-    obj->push_contents(this);
+    assert(obj == obj->forwardee(), "sanity");
 
     // Save the mark if needed
     PSScavenge::oop_promotion_failed(obj, obj_mark);
-  }  else {
-    // We lost, someone else "owns" this object
-    guarantee(obj->is_forwarded(), "Object must be forwarded if the cas failed.");
 
-    // No unallocation to worry about.
-    obj = obj->forwardee();
-  }
+#ifndef PRODUCT
+    if (TraceScavenge) {
+      ResourceMark rm;
+      gclog_or_tty->print_cr("{%s %s 0x%p (%d)}", "promotion-failure",
+          obj->klass()->internal_name(), (void*) obj, obj->size());
+    }
+#endif // PRODUCT
+
+    size_t obj_size = obj->size();
+
+    if (oopDesc::is_container(obj_mark, obj->klass())) {
+      // At this point obj is always an outermost container.
+      assert(!oopDesc::is_contained(obj_mark),
+          "object must be an outermost container");
+
+      obj_size = org_ObjectLayout_AbstractStructuredArray::
+          footprint_with_contained_objects(obj) >> LogHeapWordSize;
+
+      // The thread that "owns" a container also "owns" all its contained
+      // objects. We need to forward all of them to themselves. For this
+      // we simply need to iterate through all Java objects between
+      // ((HeapWord*) obj) + obj->size() and ((HeapWord*) obj) + obj_size
+      // and forward every object to the corresponding new location.
+      // (See PSPromotionManager::copy_to_survivor_space() for a detailed
+      // explanation.) For installing forwarding pointers we can use
+      // the non MT safe oopDesc::forward_to(), since no other thread can
+      // interfere with us.
+      HeapWord* src = ((HeapWord*) obj) + obj->size();
+      HeapWord* src_end = ((HeapWord*) obj) + obj_size;
+      while (src < src_end) {
+        markOop src_mark = ((oop) src)->mark();
+        ((oop) src)->forward_to((oop) src);
+
+        // Save the mark if needed
+        PSScavenge::oop_promotion_failed(((oop) src), src_mark);
 
 #ifndef PRODUCT
-  if (TraceScavenge) {
-    gclog_or_tty->print_cr("{%s %s 0x%x (%d)}",
-                           "promotion-failure",
-                           obj->klass()->internal_name(),
-                           (void *)obj, obj->size());
+        if (TraceScavenge) {
+          ResourceMark rm;
+          oop o = (oop) src;
+          gclog_or_tty->print_cr("{%s %s 0x%p (%d)}", "promotion-failure",
+              o->klass()->internal_name(), (void*) src, o->size());
+        }
+#endif // PRODUCT
+
+        src += ((oop) src)->size();
+      }
+      assert(src == src_end, "sanity");
+    }
+
+    _promotion_failed_info.register_copy_failure(obj_size);
 
+    obj->push_contents(this);
+  } else {
+    // We lost, someone else "owns" this object.
+    guarantee(obj->is_forwarded(), "object must be forwarded if CAS failed");
+
+    // There is no unallocation to worry about.
+    obj = obj->forwardee();
   }
-#endif
 
   return obj;
 }
--- old/src/share/vm/gc_implementation/parallelScavenge/psPromotionManager.hpp	2015-06-16 10:25:42.798473910 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psPromotionManager.hpp	2015-06-16 10:25:42.726475629 -0700
@@ -81,7 +81,6 @@
   bool                                _old_gen_is_full;
 
   OopStarTaskQueue                    _claimed_stack_depth;
-  OverflowTaskQueue<oop, mtGC>        _claimed_stack_breadth;
 
   bool                                _totally_drain;
   uint                                _target_stack_size;
@@ -96,7 +95,6 @@
   static MutableSpace* young_space() { return _young_space; }
 
   inline static PSPromotionManager* manager_array(int index);
-  template <class T> inline void claim_or_forward_internal_depth(T* p);
 
   // On the task queues we push reference locations as well as
   // partially-scanned arrays (in the latter case, we push an oop to
@@ -165,28 +163,26 @@
   }
 
   PSPromotionManager();
+  void reset();
 
   // Accessors
   OopStarTaskQueue* claimed_stack_depth() {
     return &_claimed_stack_depth;
   }
 
-  bool young_gen_is_full()             { return _young_gen_is_full; }
-
-  bool old_gen_is_full()               { return _old_gen_is_full; }
-  void set_old_gen_is_full(bool state) { _old_gen_is_full = state; }
-
   // Promotion methods
   template<bool promote_immediately> oop copy_to_survivor_space(oop o);
-  oop oop_promotion_failed(oop obj, markOop obj_mark);
 
-  void reset();
+ private:
+  inline oop allocate_in_young_gen_slow(size_t size);
+  inline oop allocate_in_old_gen_slow(size_t size);
+  oop oop_promotion_failed(oop obj, markOop obj_mark);
 
+ public:
   void flush_labs();
   void drain_stacks(bool totally_drain) {
     drain_stacks_depth(totally_drain);
   }
- public:
   void drain_stacks_cond_depth() {
     if (claimed_stack_depth()->size() > _target_stack_size) {
       drain_stacks_depth(false);
--- old/src/share/vm/gc_implementation/parallelScavenge/psPromotionManager.inline.hpp	2015-06-16 10:25:43.094466837 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psPromotionManager.inline.hpp	2015-06-16 10:25:43.022468561 -0700
@@ -38,12 +38,16 @@
 }
 
 template <class T>
-inline void PSPromotionManager::claim_or_forward_internal_depth(T* p) {
-  if (p != NULL) { // XXX: error if p != NULL here
+inline void PSPromotionManager::claim_or_forward_depth(T* p) {
+  assert(PSScavenge::should_scavenge(p, true), "revisiting object?");
+  assert(Universe::heap()->kind() == CollectedHeap::ParallelScavengeHeap,
+      "Sanity");
+  assert(Universe::heap()->is_in(p), "pointer outside heap");
+
+  if (p != NULL) {
     oop o = oopDesc::load_decode_heap_oop_not_null(p);
     if (o->is_forwarded()) {
       o = o->forwardee();
-      // Card mark
       if (PSScavenge::is_obj_in_young(o)) {
         PSScavenge::card_table()->inline_write_ref_field_gc(p, o);
       }
@@ -54,103 +58,80 @@
   }
 }
 
-template <class T>
-inline void PSPromotionManager::claim_or_forward_depth(T* p) {
-  assert(PSScavenge::should_scavenge(p, true), "revisiting object?");
-  assert(Universe::heap()->kind() == CollectedHeap::ParallelScavengeHeap,
-         "Sanity");
-  assert(Universe::heap()->is_in(p), "pointer outside heap");
-
-  claim_or_forward_internal_depth(p);
-}
-
-//
-// This method is pretty bulky. It would be nice to split it up
-// into smaller submethods, but we need to be careful not to hurt
-// performance.
-//
+// This method is pretty bulky. It would be nice to split it up into smaller
+// submethods, but we need to be careful not to hurt performance.
 template<bool promote_immediately>
 oop PSPromotionManager::copy_to_survivor_space(oop o) {
-  assert(PSScavenge::should_scavenge(&o), "Sanity");
+  assert(PSScavenge::should_scavenge(&o), "sanity");
+
+  // This method is not called if o is known to be already forwarded. The
+  // corresponding check is done by the caller just before this call.
 
   oop new_obj = NULL;
+  oop original_obj = o;
 
-  // NOTE! We must be very careful with any methods that access the mark
-  // in o. There may be multiple threads racing on it, and it may be forwarded
-  // at any time. Do not use oop methods for accessing the mark!
+  // Evacuating a live contained object triggers immediate evacuation of
+  // its outermost container. At this point, if the outermost container is not
+  // marked yet, we cannot definitely say whether it is alive or not. We always
+  // treat is as alive one. This also means that "the object is contained"
+  // property never changes during Scavenge.
+  bool need_container_copying = o->is_contained();
+  jlong offset_from_outermost_container = 0;
+  if (need_container_copying) {
+    o = o->outermost_container();
+    offset_from_outermost_container =
+        (jlong) (((address) original_obj) - ((address) o));
+  }
+
+  // NOTE! We must be very careful with any methods that access the mark in o.
+  // There may be multiple threads racing on it, and it may be forwarded
+  // at any time. Do not use oopDesc methods for accessing the mark!
   markOop test_mark = o->mark();
 
-  // The same test as "o->is_forwarded()"
+  // Check whether the object is already forwarded
   if (!test_mark->is_marked()) {
     bool new_obj_is_tenured = false;
     size_t new_obj_size = o->size();
 
+    if (oopDesc::is_container(test_mark, o->klass())) {
+      new_obj_size = org_ObjectLayout_AbstractStructuredArray::
+          footprint_with_contained_objects(o) >> LogHeapWordSize;
+    }
+
     if (!promote_immediately) {
-      // Find the objects age, MT safe.
-      uint age = (test_mark->has_displaced_mark_helper() /* o->has_displaced_mark() */) ?
-        test_mark->displaced_mark_helper()->age() : test_mark->age();
+      // Find the object's age in MT safe way
+      uint age = test_mark->has_displaced_mark_helper() ?
+          test_mark->displaced_mark_helper()->age() : test_mark->age();
 
-      // Try allocating obj in to-space (unless too old)
+      // Try to allocate the object in To space (unless it is too old)
       if (age < PSScavenge::tenuring_threshold()) {
         new_obj = (oop) _young_lab.allocate(new_obj_size);
-        if (new_obj == NULL && !_young_gen_is_full) {
-          // Do we allocate directly, or flush and refill?
-          if (new_obj_size > (YoungPLABSize / 2)) {
-            // Allocate this object directly
-            new_obj = (oop)young_space()->cas_allocate(new_obj_size);
-          } else {
-            // Flush and fill
-            _young_lab.flush();
-
-            HeapWord* lab_base = young_space()->cas_allocate(YoungPLABSize);
-            if (lab_base != NULL) {
-              _young_lab.initialize(MemRegion(lab_base, YoungPLABSize));
-              // Try the young lab allocation again.
-              new_obj = (oop) _young_lab.allocate(new_obj_size);
-            } else {
-              _young_gen_is_full = true;
-            }
-          }
+        if (new_obj == NULL) {
+          new_obj = allocate_in_young_gen_slow(new_obj_size);
         }
       }
     }
 
-    // Otherwise try allocating obj tenured
+    // Otherwise try to allocate the object in the old generation
     if (new_obj == NULL) {
 #ifndef PRODUCT
       if (Universe::heap()->promotion_should_fail()) {
-        return oop_promotion_failed(o, test_mark);
+        new_obj = oop_promotion_failed(o, test_mark);
+
+        if (need_container_copying) {
+          new_obj =
+              (oop) (((address) new_obj) + offset_from_outermost_container);
+        }
+
+        return new_obj;
       }
-#endif  // #ifndef PRODUCT
+#endif // PRODUCT
 
-      new_obj = (oop) _old_lab.allocate(new_obj_size);
       new_obj_is_tenured = true;
-
+      // TODO: Update object start array
+      new_obj = (oop) _old_lab.allocate(new_obj_size);
       if (new_obj == NULL) {
-        if (!_old_gen_is_full) {
-          // Do we allocate directly, or flush and refill?
-          if (new_obj_size > (OldPLABSize / 2)) {
-            // Allocate this object directly
-            new_obj = (oop)old_gen()->cas_allocate(new_obj_size);
-          } else {
-            // Flush and fill
-            _old_lab.flush();
-
-            HeapWord* lab_base = old_gen()->cas_allocate(OldPLABSize);
-            if(lab_base != NULL) {
-#ifdef ASSERT
-              // Delay the initialization of the promotion lab (plab).
-              // This exposes uninitialized plabs to card table processing.
-              if (GCWorkerDelayMillis > 0) {
-                os::sleep(Thread::current(), GCWorkerDelayMillis, false);
-              }
-#endif
-              _old_lab.initialize(MemRegion(lab_base, OldPLABSize));
-              // Try the old lab allocation again.
-              new_obj = (oop) _old_lab.allocate(new_obj_size);
-            }
-          }
-        }
+        new_obj = allocate_in_old_gen_slow(new_obj_size);
 
         // This is the promotion failed test, and code handling.
         // The code belongs here for two reasons. It is slightly
@@ -159,81 +140,221 @@
         // the impact on the common case fast path code.
 
         if (new_obj == NULL) {
-          _old_gen_is_full = true;
-          return oop_promotion_failed(o, test_mark);
+          new_obj = oop_promotion_failed(o, test_mark);
+
+          if (need_container_copying) {
+            new_obj =
+                (oop) (((address) new_obj) + offset_from_outermost_container);
+          }
+
+          return new_obj;
         }
       }
     }
 
     assert(new_obj != NULL, "allocation should have succeeded");
 
-    // Copy obj
-    Copy::aligned_disjoint_words((HeapWord*)o, (HeapWord*)new_obj, new_obj_size);
+    // Copy the object
+    Copy::aligned_disjoint_words((HeapWord*) o, (HeapWord*) new_obj,
+        new_obj_size);
 
     // Now we have to CAS in the header.
     if (o->cas_forward_to(new_obj, test_mark)) {
       // We won any races, we "own" this object.
-      assert(new_obj == o->forwardee(), "Sanity");
+      assert(new_obj == o->forwardee(), "sanity");
+
+      // Now that we're dealing with a markOop that cannot change, it is okay
+      // to use the non MT safe oopDesc methods.
 
-      // Increment age if obj still in new generation. Now that
-      // we're dealing with a markOop that cannot change, it is
-      // okay to use the non mt safe oop methods.
+      // Increment age if the object is still in the young generation
       if (!new_obj_is_tenured) {
+        assert(young_space()->contains(new_obj),
+            "object must belong to To space");
         new_obj->incr_age();
-        assert(young_space()->contains(new_obj), "Attempt to push non-promoted obj");
       }
 
-      // Do the size comparison first with new_obj_size, which we
+#ifndef PRODUCT
+      if (TraceScavenge) {
+        ResourceMark rm;
+        gclog_or_tty->print_cr("{%s %s 0x%p -> 0x%p (%d)}",
+            PSScavenge::should_scavenge(&new_obj) ? "copying" : "tenuring",
+            new_obj->klass()->internal_name(),
+            (void*) o, (void*) new_obj, new_obj->size());
+      }
+#endif // PRODUCT
+
+      if (new_obj->is_container()) {
+        // The thread that "owns" a container also "owns" all its contained
+        // objects. We need to forward all of them to their new locations.
+        // Let's consider the memory layout of some container, for example,
+        // a structured array.
+        // +---------+-------+-------+-------+-------+-----+-------+-------+
+        // |Container| RCO_0 | Obj_0 | RCO_1 | Obj_1 | ... |RCO_n-1|Obj_n-1|
+        // +---------+-------+-------+-------+-------+-----+-------+-------+
+        // Every cell is a single Java object, except for Obj_0 ... Obj_n-1,
+        // which may be single Java objects or nested containers having
+        // (recursively) the same memory layout. In any case, new_obj_size
+        // stores the full size of new_obj container, including (recursively)
+        // all its contents. new_obj->size() returns just the container's own
+        // size (that is the size of the first cell only). To forward all
+        // the contents of new_obj container, we simply need to iterate through
+        // all Java objects between ((HeapWord*) o) + new_obj->size() and
+        // ((HeapWord*) o) + new_obj_size and forward every object to the
+        // corresponding new location. This approach correctly works for
+        // the case of nested containers too and corresponds to depth-first
+        // traversal of the tree representing container hierarchy for new_obj.
+        // For installing forwarding pointers we can use the non MT safe
+        // oopDesc::forward_to(), since no other thread can interfere with us.
+
+        size_t size = new_obj->size();
+        HeapWord* src = ((HeapWord*) o) + size;
+        HeapWord* dst = ((HeapWord*) new_obj) + size;
+        HeapWord* src_end = ((HeapWord*) o) + new_obj_size;
+        while (src < src_end) {
+          ((oop) src)->forward_to((oop) dst);
+
+          // Increment age if the object is still in the young generation
+          if (!new_obj_is_tenured) {
+            assert(young_space()->contains((oop) dst),
+                "object must belong to To space");
+            ((oop) dst)->incr_age();
+          }
+
+#ifndef PRODUCT
+          if (TraceScavenge) {
+            ResourceMark rm;
+            oop obj = (oop) dst;
+            gclog_or_tty->print_cr("{%s %s 0x%p -> 0x%p (%d)}",
+                PSScavenge::should_scavenge(&obj) ? "copying" : "tenuring",
+                obj->klass()->internal_name(),
+                (void*) src, (void*) dst, obj->size());
+          }
+#endif // PRODUCT
+
+          size = ((oop) dst)->size();
+          src += size;
+          dst += size;
+        }
+        assert(src == src_end, "sanity");
+      }
+
+      // Firstly, we do the size comparison with new_obj_size, which we
       // already have. Hopefully, only a few objects are larger than
       // _min_array_size_for_chunking, and most of them will be arrays.
-      // So, the is->objArray() test would be very infrequent.
+      // So, the is_objArray() test would be very infrequent.
       if (new_obj_size > _min_array_size_for_chunking &&
-          new_obj->is_objArray() &&
-          PSChunkLargeArrays) {
-        // we'll chunk it
+          new_obj->is_objArray() && PSChunkLargeArrays) {
+        // We'll chunk it.
         oop* const masked_o = mask_chunked_array_oop(o);
         push_depth(masked_o);
         TASKQUEUE_STATS_ONLY(++_arrays_chunked; ++_masked_pushes);
       } else {
-        // we'll just push its contents
+        // We'll just push its contents.
         new_obj->push_contents(this);
       }
-    }  else {
-      // We lost, someone else "owns" this object
-      guarantee(o->is_forwarded(), "Object must be forwarded if the cas failed.");
-
-      // Try to deallocate the space.  If it was directly allocated we cannot
-      // deallocate it, so we have to test.  If the deallocation fails,
-      // overwrite with a filler object.
+    } else {
+      // We lost, someone else "owns" this object.
+      guarantee(o->is_forwarded(), "object must be forwarded if CAS failed");
+
+      // Here we try to deallocate the space. If it was directly allocated,
+      // we cannot deallocate it, so we have to test. If the deallocation
+      // fails, we fill the space with a filler object.
       if (new_obj_is_tenured) {
+        // TODO: Update object start array
         if (!_old_lab.unallocate_object((HeapWord*) new_obj, new_obj_size)) {
           CollectedHeap::fill_with_object((HeapWord*) new_obj, new_obj_size);
         }
-      } else if (!_young_lab.unallocate_object((HeapWord*) new_obj, new_obj_size)) {
-        CollectedHeap::fill_with_object((HeapWord*) new_obj, new_obj_size);
+      } else {
+        if (!_young_lab.unallocate_object((HeapWord*) new_obj, new_obj_size)) {
+          CollectedHeap::fill_with_object((HeapWord*) new_obj, new_obj_size);
+        }
       }
 
-      // don't update this before the unallocation!
+      // Don't update this before the unallocation!
       new_obj = o->forwardee();
     }
   } else {
-    assert(o->is_forwarded(), "Sanity");
+    assert(o->is_forwarded(), "sanity");
     new_obj = o->forwardee();
   }
 
-#ifndef PRODUCT
-  // This code must come after the CAS test, or it will print incorrect
-  // information.
-  if (TraceScavenge) {
-    gclog_or_tty->print_cr("{%s %s " PTR_FORMAT " -> " PTR_FORMAT " (%d)}",
-       PSScavenge::should_scavenge(&new_obj) ? "copying" : "tenuring",
-       new_obj->klass()->internal_name(), p2i((void *)o), p2i((void *)new_obj), new_obj->size());
+  if (need_container_copying) {
+    // The outermost container of the original object is guaranteed to be
+    // forwarded at this point, either by this thread or by another GC worker.
+    // The original object itself may be not forwarded yet (if another GC worker
+    // is still processing the contained objects of the outermost container),
+    // but we do exactly know its new location. Here we just calculate and
+    // return it; it is guaranteed to be valid when Scavenge ends.
+    new_obj = (oop) (((address) new_obj) + offset_from_outermost_container);
   }
-#endif
 
   return new_obj;
 }
 
+inline oop PSPromotionManager::allocate_in_young_gen_slow(size_t size) {
+  oop obj = NULL;
+
+  if (!_young_gen_is_full) {
+    // Do we allocate directly, or flush and fill?
+    if (size > (YoungPLABSize / 2)) {
+      // Allocate this object directly
+      obj = (oop) young_space()->cas_allocate(size);
+    } else {
+      // Flush and fill
+      _young_lab.flush();
+
+      HeapWord* lab_base = young_space()->cas_allocate(YoungPLABSize);
+      if (lab_base != NULL) {
+        _young_lab.initialize(MemRegion(lab_base, YoungPLABSize));
+
+        // Try allocation in the young generation LAB again
+        obj = (oop) _young_lab.allocate(size);
+      } else {
+        _young_gen_is_full = true;
+      }
+    }
+  }
+
+  return obj;
+}
+
+inline oop PSPromotionManager::allocate_in_old_gen_slow(size_t size) {
+  oop obj = NULL;
+
+  if (!_old_gen_is_full) {
+    // Do we allocate directly, or flush and fill?
+    if (size > (OldPLABSize / 2)) {
+      // Allocate this object directly
+      // TODO: Update object start array
+      obj = (oop) old_gen()->cas_allocate(size);
+    } else {
+      // Flush and fill
+      _old_lab.flush();
+
+      HeapWord* lab_base = old_gen()->cas_allocate(OldPLABSize);
+      if (lab_base != NULL) {
+#ifdef ASSERT
+        // Delay initialization of the promotion LAB (PLAB)
+        // This exposes uninitialized PLABs to card table processing.
+        if (GCWorkerDelayMillis > 0) {
+          os::sleep(Thread::current(), GCWorkerDelayMillis, false);
+        }
+#endif // ASSERT
+        _old_lab.initialize(MemRegion(lab_base, OldPLABSize));
+
+        // Try allocation in the old generation LAB again
+        // TODO: Update object start array
+        obj = (oop) _old_lab.allocate(size);
+      }
+    }
+  }
+
+  if (obj == NULL) {
+    _old_gen_is_full = true;
+  }
+
+  return obj;
+}
 
 inline void PSPromotionManager::process_popped_location_depth(StarTask p) {
   if (is_oop_masked(p)) {
--- old/src/share/vm/gc_implementation/parallelScavenge/psScavenge.cpp	2015-06-16 10:25:43.358460516 -0700
+++ new/src/share/vm/gc_implementation/parallelScavenge/psScavenge.cpp	2015-06-16 10:25:43.286462240 -0700
@@ -126,9 +126,10 @@
 };
 
 class PSPromotionFailedClosure : public ObjectClosure {
+ public:
   virtual void do_object(oop obj) {
     if (obj->is_forwarded()) {
-      obj->init_mark();
+      obj->convert_to_unmarked();
     }
   }
 };
@@ -231,7 +232,7 @@
 
   const bool scavenge_done = PSScavenge::invoke_no_policy();
   const bool need_full_gc = !scavenge_done ||
-    policy->should_full_GC(heap->old_gen()->free_in_bytes());
+      policy->should_full_GC(heap->old_gen()->free_in_bytes());
   bool full_gc_done = false;
 
   if (UsePerfData) {
@@ -329,12 +330,19 @@
     ResourceMark rm;
     HandleMark hm;
 
+    gclog_or_tty->date_stamp(PrintGC && PrintGCDateStamps);
     TraceCPUTime tcpu(PrintGCDetails, true, gclog_or_tty);
     GCTraceTime t1(GCCauseString("GC", gc_cause), PrintGC, !PrintGCDetails, NULL, _gc_tracer.gc_id());
     TraceCollectorStats tcs(counters());
     TraceMemoryManagerStats tms(false /* not full GC */,gc_cause);
 
-    if (TraceGen0Time) accumulated_time()->start();
+    if (TraceGen0Time) {
+      accumulated_time()->start();
+    }
+
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSScavenge::invoke_no_policy: Starting minor GC...");
+    }
 
     // Let the size policy know we're starting
     size_policy->minor_collection_begin();
@@ -436,7 +444,11 @@
 
     scavenge_midpoint.update();
 
-    // Process reference objects discovered during scavenge
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSScavenge::invoke_no_policy: Midpoint of minor GC...");
+    }
+
+    // Process discovered reference objects
     {
       GCTraceTime tm("References", false, false, &_gc_timer, _gc_tracer.gc_id());
 
@@ -656,7 +668,13 @@
       CardTableExtension::verify_all_young_refs_imprecise();
     }
 
-    if (TraceGen0Time) accumulated_time()->stop();
+    if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 1) {
+      tty->print_cr("PSScavenge::invoke_no_policy: Finished minor GC");
+    }
+
+    if (TraceGen0Time) {
+      accumulated_time()->stop();
+    }
 
     if (PrintGC) {
       if (PrintGCDetails) {
--- old/src/share/vm/gc_implementation/shared/markSweep.cpp	2015-06-16 10:25:43.638453812 -0700
+++ new/src/share/vm/gc_implementation/shared/markSweep.cpp	2015-06-16 10:25:43.566455536 -0700
@@ -124,22 +124,22 @@
 
 void MarkSweep::restore_marks() {
   assert(_preserved_oop_stack.size() == _preserved_mark_stack.size(),
-         "inconsistent preserved oop stacks");
+      "inconsistent preserved oop stacks");
   if (PrintGC && Verbose) {
     gclog_or_tty->print_cr("Restoring %d marks",
-                           _preserved_count + _preserved_oop_stack.size());
+        _preserved_count + _preserved_oop_stack.size());
   }
 
-  // restore the marks we saved earlier
+  // Restore the marks we saved earlier
   for (size_t i = 0; i < _preserved_count; i++) {
     _preserved_marks[i].restore();
   }
 
-  // deal with the overflow
+  // Deal with the overflow
   while (!_preserved_oop_stack.is_empty()) {
-    oop obj       = _preserved_oop_stack.pop();
-    markOop mark  = _preserved_mark_stack.pop();
-    obj->set_mark(mark);
+    oop obj = _preserved_oop_stack.pop();
+    markOop mark = _preserved_mark_stack.pop();
+    obj->restore_mark(mark);
   }
 }
 
--- old/src/share/vm/gc_implementation/shared/markSweep.hpp	2015-06-16 10:25:43.894447683 -0700
+++ new/src/share/vm/gc_implementation/shared/markSweep.hpp	2015-06-16 10:25:43.826449311 -0700
@@ -187,7 +187,7 @@
   }
 
   void restore() {
-    _obj->set_mark(_mark);
+    _obj->restore_mark(_mark);
   }
 };
 
--- old/src/share/vm/gc_implementation/shared/markSweep.inline.hpp	2015-06-16 10:25:44.154441448 -0700
+++ new/src/share/vm/gc_implementation/shared/markSweep.inline.hpp	2015-06-16 10:25:44.082443177 -0700
@@ -42,11 +42,11 @@
     G1StringDedup::enqueue_from_mark(obj);
   }
 #endif
-  // some marks may contain information we need to preserve so we store them away
-  // and overwrite the mark.  We'll restore it at the end of markSweep.
-  markOop mark = obj->mark();
-  obj->set_mark(markOopDesc::prototype()->set_marked());
 
+  // Some marks may contain information we need to preserve, so we store them
+  // away and overwrite the mark. We'll restore it at the end of Mark-Sweep.
+  markOop mark = obj->mark();
+  obj->convert_to_marked();
   if (mark->must_be_preserved(obj)) {
     preserve_mark(obj, mark);
   }
@@ -72,12 +72,29 @@
 }
 
 template <class T> inline void MarkSweep::mark_and_push(T* p) {
-//  assert(Universe::heap()->is_in_reserved(p), "should be in object space");
   T heap_oop = oopDesc::load_heap_oop(p);
   if (!oopDesc::is_null(heap_oop)) {
     oop obj = oopDesc::decode_heap_oop_not_null(heap_oop);
-    if (!obj->mark()->is_marked()) {
+    markOop mark = obj->mark();
+    if (!mark->is_marked()) {
+      debug_only(bool is_contained = obj->is_contained());
+      debug_only(bool is_container = obj->is_container());
       mark_object(obj);
+      assert(is_contained == obj->is_contained(),
+          "contained property not preserved");
+      assert(is_container == obj->is_container(),
+          "container property not preserved");
+      if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2)
+      {
+        ResourceMark rm;
+        tty->print_cr(
+            "MarkSweep::mark_and_push: "
+            "type=%s, addr=0x%p, %scontained, %scontainer, mark=0x%p=>0x%p",
+            obj->klass()->signature_name(), (void*) obj,
+            obj->is_contained() ? "" : "not ",
+            obj->is_container() ? "" : "not ",
+            (void*) mark, (void*) (obj->mark()));
+      }
       _marking_stack.push(obj);
     }
   }
--- old/src/share/vm/oops/instanceKlass.cpp	2015-06-16 10:25:44.426434918 -0700
+++ new/src/share/vm/oops/instanceKlass.cpp	2015-06-16 10:25:44.350436743 -0700
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
@@ -50,7 +50,6 @@
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiRedefineClassesTrace.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
-#include "prims/jvmtiThreadState.hpp"
 #include "prims/methodComparator.hpp"
 #include "runtime/fieldDescriptor.hpp"
 #include "runtime/handles.inline.hpp"
@@ -111,7 +110,7 @@
       len = name->utf8_length();                                 \
     }                                                            \
     HS_DTRACE_PROBE4(hotspot, class__initialization__##type,     \
-      data, len, (void *)(clss)->class_loader(), thread_type);           \
+      data, len, SOLARIS_ONLY((void *))(clss)->class_loader(), thread_type);           \
   }
 
 #define DTRACE_CLASSINIT_PROBE_WAIT(type, clss, thread_type, wait) \
@@ -124,7 +123,7 @@
       len = name->utf8_length();                                 \
     }                                                            \
     HS_DTRACE_PROBE5(hotspot, class__initialization__##type,     \
-      data, len, (void *)(clss)->class_loader(), thread_type, wait);     \
+      data, len, SOLARIS_ONLY((void *))(clss)->class_loader(), thread_type, wait);     \
   }
 #else /* USDT2 */
 
@@ -929,16 +928,10 @@
     // Step 10 and 11
     Handle e(THREAD, PENDING_EXCEPTION);
     CLEAR_PENDING_EXCEPTION;
-    // JVMTI has already reported the pending exception
-    // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
-    JvmtiExport::clear_detected_exception((JavaThread*)THREAD);
     {
       EXCEPTION_MARK;
       this_oop->set_initialization_state_and_notify(initialization_error, THREAD);
       CLEAR_PENDING_EXCEPTION;   // ignore any exception thrown, class initialization error is thrown below
-      // JVMTI has already reported the pending exception
-      // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
-      JvmtiExport::clear_detected_exception((JavaThread*)THREAD);
     }
     DTRACE_CLASSINIT_PROBE_WAIT(error, InstanceKlass::cast(this_oop()), -1,wait);
     if (e->is_a(SystemDictionary::Error_klass())) {
@@ -1122,19 +1115,99 @@
 
 instanceOop InstanceKlass::allocate_instance(TRAPS) {
   bool has_finalizer_flag = has_finalizer(); // Query before possible GC
-  int size = size_helper();  // Query before forming handle.
+  int size = size_helper(); // Query before forming handle
 
   KlassHandle h_k(THREAD, this);
+  instanceOop i = (instanceOop) CollectedHeap::obj_allocate(h_k, size,
+      CHECK_NULL);
+  if (has_finalizer_flag && !RegisterFinalizersAtInit) {
+    i = register_finalizer(i, CHECK_NULL);
+  }
+  return i;
+}
 
-  instanceOop i;
+instanceOop InstanceKlass::allocate_instance(int size, TRAPS) {
+  if (TraceObjectLayoutIntrinsics) {
+    ResourceMark rm;
+    tty->print_cr(
+        "InstanceKlass::allocate_instance: type=%s, size=%d",
+        signature_name(), size);
+  }
+
+  assert(size > 0, "invalid size");
 
-  i = (instanceOop)CollectedHeap::obj_allocate(h_k, size, CHECK_NULL);
+  bool has_finalizer_flag = has_finalizer(); // Query before possible GC
+
+  KlassHandle h_k(THREAD, this);
+  instanceOop i = (instanceOop) CollectedHeap::obj_allocate(h_k, size,
+      CHECK_NULL);
   if (has_finalizer_flag && !RegisterFinalizersAtInit) {
     i = register_finalizer(i, CHECK_NULL);
   }
   return i;
 }
 
+instanceOop InstanceKlass::allocate_instance_at(address obj_addr,
+    bool is_contained, bool is_container, TRAPS) {
+  if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+    ResourceMark rm;
+    tty->print_cr(
+        "InstanceKlass::allocate_instance_at: "
+        "type=%s, addr=0x%p, %scontained, %scontainer",
+        signature_name(), (void*) obj_addr,
+        is_contained ? "" : "not ",
+        is_container ? "" : "not ");
+  }
+
+  bool has_finalizer_flag = has_finalizer(); // Query before possible GC
+  int size_in_words = size_helper();
+
+  assert(!Universe::heap()->is_gc_active(),
+      "initialization during GC not allowed");
+  assert(obj_addr != NULL, "cannot initialize NULL object");
+  assert(is_ptr_aligned(obj_addr, HeapWordSize), "address not aligned");
+  assert(size_in_words > 0, "invalid size");
+
+  oop obj = (oop) obj_addr;
+  obj->set_klass_gap(0);
+  markOop mark;
+  if (UseBiasedLocking) {
+    mark = prototype_header();
+  } else {
+    mark = markOopDesc::prototype();
+  }
+  if (is_contained) {
+    mark = mark->set_contained();
+  }
+  if (is_container) {
+    mark = mark->set_container();
+  }
+  obj->set_mark(mark);
+  obj->set_klass(this);
+
+  // Support for JVMTI VMObjectAlloc event (no-op if not enabled)
+  JvmtiExport::vm_object_alloc_event_collector(obj);
+
+  if (DTraceAllocProbes) {
+    // Support for DTrace object-alloc probe (no-op most of the time)
+    if (name() != NULL) {
+      SharedRuntime::dtrace_object_alloc(obj, size_in_words);
+    }
+  }
+
+  instanceOop i = (instanceOop) obj;
+  if (has_finalizer_flag && !RegisterFinalizersAtInit) {
+    // For SA we need to do register_finalizer
+    // in a separate pass. Otherwise we might get inconsistent heap
+    // For now we just disallow SAs with finalizers
+    // See issue #28 for details.
+    guarantee( ((is_contained || is_container) == false), \
+            err_msg("SA and finalizers don't work together for now. See issue #28 for details"));
+    i = register_finalizer(i, CHECK_NULL);
+  }
+  return i;
+}
+
 void InstanceKlass::check_valid_for_instantiation(bool throwError, TRAPS) {
   if (is_interface() || is_abstract()) {
     ResourceMark rm(THREAD);
@@ -2181,26 +2254,156 @@
   }                                                                      \
 }
 
+#define InstanceKlass_CONTAINER_ITERATE_INCL_RCO(container, do_oop)         \
+{                                                                           \
+  address addr = ((address) container) +                                    \
+      org_ObjectLayout_AbstractStructuredArray::bodySize(container);        \
+  jlong length =                                                            \
+      org_ObjectLayout_AbstractStructuredArray::length(container);          \
+  jlong element_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::elementSize(container);     \
+  jlong padding_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::paddingSize(container);     \
+                                                                            \
+  oop* p;                                                                   \
+  for (jlong i = 0; i < length; i++) {                                      \
+    oop rco = (oop) addr;                                                   \
+    p = &rco;                                                               \
+    do_oop;                                                                 \
+    oop obj = (oop) (addr + padding_size);                                  \
+    p = &obj;                                                               \
+    do_oop;                                                                 \
+    addr += element_size;                                                   \
+  }                                                                         \
+}
+
+#define InstanceKlass_CONTAINER_ITERATE_EXCL_RCO(container, do_oop)         \
+{                                                                           \
+  address addr = ((address) container) +                                    \
+      org_ObjectLayout_AbstractStructuredArray::bodySize(container);        \
+  jlong length =                                                            \
+      org_ObjectLayout_AbstractStructuredArray::length(container);          \
+  jlong element_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::elementSize(container);     \
+  jlong padding_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::paddingSize(container);     \
+                                                                            \
+  oop* p;                                                                   \
+  for (jlong i = 0; i < length; i++) {                                      \
+    oop obj = (oop) (addr + padding_size);                                  \
+    p = &obj;                                                               \
+    do_oop;                                                                 \
+    addr += element_size;                                                   \
+  }                                                                         \
+}
+
+#define InstanceKlass_CONTAINER_REVERSE_ITERATE_INCL_RCO(container, do_oop) \
+{                                                                           \
+  address addr = ((address) container) +                                    \
+      org_ObjectLayout_AbstractStructuredArray::bodySize(container);        \
+  jlong length =                                                            \
+      org_ObjectLayout_AbstractStructuredArray::length(container);          \
+  jlong element_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::elementSize(container);     \
+  jlong padding_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::paddingSize(container);     \
+                                                                            \
+  addr += (length - 1) * element_size;                                      \
+  oop* p;                                                                   \
+  for (jlong i = length - 1; i >= 0; i--) {                                 \
+    oop obj = (oop) (addr + padding_size);                                  \
+    p = &obj;                                                               \
+    do_oop;                                                                 \
+    oop rco = (oop) addr;                                                   \
+    p = &rco;                                                               \
+    do_oop;                                                                 \
+    addr -= element_size;                                                   \
+  }                                                                         \
+}
+
+#define InstanceKlass_CONTAINER_REVERSE_ITERATE_EXCL_RCO(container, do_oop) \
+{                                                                           \
+  address addr = ((address) container) +                                    \
+      org_ObjectLayout_AbstractStructuredArray::bodySize(container);        \
+  jlong length =                                                            \
+      org_ObjectLayout_AbstractStructuredArray::length(container);          \
+  jlong element_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::elementSize(container);     \
+  jlong padding_size =                                                      \
+      org_ObjectLayout_AbstractStructuredArray::paddingSize(container);     \
+                                                                            \
+  addr += (length - 1) * element_size;                                      \
+  oop* p;                                                                   \
+  for (jlong i = length - 1; i >= 0; i--) {                                 \
+    oop obj = (oop) (addr + padding_size);                                  \
+    p = &obj;                                                               \
+    do_oop;                                                                 \
+    addr -= element_size;                                                   \
+  }                                                                         \
+}
+
 void InstanceKlass::oop_follow_contents(oop obj) {
-  assert(obj != NULL, "can't follow the content of NULL object");
+  assert(obj != NULL, "can't follow the contents of NULL object");
   MarkSweep::follow_klass(obj->klass());
-  InstanceKlass_OOP_MAP_ITERATE( \
-    obj, \
-    MarkSweep::mark_and_push(p), \
-    assert_is_in_closed_subset)
+  InstanceKlass_OOP_MAP_ITERATE(   \
+      obj,                         \
+      MarkSweep::mark_and_push(p), \
+      assert_is_in_closed_subset)
+
+  if (obj->is_container()) {
+    InstanceKlass_CONTAINER_ITERATE_INCL_RCO( \
+        obj,                                  \
+        MarkSweep::mark_and_push(p))
+  }
+}
+
+int InstanceKlass::oop_adjust_pointers(oop obj) {
+  int size = size_helper();
+  InstanceKlass_OOP_MAP_ITERATE(    \
+      obj,                          \
+      MarkSweep::adjust_pointer(p), \
+      assert_is_in)
+
+  // There is no need in special handling of containers here. The caller
+  // sequentially scans a space from its bottom to the end. Contained objects
+  // (and their corresponding RCO objects) will be processed one-by-one just
+  // after their containers. To the caller they look like standard Java objects.
+
+  return size;
 }
 
 #if INCLUDE_ALL_GCS
-void InstanceKlass::oop_follow_contents(ParCompactionManager* cm,
-                                        oop obj) {
-  assert(obj != NULL, "can't follow the content of NULL object");
+void InstanceKlass::oop_push_contents(PSPromotionManager* pm, oop obj) {
+  if (obj->is_container()) {
+    InstanceKlass_CONTAINER_REVERSE_ITERATE_EXCL_RCO( \
+        obj,                                          \
+        (*p)->push_contents(pm))
+  }
+
+  InstanceKlass_OOP_MAP_REVERSE_ITERATE(    \
+      obj,                                  \
+      if (PSScavenge::should_scavenge(p)) { \
+        pm->claim_or_forward_depth(p);      \
+      },                                    \
+      assert_nothing)
+}
+
+void InstanceKlass::oop_follow_contents(ParCompactionManager* cm, oop obj) {
+  assert(obj != NULL, "can't follow the contents of NULL object");
   PSParallelCompact::follow_klass(cm, obj->klass());
-  // Only mark the header and let the scan of the meta-data mark
-  // everything else.
-  InstanceKlass_OOP_MAP_ITERATE( \
-    obj, \
-    PSParallelCompact::mark_and_push(cm, p), \
-    assert_is_in)
+  InstanceKlass_OOP_MAP_ITERATE(               \
+      obj,                                     \
+      PSParallelCompact::mark_and_push(cm, p), \
+      assert_is_in)
+}
+
+int InstanceKlass::oop_update_pointers(ParCompactionManager* cm, oop obj) {
+  int size = size_helper();
+  InstanceKlass_OOP_MAP_ITERATE(            \
+      obj,                                  \
+      PSParallelCompact::adjust_pointer(p), \
+      assert_is_in)
+  return size;
 }
 #endif // INCLUDE_ALL_GCS
 
@@ -2270,36 +2473,6 @@
 ALL_OOP_OOP_ITERATE_CLOSURES_2(InstanceKlass_OOP_OOP_ITERATE_BACKWARDS_DEFN)
 #endif // INCLUDE_ALL_GCS
 
-int InstanceKlass::oop_adjust_pointers(oop obj) {
-  int size = size_helper();
-  InstanceKlass_OOP_MAP_ITERATE( \
-    obj, \
-    MarkSweep::adjust_pointer(p), \
-    assert_is_in)
-  return size;
-}
-
-#if INCLUDE_ALL_GCS
-void InstanceKlass::oop_push_contents(PSPromotionManager* pm, oop obj) {
-  InstanceKlass_OOP_MAP_REVERSE_ITERATE( \
-    obj, \
-    if (PSScavenge::should_scavenge(p)) { \
-      pm->claim_or_forward_depth(p); \
-    }, \
-    assert_nothing )
-}
-
-int InstanceKlass::oop_update_pointers(ParCompactionManager* cm, oop obj) {
-  int size = size_helper();
-  InstanceKlass_OOP_MAP_ITERATE( \
-    obj, \
-    PSParallelCompact::adjust_pointer(p), \
-    assert_is_in)
-  return size;
-}
-
-#endif // INCLUDE_ALL_GCS
-
 void InstanceKlass::clean_implementors_list(BoolObjectClosure* is_alive) {
   assert(class_loader_data()->is_alive(is_alive), "this klass should be live");
   if (is_interface()) {
@@ -2805,33 +2978,30 @@
 // not yet in the vtable due to concurrent subclass define and superinterface
 // redefinition
 // Note: those in the vtable, should have been updated via adjust_method_entries
-void InstanceKlass::adjust_default_methods(InstanceKlass* holder, bool* trace_name_printed) {
+void InstanceKlass::adjust_default_methods(Method** old_methods, Method** new_methods,
+                                           int methods_length, bool* trace_name_printed) {
   // search the default_methods for uses of either obsolete or EMCP methods
   if (default_methods() != NULL) {
-    for (int index = 0; index < default_methods()->length(); index ++) {
-      Method* old_method = default_methods()->at(index);
-      if (old_method == NULL || old_method->method_holder() != holder || !old_method->is_old()) {
-        continue; // skip uninteresting entries
-      }
-      assert(!old_method->is_deleted(), "default methods may not be deleted");
-
-      Method* new_method = holder->method_with_idnum(old_method->orig_method_idnum());
-
-      assert(new_method != NULL, "method_with_idnum() should not be NULL");
-      assert(old_method != new_method, "sanity check");
-
-      default_methods()->at_put(index, new_method);
-      if (RC_TRACE_IN_RANGE(0x00100000, 0x00400000)) {
-        if (!(*trace_name_printed)) {
-          // RC_TRACE_MESG macro has an embedded ResourceMark
-          RC_TRACE_MESG(("adjust: klassname=%s default methods from name=%s",
-                         external_name(),
-                         old_method->method_holder()->external_name()));
-          *trace_name_printed = true;
+    for (int j = 0; j < methods_length; j++) {
+      Method* old_method = old_methods[j];
+      Method* new_method = new_methods[j];
+
+      for (int index = 0; index < default_methods()->length(); index ++) {
+        if (default_methods()->at(index) == old_method) {
+          default_methods()->at_put(index, new_method);
+          if (RC_TRACE_IN_RANGE(0x00100000, 0x00400000)) {
+            if (!(*trace_name_printed)) {
+              // RC_TRACE_MESG macro has an embedded ResourceMark
+              RC_TRACE_MESG(("adjust: klassname=%s default methods from name=%s",
+                             external_name(),
+                             old_method->method_holder()->external_name()));
+              *trace_name_printed = true;
+            }
+            RC_TRACE(0x00100000, ("default method update: %s(%s) ",
+                                  new_method->name()->as_C_string(),
+                                  new_method->signature()->as_C_string()));
+          }
         }
-        RC_TRACE(0x00100000, ("default method update: %s(%s) ",
-                              new_method->name()->as_C_string(),
-                              new_method->signature()->as_C_string()));
       }
     }
   }
@@ -3754,22 +3924,6 @@
 } // end has_previous_version()
 
 
-InstanceKlass* InstanceKlass::get_klass_version(int version) {
-  if (constants()->version() == version) {
-    return this;
-  }
-  PreviousVersionWalker pvw(Thread::current(), (InstanceKlass*)this);
-  for (PreviousVersionNode * pv_node = pvw.next_previous_version();
-       pv_node != NULL; pv_node = pvw.next_previous_version()) {
-    ConstantPool* prev_cp = pv_node->prev_constant_pool();
-    if (prev_cp->version() == version) {
-      return prev_cp->pool_holder();
-    }
-  }
-  return NULL; // None found
-}
-
-
 Method* InstanceKlass::method_with_idnum(int idnum) {
   Method* m = NULL;
   if (idnum < methods()->length()) {
@@ -3788,37 +3942,6 @@
   return m;
 }
 
-
-Method* InstanceKlass::method_with_orig_idnum(int idnum) {
-  if (idnum >= methods()->length()) {
-    return NULL;
-  }
-  Method* m = methods()->at(idnum);
-  if (m != NULL && m->orig_method_idnum() == idnum) {
-    return m;
-  }
-  // Obsolete method idnum does not match the original idnum
-  for (int index = 0; index < methods()->length(); ++index) {
-    m = methods()->at(index);
-    if (m->orig_method_idnum() == idnum) {
-      return m;
-    }
-  }
-  // None found, return null for the caller to handle.
-  return NULL;
-}
-
-
-Method* InstanceKlass::method_with_orig_idnum(int idnum, int version) {
-  InstanceKlass* holder = get_klass_version(version);
-  if (holder == NULL) {
-    return NULL; // The version of klass is gone, no method is found
-  }
-  Method* method = holder->method_with_orig_idnum(idnum);
-  return method;
-}
-
-
 jint InstanceKlass::get_cached_class_file_len() {
   return VM_RedefineClasses::get_cached_class_file_len(_cached_class_file);
 }
--- old/src/share/vm/oops/instanceKlass.hpp	2015-06-16 10:25:44.742427332 -0700
+++ new/src/share/vm/oops/instanceKlass.hpp	2015-06-16 10:25:44.674428965 -0700
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2014, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
@@ -358,8 +358,6 @@
   Array<Method*>* methods() const          { return _methods; }
   void set_methods(Array<Method*>* a)      { _methods = a; }
   Method* method_with_idnum(int idnum);
-  Method* method_with_orig_idnum(int idnum);
-  Method* method_with_orig_idnum(int idnum, int version);
 
   // method ordering
   Array<int>* method_ordering() const     { return _method_ordering; }
@@ -660,7 +658,6 @@
     return _previous_versions;
   }
 
-  InstanceKlass* get_klass_version(int version);
   static void purge_previous_versions(InstanceKlass* ik);
 
   // JVMTI: Support for caching a class file before it is modified by an agent that can do retransformation
@@ -754,11 +751,25 @@
   Array<AnnotationArray*>* fields_type_annotations() const {
     return (_annotations != NULL) ? _annotations->fields_type_annotations() : NULL;
   }
-  // allocation
+
+  // Instance allocation
   instanceOop allocate_instance(TRAPS);
+  instanceHandle allocate_instance_handle(TRAPS) {
+    return instanceHandle(THREAD, allocate_instance(THREAD));
+  }
+  instanceOop allocate_instance(int size, TRAPS);
+  instanceHandle allocate_instance_handle(int size, TRAPS) {
+    return instanceHandle(THREAD, allocate_instance(size, THREAD));
+  }
 
-  // additional member function to return a handle
-  instanceHandle allocate_instance_handle(TRAPS)      { return instanceHandle(THREAD, allocate_instance(THREAD)); }
+  // Instance allocation at the given address (in preallocated memory region)
+  instanceOop allocate_instance_at(address obj_addr,
+      bool is_contained, bool is_container, TRAPS);
+  instanceHandle allocate_instance_at_handle(address obj_addr,
+      bool is_contained, bool is_container, TRAPS) {
+    return instanceHandle(THREAD, allocate_instance_at(obj_addr,
+        is_contained, is_container, THREAD));
+  }
 
   objArrayOop allocate_objArray(int n, int length, TRAPS);
   // Helper function
@@ -962,12 +973,17 @@
   Method* method_at_itable(Klass* holder, int index, TRAPS);
 
 #if INCLUDE_JVMTI
-  void adjust_default_methods(InstanceKlass* holder, bool* trace_name_printed);
+  void adjust_default_methods(Method** old_methods, Method** new_methods,
+                              int methods_length, bool* trace_name_printed);
 #endif // INCLUDE_JVMTI
 
-  // Garbage collection
+  // Garbage collection support
+  // Mark-Sweep GCs
   void oop_follow_contents(oop obj);
-  int  oop_adjust_pointers(oop obj);
+  int oop_adjust_pointers(oop obj);
+
+  // Parallel Scavenge and Parallel Old GCs
+  PARALLEL_GC_DECLS
 
   void clean_implementors_list(BoolObjectClosure* is_alive);
   void clean_method_data(BoolObjectClosure* is_alive);
@@ -992,9 +1008,6 @@
   static void notify_unload_class(InstanceKlass* ik);
   static void release_C_heap_structures(InstanceKlass* ik);
 
-  // Parallel Scavenge and Parallel Old
-  PARALLEL_GC_DECLS
-
   // Naming
   const char* signature_name() const;
 
--- old/src/share/vm/oops/klass.cpp	2015-06-16 10:25:45.014420801 -0700
+++ new/src/share/vm/oops/klass.cpp	2015-06-16 10:25:44.946422434 -0700
@@ -392,6 +392,10 @@
   debug_only(verify();)
 }
 
+bool Klass::oop_is_container() const {
+  return is_subclass_of(SystemDictionary::AbstractStructuredArray_klass());
+}
+
 bool Klass::is_loader_alive(BoolObjectClosure* is_alive) {
 #ifdef ASSERT
   // The class is alive iff the class loader is alive.
--- old/src/share/vm/oops/klass.hpp	2015-06-16 10:25:45.266414735 -0700
+++ new/src/share/vm/oops/klass.hpp	2015-06-16 10:25:45.198416373 -0700
@@ -506,11 +506,12 @@
   //     and the package separators as '/'.
   virtual const char* signature_name() const;
 
-  // garbage collection support
+  // Garbage collection support
+  // Mark-Sweep GCs
   virtual void oop_follow_contents(oop obj) = 0;
-  virtual int  oop_adjust_pointers(oop obj) = 0;
+  virtual int oop_adjust_pointers(oop obj) = 0;
 
-  // Parallel Scavenge and Parallel Old
+  // Parallel Scavenge and Parallel Old GCs
   PARALLEL_GC_DECLS_PV
 
   // type testing operations
@@ -535,18 +536,23 @@
   }
  public:
   #endif
-  inline  bool oop_is_instance()            const { return assert_same_query(
-                                                    layout_helper_is_instance(layout_helper()),
-                                                    oop_is_instance_slow()); }
-  inline  bool oop_is_array()               const { return assert_same_query(
-                                                    layout_helper_is_array(layout_helper()),
-                                                    oop_is_array_slow()); }
-  inline  bool oop_is_objArray()            const { return assert_same_query(
-                                                    layout_helper_is_objArray(layout_helper()),
-                                                    oop_is_objArray_slow()); }
-  inline  bool oop_is_typeArray()           const { return assert_same_query(
-                                                    layout_helper_is_typeArray(layout_helper()),
-                                                    oop_is_typeArray_slow()); }
+  inline bool oop_is_instance() const {
+    return assert_same_query(layout_helper_is_instance(layout_helper()),
+        oop_is_instance_slow());
+  }
+  bool oop_is_container() const;
+  inline bool oop_is_array() const {
+    return assert_same_query(layout_helper_is_array(layout_helper()),
+        oop_is_array_slow());
+  }
+  inline bool oop_is_objArray() const {
+    return assert_same_query(layout_helper_is_objArray(layout_helper()),
+        oop_is_objArray_slow());
+  }
+  inline bool oop_is_typeArray() const {
+    return assert_same_query(layout_helper_is_typeArray(layout_helper()),
+        oop_is_typeArray_slow());
+  }
   #undef assert_same_query
 
   // Access flags
--- old/src/share/vm/oops/klassPS.hpp	2015-06-16 10:25:45.538408187 -0700
+++ new/src/share/vm/oops/klassPS.hpp	2015-06-16 10:25:45.462410018 -0700
@@ -25,28 +25,23 @@
 #ifndef SHARE_VM_OOPS_KLASSPS_HPP
 #define SHARE_VM_OOPS_KLASSPS_HPP
 
-  // Expands to Parallel Scavenge and Parallel Old declarations
-
 #include "utilities/macros.hpp"
 
+// Macros that expand to Parallel Scavenge and Parallel Old GC related
+// declarations
+
 #if INCLUDE_ALL_GCS
-#define PARALLEL_GC_DECLS \
-  virtual void oop_push_contents(PSPromotionManager* pm, oop obj);          \
-  /* Parallel Old GC support                                                \
-                                                                            \
-   The 2-arg version of oop_update_pointers is for objects that are         \
-   known not to cross chunk boundaries.  The 4-arg version is for           \
-   objects that do (or may) cross chunk boundaries; it updates only those   \
-   oops that are in the region [beg_addr, end_addr).  */                    \
-  virtual void oop_follow_contents(ParCompactionManager* cm, oop obj);      \
-  virtual int  oop_update_pointers(ParCompactionManager* cm, oop obj);
+#define PARALLEL_GC_DECLS                                              \
+  virtual void oop_push_contents(PSPromotionManager* pm, oop obj);     \
+  virtual void oop_follow_contents(ParCompactionManager* cm, oop obj); \
+  virtual int oop_update_pointers(ParCompactionManager* cm, oop obj);
 
 // Pure virtual version for klass.hpp
-#define PARALLEL_GC_DECLS_PV \
-  virtual void oop_push_contents(PSPromotionManager* pm, oop obj) = 0;      \
-  virtual void oop_follow_contents(ParCompactionManager* cm, oop obj) = 0;  \
-  virtual int  oop_update_pointers(ParCompactionManager* cm, oop obj) = 0;
-#else  // INCLUDE_ALL_GCS
+#define PARALLEL_GC_DECLS_PV                                               \
+  virtual void oop_push_contents(PSPromotionManager* pm, oop obj) = 0;     \
+  virtual void oop_follow_contents(ParCompactionManager* cm, oop obj) = 0; \
+  virtual int oop_update_pointers(ParCompactionManager* cm, oop obj) = 0;
+#else // INCLUDE_ALL_GCS
 #define PARALLEL_GC_DECLS
 #define PARALLEL_GC_DECLS_PV
 #endif // INCLUDE_ALL_GCS
--- old/src/share/vm/oops/markOop.hpp	2015-06-16 10:25:45.802401832 -0700
+++ new/src/share/vm/oops/markOop.hpp	2015-06-16 10:25:45.730403566 -0700
@@ -53,6 +53,25 @@
 //  narrowOop:32 unused:24 cms_free:1 unused:4 promo_bits:3 ----->| (COOPs && CMS promoted object)
 //  unused:21 size:35 -->| cms_free:1 unused:7 ------------------>| (COOPs && CMS free block)
 //
+//
+//  * SPECIAL NOTE ON THE CURRENT IMPLEMENTATION OF STRUCTURED ARRAYS AND OBJECTLAYOUT *
+//
+//  Structured arrays ares only implemented for 64-bit VM without Compressed OOPs
+//  At this moment biased locking is not supported
+//
+//  Here is the bit encoding of the mark word
+//
+//  64 bits:
+//  --------
+//  unused:25 hash:31 -->| unused:1 | container:1 | contained:1 | age:2 |biased_lock:1 lock:2 (normal object)
+//
+//  There is a TODO item to investigate and to relocate container and 
+//  contained bits to bits 62 and 63 and restore 4 bits for age description.
+//
+//  * END OF SPECIAL NOTE ON THE CURRENT IMPLEMENTATION OF STRUCTURED ARRAYS AND OBJECTLAYOUT *
+//
+//
+//
 //  - hash contains the identity hash value: largest value is
 //    31 bits, see os::random().  Also, 64-bit vm's require
 //    a hash value no bigger than 32 bits because they will not
@@ -101,19 +120,25 @@
 class ObjectMonitor;
 class JavaThread;
 
-class markOopDesc: public oopDesc {
- private:
+class markOopDesc : public oopDesc {
+private:
   // Conversion
-  uintptr_t value() const { return (uintptr_t) this; }
+  uintptr_t value() const {
+    return (uintptr_t) this;
+  }
 
- public:
+public:
   // Constants
-  enum { age_bits                 = 4,
-         lock_bits                = 2,
+  enum { lock_bits                = 2,
          biased_lock_bits         = 1,
-         max_hash_bits            = BitsPerWord - age_bits - lock_bits - biased_lock_bits,
-         hash_bits                = max_hash_bits > 31 ? 31 : max_hash_bits,
+         age_bits                 = 2,
+         contained_bits           = 1,
+         container_bits           = 1,
          cms_bits                 = LP64_ONLY(1) NOT_LP64(0),
+         max_hash_bits            = BitsPerWord - lock_bits - biased_lock_bits -
+                                    age_bits - contained_bits - container_bits -
+                                    cms_bits,
+         hash_bits                = max_hash_bits > 31 ? 31 : max_hash_bits,
          epoch_bits               = 2
   };
 
@@ -121,8 +146,10 @@
   // contiguous to the lock bits.
   enum { lock_shift               = 0,
          biased_lock_shift        = lock_bits,
-         age_shift                = lock_bits + biased_lock_bits,
-         cms_shift                = age_shift + age_bits,
+         age_shift                = biased_lock_shift + biased_lock_bits,
+         contained_shift          = age_shift + age_bits,
+         container_shift          = contained_shift + contained_bits,
+         cms_shift                = container_shift + container_bits,
          hash_shift               = cms_shift + cms_bits,
          epoch_shift              = hash_shift
   };
@@ -130,30 +157,36 @@
   enum { lock_mask                = right_n_bits(lock_bits),
          lock_mask_in_place       = lock_mask << lock_shift,
          biased_lock_mask         = right_n_bits(lock_bits + biased_lock_bits),
-         biased_lock_mask_in_place= biased_lock_mask << lock_shift,
-         biased_lock_bit_in_place = 1 << biased_lock_shift,
+         biased_lock_mask_in_place = biased_lock_mask << lock_shift,
+         biased_lock_bit_in_place = nth_bit(biased_lock_shift),
          age_mask                 = right_n_bits(age_bits),
          age_mask_in_place        = age_mask << age_shift,
-         epoch_mask               = right_n_bits(epoch_bits),
-         epoch_mask_in_place      = epoch_mask << epoch_shift,
+         contained_mask           = right_n_bits(contained_bits),
+         contained_mask_in_place  = contained_mask << contained_shift,
+         container_mask           = right_n_bits(container_bits),
+         container_mask_in_place  = container_mask << container_shift,
          cms_mask                 = right_n_bits(cms_bits),
-         cms_mask_in_place        = cms_mask << cms_shift
+         cms_mask_in_place        = cms_mask << cms_shift,
 #ifndef _WIN64
-         ,hash_mask               = right_n_bits(hash_bits),
-         hash_mask_in_place       = (address_word)hash_mask << hash_shift
-#endif
-  };
-
-  // Alignment of JavaThread pointers encoded in object header required by biased locking
-  enum { biased_lock_alignment    = 2 << (epoch_shift + epoch_bits)
+         hash_mask                = right_n_bits(hash_bits),
+         hash_mask_in_place       = (address_word) hash_mask << hash_shift,
+#endif // _WIN64
+         epoch_mask               = right_n_bits(epoch_bits),
+         epoch_mask_in_place      = epoch_mask << epoch_shift
   };
 
 #ifdef _WIN64
-    // These values are too big for Win64
-    const static uintptr_t hash_mask = right_n_bits(hash_bits);
-    const static uintptr_t hash_mask_in_place  =
-                            (address_word)hash_mask << hash_shift;
-#endif
+  // These values are too big for Win64.
+  const static uintptr_t hash_mask = right_n_bits(hash_bits);
+  const static uintptr_t hash_mask_in_place =
+      (address_word) hash_mask << hash_shift;
+#endif // _WIN64
+
+  enum { contained_mask_in_place_when_forwarded = nth_bit(2) };
+
+  // Alignment of JavaThread pointers encoded in object header required by
+  // biased locking
+  enum { biased_lock_alignment    = 2 << (epoch_shift + epoch_bits) };
 
   enum { locked_value             = 0,
          unlocked_value           = 1,
@@ -162,9 +195,9 @@
          biased_lock_pattern      = 5
   };
 
-  enum { no_hash                  = 0 };  // no hash value assigned
+  enum { no_hash                  = 0 }; // No hash value assigned
 
-  enum { no_hash_in_place         = (address_word)no_hash << hash_shift,
+  enum { no_hash_in_place         = (address_word) no_hash << hash_shift,
          no_lock_in_place         = unlocked_value
   };
 
@@ -172,56 +205,61 @@
 
   enum { max_bias_epoch           = epoch_mask };
 
-  // Biased Locking accessors.
-  // These must be checked by all code which calls into the
-  // ObjectSynchronizer and other code. The biasing is not understood
-  // by the lower-level CAS-based locking code, although the runtime
-  // fixes up biased locks to be compatible with it when a bias is
-  // revoked.
+  // Lock accessors (note that these assume lock_shift == 0)
+  bool is_locked() const {
+    return mask_bits(value(), lock_mask_in_place) != unlocked_value;
+  }
+  bool is_unlocked() const {
+    return mask_bits(value(), biased_lock_mask_in_place) == unlocked_value;
+  }
+  bool is_marked() const {
+    return mask_bits(value(), lock_mask_in_place) == marked_value;
+  }
+  bool is_neutral() const {
+    return mask_bits(value(), biased_lock_mask_in_place) == unlocked_value;
+  }
+
+  // Biased locking accessors
+  // These must be checked by all code which calls into the ObjectSynchronizer
+  // and other code. The biasing is not understood by the lower-level CAS-based
+  // locking code, although the runtime fixes up biased locks to be compatible
+  // with it when a bias is revoked.
   bool has_bias_pattern() const {
-    return (mask_bits(value(), biased_lock_mask_in_place) == biased_lock_pattern);
+    return mask_bits(value(), biased_lock_mask_in_place) == biased_lock_pattern;
   }
   JavaThread* biased_locker() const {
     assert(has_bias_pattern(), "should not call this otherwise");
-    return (JavaThread*) ((intptr_t) (mask_bits(value(), ~(biased_lock_mask_in_place | age_mask_in_place | epoch_mask_in_place))));
+    return (JavaThread*) ((intptr_t) mask_bits(value(),
+        ~(biased_lock_mask_in_place | age_mask_in_place |
+        contained_mask_in_place | container_mask_in_place |
+        epoch_mask_in_place)));
   }
   // Indicates that the mark has the bias bit set but that it has not
   // yet been biased toward a particular thread
   bool is_biased_anonymously() const {
-    return (has_bias_pattern() && (biased_locker() == NULL));
+    return has_bias_pattern() && (biased_locker() == NULL);
   }
   // Indicates epoch in which this bias was acquired. If the epoch
   // changes due to too many bias revocations occurring, the biases
   // from the previous epochs are all considered invalid.
   int bias_epoch() const {
     assert(has_bias_pattern(), "should not call this otherwise");
-    return (mask_bits(value(), epoch_mask_in_place) >> epoch_shift);
+    return mask_bits(value(), epoch_mask_in_place) >> epoch_shift;
   }
-  markOop set_bias_epoch(int epoch) {
+  markOop set_bias_epoch(int epoch) const {
     assert(has_bias_pattern(), "should not call this otherwise");
-    assert((epoch & (~epoch_mask)) == 0, "epoch overflow");
-    return markOop(mask_bits(value(), ~epoch_mask_in_place) | (epoch << epoch_shift));
+    assert((epoch & ~epoch_mask) == 0, "epoch overflow");
+    return markOop(mask_bits(value(), ~epoch_mask_in_place) |
+        (epoch << epoch_shift));
   }
-  markOop incr_bias_epoch() {
-    return set_bias_epoch((1 + bias_epoch()) & epoch_mask);
+  markOop incr_bias_epoch() const {
+    return set_bias_epoch((bias_epoch() + 1) & epoch_mask);
   }
   // Prototype mark for initialization
   static markOop biased_locking_prototype() {
-    return markOop( biased_lock_pattern );
+    return markOop(biased_lock_pattern);
   }
 
-  // lock accessors (note that these assume lock_shift == 0)
-  bool is_locked()   const {
-    return (mask_bits(value(), lock_mask_in_place) != unlocked_value);
-  }
-  bool is_unlocked() const {
-    return (mask_bits(value(), biased_lock_mask_in_place) == unlocked_value);
-  }
-  bool is_marked()   const {
-    return (mask_bits(value(), lock_mask_in_place) == marked_value);
-  }
-  bool is_neutral()  const { return (mask_bits(value(), biased_lock_mask_in_place) == unlocked_value); }
-
   // Special temporary state of the markOop while being inflated.
   // Code that looks at mark outside a lock need to take this into account.
   bool is_being_inflated() const { return (value() == 0); }
@@ -326,45 +364,95 @@
     return (markOop) (tmp | (bias_epoch << epoch_shift) | (age << age_shift) | biased_lock_pattern);
   }
 
-  // used to encode pointers during GC
-  markOop clear_lock_bits() { return markOop(value() & ~lock_mask_in_place); }
+  markOop set_marked() const {
+    return markOop((value() & ~lock_mask_in_place) | marked_value);
+  }
+  markOop set_unmarked() const {
+    return markOop((value() & ~lock_mask_in_place) | unlocked_value);
+  }
+  markOop clear_lock_and_contained_bits() const {
+    return markOop(value() & ~(lock_mask_in_place |
+        contained_mask_in_place_when_forwarded));
+  }
 
-  // age operations
-  markOop set_marked()   { return markOop((value() & ~lock_mask_in_place) | marked_value); }
-  markOop set_unmarked() { return markOop((value() & ~lock_mask_in_place) | unlocked_value); }
+  // Age operations
+  uint age() const {
+    return mask_bits(value(), age_mask_in_place) >> age_shift;
+  }
+  markOop set_age(uint age) const {
+    assert((age & ~age_mask) == 0, "age overflow");
+    return markOop(mask_bits(value(), ~age_mask_in_place) |
+        (age << age_shift));
+  }
+  markOop incr_age() const {
+    return age() == max_age ? markOop(this) : set_age(age() + 1);
+  }
 
-  uint    age()               const { return mask_bits(value() >> age_shift, age_mask); }
-  markOop set_age(uint v) const {
-    assert((v & ~age_mask) == 0, "shouldn't overflow age field");
-    return markOop((value() & ~age_mask_in_place) | (((uintptr_t)v & age_mask) << age_shift));
+  // Containment bits operations
+  bool is_contained() const {
+    return mask_bits(value(), contained_mask_in_place) != 0;
+  }
+  markOop set_contained() const {
+    return markOop(value() | contained_mask_in_place);
+  }
+  markOop clear_contained() const {
+    return markOop(value() & ~contained_mask_in_place);
+  }
+  bool is_container() const {
+    return mask_bits(value(), container_mask_in_place) != 0;
+  }
+  markOop set_container() const {
+    return markOop(value() | container_mask_in_place);
+  }
+  markOop clear_container() const {
+    return markOop(value() & ~container_mask_in_place);
+  }
+  bool is_contained_when_forwarded() const {
+    return mask_bits(value(), contained_mask_in_place_when_forwarded) != 0;
+  }
+  markOop set_contained_when_forwarded() const {
+    return markOop(value() | contained_mask_in_place_when_forwarded);
+  }
+  markOop clear_contained_when_forwarded() const {
+    return markOop(value() & ~contained_mask_in_place_when_forwarded);
   }
-  markOop incr_age()          const { return age() == max_age ? markOop(this) : set_age(age() + 1); }
 
-  // hash operations
+  // Hash operations
   intptr_t hash() const {
     return mask_bits(value() >> hash_shift, hash_mask);
   }
-
   bool has_no_hash() const {
     return hash() == no_hash;
   }
 
   // Prototype mark for initialization
   static markOop prototype() {
-    return markOop( no_hash_in_place | no_lock_in_place );
+    return markOop(no_hash_in_place | no_lock_in_place);
   }
 
-  // Helper function for restoration of unmarked mark oops during GC
-  static inline markOop prototype_for_object(oop obj);
+  // Helper function for restoration of unmarked markOop's during GC
+  static markOop prototype_for_object(oop obj);
 
   // Debugging
   void print_on(outputStream* st) const;
 
   // Prepare address of oop for placement into mark
-  inline static markOop encode_pointer_as_mark(void* p) { return markOop(p)->set_marked(); }
+  static markOop encode_pointer_as_mark(void* p, bool is_contained) {
+    assert(!markOop(p)->is_contained_when_forwarded(), "encoding will fail");
+    markOop m = markOop(p)->set_marked();
+    if (is_contained) {
+      m = m->set_contained_when_forwarded();
+    }
+    return m;
+  }
 
   // Recover address of oop from encoded form used in mark
-  inline void* decode_pointer() { if (UseBiasedLocking && has_bias_pattern()) return NULL; return clear_lock_bits(); }
+  void* decode_pointer() {
+    if (UseBiasedLocking && has_bias_pattern()) {
+      return NULL;
+    }
+    return clear_lock_and_contained_bits();
+  }
 
   // These markOops indicate cms free chunk blocks and not objects.
   // In 64 bit, the markOop is set to distinguish them from oops.
--- old/src/share/vm/oops/oop.hpp	2015-06-16 10:25:46.074395281 -0700
+++ new/src/share/vm/oops/oop.hpp	2015-06-16 10:25:45.994397211 -0700
@@ -77,8 +77,8 @@
   void    release_set_mark(markOop m);
   markOop cas_set_mark(markOop new_mark, markOop old_mark);
 
-  // Used only to re-initialize the mark word (e.g., of promoted
-  // objects during a GC) -- requires a valid klass pointer
+  // Used only to re-initialize the markword (e.g., of promoted
+  // objects during a GC); requires a valid klass pointer
   void init_mark();
 
   Klass* klass() const;
@@ -275,11 +275,6 @@
   void verify_on(outputStream* st);
   void verify();
 
-  // locking operations
-  bool is_locked()   const;
-  bool is_unlocked() const;
-  bool has_bias_pattern() const;
-
   // asserts
   bool is_oop(bool ignore_mark_word = false) const;
   bool is_oop_or_null(bool ignore_mark_word = false) const;
@@ -287,50 +282,92 @@
   bool is_unlocked_oop() const;
 #endif
 
-  // garbage collection
+  // Locking operations
+  bool is_locked() const;
+  bool is_unlocked() const;
+  bool has_bias_pattern() const;
+
+  // The mark is forwarded to stack when the object is locked; in this case
+  // it is said that the mark is displaced.
+  bool has_displaced_mark() const;
+  markOop displaced_mark() const;
+  void set_displaced_mark(markOop m);
+
+  // Garbage collection support
   bool is_gc_marked() const;
-  // Apply "MarkSweep::mark_and_push" to (the address of) every non-NULL
-  // reference field in "this".
-  void follow_contents(void);
+  bool is_scavengable() const;
+
+  // Mark-Sweep GCs
+  // Apply MarkSweep::mark_and_push() to (the address of) every reference
+  // field in this object
+  void follow_contents();
+
+  // Adjust all pointers in this object to point at their forwarded locations
+  // and return the size of this object
+  int adjust_pointers();
 
 #if INCLUDE_ALL_GCS
-  // Parallel Scavenge
+  // Parallel Scavenge GC
   void push_contents(PSPromotionManager* pm);
 
-  // Parallel Old
-  void update_contents(ParCompactionManager* cm);
-
+  // Parallel Old GC
   void follow_contents(ParCompactionManager* cm);
+  void update_contents(ParCompactionManager* cm);
 #endif // INCLUDE_ALL_GCS
 
-  bool is_scavengable() const;
-
-  // Forward pointer operations for scavenge
+  // Forwarding pointer operations for Mark-Sweep and Scavenge
   bool is_forwarded() const;
-
   void forward_to(oop p);
   bool cas_forward_to(oop p, markOop compare);
-
 #if INCLUDE_ALL_GCS
-  // Like "forward_to", but inserts the forwarding pointer atomically.
+  // Like forward_to(), but inserts the forwarding pointer atomically.
   // Exactly one thread succeeds in inserting the forwarding pointer, and
-  // this call returns "NULL" for that thread; any other thread has the
-  // value of the forwarding pointer returned and does not modify "this".
+  // this call returns NULL for that thread; any other thread has the value
+  // of the forwarding pointer returned and does not modify this object.
   oop forward_to_atomic(oop p);
 #endif // INCLUDE_ALL_GCS
-
   oop forwardee() const;
 
-  // Age of object during scavenge
+  // Age of object used by Scavenge
   uint age() const;
   void incr_age();
 
-  // Adjust all pointers in this object to point at it's forwarded location and
-  // return the size of this oop.  This is used by the MarkSweep collector.
-  int adjust_pointers();
-
-  // mark-sweep support
-  void follow_body(int begin, int end);
+  // ObjectLayout support
+  // Operations with the flag whether the object is contained
+  bool is_contained() const;
+  static bool is_contained(markOop mark);
+  void set_contained();
+  void clear_contained();
+
+  // Operations with the flag whether the object is a container
+  bool is_container() const;
+  static bool is_container(markOop mark, Klass* klass);
+
+  // Manipulations with the object's mark that preserve the current values of
+  // its "contained" and "container" properties
+  void convert_to_marked();
+  void convert_to_unmarked();
+  void restore_mark(markOop saved_value);
+
+  // Operations with the relative container offset (RCO) object corresponding
+  // to this object (which must be contained). The only mission of the RCO
+  // object is to store the offset of this object from its immediate container.
+  // RCO objects are transparently created by the VM for every contained object
+  // (including contained containers) at the time of their construction. They
+  // look like "prepadding" objects taking several words just before the
+  // corresponding contained objects.
+  jlong relative_container_offset() const;
+  void set_relative_container_offset(jlong offset);
+
+  // Return the immediate container of this object (which must be contained)
+  oop container() const;
+
+  // Return the outermost container of this object (which must be contained)
+  oop outermost_container();
+
+  // Return the outermost GC-marked container of this object (which must be
+  // contained and also GC-marked)
+  oop outermost_alive_container();
 
   // Fast access to barrier set
   static BarrierSet* bs()            { return _bs; }
@@ -365,11 +402,6 @@
   // Alternate hashing code if string table is rehashed
   unsigned int new_hash(juint seed);
 
-  // marks are forwarded to stack when object is locked
-  bool     has_displaced_mark() const;
-  markOop  displaced_mark() const;
-  void     set_displaced_mark(markOop m);
-
   // for code generation
   static int mark_offset_in_bytes()    { return offset_of(oopDesc, _mark); }
   static int klass_offset_in_bytes()   { return offset_of(oopDesc, _metadata._klass); }
--- old/src/share/vm/oops/oop.inline.hpp	2015-06-16 10:25:46.338388909 -0700
+++ new/src/share/vm/oops/oop.inline.hpp	2015-06-16 10:25:46.258390839 -0700
@@ -144,7 +144,9 @@
   }
 }
 
-inline void   oopDesc::init_mark()                 { set_mark(markOopDesc::prototype_for_object(this)); }
+inline void oopDesc::init_mark() {
+  set_mark(markOopDesc::prototype_for_object(this));
+}
 
 inline bool oopDesc::is_a(Klass* k)        const { return klass()->is_subtype_of(k); }
 
@@ -559,24 +561,6 @@
   }
 }
 
-// Used only for markSweep, scavenging
-inline bool oopDesc::is_gc_marked() const {
-  return mark()->is_marked();
-}
-
-inline bool oopDesc::is_locked() const {
-  return mark()->is_locked();
-}
-
-inline bool oopDesc::is_unlocked() const {
-  return mark()->is_unlocked();
-}
-
-inline bool oopDesc::has_bias_pattern() const {
-  return mark()->has_bias_pattern();
-}
-
-
 // used only for asserts
 inline bool oopDesc::is_oop(bool ignore_mark_word) const {
   oop obj = (oop) this;
@@ -598,7 +582,6 @@
   return !SafepointSynchronize::is_at_safepoint();
 }
 
-
 // used only for asserts
 inline bool oopDesc::is_oop_or_null(bool ignore_mark_word) const {
   return this == NULL ? true : is_oop(ignore_mark_word);
@@ -612,63 +595,86 @@
 }
 #endif // PRODUCT
 
-inline void oopDesc::follow_contents(void) {
-  assert (is_gc_marked(), "should be marked");
+inline bool oopDesc::is_locked() const {
+  return mark()->is_locked();
+}
+
+inline bool oopDesc::is_unlocked() const {
+  return mark()->is_unlocked();
+}
+
+inline bool oopDesc::has_bias_pattern() const {
+  return mark()->has_bias_pattern();
+}
+
+inline bool oopDesc::has_displaced_mark() const {
+  return mark()->has_displaced_mark_helper();
+}
+
+inline markOop oopDesc::displaced_mark() const {
+  return mark()->displaced_mark_helper();
+}
+
+inline void oopDesc::set_displaced_mark(markOop m) {
+  mark()->set_displaced_mark_helper(m);
+}
+
+inline bool oopDesc::is_gc_marked() const {
+  return mark()->is_marked();
+}
+
+inline void oopDesc::follow_contents() {
+  assert(is_gc_marked(), "should be marked");
   klass()->oop_follow_contents(this);
 }
 
-// Used by scavengers
+inline int oopDesc::adjust_pointers() {
+  debug_only(int check_size = size());
+  int s = klass()->oop_adjust_pointers(this);
+  assert(s == check_size, "should be the same");
+  return s;
+}
 
+// Used by scavengers
 inline bool oopDesc::is_forwarded() const {
-  // The extra heap check is needed since the obj might be locked, in which case the
-  // mark would point to a stack location and have the sentinel bit cleared
+  // The extra heap check is needed since the object might be locked, in which
+  // case the mark would point to a stack location and have the sentinel bit
+  // cleared.
   return mark()->is_marked();
 }
 
 // Used by scavengers
 inline void oopDesc::forward_to(oop p) {
   assert(check_obj_alignment(p),
-         "forwarding to something not aligned");
+      "forwarding to something not aligned");
   assert(Universe::heap()->is_in_reserved(p),
-         "forwarding to something not in heap");
-  markOop m = markOopDesc::encode_pointer_as_mark(p);
-  assert(m->decode_pointer() == p, "encoding must be reversable");
+      "forwarding to something not in heap");
+  markOop m = markOopDesc::encode_pointer_as_mark(p, is_contained());
+  assert(m->decode_pointer() == p, "encoding must be reversible");
   set_mark(m);
 }
 
 // Used by parallel scavengers
 inline bool oopDesc::cas_forward_to(oop p, markOop compare) {
   assert(check_obj_alignment(p),
-         "forwarding to something not aligned");
+      "forwarding to something not aligned");
   assert(Universe::heap()->is_in_reserved(p),
-         "forwarding to something not in heap");
-  markOop m = markOopDesc::encode_pointer_as_mark(p);
-  assert(m->decode_pointer() == p, "encoding must be reversable");
+      "forwarding to something not in heap");
+  markOop m = markOopDesc::encode_pointer_as_mark(p,
+      oopDesc::is_contained(compare));
+  assert(m->decode_pointer() == p, "encoding must be reversible");
   return cas_set_mark(m, compare) == compare;
 }
 
-// Note that the forwardee is not the same thing as the displaced_mark.
-// The forwardee is used when copying during scavenge and mark-sweep.
-// It does need to clear the low two locking- and GC-related bits.
+// Note that the forwardee is not the same thing as the displaced mark.
+// The forwardee is used when copying during Scavenge and Mark-Sweep. It does
+// need to clear the low three locking, GC, and ObjectLayout related bits.
 inline oop oopDesc::forwardee() const {
   return (oop) mark()->decode_pointer();
 }
 
-inline bool oopDesc::has_displaced_mark() const {
-  return mark()->has_displaced_mark_helper();
-}
-
-inline markOop oopDesc::displaced_mark() const {
-  return mark()->displaced_mark_helper();
-}
-
-inline void oopDesc::set_displaced_mark(markOop m) {
-  mark()->set_displaced_mark_helper(m);
-}
-
-// The following method needs to be MT safe.
 inline uint oopDesc::age() const {
-  assert(!is_forwarded(), "Attempt to read age from forwarded mark");
+  assert(!is_forwarded(), "attempt to read age from forwarded mark");
   if (has_displaced_mark()) {
     return displaced_mark()->age();
   } else {
@@ -677,7 +683,7 @@
 }
 
 inline void oopDesc::incr_age() {
-  assert(!is_forwarded(), "Attempt to increment age of forwarded mark");
+  assert(!is_forwarded(), "attempt to increment age of forwarded mark");
   if (has_displaced_mark()) {
     set_displaced_mark(displaced_mark()->incr_age());
   } else {
@@ -685,6 +691,136 @@
   }
 }
 
+inline bool oopDesc::is_contained() const {
+  return is_contained(mark());
+}
+
+inline bool oopDesc::is_contained(markOop mark) {
+  if (mark->is_marked()) {
+    return mark->is_contained_when_forwarded();
+  } else if (mark->has_displaced_mark_helper()) {
+    return mark->displaced_mark_helper()->is_contained();
+  } else {
+    return mark->is_contained();
+  }
+}
+
+inline void oopDesc::set_contained() {
+  if (is_forwarded()) {
+    set_mark(mark()->set_contained_when_forwarded());
+  } else if (has_displaced_mark()) {
+    set_displaced_mark(displaced_mark()->set_contained());
+  } else {
+    set_mark(mark()->set_contained());
+  }
+}
+
+inline void oopDesc::clear_contained() {
+  if (is_forwarded()) {
+    set_mark(mark()->clear_contained_when_forwarded());
+  } else if (has_displaced_mark()) {
+    set_displaced_mark(displaced_mark()->clear_contained());
+  } else {
+    set_mark(mark()->clear_contained());
+  }
+}
+
+inline bool oopDesc::is_container() const {
+  return is_container(mark(), klass());
+}
+
+inline bool oopDesc::is_container(markOop mark, Klass* klass) {
+  if (mark->is_marked()) {
+    return klass->oop_is_container();
+  } else if (mark->has_displaced_mark_helper()) {
+    return mark->displaced_mark_helper()->is_container();
+  } else {
+    return mark->is_container();
+  }
+}
+
+inline void oopDesc::convert_to_marked() {
+  assert(!is_gc_marked(), "object already marked");
+  markOop new_mark = markOopDesc::prototype()->set_marked();
+  if (is_contained()) {
+    new_mark = new_mark->set_contained_when_forwarded();
+  }
+  set_mark(new_mark);
+}
+
+inline void oopDesc::convert_to_unmarked() {
+  assert(is_gc_marked(), "object already unmarked");
+  markOop new_mark = markOopDesc::prototype_for_object(this);
+  if (is_contained()) {
+    new_mark = new_mark->set_contained();
+  }
+  if (is_container()) {
+    new_mark = new_mark->set_container();
+  }
+  set_mark(new_mark);
+}
+
+inline void oopDesc::restore_mark(markOop saved_value) {
+  assert(!is_gc_marked(), "object must not be marked");
+  assert(!saved_value->is_marked(), "value to be restored must not be marked");
+  bool is_contained = this->is_contained();
+  // The property whether the object is a container cannot change during GC.
+  // Hence, there is no need in updating the corresponding bit in the given
+  // saved markword value.
+  if (saved_value->has_displaced_mark_helper()) {
+    markOop displaced_value = saved_value->displaced_mark_helper();
+    saved_value->set_displaced_mark_helper(is_contained ?
+        displaced_value->set_contained() :
+        displaced_value->clear_contained());
+    set_mark(saved_value);
+  } else {
+    set_mark(is_contained ?
+        saved_value->set_contained() :
+        saved_value->clear_contained());
+  }
+}
+
+inline jlong oopDesc::relative_container_offset() const {
+  assert(is_contained(), "object not contained");
+  return *((jlong*) (((address) this) - sizeof(jlong)));
+}
+
+inline void oopDesc::set_relative_container_offset(jlong offset) {
+  assert(is_contained(), "object not contained");
+  *((jlong*) (((address) this) - sizeof(jlong))) = offset;
+}
+
+inline oop oopDesc::container() const {
+  assert(is_contained(), "object not contained");
+  return (oop) (((address) this) - relative_container_offset());
+}
+
+inline oop oopDesc::outermost_container() {
+  assert(is_contained(), "object not contained");
+  oop container = this;
+  do {
+    container = container->container();
+    assert(container->is_container(), "container broken");
+  } while (container->is_contained());
+  return container;
+}
+
+inline oop oopDesc::outermost_alive_container() {
+  assert(is_gc_marked(), "object not marked");
+  assert(is_contained(), "object not contained");
+  oop result = NULL;
+  oop container = this;
+  do {
+    container = container->container();
+    assert(container->is_container(), "container broken");
+    if (container->is_gc_marked()) {
+      result = container;
+    } else {
+      break;
+    }
+  } while (container->is_contained());
+  return result;
+}
 
 inline intptr_t oopDesc::identity_hash() {
   // Fast case; if the object is unlocked and the hash value is set, no locking is needed
@@ -699,13 +835,6 @@
   }
 }
 
-inline int oopDesc::adjust_pointers() {
-  debug_only(int check_size = size());
-  int s = klass()->oop_adjust_pointers(this);
-  assert(s == check_size, "should be the same");
-  return s;
-}
-
 #define OOP_ITERATE_DEFN(OopClosureType, nv_suffix)                        \
                                                                            \
 inline int oopDesc::oop_iterate(OopClosureType* blk) {                     \
--- old/src/share/vm/oops/oop.pcgc.inline.hpp	2015-06-16 10:25:46.602382537 -0700
+++ new/src/share/vm/oops/oop.pcgc.inline.hpp	2015-06-16 10:25:46.526384371 -0700
@@ -57,8 +57,10 @@
 inline oop oopDesc::forward_to_atomic(oop p) {
   assert(ParNewGeneration::is_legal_forward_ptr(p),
          "illegal forwarding pointer value.");
+
   markOop oldMark = mark();
-  markOop forwardPtrMark = markOopDesc::encode_pointer_as_mark(p);
+  markOop forwardPtrMark =
+      markOopDesc::encode_pointer_as_mark(p, /* is_contained = */ false);
   markOop curMark;
 
   assert(forwardPtrMark->decode_pointer() == p, "encoding must be reversable");
--- old/src/share/vm/oops/oop.psgc.inline.hpp	2015-06-16 10:25:46.866376166 -0700
+++ new/src/share/vm/oops/oop.psgc.inline.hpp	2015-06-16 10:25:46.790378000 -0700
@@ -32,15 +32,15 @@
 #include "gc_implementation/parallelScavenge/psScavenge.inline.hpp"
 #endif // INCLUDE_ALL_GCS
 
-// ParallelScavengeHeap methods
+// Parallel Scavenge GC related methods
 
 inline void oopDesc::push_contents(PSPromotionManager* pm) {
   Klass* k = klass();
   if (!k->oop_is_typeArray()) {
-    // It might contain oops beyond the header, so take the virtual call.
+    // It might contain oops beyond the header, so take the virtual call
     k->oop_push_contents(pm, this);
   }
-  // Else skip it.  The TypeArrayKlass in the header never needs scavenging.
+  // Else skip it. TypeArrayKlass instances never need scavenging.
 }
 
 #endif // SHARE_VM_OOPS_OOP_PSGC_INLINE_HPP
--- old/src/share/vm/opto/classes.hpp	2015-06-16 10:25:47.126369883 -0700
+++ new/src/share/vm/opto/classes.hpp	2015-06-16 10:25:47.050371722 -0700
@@ -53,6 +53,7 @@
 macro(CallLeafNoFP)
 macro(CallRuntime)
 macro(CallStaticJava)
+macro(CastDerived)
 macro(CastII)
 macro(CastX2P)
 macro(CastP2X)
@@ -199,7 +200,6 @@
 macro(Opaque1)
 macro(Opaque2)
 macro(Opaque3)
-macro(ProfileBoolean)
 macro(OrI)
 macro(OrL)
 macro(OverflowAddI)
--- old/src/share/vm/opto/connode.hpp	2015-06-16 10:25:47.386363592 -0700
+++ new/src/share/vm/opto/connode.hpp	2015-06-16 10:25:47.310365430 -0700
@@ -266,12 +266,23 @@
 // cast pointer to pointer (different type)
 class CastPPNode: public ConstraintCastNode {
 public:
-  CastPPNode (Node *n, const Type *t ): ConstraintCastNode(n, t) {}
+  CastPPNode (Node *n, const Type *t ): ConstraintCastNode(n, t) { init_class_id(Class_CastPP); }
   virtual int Opcode() const;
   virtual uint ideal_reg() const { return Op_RegP; }
   virtual Node *Ideal_DU_postCCP( PhaseCCP * );
 };
 
+//------------------------------CastDerivedNode-------------------------------------
+// cast pointer of a container object with offset to a pointer of a contained object
+class CastDerivedNode: public CastPPNode {
+public:
+  CastDerivedNode(Node* n, const Type* t): CastPPNode(n, t) { init_class_id(Class_CastDerived); }
+  virtual const Type* Value(PhaseTransform* phase) const { return _type; }
+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape) { return NULL; } // don't optimize away
+  virtual int Opcode() const;
+  virtual Node* Ideal_DU_postCCP(PhaseCCP* ccp) { return this; } // don't optimize away
+};
+
 //------------------------------CheckCastPPNode--------------------------------
 // for _checkcast, cast pointer to pointer (different type), without JOIN,
 class CheckCastPPNode: public TypeNode {
@@ -669,31 +680,6 @@
   bool rtm_opt() const { return (_opt == RTM_OPT); }
 };
 
-//------------------------------ProfileBooleanNode-------------------------------
-// A node represents value profile for a boolean during parsing.
-// Once parsing is over, the node goes away (during IGVN).
-// It is used to override branch frequencies from MDO (see has_injected_profile in parse2.cpp).
-class ProfileBooleanNode : public Node {
-  uint _false_cnt;
-  uint _true_cnt;
-  bool _consumed;
-  bool _delay_removal;
-  virtual uint hash() const ;                  // { return NO_HASH; }
-  virtual uint cmp( const Node &n ) const;
-  public:
-  ProfileBooleanNode(Node *n, uint false_cnt, uint true_cnt) : Node(0, n),
-          _false_cnt(false_cnt), _true_cnt(true_cnt), _delay_removal(true), _consumed(false) {}
-
-  uint false_count() const { return _false_cnt; }
-  uint  true_count() const { return  _true_cnt; }
-
-  void consume() { _consumed = true;  }
-
-  virtual int Opcode() const;
-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
-  virtual Node *Identity(PhaseTransform *phase);
-  virtual const Type *bottom_type() const { return TypeInt::BOOL; }
-};
 
 //----------------------PartialSubtypeCheckNode--------------------------------
 // The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary superklass
--- old/src/share/vm/opto/library_call.cpp	2015-06-16 10:25:47.650357204 -0700
+++ new/src/share/vm/opto/library_call.cpp	2015-06-16 10:25:47.578358946 -0700
@@ -31,7 +31,6 @@
 #include "opto/addnode.hpp"
 #include "opto/callGenerator.hpp"
 #include "opto/cfgnode.hpp"
-#include "opto/connode.hpp"
 #include "opto/idealKit.hpp"
 #include "opto/mathexactnode.hpp"
 #include "opto/mulnode.hpp"
@@ -230,6 +229,7 @@
   // Generates the guards that check whether the result of
   // Unsafe.getObject should be recorded in an SATB log buffer.
   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
+  bool inline_unsafe_deriveContainedObjectAtOffset();
   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, bool is_volatile);
   bool inline_unsafe_prefetch(bool is_native_ptr, bool is_store, bool is_static);
   static bool klass_needs_init_guard(Node* kls);
@@ -306,6 +306,9 @@
   bool inline_fp_conversions(vmIntrinsics::ID id);
   bool inline_number_methods(vmIntrinsics::ID id);
   bool inline_reference_get();
+  bool inline_derive_contained_object();
+  bool inline_asa_get();
+  Node* load_container_class(Node* ctrObj);
   bool inline_aescrypt_Block(vmIntrinsics::ID id);
   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
@@ -324,8 +327,6 @@
   bool inline_updateBytesCRC32();
   bool inline_updateByteBufferCRC32();
   bool inline_multiplyToLen();
-
-  bool inline_profileBoolean();
 };
 
 
@@ -518,6 +519,10 @@
     break;
 #endif
 
+  case vmIntrinsics::_deriveContainedObjectAtOffset:
+    if (!UseObjectLayoutIntrinsics) return NULL;
+    break;
+
   case vmIntrinsics::_aescrypt_encryptBlock:
   case vmIntrinsics::_aescrypt_decryptBlock:
     if (!UseAESIntrinsics) return NULL;
@@ -909,6 +914,9 @@
 
   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 
+  case vmIntrinsics::_deriveContainedObjectAtOffset:
+                                                return inline_unsafe_deriveContainedObjectAtOffset();
+
   case vmIntrinsics::_aescrypt_encryptBlock:
   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 
@@ -937,9 +945,6 @@
   case vmIntrinsics::_updateByteBufferCRC32:
     return inline_updateByteBufferCRC32();
 
-  case vmIntrinsics::_profileBoolean:
-    return inline_profileBoolean();
-
   default:
     // If you get here, it may be that someone has added a new intrinsic
     // to the list in vmSymbols.hpp without implementing it here.
@@ -2378,6 +2383,36 @@
   return true;
 }
 
+//------------------inline_unsafe_deriveContainedObjectAtOffset---------------
+
+bool LibraryCallKit::inline_unsafe_deriveContainedObjectAtOffset() {
+  Node* receiver = argument(0); // the unsafe instance
+  Node* base     = argument(1);
+  Node* offset   = argument(2);
+
+  // null check unsafe: must have capability
+  receiver = null_check(receiver);
+  if (stopped()) {
+    return true;
+  }
+
+  // null check base
+  base = null_check(base);
+  if (stopped()) {
+    return true;
+  }
+
+  // if (!is_size_aligned((size_t) offset, HeapWordSize))
+  //   throw new IllegalArgumentException();
+  // TODO
+
+  Node* adr = basic_plus_adr(top(), base, offset); // don't want to keep base-derived relationship here
+  Node* cast = new (C) CastDerivedNode(adr, TypeInstPtr::NOTNULL); // assuming a non-null Object
+  cast = _gvn.transform(cast);
+  set_result(cast);
+  return true;
+}
+
 //----------------------------inline_unsafe_access----------------------------
 
 const static BasicType T_ADDRESS_HOLDER = T_LONG;
@@ -6049,6 +6084,151 @@
   return loadedField;
 }
 
+Node * LibraryCallKit::load_container_class(Node* ctrObj) {
+
+  const TypeInstPtr* tinst = _gvn.type(ctrObj)->isa_instptr();
+  assert(tinst != NULL, "obj is null");
+  assert(tinst->klass()->is_loaded(), "obj is not loaded");
+
+  ciField* field = tinst->klass()->as_instance_klass()->get_field_by_name(
+          ciSymbol::make("elementClass"), ciSymbol::make("Ljava/lang/Class;"), false);
+  if (field == NULL) return (Node *) NULL;
+  assert (field != NULL, "undefined field");
+
+  ciType* field_klass = field->type();
+  assert(field_klass->is_loaded(), "should be loaded");
+  const TypePtr* adr_type = C->alias_type(field)->adr_type();
+  int offset  = field->offset_in_bytes();
+  Node* adr = basic_plus_adr(ctrObj, ctrObj, offset);
+  BasicType bt = field->layout_type();
+  assert(bt == T_OBJECT, "");
+
+  const Type* type = TypeOopPtr::make_from_klass(field_klass->as_klass());
+  Node* loadedField = make_load(NULL, adr, type, bt, adr_type, MemNode::unordered, false);
+  return loadedField;
+}
+
+bool LibraryCallKit::inline_asa_get() {
+    assert(UseObjectLayoutIntrinsics, "not implemented on this platform");
+
+#ifndef PRODUCT
+  tty->print_cr("Attempting to inline org.ObjectLayout.AbstractStructuredArray.get(long) ...");
+  {
+    ResourceMark rm;
+    // Check the signature
+    ciSignature* sig = callee()->signature();
+    BasicType rtype = sig->return_type()->basic_type();
+    assert(rtype == T_OBJECT, "return value is object");
+    assert(sig->count() == 1, "1 arguments");
+    assert(sig->type_at(0)->basic_type() == T_LONG,   "sanity");
+  }
+#endif // PRODUCT
+  Node* receiver = argument(0);
+  Node* index    = argument(1);
+
+  receiver = null_check_receiver();
+  if (stopped()) return true;
+
+  Node* bodySize = NULL;
+  Node* elemtSize = NULL;
+  Node* padSize = NULL;
+
+  int len_off = org_ObjectLayout_AbstractStructuredArray::length_offset();
+  int bs_off  = org_ObjectLayout_AbstractStructuredArray::bodySize_offset();
+  int es_off  = org_ObjectLayout_AbstractStructuredArray::elementSize_offset();
+  int ps_off  = org_ObjectLayout_AbstractStructuredArray::paddingSize_offset();
+
+  Node* lnp = basic_plus_adr(top(), receiver, len_off);
+  if (lnp == NULL) return false; // cannot happen?
+  Node* lnv = make_load(NULL, lnp, TypeLong::LONG, T_LONG, MemNode::unordered);
+
+  Node* bsp = basic_plus_adr(top(), receiver, bs_off);
+  if (bsp == NULL) return false; // cannot happen?
+  Node* bsv = make_load(NULL, bsp, TypeInt::INT, T_INT, MemNode::unordered);
+
+  Node* esp = basic_plus_adr(top(), receiver, es_off);
+  if (esp == NULL) return false; // cannot happen?
+  Node* esv = make_load(NULL, esp, TypeLong::LONG, T_LONG, MemNode::unordered);
+
+  Node* psp = basic_plus_adr(top(), receiver, ps_off);
+  if (psp == NULL) return false; // cannot happen?
+  Node* psv = make_load(NULL, psp, TypeLong::LONG, T_LONG, MemNode::unordered);
+
+  /* long offset = getBodySize() + index*getElementSize() + getPaddingSize(); */
+
+  Node* bs_ps_sum = _gvn.transform(new (C) AddLNode(bsv, psv));
+  Node* idx_es_mul = _gvn.transform(new (C) MulLNode(index, esv));
+  Node* offset = _gvn.transform(new (C) AddLNode(bs_ps_sum, idx_es_mul));
+
+  const Type* t = TypeOopPtr::BOTTOM; // FIXME
+
+  Node* result = make_load(NULL, offset, t, T_OBJECT, MemNode::unordered);
+
+  _gvn.set_type(result, t);
+
+  set_result(result);
+#ifndef PRODUCT
+  tty->print_cr("Done.");
+#endif
+  return false;
+}
+
+/*
+ * Derive contained object at offset.
+ * Object deriveContainedObjectAtOffset(Object container, long index)
+ */
+bool LibraryCallKit::inline_derive_contained_object() {
+    assert(UseObjectLayoutIntrinsics, "not implemented on this platform");
+
+#ifndef PRODUCT
+  tty->print_cr("Attempting to inline sun.misc.Unsafe.deriveContainedObjectAtOffset(Object,long) ...");
+  {
+    ResourceMark rm;
+    // Check the signature
+    ciSignature* sig = callee()->signature();
+    BasicType rtype = sig->return_type()->basic_type();
+    assert(rtype == T_OBJECT, "return value is object");
+    assert(sig->count() == 2, "2 arguments");
+    assert(sig->type_at(0)->basic_type() == T_OBJECT, "sanity");
+    assert(sig->type_at(1)->basic_type() == T_LONG,   "sanity");
+  }
+#endif // PRODUCT
+
+  Node* receiver = argument(0); // type: oop
+  Node* container = argument(1); // type: oop
+  Node* offset = argument(2); // type: long
+
+  receiver  = null_check(receiver);
+  container = null_check(container);
+  if (stopped()) {
+    return true;
+  }
+
+  Node* result = basic_plus_adr(container, container, offset);
+
+  const TypePtr *adr_type = _gvn.type(result)->isa_ptr();
+  Compile::AliasType* alias_type = C->alias_type(adr_type);
+  assert(alias_type->index() != Compile::AliasIdxBot, "no bare pointers here");
+
+  bool need_mem_bar = (alias_type->adr_type() == TypeOopPtr::BOTTOM); // it's true
+  
+  Node* heap_base_oop = container;
+  bool need_read_barrier = offset != top() && heap_base_oop != top(); // it's true
+  
+  if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
+
+  if (need_read_barrier) {
+    insert_pre_barrier(heap_base_oop, offset, result, !(need_mem_bar));
+  }
+
+  set_result(result);
+
+  if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
+#ifndef PRODUCT
+  tty->print_cr("Done.");
+#endif
+  return true;
+}
 
 //------------------------------inline_aescrypt_Block-----------------------
 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
@@ -6550,81 +6730,3 @@
 
   return instof_false;  // even if it is NULL
 }
-
-bool LibraryCallKit::inline_profileBoolean() {
-  Node* counts = argument(1);
-  const TypeAryPtr* ary = NULL;
-  ciArray* aobj = NULL;
-  if (counts->is_Con()
-      && (ary = counts->bottom_type()->isa_aryptr()) != NULL
-      && (aobj = ary->const_oop()->as_array()) != NULL
-      && (aobj->length() == 2)) {
-    // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
-    jint false_cnt = aobj->element_value(0).as_int();
-    jint  true_cnt = aobj->element_value(1).as_int();
-
-    method()->set_injected_profile(true);
-
-    if (C->log() != NULL) {
-      C->log()->elem("observe source='profileBoolean' false='%d' true='%d'",
-                     false_cnt, true_cnt);
-    }
-
-    if (false_cnt + true_cnt == 0) {
-      // According to profile, never executed.
-      uncommon_trap_exact(Deoptimization::Reason_intrinsic,
-                          Deoptimization::Action_reinterpret);
-      return true;
-    }
-
-    // result is a boolean (0 or 1) and its profile (false_cnt & true_cnt)
-    // is a number of each value occurrences.
-    Node* result = argument(0);
-    if (false_cnt == 0 || true_cnt == 0) {
-      // According to profile, one value has been never seen.
-      int expected_val = (false_cnt == 0) ? 1 : 0;
-
-      Node* cmp  = _gvn.transform(new (C) CmpINode(result, intcon(expected_val)));
-      Node* test = _gvn.transform(new (C) BoolNode(cmp, BoolTest::eq));
-
-      IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
-      Node* fast_path = _gvn.transform(new (C) IfTrueNode(check));
-      Node* slow_path = _gvn.transform(new (C) IfFalseNode(check));
-
-      { // Slow path: uncommon trap for never seen value and then reexecute
-        // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
-        // the value has been seen at least once.
-        PreserveJVMState pjvms(this);
-        PreserveReexecuteState preexecs(this);
-        jvms()->set_should_reexecute(true);
-
-        set_control(slow_path);
-        set_i_o(i_o());
-
-        uncommon_trap_exact(Deoptimization::Reason_intrinsic,
-                            Deoptimization::Action_reinterpret);
-      }
-      // The guard for never seen value enables sharpening of the result and
-      // returning a constant. It allows to eliminate branches on the same value
-      // later on.
-      set_control(fast_path);
-      result = intcon(expected_val);
-    }
-    // Stop profiling.
-    // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
-    // By replacing method body with profile data (represented as ProfileBooleanNode
-    // on IR level) we effectively disable profiling.
-    // It enables full speed execution once optimized code is generated.
-    Node* profile = _gvn.transform(new (C) ProfileBooleanNode(result, false_cnt, true_cnt));
-    C->record_for_igvn(profile);
-    set_result(profile);
-    return true;
-  } else {
-    // Continue profiling.
-    // Profile data isn't available at the moment. So, execute method's bytecode version.
-    // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
-    // is compiled and counters aren't available since corresponding MethodHandle
-    // isn't a compile-time constant.
-    return false;
-  }
-}
--- old/src/share/vm/opto/memnode.cpp	2015-06-16 10:25:47.986349073 -0700
+++ new/src/share/vm/opto/memnode.cpp	2015-06-16 10:25:47.914350815 -0700
@@ -756,6 +756,7 @@
         return n;
 
       case Op_CastPP:
+      case Op_CastDerived:
         // If the CastPP is useless, just peek on through it.
         if( ccp->type(adr) == ccp->type(adr->in(1)) ) {
           // Remember the cast that we've peeked though. If we peek
--- old/src/share/vm/opto/node.hpp	2015-06-16 10:25:48.298341506 -0700
+++ new/src/share/vm/opto/node.hpp	2015-06-16 10:25:48.218343446 -0700
@@ -51,6 +51,8 @@
 class CallNode;
 class CallRuntimeNode;
 class CallStaticJavaNode;
+class CastDerivedNode;
+class CastPPNode;
 class CatchNode;
 class CatchProjNode;
 class CheckCastPPNode;
@@ -98,7 +100,6 @@
 class MachSafePointNode;
 class MachSpillCopyNode;
 class MachTempNode;
-class MachMergeNode;
 class Matcher;
 class MemBarNode;
 class MemBarStoreStoreNode;
@@ -592,11 +593,12 @@
       DEFINE_CLASS_ID(MachTemp,         Mach, 3)
       DEFINE_CLASS_ID(MachConstantBase, Mach, 4)
       DEFINE_CLASS_ID(MachConstant,     Mach, 5)
-      DEFINE_CLASS_ID(MachMerge,        Mach, 6)
 
     DEFINE_CLASS_ID(Type,  Node, 2)
       DEFINE_CLASS_ID(Phi,   Type, 0)
       DEFINE_CLASS_ID(ConstraintCast, Type, 1)
+        DEFINE_CLASS_ID(CastPP, ConstraintCast, 0)
+          DEFINE_CLASS_ID(CastDerived, CastPP, 0)
       DEFINE_CLASS_ID(CheckCastPP, Type, 2)
       DEFINE_CLASS_ID(CMove, Type, 3)
       DEFINE_CLASS_ID(SafePointScalarObject, Type, 4)
@@ -718,6 +720,7 @@
   DEFINE_CLASS_QUERY(CallLeaf)
   DEFINE_CLASS_QUERY(CallRuntime)
   DEFINE_CLASS_QUERY(CallStaticJava)
+  DEFINE_CLASS_QUERY(CastDerived)
   DEFINE_CLASS_QUERY(Catch)
   DEFINE_CLASS_QUERY(CatchProj)
   DEFINE_CLASS_QUERY(CheckCastPP)
@@ -763,7 +766,6 @@
   DEFINE_CLASS_QUERY(MachSafePoint)
   DEFINE_CLASS_QUERY(MachSpillCopy)
   DEFINE_CLASS_QUERY(MachTemp)
-  DEFINE_CLASS_QUERY(MachMerge)
   DEFINE_CLASS_QUERY(Mem)
   DEFINE_CLASS_QUERY(MemBar)
   DEFINE_CLASS_QUERY(MemBarStoreStore)
--- old/src/share/vm/prims/unsafe.cpp	2015-06-16 10:25:48.610333937 -0700
+++ new/src/share/vm/prims/unsafe.cpp	2015-06-16 10:25:48.530335878 -0700
@@ -322,33 +322,10 @@
 UNSAFE_END
 
 #ifndef SUPPORTS_NATIVE_CX8
+// Keep old code for platforms which may not have atomic jlong (8 bytes) instructions
 
-// VM_Version::supports_cx8() is a surrogate for 'supports atomic long memory ops'.
-//
-// On platforms which do not support atomic compare-and-swap of jlong (8 byte)
-// values we have to use a lock-based scheme to enforce atomicity. This has to be
-// applied to all Unsafe operations that set the value of a jlong field. Even so
-// the compareAndSwapLong operation will not be atomic with respect to direct stores
-// to the field from Java code. It is important therefore that any Java code that
-// utilizes these Unsafe jlong operations does not perform direct stores. To permit
-// direct loads of the field from Java code we must also use Atomic::store within the
-// locked regions. And for good measure, in case there are direct stores, we also
-// employ Atomic::load within those regions. Note that the field in question must be
-// volatile and so must have atomic load/store accesses applied at the Java level.
-//
-// The locking scheme could utilize a range of strategies for controlling the locking
-// granularity: from a lock per-field through to a single global lock. The latter is
-// the simplest and is used for the current implementation. Note that the Java object
-// that contains the field, can not, in general, be used for locking. To do so can lead
-// to deadlocks as we may introduce locking into what appears to the Java code to be a
-// lock-free path.
-//
-// As all the locked-regions are very short and themselves non-blocking we can treat
-// them as leaf routines and elide safepoint checks (ie we don't perform any thread
-// state transitions even when blocking for the lock). Note that if we do choose to
-// add safepoint checks and thread state transitions, we must ensure that we calculate
-// the address of the field _after_ we have acquired the lock, else the object may have
-// been moved by the GC
+// Volatile long versions must use locks if !VM_Version::supports_cx8().
+// support_cx8 is a surrogate for 'supports atomic long memory ops'.
 
 UNSAFE_ENTRY(jlong, Unsafe_GetLongVolatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset))
   UnsafeWrapper("Unsafe_GetLongVolatile");
@@ -360,8 +337,8 @@
     else {
       Handle p (THREAD, JNIHandles::resolve(obj));
       jlong* addr = (jlong*)(index_oop_from_field_offset_long(p(), offset));
-      MutexLockerEx mu(UnsafeJlong_lock, Mutex::_no_safepoint_check_flag);
-      jlong value = Atomic::load(addr);
+      ObjectLocker ol(p, THREAD);
+      jlong value = *addr;
       return value;
     }
   }
@@ -376,8 +353,8 @@
     else {
       Handle p (THREAD, JNIHandles::resolve(obj));
       jlong* addr = (jlong*)(index_oop_from_field_offset_long(p(), offset));
-      MutexLockerEx mu(UnsafeJlong_lock, Mutex::_no_safepoint_check_flag);
-      Atomic::store(x, addr);
+      ObjectLocker ol(p, THREAD);
+      *addr = x;
     }
   }
 UNSAFE_END
@@ -486,8 +463,8 @@
     else {
       Handle p (THREAD, JNIHandles::resolve(obj));
       jlong* addr = (jlong*)(index_oop_from_field_offset_long(p(), offset));
-      MutexLockerEx mu(UnsafeJlong_lock, Mutex::_no_safepoint_check_flag);
-      Atomic::store(x, addr);
+      ObjectLocker ol(p, THREAD);
+      *addr = x;
     }
   }
 #endif
@@ -1236,19 +1213,14 @@
   UnsafeWrapper("Unsafe_CompareAndSwapLong");
   Handle p (THREAD, JNIHandles::resolve(obj));
   jlong* addr = (jlong*)(index_oop_from_field_offset_long(p(), offset));
-#ifdef SUPPORTS_NATIVE_CX8
-  return (jlong)(Atomic::cmpxchg(x, addr, e)) == e;
-#else
   if (VM_Version::supports_cx8())
     return (jlong)(Atomic::cmpxchg(x, addr, e)) == e;
   else {
     jboolean success = false;
-    MutexLockerEx mu(UnsafeJlong_lock, Mutex::_no_safepoint_check_flag);
-    jlong val = Atomic::load(addr);
-    if (val == e) { Atomic::store(x, addr); success = true; }
+    ObjectLocker ol(p, THREAD);
+    if (*addr == e) { *addr = x; success = true; }
     return success;
   }
-#endif
 UNSAFE_END
 
 UNSAFE_ENTRY(void, Unsafe_Park(JNIEnv *env, jobject unsafe, jboolean isAbsolute, jlong time))
@@ -1358,6 +1330,568 @@
   Prefetch::write(addr, (intx)offset);
 UNSAFE_END
 
+// Implementation of ObjectLayout intrinsics
+
+// Return the size, in words, of an instance of the given class. The class is
+// specified by the object that is an instance of java.lang.Class.
+static jlong get_instance_size_helper(oop obj, TRAPS) {
+  instanceKlassHandle h_klass(THREAD, java_lang_Class::as_Klass(obj));
+
+  // Make sure we are not fetching the instance size of an abstract klass
+  h_klass->check_valid_for_instantiation(true, CHECK_0);
+
+  // Make sure klass is initialized
+  h_klass->initialize(CHECK_0);
+
+  return (jlong) h_klass->size_helper();
+}
+
+// Return the size, in words, of an instance of the given class
+static jlong get_instance_size_helper(jclass cls, TRAPS) {
+  return get_instance_size_helper(JNIHandles::resolve_non_null(cls), THREAD);
+}
+
+// Return the size, in words, of prepadding for a contained object, which is
+// used to allocate the corresponding relative container offset (RCO) object.
+// RCO objects are implemented as standard filler objects (arrays of integers)
+// able to store the payload of 8 bytes.
+static jlong get_prepadding_helper() {
+  jlong size_in_bytes =
+      arrayOopDesc::base_offset_in_bytes(T_INT) + sizeof(jlong);
+  jlong size_in_words =
+      (size_in_bytes + (HeapWordSize - 1)) >> LogHeapWordSize;
+  return (jlong) align_object_size((intptr_t) size_in_words);
+}
+
+// Create the RCO object for the object that will be allocated at the given
+// address and initialize it with the given offset value
+static void create_rco_object(address obj_addr, jlong offset_in_bytes) {
+  // Create the RCO object
+  jlong prepadding_in_words = get_prepadding_helper();
+  address rco_obj_addr = obj_addr - (prepadding_in_words << LogHeapWordSize);
+  CollectedHeap::fill_with_object((HeapWord*) rco_obj_addr,
+      (size_t) prepadding_in_words, false);
+
+  // Store the offset value in the last 8 bytes of the RCO object
+  *((jlong*) (obj_addr - sizeof(jlong))) = offset_in_bytes;
+}
+
+// Utility class describing the configuration of a structured array. It is used
+// to configure objects extending org.ObjectLayout.AbstractStructuredArray class
+// before their actual construction. All fields have exactly the same meanings
+// as the corresponding fields of org.ObjectLayout.AbstractStructuredArray
+// class. All sizes are in bytes.
+class AbstractStructuredArrayConfiguration VALUE_OBJ_CLASS_SPEC {
+ public:
+  jint body_size;
+  jlong length;
+  jlong element_size;
+  jlong padding_size;
+  instanceHandle h_element_class;
+
+ public:
+  // Configure the given object extending
+  // org.ObjectLayout.AbstractStructuredArray class
+  void configure_instance(oop obj) {
+    org_ObjectLayout_AbstractStructuredArray::set_bodySize(obj, body_size);
+    org_ObjectLayout_AbstractStructuredArray::set_length(obj, length);
+    org_ObjectLayout_AbstractStructuredArray::set_elementSize(
+        obj, element_size);
+    org_ObjectLayout_AbstractStructuredArray::set_paddingSize(
+        obj, padding_size);
+    org_ObjectLayout_AbstractStructuredArray::set_elementClass(
+        obj, h_element_class());
+  }
+};
+
+// Utility class describing the configurations of all nesting levels of
+// a structured array. Numbering of levels starts from the outermost one.
+class UnfoldedStructuredArrayConfiguration VALUE_OBJ_CLASS_SPEC {
+ private:
+  int dims;
+  AbstractStructuredArrayConfiguration* configs;
+
+ public:
+  UnfoldedStructuredArrayConfiguration() {
+    dims = 0;
+    configs = NULL;
+  }
+
+  ~UnfoldedStructuredArrayConfiguration() {
+    if (configs != NULL) {
+      FREE_C_HEAP_ARRAY(AbstractStructuredArrayConfiguration, configs,
+          mtInternal);
+    }
+  }
+
+  // Initialize the configurations from the given outermost structured array
+  // class and two parallel arrays determining counts of the elements and their
+  // classes for all nesting levels, starting from the outermost one. The caller
+  // is responsible for providing the correct input data.
+  void initialize(jclass container_cls, typeArrayHandle h_element_counts,
+      objArrayHandle h_element_classes, TRAPS) {
+    dims = h_element_counts->length();
+
+    configs = NEW_C_HEAP_ARRAY(AbstractStructuredArrayConfiguration, dims,
+        mtInternal);
+    if (configs == NULL) {
+      dims = 0;
+      THROW(vmSymbols::java_lang_OutOfMemoryError());
+    }
+
+    int i = dims - 1;
+    jlong prepadding_in_words = get_prepadding_helper();
+    jlong body_size_in_words = get_instance_size_helper(
+        h_element_classes->obj_at(i), THREAD);
+    jlong size_in_words = prepadding_in_words + body_size_in_words;
+    configs[i].length = h_element_counts->long_at(i);
+    configs[i].element_size = size_in_words << LogHeapWordSize;
+    configs[i].padding_size = prepadding_in_words << LogHeapWordSize;
+    instanceHandle h_temp_element_class(THREAD,
+        (instanceOop) (h_element_classes->obj_at(i)));
+    configs[i].h_element_class = h_temp_element_class;
+
+    while (--i >= 0) {
+      body_size_in_words = get_instance_size_helper(
+          h_element_classes->obj_at(i), THREAD);
+      size_in_words = prepadding_in_words + body_size_in_words +
+          (h_element_counts->long_at(i + 1) * size_in_words);
+      configs[i + 1].body_size = (jint) (body_size_in_words << LogHeapWordSize);
+      configs[i].length = h_element_counts->long_at(i);
+      configs[i].element_size = size_in_words << LogHeapWordSize;
+      configs[i].padding_size = prepadding_in_words << LogHeapWordSize;
+      instanceHandle h_temp_element_class(THREAD,
+          (instanceOop) (h_element_classes->obj_at(i)));
+      configs[i].h_element_class = h_temp_element_class;
+    }
+
+    body_size_in_words = get_instance_size_helper(container_cls, THREAD);
+    configs[0].body_size = (jint) (body_size_in_words << LogHeapWordSize);
+  }
+
+  // Accessors
+
+  int dimensions() {
+    assert(configs != NULL, "object must be initialized");
+    return dims;
+  }
+
+  jint body_size(int level) {
+    assert(configs != NULL, "object must be initialized");
+    assert(level >= 0 && level < dims, "level out of bounds");
+    return configs[level].body_size;
+  }
+
+  jlong length(int level) {
+    assert(configs != NULL, "object must be initialized");
+    assert(level >= 0 && level < dims, "level out of bounds");
+    return configs[level].length;
+  }
+
+  jlong element_size(int level) {
+    assert(configs != NULL, "object must be initialized");
+    assert(level >= 0 && level < dims, "level out of bounds");
+    return configs[level].element_size;
+  }
+
+  jlong padding_size(int level) {
+    assert(configs != NULL, "object must be initialized");
+    assert(level >= 0 && level < dims, "level out of bounds");
+    return configs[level].padding_size;
+  }
+
+  instanceHandle element_class(int level) {
+    assert(configs != NULL, "object must be initialized");
+    assert(level >= 0 && level < dims, "level out of bounds");
+    return configs[level].h_element_class;
+  }
+
+  // Configure the given object extending
+  // org.ObjectLayout.AbstractStructuredArray class and belonging to the given
+  // nesting level
+  void configure_instance(oop obj, int level) {
+    assert(configs != NULL, "object must be initialized");
+    assert(level >= 0 && level < dims, "level out of bounds");
+    configs[level].configure_instance(obj);
+  }
+};
+
+UNSAFE_ENTRY(jlong, Unsafe_GetInstanceSize(JNIEnv* env, jobject unsafe,
+    jclass cls))
+  UnsafeWrapper("Unsafe_GetInstanceSize");
+  if (TraceObjectLayoutIntrinsics) {
+    ResourceMark rm;
+    tty->print("Unsafe_GetInstanceSize: cls=");
+    if (cls != NULL) {
+      java_lang_Class::print_signature(JNIHandles::resolve_non_null(cls), tty);
+      tty->cr();
+    } else {
+      tty->print_cr("<null>");
+    }
+  }
+  if (cls == NULL) {
+    THROW_0(vmSymbols::java_lang_NullPointerException());
+  }
+
+  jlong size_in_words = get_instance_size_helper(cls, CHECK_0);
+  return size_in_words << LogHeapWordSize;
+UNSAFE_END
+
+UNSAFE_ENTRY(jlong, Unsafe_GetInstanceFootprintWhenContained(JNIEnv* env,
+    jobject unsafe, jclass cls))
+  UnsafeWrapper("Unsafe_GetInstanceFootprintWhenContained");
+  if (TraceObjectLayoutIntrinsics) {
+    ResourceMark rm;
+    tty->print("Unsafe_GetInstanceFootprintWhenContained: cls=");
+    if (cls != NULL) {
+      java_lang_Class::print_signature(JNIHandles::resolve_non_null(cls), tty);
+      tty->cr();
+    } else {
+      tty->print_cr("<null>");
+    }
+  }
+  if (cls == NULL) {
+    THROW_0(vmSymbols::java_lang_NullPointerException());
+  }
+
+  jlong size_in_words = get_prepadding_helper() +
+      get_instance_size_helper(cls, CHECK_0);
+  return size_in_words << LogHeapWordSize;
+UNSAFE_END
+
+UNSAFE_ENTRY(jlong, Unsafe_GetContainingObjectFootprint(JNIEnv* env,
+    jobject unsafe, jclass container_cls, jlong element_size_in_bytes,
+    jlong number_of_elements))
+  UnsafeWrapper("Unsafe_GetContainingObjectFootprint");
+  if (TraceObjectLayoutIntrinsics) {
+    ResourceMark rm;
+    tty->print("Unsafe_GetContainingObjectFootprint: container_cls=");
+    if (container_cls != NULL) {
+      java_lang_Class::print_signature(
+          JNIHandles::resolve_non_null(container_cls), tty);
+    } else {
+      tty->print("<null>");
+    }
+    tty->print_cr(", element_size_in_bytes=%ld(0x%lX), number_of_elements=%ld",
+        element_size_in_bytes, element_size_in_bytes, number_of_elements);
+  }
+  if (container_cls == NULL) {
+    THROW_0(vmSymbols::java_lang_NullPointerException());
+  }
+  if (!is_size_aligned((size_t) element_size_in_bytes, HeapWordSize) ||
+      element_size_in_bytes <= 0 || number_of_elements < 0) {
+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());
+  }
+
+  jlong body_size_in_words = get_instance_size_helper(container_cls, CHECK_0);
+  jlong element_size_in_words = element_size_in_bytes >> LogHeapWordSize;
+  return (body_size_in_words + (number_of_elements * element_size_in_words)) <<
+      LogHeapWordSize;
+UNSAFE_END
+
+UNSAFE_ENTRY(jlong, Unsafe_GetContainingObjectFootprintWhenContained(
+    JNIEnv* env, jobject unsafe, jclass container_cls,
+    jlong element_size_in_bytes, jlong number_of_elements))
+  UnsafeWrapper("Unsafe_GetContainingObjectFootprintWhenContained");
+  if (TraceObjectLayoutIntrinsics) {
+    ResourceMark rm;
+    tty->print("Unsafe_GetContainingObjectFootprintWhenContained: "
+        "container_cls=");
+    if (container_cls != NULL) {
+      java_lang_Class::print_signature(
+          JNIHandles::resolve_non_null(container_cls), tty);
+    } else {
+      tty->print("<null>");
+    }
+    tty->print_cr(", element_size_in_bytes=%ld(0x%lX), number_of_elements=%ld",
+        element_size_in_bytes, element_size_in_bytes, number_of_elements);
+  }
+  if (container_cls == NULL) {
+    THROW_0(vmSymbols::java_lang_NullPointerException());
+  }
+  if (!is_size_aligned((size_t) element_size_in_bytes, HeapWordSize) ||
+      element_size_in_bytes <= 0 || number_of_elements < 0) {
+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());
+  }
+
+  jlong prepadding_in_words = get_prepadding_helper();
+  jlong body_size_in_words = get_instance_size_helper(container_cls, CHECK_0);
+  jlong element_size_in_words = element_size_in_bytes >> LogHeapWordSize;
+  return (prepadding_in_words + body_size_in_words +
+      (number_of_elements * element_size_in_words)) << LogHeapWordSize;
+UNSAFE_END
+
+UNSAFE_ENTRY(jlong, Unsafe_GetPrePaddingInObjectFootprint(JNIEnv* env,
+    jobject unsafe, jlong obj_footprint_in_bytes))
+  UnsafeWrapper("Unsafe_GetPrePaddingInObjectFootprint");
+  if (TraceObjectLayoutIntrinsics) {
+    tty->print_cr("Unsafe_GetPrePaddingInObjectFootprint: "
+        "obj_footprint_in_bytes=%ld(0x%lX)", obj_footprint_in_bytes,
+        obj_footprint_in_bytes);
+  }
+
+  return get_prepadding_helper() << LogHeapWordSize;
+UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_AllocateHeapForElementArrayClass(JNIEnv* env,
+    jobject unsafe, jclass container_cls, jlongArray element_counts,
+    jobjectArray element_classes))
+  UnsafeWrapper("Unsafe_AllocateHeapForElementArrayClass");
+  if (TraceObjectLayoutIntrinsics) {
+    ResourceMark rm;
+    tty->print("Unsafe_AllocateHeapForElementArrayClass: container_cls=");
+    if (container_cls != NULL) {
+      java_lang_Class::print_signature(
+          JNIHandles::resolve_non_null(container_cls), tty);
+    } else {
+      tty->print("<null>");
+    }
+    tty->print_cr(", element_counts=" PTR_FORMAT
+        ", element_classes=" PTR_FORMAT,
+        p2i((void*) JNIHandles::resolve(element_counts)),
+        p2i((void*) JNIHandles::resolve(element_classes)));
+  }
+  if (container_cls == NULL || element_counts == NULL ||
+      element_classes == NULL) {
+    THROW_NULL(vmSymbols::java_lang_NullPointerException());
+  }
+
+  typeArrayHandle h_element_counts(THREAD,
+      typeArrayOop(JNIHandles::resolve_non_null(element_counts)));
+  objArrayHandle h_element_classes(THREAD,
+      objArrayOop(JNIHandles::resolve_non_null(element_classes)));
+  int dims = h_element_counts->length();
+
+  if (dims == 0 || dims != h_element_classes->length()) {
+    THROW_NULL(vmSymbols::java_lang_IllegalArgumentException());
+  }
+  for (int i = 0; i < dims; i++) {
+    if (h_element_counts->long_at(i) < 0 ||
+        h_element_classes->obj_at(i) == NULL) {
+      THROW_NULL(vmSymbols::java_lang_IllegalArgumentException());
+    }
+  }
+
+  // Make sure that all the involved klasses are valid for instantiation
+  // and initialized
+  instanceKlassHandle h_container_klass(THREAD,
+      java_lang_Class::as_Klass(JNIHandles::resolve_non_null(container_cls)));
+  h_container_klass->check_valid_for_instantiation(true, CHECK_NULL);
+  h_container_klass->initialize(CHECK_NULL);
+  for (int i = 0; i < dims; i++) {
+    instanceKlassHandle h_element_klass(THREAD,
+        java_lang_Class::as_Klass(h_element_classes->obj_at(i)));
+    h_element_klass->check_valid_for_instantiation(true, CHECK_NULL);
+    h_element_klass->initialize(CHECK_NULL);
+  }
+
+  // We need to allocate in the Java heap a memory region for a multi-
+  // dimensional structured array which is described by the given specification.
+  // The specification consists of two parallel arrays determining counts
+  // of the elements (h_element_counts) and their classes (h_element_classes)
+  // for all nesting levels, starting from the outermost one. The number of
+  // dimensions is determined by the length of these arrays.
+  //
+  // Here are the steps we need to do:
+  // 1. Calculate the total memory footprint of the multi-dimensional
+  //    structured array (including containing, contained, and RCO objects
+  //    on all nesting levels).
+  // 2. Allocate a memory region with the calculated size.
+  // 3. Initialize the headers of all the objects that make up the multi-
+  //    dimensional structured array (including containing, contained, and RCO
+  //    objects on all nesting levels); initialize special fields of all the
+  //    containing objects; set up the offset values in all the RCO objects.
+  //    It is important to complete all this initialization work before the
+  //    return from this method, otherwise the uninitialized contents get
+  //    exposed to the Java heap scanners and garbage collectors. An attempt
+  //    to scan an uninitialized or partially initialized structured array
+  //    would most likely result in a crash. It is also important to note that
+  //    by initialization here we don't mean actual creation of involved
+  //    Java objects (calling their constructors, etc.), but just doing
+  //    a specific set of steps for making the region of memory representing
+  //    the multi-dimensional structured array parsable by heap scanners and
+  //    GCs.
+
+  // Initialize configurations of all nesting levels of the structured array
+  UnfoldedStructuredArrayConfiguration configuration;
+  configuration.initialize(container_cls, h_element_counts, h_element_classes,
+      CHECK_NULL);
+
+  // Multi-dimensional cursor able to address elements of the most inner level
+  jlong* cursors = NEW_C_HEAP_ARRAY(jlong, dims, mtInternal);
+  if (cursors == NULL) {
+    THROW_NULL(vmSymbols::java_lang_OutOfMemoryError());
+  }
+  for (int i = 0; i < dims; i++) {
+    cursors[i] = 0;
+  }
+
+  // Calculate the total memory footprint of the multi-dimensional structured
+  // array
+  jlong size_in_words = (configuration.body_size(0) +
+      (configuration.length(0) * configuration.element_size(0))) >>
+      LogHeapWordSize;
+
+#ifdef ASSERT
+  if (TraceObjectLayoutIntrinsics) {
+    tty->print_cr("  Unsafe_AllocateHeapForElementArrayClass: "
+        "size=0x%lX", size_in_words << LogHeapWordSize);
+  }
+#endif // ASSERT
+
+  // Allocate a memory region with the calculated size
+  oop container_obj =
+      h_container_klass->allocate_instance((int) size_in_words, CHECK_NULL);
+  configuration.configure_instance(container_obj, 0);
+
+  // Initialize the headers of all the objects that make up the multi-
+  // dimensional structured array; initialize special fields of all the
+  // containing objects; set up the offset values in all the RCO objects
+
+  address last_container_addr = (address) container_obj;
+  address derived_obj_addr = ((address) container_obj) +
+      configuration.body_size(0) + configuration.padding_size(0);
+
+  // This cycle initializes one innermost structured array per iteration. It
+  // also correctly handles situations when there are arrays with zero lengths
+  // somewhere in the hierarchy. In this case the cycle initializes one
+  // outermost structured array with zero length per cycle.
+  int cur_dim = 0;
+  do {
+    // Initialize one outermost structured array with zero length or, if there
+    // are no such arrays, one innermost structured array, including all its
+    // containers that are not initialized yet
+    while (cur_dim < dims - 1) {
+      if (configuration.length(cur_dim) == 0) {
+        break;
+      }
+      create_rco_object(derived_obj_addr,
+          derived_obj_addr - last_container_addr);
+      instanceKlassHandle h_element_klass(THREAD,
+          java_lang_Class::as_Klass(configuration.element_class(cur_dim)()));
+      oop derived_obj = h_element_klass->allocate_instance_at(derived_obj_addr,
+          true, true, CHECK_NULL);
+      configuration.configure_instance(derived_obj, cur_dim + 1);
+      last_container_addr = derived_obj_addr;
+      derived_obj_addr += configuration.body_size(cur_dim + 1) +
+          configuration.padding_size(cur_dim + 1);
+      cur_dim++;
+    }
+
+    // If we got to the innermost structured array, initialize its elements
+    if (cur_dim == dims - 1) {
+      instanceKlassHandle h_element_klass(THREAD,
+          java_lang_Class::as_Klass(configuration.element_class(cur_dim)()));
+      for (jlong i = 0; i < configuration.length(cur_dim); i++) {
+        create_rco_object(derived_obj_addr,
+            derived_obj_addr - last_container_addr);
+        h_element_klass->allocate_instance_at(derived_obj_addr, true, false,
+            CHECK_NULL);
+        derived_obj_addr += configuration.element_size(cur_dim);
+      }
+    }
+
+    // Move the cursor to the next structured array. The cursor will overflow
+    // and reset to the vector of zeros after all the hierarchy is traversed.
+    while (cur_dim > 0) {
+      cur_dim--;
+      cursors[cur_dim]++;
+      if (cursors[cur_dim] < configuration.length(cur_dim)) {
+        break;
+      }
+      cursors[cur_dim] = 0;
+    }
+  } while (cur_dim != 0 || cursors[cur_dim] != 0);
+
+  FREE_C_HEAP_ARRAY(jlong, cursors, mtInternal);
+
+  return JNIHandles::make_local(env, container_obj);
+UNSAFE_END
+
+UNSAFE_ENTRY(void, Unsafe_ConstructObjectAtOffset(JNIEnv* env, jobject unsafe,
+    jobject container, jlong offset, jlong obj_prepadding,
+    jboolean is_contained, jboolean is_container, jlong obj_footprint,
+    jobject ctor, jobjectArray ctor_args))
+  UnsafeWrapper("Unsafe_ConstructObjectAtOffset");
+  if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+    tty->print_cr("Unsafe_ConstructObjectAtOffset: "
+        "container=" PTR_FORMAT ", offset=0x%lX, "
+        "obj_prepadding=0x%lX, %scontained, %scontainer, obj_footprint=0x%lX, "
+        "ctor=" PTR_FORMAT ", ctor_args=" PTR_FORMAT,
+        p2i((void*) JNIHandles::resolve(container)),
+        offset,
+        obj_prepadding,
+        is_contained ? "" : "not ",
+        is_container ? "" : "not ",
+        obj_footprint,
+        p2i((void*) JNIHandles::resolve(ctor)),
+        p2i((void*) JNIHandles::resolve(ctor_args)));
+  }
+  if (container == NULL || ctor == NULL) {
+    THROW(vmSymbols::java_lang_NullPointerException());
+  }
+  if (!is_size_aligned((size_t) offset, HeapWordSize) || offset < 0 ||
+      ((is_contained == JNI_FALSE) && (obj_prepadding != 0)) ||
+      ((is_contained != JNI_FALSE) &&
+      (obj_prepadding != (get_prepadding_helper() << LogHeapWordSize)))) {
+    THROW(vmSymbols::java_lang_IllegalArgumentException());
+  }
+
+  // Calculate the address at which the object will be created
+  oop container_obj = JNIHandles::resolve_non_null(container);
+  address derived_obj_addr = ((address) container_obj) + offset;
+
+  // Create the object
+  oop ctor_mirror = JNIHandles::resolve_non_null(ctor);
+
+  objArrayHandle h_ctor_args(THREAD,
+      objArrayOop(JNIHandles::resolve(ctor_args)));
+
+  oop result = Reflection::invoke_constructor_in_place(derived_obj_addr,
+      (is_contained != JNI_FALSE), (is_container != JNI_FALSE),
+      ctor_mirror, h_ctor_args, CHECK);
+
+  if (JvmtiExport::should_post_vm_object_alloc()) {
+    JvmtiExport::post_vm_object_alloc(JavaThread::current(), result);
+  }
+UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_DeriveContainedObjectAtOffset(JNIEnv* env,
+    jobject unsafe, jobject container, jlong offset_in_bytes))
+  UnsafeWrapper("Unsafe_DeriveContainedObjectAtOffset");
+  if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+    tty->print_cr("Unsafe_DeriveContainedObjectAtOffset: "
+        "container=" PTR_FORMAT ", offset_in_bytes=%ld(0x%lX)",
+        p2i((void*) JNIHandles::resolve(container)), offset_in_bytes,
+        offset_in_bytes);
+  }
+  if (container == NULL) {
+    THROW_NULL(vmSymbols::java_lang_NullPointerException());
+  }
+  if (!is_size_aligned((size_t) offset_in_bytes, HeapWordSize) ||
+      offset_in_bytes < 0) {
+    THROW_NULL(vmSymbols::java_lang_IllegalArgumentException());
+  }
+
+  oop container_obj = JNIHandles::resolve_non_null(container);
+  oop derived_obj = (oop) (((address) container_obj) + offset_in_bytes);
+
+#ifdef ASSERT
+  if (TraceObjectLayoutIntrinsics && ObjectLayoutIntrinsicsTraceLevel >= 2) {
+    if (derived_obj->is_contained()) {
+      jlong stored_offset_in_bytes = derived_obj->relative_container_offset();
+      tty->print_cr("  Unsafe_DeriveContainedObjectAtOffset: "
+          "derived object contained, RCO=%ld(0x%lX)",
+          stored_offset_in_bytes, stored_offset_in_bytes);
+    } else {
+      tty->print_cr("  Unsafe_DeriveContainedObjectAtOffset: "
+          "derived object not contained");
+    }
+  }
+#endif // ASSERT
+
+  return JNIHandles::make_local(env, derived_obj);
+UNSAFE_END
 
 /// JVM_RegisterUnsafeMethods
 
@@ -1697,6 +2231,17 @@
     {CC"fullFence",          CC"()V",                    FN_PTR(Unsafe_FullFence)},
 };
 
+static JNINativeMethod objectlayout_methods[] = {
+  { CC "getInstanceSize",                           CC "("CLS")J",                 FN_PTR(Unsafe_GetInstanceSize) },
+  { CC "getInstanceFootprintWhenContained",         CC "("CLS")J",                 FN_PTR(Unsafe_GetInstanceFootprintWhenContained) },
+  { CC "getContainingObjectFootprint",              CC "("CLS"JJ)J",               FN_PTR(Unsafe_GetContainingObjectFootprint) },
+  { CC "getContainingObjectFootprintWhenContained", CC "("CLS"JJ)J",               FN_PTR(Unsafe_GetContainingObjectFootprintWhenContained) },
+  { CC "getPrePaddingInObjectFootprint",            CC "(J)J",                     FN_PTR(Unsafe_GetPrePaddingInObjectFootprint) },
+  { CC "allocateHeapForElementArrayClass",          CC "("CLS"[J["CLS")"OBJ,       FN_PTR(Unsafe_AllocateHeapForElementArrayClass) },
+  { CC "constructObjectAtOffset",                   CC "("OBJ"JJZZJ"CTR"["OBJ")V", FN_PTR(Unsafe_ConstructObjectAtOffset) },
+  { CC "deriveContainedObjectAtOffset",             CC "("OBJ"J)"OBJ,              FN_PTR(Unsafe_DeriveContainedObjectAtOffset) }
+};
+
 #undef CC
 #undef FN_PTR
 
@@ -1796,5 +2341,10 @@
 
     // Fence methods
     register_natives("1.8 fence methods", env, unsafecls, fence_methods, sizeof(fence_methods)/sizeof(JNINativeMethod));
+
+    // ObjectLayout intrinsics
+    register_natives("ObjectLayout intrinsics", env, unsafecls,
+      objectlayout_methods,
+      sizeof(objectlayout_methods) / sizeof(JNINativeMethod));
   }
 JVM_END
--- old/src/share/vm/runtime/arguments.cpp	2015-06-16 10:25:48.902326853 -0700
+++ new/src/share/vm/runtime/arguments.cpp	2015-06-16 10:25:48.830328600 -0700
@@ -1254,7 +1254,7 @@
                                     CardTableRS::ct_max_alignment_constraint());
 
   // Now make adjustments for CMS
-  intx   tenuring_default = (intx)6;
+  uintx tenuring_default = (uintx) 3;
   size_t young_gen_per_worker = CMSYoungGenPerWorker;
 
   // Preferred young gen size for "short" pauses:
@@ -1572,7 +1572,7 @@
 
 void Arguments::select_gc() {
   if (!gc_selected()) {
-    select_gc_ergonomically();
+    ArgumentsExt::select_gc_ergonomically();
   }
 }
 
@@ -1611,7 +1611,7 @@
   assert(UseParallelGC || UseParallelOldGC, "Error");
   // Enable ParallelOld unless it was explicitly disabled (cmd line or rc file).
   if (FLAG_IS_DEFAULT(UseParallelOldGC)) {
-    FLAG_SET_DEFAULT(UseParallelOldGC, true);
+    FLAG_SET_DEFAULT(UseParallelOldGC, false);
   }
   FLAG_SET_DEFAULT(UseParallelGC, true);
 
@@ -1714,6 +1714,17 @@
 #endif // ASSERT
 #endif // INCLUDE_ALL_GCS
 
+void Arguments::set_object_layout_flags() {
+    if (UseObjectLayoutIntrinsics) {
+      FLAG_SET_DEFAULT(UseParallelGC,true);
+      FLAG_SET_DEFAULT(UseParallelOldGC,false);
+      FLAG_SET_DEFAULT(UseCompressedOops, false);
+      FLAG_SET_DEFAULT(UseBiasedLocking, false);
+    }
+}
+
+
+
 void Arguments::set_gc_specific_flags() {
 #if INCLUDE_ALL_GCS
   // Set per-collector flags
@@ -2067,7 +2078,7 @@
 }
 
 // Check consistency of GC selection
-bool Arguments::check_gc_consistency() {
+bool Arguments::check_gc_consistency_user() {
   check_gclog_consistency();
   bool status = true;
   // Ensure that the user has not selected conflicting sets
@@ -2233,7 +2244,7 @@
     FLAG_SET_DEFAULT(UseGCOverheadLimit, false);
   }
 
-  status = status && check_gc_consistency();
+  status = status && check_gc_consistency_user();
   status = status && check_stack_pages();
 
   if (CMSIncrementalMode) {
@@ -2408,12 +2419,16 @@
   status = status && verify_percentage(YoungGenerationSizeSupplement, "YoungGenerationSizeSupplement");
   status = status && verify_percentage(TenuredGenerationSizeSupplement, "TenuredGenerationSizeSupplement");
 
-  // the "age" field in the oop header is 4 bits; do not want to pull in markOop.hpp
-  // just for that, so hardcode here.
-  status = status && verify_interval(MaxTenuringThreshold, 0, 15, "MaxTenuringThreshold");
-  status = status && verify_interval(InitialTenuringThreshold, 0, MaxTenuringThreshold, "MaxTenuringThreshold");
-  status = status && verify_percentage(TargetSurvivorRatio, "TargetSurvivorRatio");
-  status = status && verify_percentage(MarkSweepDeadRatio, "MarkSweepDeadRatio");
+  // The age field in the object header has 2 bits. We do not want to pull in
+  // markOop.hpp just for that, so hardcode here.
+  status = status && verify_interval(MaxTenuringThreshold,
+      0, 3, "MaxTenuringThreshold");
+  status = status && verify_interval(InitialTenuringThreshold,
+      0, MaxTenuringThreshold, "InitialTenuringThreshold");
+  status = status && verify_percentage(TargetSurvivorRatio,
+      "TargetSurvivorRatio");
+  status = status && verify_percentage(MarkSweepDeadRatio,
+      "MarkSweepDeadRatio");
 
   status = status && verify_min_value(MarkSweepAlwaysCompactCount, 1, "MarkSweepAlwaysCompactCount");
 #ifdef COMPILER1
@@ -3837,8 +3852,8 @@
       CommandLineFlags::printFlags(tty, false);
       vm_exit(0);
     }
-    if (match_option(option, "-XX:NativeMemoryTracking", &tail)) {
 #if INCLUDE_NMT
+    if (match_option(option, "-XX:NativeMemoryTracking", &tail)) {
       // The launcher did not setup nmt environment variable properly.
       if (!MemTracker::check_launcher_nmt_support(tail)) {
         warning("Native Memory Tracking did not setup properly, using wrong launcher?");
@@ -3853,12 +3868,8 @@
       } else {
         vm_exit_during_initialization("Syntax error, expecting -XX:NativeMemoryTracking=[off|summary|detail]", NULL);
       }
-#else
-      jio_fprintf(defaultStream::error_stream(),
-        "Native Memory Tracking is not supported in this VM\n");
-      return JNI_ERR;
-#endif
     }
+#endif
 
 
 #ifndef PRODUCT
@@ -4004,13 +4015,17 @@
 
 jint Arguments::apply_ergo() {
 
+  // We need to set all object layout flags
+  // before ergonomics possibly enables CompressedOops
+  set_object_layout_flags();
+  
   // Set flags based on ergonomics.
   set_ergonomics_flags();
 
   set_shared_spaces_flags();
 
   // Check the GC selections again.
-  if (!check_gc_consistency()) {
+  if (!ArgumentsExt::check_gc_consistency_ergo()) {
     return JNI_EINVAL;
   }
 
--- old/src/share/vm/runtime/arguments.hpp	2015-06-16 10:25:49.214319272 -0700
+++ new/src/share/vm/runtime/arguments.hpp	2015-06-16 10:25:49.146320926 -0700
@@ -342,6 +342,7 @@
   static void select_gc();
   static void set_ergonomics_flags();
   static void set_shared_spaces_flags();
+  static void set_object_layout_flags();
   // limits the given memory size by the maximum amount of memory this process is
   // currently allowed to allocate or reserve.
   static julong limit_by_allocatable_memory(julong size);
@@ -466,7 +467,8 @@
   static bool verify_MaxHeapFreeRatio(FormatBuffer<80>& err_msg, uintx max_heap_free_ratio);
 
   // Check for consistency in the selection of the garbage collector.
-  static bool check_gc_consistency();        // Check user-selected gc
+  static bool check_gc_consistency_user();        // Check user-selected gc
+  static inline bool check_gc_consistency_ergo(); // Check ergonomic-selected gc
   static void check_deprecated_gcs();
   static void check_deprecated_gc_flags();
   // Check consistecy or otherwise of VM argument settings
@@ -614,4 +616,8 @@
     UseParNewGC || UseSerialGC;
 }
 
+bool Arguments::check_gc_consistency_ergo() {
+  return check_gc_consistency_user();
+}
+
 #endif // SHARE_VM_RUNTIME_ARGUMENTS_HPP
--- old/src/share/vm/runtime/globals.hpp	2015-06-16 10:25:49.478312852 -0700
+++ new/src/share/vm/runtime/globals.hpp	2015-06-16 10:25:49.406314603 -0700
@@ -718,7 +718,16 @@
           "Use intrinsics for SHA-384 and SHA-512 crypto hash functions")   \
                                                                             \
   product(bool, UseCRC32Intrinsics, false,                                  \
-          "use intrinsics for java.util.zip.CRC32")                         \
+          "Use intrinsics for java.util.zip.CRC32")                         \
+                                                                            \
+  product(bool, UseObjectLayoutIntrinsics, false,                           \
+          "Use intrinsics enabling optimized ObjectLayout implementation")  \
+                                                                            \
+  product(bool, TraceObjectLayoutIntrinsics, false,                         \
+          "Trace intrinsics specific to ObjectLayout implementation")       \
+                                                                            \
+  product(intx, ObjectLayoutIntrinsicsTraceLevel, 0,                        \
+          "Trace level for ObjectLayout related intrinsics")                \
                                                                             \
   develop(bool, TraceCallFixup, false,                                      \
           "Trace all call fixups")                                          \
@@ -1246,7 +1255,7 @@
   product(bool, CompactFields, true,                                        \
           "Allocate nonstatic fields in gaps between previous fields")      \
                                                                             \
-  notproduct(bool, PrintFieldLayout, false,                                 \
+  product(bool, PrintFieldLayout, false,                                    \
           "Print field layout for each class")                              \
                                                                             \
   product(intx, ContendedPaddingWidth, 128,                                 \
@@ -1258,7 +1267,7 @@
   product(bool, RestrictContended, true,                                    \
           "Restrict @Contended to trusted classes")                         \
                                                                             \
-  product(bool, UseBiasedLocking, true,                                     \
+  product(bool, UseBiasedLocking, false,                                     \
           "Enable biased locking in JVM")                                   \
                                                                             \
   product(intx, BiasedLockingStartupDelay, 4000,                            \
@@ -1494,7 +1503,7 @@
           "How much the GC can expand the eden by while the GC locker "     \
           "is active (as a percentage)")                                    \
                                                                             \
-  diagnostic(uintx, GCLockerRetryAllocationCount, 2,                        \
+  diagnostic(intx, GCLockerRetryAllocationCount, 2,                         \
           "Number of times to retry allocations when "                      \
           "blocked by the GC locker")                                       \
                                                                             \
@@ -2048,6 +2057,9 @@
           "Provide more detailed and expensive TLAB statistics "            \
           "(with PrintTLAB)")                                               \
                                                                             \
+  EMBEDDED_ONLY(product(bool, LowMemoryProtection, true,                    \
+          "Enable LowMemoryProtection"))                                    \
+                                                                            \
   product_pd(bool, NeverActAsServerClassMachine,                            \
           "Never act like a server-class machine")                          \
                                                                             \
@@ -3233,10 +3245,10 @@
   diagnostic(intx, VerifyGCLevel,     0,                                    \
           "Generation level at which to start +VerifyBefore/AfterGC")       \
                                                                             \
-  product(uintx, MaxTenuringThreshold,    15,                               \
+  product(uintx, MaxTenuringThreshold, 3,                                   \
           "Maximum value for tenuring threshold")                           \
                                                                             \
-  product(uintx, InitialTenuringThreshold,    7,                            \
+  product(uintx, InitialTenuringThreshold, 3,                               \
           "Initial value for tenuring threshold")                           \
                                                                             \
   product(uintx, TargetSurvivorRatio,    50,                                \
--- old/src/share/vm/runtime/reflection.cpp	2015-06-16 10:25:49.782305460 -0700
+++ new/src/share/vm/runtime/reflection.cpp	2015-06-16 10:25:49.710307210 -0700
@@ -36,7 +36,6 @@
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "prims/jvm.h"
-#include "prims/jvmtiExport.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/javaCalls.hpp"
@@ -943,11 +942,6 @@
           // Method resolution threw an exception; wrap it in an InvocationTargetException
             oop resolution_exception = PENDING_EXCEPTION;
             CLEAR_PENDING_EXCEPTION;
-            // JVMTI has already reported the pending exception
-            // JVMTI internal flag reset is needed in order to report InvocationTargetException
-            if (THREAD->is_Java_thread()) {
-              JvmtiExport::clear_detected_exception((JavaThread*) THREAD);
-            }
             JavaCallArguments args(Handle(THREAD, resolution_exception));
             THROW_ARG_0(vmSymbols::java_lang_reflect_InvocationTargetException(),
                 vmSymbols::throwable_void_signature(),
@@ -1080,12 +1074,6 @@
     // Method threw an exception; wrap it in an InvocationTargetException
     oop target_exception = PENDING_EXCEPTION;
     CLEAR_PENDING_EXCEPTION;
-    // JVMTI has already reported the pending exception
-    // JVMTI internal flag reset is needed in order to report InvocationTargetException
-    if (THREAD->is_Java_thread()) {
-      JvmtiExport::clear_detected_exception((JavaThread*) THREAD);
-    }
-
     JavaCallArguments args(Handle(THREAD, target_exception));
     THROW_ARG_0(vmSymbols::java_lang_reflect_InvocationTargetException(),
                 vmSymbols::throwable_void_signature(),
@@ -1127,11 +1115,13 @@
 // This would be nicer if, say, java.lang.reflect.Method was a subclass
 // of java.lang.reflect.Constructor
 
-oop Reflection::invoke_method(oop method_mirror, Handle receiver, objArrayHandle args, TRAPS) {
-  oop mirror             = java_lang_reflect_Method::clazz(method_mirror);
-  int slot               = java_lang_reflect_Method::slot(method_mirror);
-  bool override          = java_lang_reflect_Method::override(method_mirror) != 0;
-  objArrayHandle ptypes(THREAD, objArrayOop(java_lang_reflect_Method::parameter_types(method_mirror)));
+oop Reflection::invoke_method(oop method_mirror, Handle receiver,
+    objArrayHandle method_args, TRAPS) {
+  oop mirror = java_lang_reflect_Method::clazz(method_mirror);
+  int slot = java_lang_reflect_Method::slot(method_mirror);
+  bool override = java_lang_reflect_Method::override(method_mirror) != 0;
+  objArrayHandle ptypes(THREAD,
+      objArrayOop(java_lang_reflect_Method::parameter_types(method_mirror)));
 
   oop return_type_mirror = java_lang_reflect_Method::return_type(method_mirror);
   BasicType rtype;
@@ -1144,36 +1134,75 @@
   instanceKlassHandle klass(THREAD, java_lang_Class::as_Klass(mirror));
   Method* m = klass->method_with_idnum(slot);
   if (m == NULL) {
-    THROW_MSG_0(vmSymbols::java_lang_InternalError(), "invoke");
+    THROW_MSG_NULL(vmSymbols::java_lang_InternalError(), "invoke");
   }
   methodHandle method(THREAD, m);
 
-  return invoke(klass, method, receiver, override, ptypes, rtype, args, true, THREAD);
+  return invoke(klass, method, receiver, override, ptypes, rtype, method_args,
+      true, THREAD);
 }
 
-
-oop Reflection::invoke_constructor(oop constructor_mirror, objArrayHandle args, TRAPS) {
-  oop mirror             = java_lang_reflect_Constructor::clazz(constructor_mirror);
-  int slot               = java_lang_reflect_Constructor::slot(constructor_mirror);
-  bool override          = java_lang_reflect_Constructor::override(constructor_mirror) != 0;
-  objArrayHandle ptypes(THREAD, objArrayOop(java_lang_reflect_Constructor::parameter_types(constructor_mirror)));
+oop Reflection::invoke_constructor(oop ctor_mirror, objArrayHandle ctor_args,
+    TRAPS) {
+  oop mirror = java_lang_reflect_Constructor::clazz(ctor_mirror);
+  int slot = java_lang_reflect_Constructor::slot(ctor_mirror);
+  bool override = java_lang_reflect_Constructor::override(ctor_mirror) != 0;
+  objArrayHandle ptypes(THREAD,
+      objArrayOop(java_lang_reflect_Constructor::parameter_types(ctor_mirror)));
 
   instanceKlassHandle klass(THREAD, java_lang_Class::as_Klass(mirror));
   Method* m = klass->method_with_idnum(slot);
   if (m == NULL) {
-    THROW_MSG_0(vmSymbols::java_lang_InternalError(), "invoke");
+    THROW_MSG_NULL(vmSymbols::java_lang_InternalError(), "invoke");
   }
   methodHandle method(THREAD, m);
-  assert(method->name() == vmSymbols::object_initializer_name(), "invalid constructor");
+  assert(method->name() == vmSymbols::object_initializer_name(),
+      "invalid constructor");
+
+  // Make sure we are not trying to instantiate an abstract klass
+  klass->check_valid_for_instantiation(false, CHECK_NULL);
 
-  // Make sure klass gets initialize
+  // Make sure klass is initialized
   klass->initialize(CHECK_NULL);
 
   // Create new instance (the receiver)
-  klass->check_valid_for_instantiation(false, CHECK_NULL);
   Handle receiver = klass->allocate_instance_handle(CHECK_NULL);
 
-  // Ignore result from call and return receiver
-  invoke(klass, method, receiver, override, ptypes, T_VOID, args, false, CHECK_NULL);
+  invoke(klass, method, receiver, override, ptypes, T_VOID, ctor_args, false,
+      CHECK_NULL);
+  return receiver();
+}
+
+oop Reflection::invoke_constructor_in_place(address obj_addr,
+    bool is_contained, bool is_container, oop ctor_mirror,
+    objArrayHandle ctor_args, TRAPS) {
+  oop mirror = java_lang_reflect_Constructor::clazz(ctor_mirror);
+  int slot = java_lang_reflect_Constructor::slot(ctor_mirror);
+  bool override = java_lang_reflect_Constructor::override(ctor_mirror) != 0;
+  objArrayHandle ptypes(THREAD,
+      objArrayOop(java_lang_reflect_Constructor::parameter_types(ctor_mirror)));
+
+  instanceKlassHandle klass(THREAD, java_lang_Class::as_Klass(mirror));
+  Method* m = klass->method_with_idnum(slot);
+  if (m == NULL) {
+    THROW_MSG_NULL(vmSymbols::java_lang_InternalError(), "invoke");
+  }
+  methodHandle method(THREAD, m);
+  assert(method->name() == vmSymbols::object_initializer_name(),
+      "invalid constructor");
+
+  // Make sure we are not trying to instantiate an abstract klass
+  klass->check_valid_for_instantiation(false, CHECK_NULL);
+
+  // Make sure klass is initialized
+  klass->initialize(CHECK_NULL);
+
+  // Create new instance (the receiver) at the given address (in preallocated
+  // memory region)
+  Handle receiver = klass->allocate_instance_at_handle(obj_addr, is_contained,
+      is_container, CHECK_NULL);
+
+  invoke(klass, method, receiver, override, ptypes, T_VOID, ctor_args, false,
+      CHECK_NULL);
   return receiver();
 }
--- old/src/share/vm/runtime/reflection.hpp	2015-06-16 10:25:50.046299036 -0700
+++ new/src/share/vm/runtime/reflection.hpp	2015-06-16 10:25:49.974300789 -0700
@@ -138,11 +138,18 @@
   static BasicType basic_type_mirror_to_basic_type(oop basic_type_mirror, TRAPS);
 
 public:
-  // Method invokation through java.lang.reflect.Method
-  static oop      invoke_method(oop method_mirror, Handle receiver, objArrayHandle args, TRAPS);
-  // Method invokation through java.lang.reflect.Constructor
-  static oop      invoke_constructor(oop method_mirror, objArrayHandle args, TRAPS);
-
+  // Method invocation through java.lang.reflect.Method
+  static oop invoke_method(oop method_mirror, Handle receiver,
+      objArrayHandle method_args, TRAPS);
+  // Allocation of a new object, its initialization, and invocation of its
+  // constructor through java.lang.reflect.Constructor
+  static oop invoke_constructor(oop ctor_mirror, objArrayHandle ctor_args,
+      TRAPS);
+  // Initialization of an already allocated object and invocation of its
+  // constructor through java.lang.reflect.Constructor
+  static oop invoke_constructor_in_place(address obj_addr,
+      bool is_contained, bool is_container, oop ctor_mirror,
+      objArrayHandle ctor_args, TRAPS);
 };
 
 #endif // SHARE_VM_RUNTIME_REFLECTION_HPP
--- old/src/share/vm/runtime/virtualspace.cpp	2015-06-16 10:25:50.306292698 -0700
+++ new/src/share/vm/runtime/virtualspace.cpp	2015-06-16 10:25:50.226294648 -0700
@@ -52,22 +52,12 @@
     _alignment(0), _special(false), _executable(false) {
 }
 
-ReservedSpace::ReservedSpace(size_t size, size_t preferred_page_size) {
-  bool has_preferred_page_size = preferred_page_size != 0;
-  // Want to use large pages where possible and pad with small pages.
-  size_t page_size = has_preferred_page_size ? preferred_page_size : os::page_size_for_region_unaligned(size, 1);
+ReservedSpace::ReservedSpace(size_t size) {
+  size_t page_size = os::page_size_for_region(size, size, 1);
   bool large_pages = page_size != (size_t)os::vm_page_size();
-  size_t alignment;
-  if (large_pages && has_preferred_page_size) {
-    alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
-    // ReservedSpace initialization requires size to be aligned to the given
-    // alignment. Align the size up.
-    size = align_size_up(size, alignment);
-  } else {
-    // Don't force the alignment to be large page aligned,
-    // since that will waste memory.
-    alignment = os::vm_allocation_granularity();
-  }
+  // Don't force the alignment to be large page aligned,
+  // since that will waste memory.
+  size_t alignment = os::vm_allocation_granularity();
   initialize(size, alignment, large_pages, NULL, 0, false);
 }
 
@@ -228,10 +218,12 @@
   assert(noaccess_prefix == 0 ||
          noaccess_prefix == _alignment, "noaccess prefix wrong");
 
-  assert(markOopDesc::encode_pointer_as_mark(_base)->decode_pointer() == _base,
-         "area must be distinguisable from marks for mark-sweep");
-  assert(markOopDesc::encode_pointer_as_mark(&_base[size])->decode_pointer() == &_base[size],
-         "area must be distinguisable from marks for mark-sweep");
+  assert(markOopDesc::encode_pointer_as_mark(_base,
+      /* is_contained = */ true)->decode_pointer() == _base,
+      "encoding space start address as mark is not reversible");
+  assert(markOopDesc::encode_pointer_as_mark(&_base[size],
+      /* is_contained = */ true)->decode_pointer() == &_base[size],
+      "encoding space end address as mark is not reversible");
 }
 
 
@@ -382,7 +374,7 @@
 
 
 bool VirtualSpace::initialize(ReservedSpace rs, size_t committed_size) {
-  const size_t max_commit_granularity = os::page_size_for_region_unaligned(rs.size(), 1);
+  const size_t max_commit_granularity = os::page_size_for_region(rs.size(), rs.size(), 1);
   return initialize_with_granularity(rs, committed_size, max_commit_granularity);
 }
 
@@ -640,7 +632,19 @@
   }
 
   if (pre_touch || AlwaysPreTouch) {
-    os::pretouch_memory(previous_high, unaligned_new_high);
+    int vm_ps = os::vm_page_size();
+    for (char* curr = previous_high;
+         curr < unaligned_new_high;
+         curr += vm_ps) {
+      // Note the use of a write here; originally we tried just a read, but
+      // since the value read was unused, the optimizer removed the read.
+      // If we ever have a concurrent touchahead thread, we'll want to use
+      // a read, to avoid the potential of overwriting data (if a mutator
+      // thread beats the touchahead thread to a page).  There are various
+      // ways of making sure this read is not optimized away: for example,
+      // generating the code for a read procedure at runtime.
+      *curr = 0;
+    }
   }
 
   _high += bytes;
@@ -1005,7 +1009,7 @@
     case Disable:
       return vs.initialize_with_granularity(rs, 0, os::vm_page_size());
     case Commit:
-      return vs.initialize_with_granularity(rs, 0, os::page_size_for_region_unaligned(rs.size(), 1));
+      return vs.initialize_with_granularity(rs, 0, os::page_size_for_region(rs.size(), rs.size(), 1));
     }
   }
 
